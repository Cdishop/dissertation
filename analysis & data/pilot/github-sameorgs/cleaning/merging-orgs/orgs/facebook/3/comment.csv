owner,repo,issue_id,url,id,node_id,user_id,created_at,updated_at,author_association,body,performed_via_github_app,comments_url,html_url,issue_url
mashlol,notify,837150117,https://api.github.com/repos/mashlol/notify/issues/comments/813363432,813363432,MDEyOklzc3VlQ29tbWVudDgxMzM2MzQzMg==,63560258,2021-04-05T12:12:56Z,2021-04-05T12:12:56Z,NONE,"It stopped working for me too! :-/
",NA,https://api.github.com/repos/mashlol/notify/issues/58/comments,https://github.com/mashlol/notify/issues/58#issuecomment-813363432,https://api.github.com/repos/mashlol/notify/issues/58
mashlol,notify,837150117,https://api.github.com/repos/mashlol/notify/issues/comments/813775796,813775796,MDEyOklzc3VlQ29tbWVudDgxMzc3NTc5Ng==,1039240,2021-04-06T02:24:58Z,2021-04-06T02:24:58Z,OWNER,"Looks like firebase doesn't allow node 6 anymore. I think newer node versions require a paid plan on firebase too, so not sure if the concept works anymore really.

![image](https://user-images.githubusercontent.com/1039240/113650149-99f75d80-9644-11eb-8991-464a33f65639.png)",NA,https://api.github.com/repos/mashlol/notify/issues/58/comments,https://github.com/mashlol/notify/issues/58#issuecomment-813775796,https://api.github.com/repos/mashlol/notify/issues/58
mashlol,notify,837150117,https://api.github.com/repos/mashlol/notify/issues/comments/813779777,813779777,MDEyOklzc3VlQ29tbWVudDgxMzc3OTc3Nw==,1039240,2021-04-06T02:37:54Z,2021-04-06T02:37:54Z,OWNER,"Eh, w.e, I set up billing since the rates are pretty high until they charge - hopefully I won't get charged. Hopefully it should work now - let me know if it's still broken.",NA,https://api.github.com/repos/mashlol/notify/issues/58/comments,https://github.com/mashlol/notify/issues/58#issuecomment-813779777,https://api.github.com/repos/mashlol/notify/issues/58
mashlol,notify,665493211,https://api.github.com/repos/mashlol/notify/issues/comments/663793623,663793623,MDEyOklzc3VlQ29tbWVudDY2Mzc5MzYyMw==,1039240,2020-07-25T02:01:03Z,2020-07-25T02:01:03Z,OWNER,Looks like it already got in the other PR,NA,https://api.github.com/repos/mashlol/notify/issues/57/comments,https://github.com/mashlol/notify/pull/57#issuecomment-663793623,https://api.github.com/repos/mashlol/notify/issues/57
mashlol,notify,665493211,https://api.github.com/repos/mashlol/notify/issues/comments/663794429,663794429,MDEyOklzc3VlQ29tbWVudDY2Mzc5NDQyOQ==,7108843,2020-07-25T02:09:05Z,2020-07-25T02:12:01Z,CONTRIBUTOR,"Well,
- the other [pull request](https://github.com/mashlol/notify/pull/28) is about uploading the binary file to some unknown untrustworthy 3rd party website, I wouldn't trust any 3rd party website to host nor to give the official link to that binary file due to it can be hijacked/modified, except if it's not Play Store / GitHub.
- the file is missing in that 3rd party website which was promised that will be uploaded.

I would recommend you to host the generated apk file right here in GitHub's repository with md5 or sha256 hash attached to it.",NA,https://api.github.com/repos/mashlol/notify/issues/57/comments,https://github.com/mashlol/notify/pull/57#issuecomment-663794429,https://api.github.com/repos/mashlol/notify/issues/57
mashlol,notify,665493211,https://api.github.com/repos/mashlol/notify/issues/comments/663794815,663794815,MDEyOklzc3VlQ29tbWVudDY2Mzc5NDgxNQ==,1039240,2020-07-25T02:13:31Z,2020-07-25T02:13:31Z,OWNER,"Sorry, I meant you included this commit in #56",NA,https://api.github.com/repos/mashlol/notify/issues/57/comments,https://github.com/mashlol/notify/pull/57#issuecomment-663794815,https://api.github.com/repos/mashlol/notify/issues/57
mashlol,notify,665493211,https://api.github.com/repos/mashlol/notify/issues/comments/663794922,663794922,MDEyOklzc3VlQ29tbWVudDY2Mzc5NDkyMg==,7108843,2020-07-25T02:14:36Z,2020-07-25T02:14:36Z,CONTRIBUTOR,Oh. My apologies.. didn't notice that. Very well :+1: ,NA,https://api.github.com/repos/mashlol/notify/issues/57/comments,https://github.com/mashlol/notify/pull/57#issuecomment-663794922,https://api.github.com/repos/mashlol/notify/issues/57
mashlol,notify,646885444,https://api.github.com/repos/mashlol/notify/issues/comments/652601141,652601141,MDEyOklzc3VlQ29tbWVudDY1MjYwMTE0MQ==,67541101,2020-07-01T19:19:51Z,2020-07-01T19:19:51Z,NONE,"I'v found the soultion.
Sorry for interrupt",NA,https://api.github.com/repos/mashlol/notify/issues/55/comments,https://github.com/mashlol/notify/issues/55#issuecomment-652601141,https://api.github.com/repos/mashlol/notify/issues/55
mashlol,notify,646885444,https://api.github.com/repos/mashlol/notify/issues/comments/653344694,653344694,MDEyOklzc3VlQ29tbWVudDY1MzM0NDY5NA==,1039240,2020-07-03T04:48:40Z,2020-07-03T04:48:52Z,OWNER,Note that I would not rely on this project for emergency situations! If you need to notify the firefighters urgently or reliably I would try to find a paid solution like texting/calling them with an automated service that provides reliability guarantees!,NA,https://api.github.com/repos/mashlol/notify/issues/55/comments,https://github.com/mashlol/notify/issues/55#issuecomment-653344694,https://api.github.com/repos/mashlol/notify/issues/55
mashlol,notify,646885444,https://api.github.com/repos/mashlol/notify/issues/comments/653356517,653356517,MDEyOklzc3VlQ29tbWVudDY1MzM1NjUxNw==,67541101,2020-07-03T05:25:17Z,2020-07-03T05:25:17Z,NONE,"> Note that I would not rely on this project for emergency situations! If you need to notify the firefighters urgently or reliably I would try to find a paid solution like texting/calling them with an automated service that provides reliability guarantees!

Yes, you have right.
Thatâ€™s why i using notify as a stare channel
We have got very good solution based on RF channel to inform us about energency situation, but we would like to have spare - unofficial - channel
Thanks for reply and very nice app!",NA,https://api.github.com/repos/mashlol/notify/issues/55/comments,https://github.com/mashlol/notify/issues/55#issuecomment-653356517,https://api.github.com/repos/mashlol/notify/issues/55
mashlol,notify,646885444,https://api.github.com/repos/mashlol/notify/issues/comments/653363003,653363003,MDEyOklzc3VlQ29tbWVudDY1MzM2MzAwMw==,1039240,2020-07-03T05:44:51Z,2020-07-03T05:52:32Z,OWNER,"Haha great to hear it, just wanted to make sure :) ! Glad you figured it out and are able to make use of it!",NA,https://api.github.com/repos/mashlol/notify/issues/55/comments,https://github.com/mashlol/notify/issues/55#issuecomment-653363003,https://api.github.com/repos/mashlol/notify/issues/55
mashlol,notify,443401699,https://api.github.com/repos/mashlol/notify/issues/comments/498685892,498685892,MDEyOklzc3VlQ29tbWVudDQ5ODY4NTg5Mg==,1698456,2019-06-04T14:04:22Z,2019-06-04T14:04:22Z,NONE,"The backend seems to be ""down"". I tried hitting the right URL with curl and I get a 429 ""Error: could not handle the request"".

```
curl -v -g -s -G \
         'https://us-central1-notify-b7652.cloudfunctions.net/sendNotification' \
         --data-urlencode ""to=hunter2"" \
         --data-urlencode ""text=TEXT"" \
         --data-urlencode ""title=TITLE"" 
```
```
...
> GET /sendNotification?to=hunter2&text=TEXT&title=TITLE HTTP/2
...
< HTTP/2 429 
...
Error: could not handle the request
```
Full curl log [here](https://gist.github.com/tasinet/0a6366ee3f0d874e314f9e5a200c89bb)",NA,https://api.github.com/repos/mashlol/notify/issues/54/comments,https://github.com/mashlol/notify/issues/54#issuecomment-498685892,https://api.github.com/repos/mashlol/notify/issues/54
mashlol,notify,443401699,https://api.github.com/repos/mashlol/notify/issues/comments/499283258,499283258,MDEyOklzc3VlQ29tbWVudDQ5OTI4MzI1OA==,1598067,2019-06-05T22:44:47Z,2019-06-05T22:44:47Z,NONE,"FWIW, given that the app has disappeared from the app store and notifications from the CLI are no longer working, I decided to see if I could switch to using IFTTT to send messages from Linux to my Android phone. Indeed, it appears that you can. You can set up an IFTTT applet which accepts HTTPS requests using the Webhooks service and transmits notifications to the IFTTT app on your phone using the Notifications service. The incoming Webhooks requests can set variables for, e.g., the notification title and content, and the Notifications service can then access those variables to transmit the content to your phone. Just mentioning this here in case it proves useful to someone else.",NA,https://api.github.com/repos/mashlol/notify/issues/54/comments,https://github.com/mashlol/notify/issues/54#issuecomment-499283258,https://api.github.com/repos/mashlol/notify/issues/54
mashlol,notify,443401699,https://api.github.com/repos/mashlol/notify/issues/comments/782602038,782602038,MDEyOklzc3VlQ29tbWVudDc4MjYwMjAzOA==,79359434,2021-02-20T10:26:15Z,2021-02-20T10:27:08Z,NONE,"Hi, notify worked perfectly for me until a month ago. Is the service down now?
Now I have the same problem like korenmic had.
",NA,https://api.github.com/repos/mashlol/notify/issues/54/comments,https://github.com/mashlol/notify/issues/54#issuecomment-782602038,https://api.github.com/repos/mashlol/notify/issues/54
mashlol,notify,426979544,https://api.github.com/repos/mashlol/notify/issues/comments/478052458,478052458,MDEyOklzc3VlQ29tbWVudDQ3ODA1MjQ1OA==,1039240,2019-03-29T15:59:21Z,2019-03-29T15:59:21Z,OWNER,"Yes Google removed it due to ""collecting advertising ID"" although I have no idea what is collecting that - I already turned off the firebase automatic collection of advertising ID. Some other google project is probably collecting it automatically...I'll try to get it back up when I can",NA,https://api.github.com/repos/mashlol/notify/issues/53/comments,https://github.com/mashlol/notify/issues/53#issuecomment-478052458,https://api.github.com/repos/mashlol/notify/issues/53
mashlol,notify,426979544,https://api.github.com/repos/mashlol/notify/issues/comments/478771454,478771454,MDEyOklzc3VlQ29tbWVudDQ3ODc3MTQ1NA==,35245840,2019-04-01T22:33:12Z,2019-04-01T22:33:12Z,NONE,Is there another way to get it since it is no longer on Google Play? I just heard about this app and I would love to have it on my phone.,NA,https://api.github.com/repos/mashlol/notify/issues/53/comments,https://github.com/mashlol/notify/issues/53#issuecomment-478771454,https://api.github.com/repos/mashlol/notify/issues/53
mashlol,notify,426979544,https://api.github.com/repos/mashlol/notify/issues/comments/483636396,483636396,MDEyOklzc3VlQ29tbWVudDQ4MzYzNjM5Ng==,25059103,2019-04-16T12:21:54Z,2019-04-16T12:21:54Z,NONE,You could build it yourself if there aren't any mirrors anywhere.,NA,https://api.github.com/repos/mashlol/notify/issues/53/comments,https://github.com/mashlol/notify/issues/53#issuecomment-483636396,https://api.github.com/repos/mashlol/notify/issues/53
mashlol,notify,426979544,https://api.github.com/repos/mashlol/notify/issues/comments/484462712,484462712,MDEyOklzc3VlQ29tbWVudDQ4NDQ2MjcxMg==,23291749,2019-04-18T11:24:13Z,2019-04-18T11:32:03Z,NONE,"Yeah i wold like to see it back aswell.
Wonderfull little app that tells me my pc is still up and running.
Great work btw.

Edit:
I just had a Quick look on the net about what u wrote. Maybe this could Help.

https://stackoverflow.com/questions/52610352/what-library-is-collecting-advertising-id-for-my-app",NA,https://api.github.com/repos/mashlol/notify/issues/53/comments,https://github.com/mashlol/notify/issues/53#issuecomment-484462712,https://api.github.com/repos/mashlol/notify/issues/53
mashlol,notify,426979544,https://api.github.com/repos/mashlol/notify/issues/comments/499283313,499283313,MDEyOklzc3VlQ29tbWVudDQ5OTI4MzMxMw==,1598067,2019-06-05T22:45:02Z,2019-06-05T22:45:02Z,NONE,"FWIW, given that the app has disappeared from the app store and notifications from the CLI are no longer working, I decided to see if I could switch to using IFTTT to send messages from Linux to my Android phone. Indeed, it appears that you can. You can set up an IFTTT applet which accepts HTTPS requests using the Webhooks service and transmits notifications to the IFTTT app on your phone using the Notifications service. The incoming Webhooks requests can set variables for, e.g., the notification title and content, and the Notifications service can then access those variables to transmit the content to your phone. Just mentioning this here in case it proves useful to someone else.",NA,https://api.github.com/repos/mashlol/notify/issues/53/comments,https://github.com/mashlol/notify/issues/53#issuecomment-499283313,https://api.github.com/repos/mashlol/notify/issues/53
mashlol,notify,426979544,https://api.github.com/repos/mashlol/notify/issues/comments/506935640,506935640,MDEyOklzc3VlQ29tbWVudDUwNjkzNTY0MA==,36999173,2019-06-29T07:26:35Z,2019-06-29T07:26:35Z,NONE,"Thia may help too 

https://blog.usejournal.com/how-to-fix-advertising-id-policy-violation-in-google-play-store-6d9cf92d335d",NA,https://api.github.com/repos/mashlol/notify/issues/53/comments,https://github.com/mashlol/notify/issues/53#issuecomment-506935640,https://api.github.com/repos/mashlol/notify/issues/53
mashlol,notify,426979544,https://api.github.com/repos/mashlol/notify/issues/comments/513473310,513473310,MDEyOklzc3VlQ29tbWVudDUxMzQ3MzMxMA==,5004545,2019-07-20T14:42:51Z,2019-07-20T14:42:51Z,NONE,"W.R.T. the play store, would F-Droid be a viable alternative in the interim?",NA,https://api.github.com/repos/mashlol/notify/issues/53/comments,https://github.com/mashlol/notify/issues/53#issuecomment-513473310,https://api.github.com/repos/mashlol/notify/issues/53
mashlol,notify,426979544,https://api.github.com/repos/mashlol/notify/issues/comments/513515418,513515418,MDEyOklzc3VlQ29tbWVudDUxMzUxNTQxOA==,25059103,2019-07-21T03:08:07Z,2019-07-21T03:08:07Z,NONE,"Maybe on a separate repository, the part where it relies on a central
server is an anti-feature. Unless I completely misunderstood how this
project works.

On Sat, Jul 20, 2019, 10:42 AM stellarpower <notifications@github.com>
wrote:

> W.R.T. the play store, would F-Droid be a viable alternative in the
> interim?
>
> â€”
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/mashlol/notify/issues/53?email_source=notifications&email_token=AF7F6HZ3BJ6VDLGT7PVTWX3QAMP6ZA5CNFSM4HCJN43KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2NPWHQ#issuecomment-513473310>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AF7F6H4D3EP57M43QHENAQ3QAMP6ZANCNFSM4HCJN43A>
> .
>
",NA,https://api.github.com/repos/mashlol/notify/issues/53/comments,https://github.com/mashlol/notify/issues/53#issuecomment-513515418,https://api.github.com/repos/mashlol/notify/issues/53
mashlol,notify,426979544,https://api.github.com/repos/mashlol/notify/issues/comments/513515528,513515528,MDEyOklzc3VlQ29tbWVudDUxMzUxNTUyOA==,25059103,2019-07-21T03:10:57Z,2019-07-21T03:10:57Z,NONE,"In the meantime I've found wirepusher to be extremely helpful (on the play
store)

On Sat, Jul 20, 2019, 11:07 PM Jake Marin <jmarincic9@gmail.com> wrote:

> Maybe on a separate repository, the part where it relies on a central
> server is an anti-feature. Unless I completely misunderstood how this
> project works.
>
> On Sat, Jul 20, 2019, 10:42 AM stellarpower <notifications@github.com>
> wrote:
>
>> W.R.T. the play store, would F-Droid be a viable alternative in the
>> interim?
>>
>> â€”
>> You are receiving this because you commented.
>> Reply to this email directly, view it on GitHub
>> <https://github.com/mashlol/notify/issues/53?email_source=notifications&email_token=AF7F6HZ3BJ6VDLGT7PVTWX3QAMP6ZA5CNFSM4HCJN43KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2NPWHQ#issuecomment-513473310>,
>> or mute the thread
>> <https://github.com/notifications/unsubscribe-auth/AF7F6H4D3EP57M43QHENAQ3QAMP6ZANCNFSM4HCJN43A>
>> .
>>
>
",NA,https://api.github.com/repos/mashlol/notify/issues/53/comments,https://github.com/mashlol/notify/issues/53#issuecomment-513515528,https://api.github.com/repos/mashlol/notify/issues/53
mashlol,notify,426979544,https://api.github.com/repos/mashlol/notify/issues/comments/536008588,536008588,MDEyOklzc3VlQ29tbWVudDUzNjAwODU4OA==,55889263,2019-09-27T16:26:27Z,2019-09-27T16:44:03Z,NONE,"you can download  notify-android-app.apk  for android phones via:

https://drive.google.com/open?id=16WFx1qz6uMu1QWyL4pEVap6Ov8-5skn0

![notify android app token screen](https://user-images.githubusercontent.com/55889263/65785166-70f73200-e15c-11e9-8c7b-f72daf0a2b6a.jpg)",NA,https://api.github.com/repos/mashlol/notify/issues/53/comments,https://github.com/mashlol/notify/issues/53#issuecomment-536008588,https://api.github.com/repos/mashlol/notify/issues/53
mashlol,notify,426979544,https://api.github.com/repos/mashlol/notify/issues/comments/680836557,680836557,MDEyOklzc3VlQ29tbWVudDY4MDgzNjU1Nw==,39578914,2020-08-26T12:02:57Z,2020-08-26T12:02:57Z,NONE,Thanks @ctankutay . This works,NA,https://api.github.com/repos/mashlol/notify/issues/53/comments,https://github.com/mashlol/notify/issues/53#issuecomment-680836557,https://api.github.com/repos/mashlol/notify/issues/53
mashlol,notify,410853906,https://api.github.com/repos/mashlol/notify/issues/comments/466462614,466462614,MDEyOklzc3VlQ29tbWVudDQ2NjQ2MjYxNA==,7959559,2019-02-22T16:44:06Z,2019-02-22T16:44:06Z,NONE,"Yeah, same thing here.
Can't copy paste the text. Just dismiss.",NA,https://api.github.com/repos/mashlol/notify/issues/52/comments,https://github.com/mashlol/notify/issues/52#issuecomment-466462614,https://api.github.com/repos/mashlol/notify/issues/52
mashlol,notify,405582968,https://api.github.com/repos/mashlol/notify/issues/comments/459818718,459818718,MDEyOklzc3VlQ29tbWVudDQ1OTgxODcxOA==,1039240,2019-02-01T18:22:14Z,2019-02-01T18:22:14Z,OWNER,Can you add a summary of what this is for? Any docs or articles explaining? Thanks!,NA,https://api.github.com/repos/mashlol/notify/issues/51/comments,https://github.com/mashlol/notify/pull/51#issuecomment-459818718,https://api.github.com/repos/mashlol/notify/issues/51
mashlol,notify,404340959,https://api.github.com/repos/mashlol/notify/issues/comments/458598242,458598242,MDEyOklzc3VlQ29tbWVudDQ1ODU5ODI0Mg==,10133790,2019-01-29T16:05:35Z,2019-01-29T16:05:35Z,NONE,"I figured out what's going on.

`getopt` (line 19) is [not supported in MacOS by default](https://stackoverflow.com/questions/12152077/how-can-i-make-bash-deal-with-long-param-using-getopt-command-in-mac). My fix was to install gnu-getopt with Homebrew (`brew install gnu-getopt`), and then modify the script to put the command's full location in place of the getopt command on line 19 (`/usr/local/Cellar/gnu-getopt/1.1.6/bin/getopt`).

It would be neat if this used a different method to parse arguments - one which is supported in MacOS - but I don't know that that's in the scope of this project, since it's intended for Linux distributions.",NA,https://api.github.com/repos/mashlol/notify/issues/49/comments,https://github.com/mashlol/notify/issues/49#issuecomment-458598242,https://api.github.com/repos/mashlol/notify/issues/49
mashlol,notify,392539916,https://api.github.com/repos/mashlol/notify/issues/comments/467593493,467593493,MDEyOklzc3VlQ29tbWVudDQ2NzU5MzQ5Mw==,20089138,2019-02-26T20:08:47Z,2019-02-26T20:08:47Z,NONE,I have Glimpse and I get a heads-up notification with Notify,NA,https://api.github.com/repos/mashlol/notify/issues/48/comments,https://github.com/mashlol/notify/issues/48#issuecomment-467593493,https://api.github.com/repos/mashlol/notify/issues/48
mashlol,notify,368820391,https://api.github.com/repos/mashlol/notify/issues/comments/428713083,428713083,MDEyOklzc3VlQ29tbWVudDQyODcxMzA4Mw==,1039240,2018-10-10T20:09:56Z,2018-10-10T20:09:56Z,OWNER,Thanks,NA,https://api.github.com/repos/mashlol/notify/issues/44/comments,https://github.com/mashlol/notify/pull/44#issuecomment-428713083,https://api.github.com/repos/mashlol/notify/issues/44
mashlol,notify,361931355,https://api.github.com/repos/mashlol/notify/issues/comments/425571621,425571621,MDEyOklzc3VlQ29tbWVudDQyNTU3MTYyMQ==,3803896,2018-09-28T21:30:02Z,2018-09-28T21:30:02Z,NONE,It's no longer on the Play store. Does anyone know if it's available somewhere else?,NA,https://api.github.com/repos/mashlol/notify/issues/43/comments,https://github.com/mashlol/notify/issues/43#issuecomment-425571621,https://api.github.com/repos/mashlol/notify/issues/43
mashlol,notify,361931355,https://api.github.com/repos/mashlol/notify/issues/comments/425572375,425572375,MDEyOklzc3VlQ29tbWVudDQyNTU3MjM3NQ==,1039240,2018-09-28T21:33:14Z,2018-09-28T21:33:14Z,OWNER,Unfortunately Google took it down yesterday and I'm on vacation right now. Probably will sort it out in the next couple weeks. Until then you could build the apk yourself from source if you're comfortable doing that.,NA,https://api.github.com/repos/mashlol/notify/issues/43/comments,https://github.com/mashlol/notify/issues/43#issuecomment-425572375,https://api.github.com/repos/mashlol/notify/issues/43
mashlol,notify,361931355,https://api.github.com/repos/mashlol/notify/issues/comments/428376863,428376863,MDEyOklzc3VlQ29tbWVudDQyODM3Njg2Mw==,20277627,2018-10-09T22:36:43Z,2018-10-09T22:36:43Z,NONE,any news about android app ?,NA,https://api.github.com/repos/mashlol/notify/issues/43/comments,https://github.com/mashlol/notify/issues/43#issuecomment-428376863,https://api.github.com/repos/mashlol/notify/issues/43
mashlol,notify,361931355,https://api.github.com/repos/mashlol/notify/issues/comments/429584412,429584412,MDEyOklzc3VlQ29tbWVudDQyOTU4NDQxMg==,1039240,2018-10-13T23:47:01Z,2018-10-13T23:47:01Z,OWNER,"All right, submitted a new version to Google, we'll see how long it takes them to accept it. Firebase automatically collects ad ID, and apparently you need some privacy policy if it does that...so I turned it off. I don't know why it collects that anyway...",NA,https://api.github.com/repos/mashlol/notify/issues/43/comments,https://github.com/mashlol/notify/issues/43#issuecomment-429584412,https://api.github.com/repos/mashlol/notify/issues/43
mashlol,notify,361931355,https://api.github.com/repos/mashlol/notify/issues/comments/429924655,429924655,MDEyOklzc3VlQ29tbWVudDQyOTkyNDY1NQ==,1039240,2018-10-15T16:35:23Z,2018-10-15T16:35:23Z,OWNER,Seems to be approved,NA,https://api.github.com/repos/mashlol/notify/issues/43/comments,https://github.com/mashlol/notify/issues/43#issuecomment-429924655,https://api.github.com/repos/mashlol/notify/issues/43
mashlol,notify,348058765,https://api.github.com/repos/mashlol/notify/issues/comments/410868898,410868898,MDEyOklzc3VlQ29tbWVudDQxMDg2ODg5OA==,29665732,2018-08-06T22:09:33Z,2018-08-06T22:09:33Z,NONE,Closed  in lieu of #41.,NA,https://api.github.com/repos/mashlol/notify/issues/40/comments,https://github.com/mashlol/notify/issues/40#issuecomment-410868898,https://api.github.com/repos/mashlol/notify/issues/40
mashlol,notify,343649508,https://api.github.com/repos/mashlol/notify/issues/comments/410430295,410430295,MDEyOklzc3VlQ29tbWVudDQxMDQzMDI5NQ==,1039240,2018-08-04T07:20:50Z,2018-08-04T07:20:50Z,OWNER,"Thanks @ptdev ! I saw this but forgot about it until now, I'll merge it in and perhaps make a new release in a bit - may take a few weeks though.",NA,https://api.github.com/repos/mashlol/notify/issues/39/comments,https://github.com/mashlol/notify/pull/39#issuecomment-410430295,https://api.github.com/repos/mashlol/notify/issues/39
mashlol,notify,343435214,https://api.github.com/repos/mashlol/notify/issues/comments/406924447,406924447,MDEyOklzc3VlQ29tbWVudDQwNjkyNDQ0Nw==,1039240,2018-07-23T02:51:26Z,2018-07-23T02:51:26Z,OWNER,Yeah of course! Would love to see some test coverage :),NA,https://api.github.com/repos/mashlol/notify/issues/38/comments,https://github.com/mashlol/notify/issues/38#issuecomment-406924447,https://api.github.com/repos/mashlol/notify/issues/38
mashlol,notify,339547337,https://api.github.com/repos/mashlol/notify/issues/comments/403685224,403685224,MDEyOklzc3VlQ29tbWVudDQwMzY4NTIyNA==,1039240,2018-07-10T03:07:06Z,2018-07-10T03:09:57Z,OWNER,"I like this structure - but I'd love for it to be backward compatible. Perhaps we could read the old txt format first, see if it matches and if so convert over to the yaml format, whenever a key is registered? 

Another option, to keep it lightweight, we could just adjust the existing format a bit:

```
key:alias1,alias2
key2:alias3,alias4
```

Search time is probably not really a factor in the format, since it's unlikely for anyone to have many aliases/keys",NA,https://api.github.com/repos/mashlol/notify/issues/37/comments,https://github.com/mashlol/notify/issues/37#issuecomment-403685224,https://api.github.com/repos/mashlol/notify/issues/37
mashlol,notify,339547337,https://api.github.com/repos/mashlol/notify/issues/comments/403697181,403697181,MDEyOklzc3VlQ29tbWVudDQwMzY5NzE4MQ==,29665732,2018-07-10T04:33:16Z,2018-07-10T04:33:16Z,NONE,Okay! I had already started working on the yaml schema but I can change it back if need be. What do you think about [this implementation](https://github.com/taylorjdawson/notify/blob/reg-file-storage-revision/node/utils.js)?,NA,https://api.github.com/repos/mashlol/notify/issues/37/comments,https://github.com/mashlol/notify/issues/37#issuecomment-403697181,https://api.github.com/repos/mashlol/notify/issues/37
mashlol,notify,339547337,https://api.github.com/repos/mashlol/notify/issues/comments/403698630,403698630,MDEyOklzc3VlQ29tbWVudDQwMzY5ODYzMA==,1039240,2018-07-10T04:44:11Z,2018-07-10T04:44:11Z,OWNER,"Yeah looking good! The only thing I don't like that much is that we're writing in a `get` method, so it has a side effect that may be unexpected. Perhaps simply changing the name to `getYamlRegFileOrMigrate` or something would be enough to solve that problem.",NA,https://api.github.com/repos/mashlol/notify/issues/37/comments,https://github.com/mashlol/notify/issues/37#issuecomment-403698630,https://api.github.com/repos/mashlol/notify/issues/37
mashlol,notify,339547337,https://api.github.com/repos/mashlol/notify/issues/comments/406893065,406893065,MDEyOklzc3VlQ29tbWVudDQwNjg5MzA2NQ==,29665732,2018-07-22T20:07:46Z,2018-07-22T20:07:46Z,NONE,@mashlol okay will do!,NA,https://api.github.com/repos/mashlol/notify/issues/37/comments,https://github.com/mashlol/notify/issues/37#issuecomment-406893065,https://api.github.com/repos/mashlol/notify/issues/37
mashlol,notify,339165976,https://api.github.com/repos/mashlol/notify/issues/comments/403237841,403237841,MDEyOklzc3VlQ29tbWVudDQwMzIzNzg0MQ==,1039240,2018-07-07T19:26:23Z,2018-07-07T19:26:23Z,OWNER,Thanks!,NA,https://api.github.com/repos/mashlol/notify/issues/34/comments,https://github.com/mashlol/notify/pull/34#issuecomment-403237841,https://api.github.com/repos/mashlol/notify/issues/34
mashlol,notify,339165976,https://api.github.com/repos/mashlol/notify/issues/comments/403239502,403239502,MDEyOklzc3VlQ29tbWVudDQwMzIzOTUwMg==,1280528,2018-07-07T19:58:03Z,2018-07-07T19:58:03Z,CONTRIBUTOR,"No problem, and thank you for this app! Very useful ðŸ‘ ",NA,https://api.github.com/repos/mashlol/notify/issues/34/comments,https://github.com/mashlol/notify/pull/34#issuecomment-403239502,https://api.github.com/repos/mashlol/notify/issues/34
mashlol,notify,339165976,https://api.github.com/repos/mashlol/notify/issues/comments/403309729,403309729,MDEyOklzc3VlQ29tbWVudDQwMzMwOTcyOQ==,29665732,2018-07-08T19:14:13Z,2018-07-08T19:14:13Z,NONE,"I appreciate this, thanks!",NA,https://api.github.com/repos/mashlol/notify/issues/34/comments,https://github.com/mashlol/notify/pull/34#issuecomment-403309729,https://api.github.com/repos/mashlol/notify/issues/34
mashlol,notify,338350297,https://api.github.com/repos/mashlol/notify/issues/comments/402549123,402549123,MDEyOklzc3VlQ29tbWVudDQwMjU0OTEyMw==,1039240,2018-07-04T19:44:04Z,2018-07-04T19:44:04Z,OWNER,"Hmm, it works for me on Android 8. Which version of the app are you using? There were some issues in older versions",NA,https://api.github.com/repos/mashlol/notify/issues/33/comments,https://github.com/mashlol/notify/issues/33#issuecomment-402549123,https://api.github.com/repos/mashlol/notify/issues/33
mashlol,notify,338350297,https://api.github.com/repos/mashlol/notify/issues/comments/402552355,402552355,MDEyOklzc3VlQ29tbWVudDQwMjU1MjM1NQ==,5488003,2018-07-04T20:09:50Z,2018-07-04T20:09:50Z,NONE,I'm using latest version from the playstore (1.4),NA,https://api.github.com/repos/mashlol/notify/issues/33/comments,https://github.com/mashlol/notify/issues/33#issuecomment-402552355,https://api.github.com/repos/mashlol/notify/issues/33
mashlol,notify,338350297,https://api.github.com/repos/mashlol/notify/issues/comments/402555637,402555637,MDEyOklzc3VlQ29tbWVudDQwMjU1NTYzNw==,1039240,2018-07-04T20:36:18Z,2018-07-04T20:36:49Z,OWNER,"Oh, what about the CLI version? I think you also need the latest notify-cli npm version to get it to work properly in background.

Alternatively, could you try using the shell script included in the repo? That should work.",NA,https://api.github.com/repos/mashlol/notify/issues/33/comments,https://github.com/mashlol/notify/issues/33#issuecomment-402555637,https://api.github.com/repos/mashlol/notify/issues/33
mashlol,notify,338350297,https://api.github.com/repos/mashlol/notify/issues/comments/402557821,402557821,MDEyOklzc3VlQ29tbWVudDQwMjU1NzgyMQ==,5488003,2018-07-04T20:53:45Z,2018-07-04T20:53:45Z,NONE,"I was using version 0.1.3, updated to 0.1.4 and now it works, thank you! ",NA,https://api.github.com/repos/mashlol/notify/issues/33/comments,https://github.com/mashlol/notify/issues/33#issuecomment-402557821,https://api.github.com/repos/mashlol/notify/issues/33
mashlol,notify,338335916,https://api.github.com/repos/mashlol/notify/issues/comments/402549041,402549041,MDEyOklzc3VlQ29tbWVudDQwMjU0OTA0MQ==,1039240,2018-07-04T19:43:27Z,2018-07-04T19:43:27Z,OWNER,"When you registered, did it print ""[notify] Your registration code has been saved to ~/.notifyreg""?  Can you open the `~/.notifyreg` file and ensure the key is in there?

When you use the CLI, does it say ""[notify] Notifying {key}"" or nothing at all?

What platform are you on?",NA,https://api.github.com/repos/mashlol/notify/issues/32/comments,https://github.com/mashlol/notify/issues/32#issuecomment-402549041,https://api.github.com/repos/mashlol/notify/issues/32
mashlol,notify,338335916,https://api.github.com/repos/mashlol/notify/issues/comments/403685312,403685312,MDEyOklzc3VlQ29tbWVudDQwMzY4NTMxMg==,1039240,2018-07-10T03:07:41Z,2018-07-10T03:07:41Z,OWNER,"Closing for now, please reopen if you are still encountering and can answer above questions",NA,https://api.github.com/repos/mashlol/notify/issues/32/comments,https://github.com/mashlol/notify/issues/32#issuecomment-403685312,https://api.github.com/repos/mashlol/notify/issues/32
mashlol,notify,338335916,https://api.github.com/repos/mashlol/notify/issues/comments/403763874,403763874,MDEyOklzc3VlQ29tbWVudDQwMzc2Mzg3NA==,5004545,2018-07-10T09:41:41Z,2018-07-10T09:43:31Z,NONE,"Sorry for the delay, no, it doesn't print anything it just ends silently. I don't have a .notifyreg file in my home folder. It works fine on my other machine. I'm on Mint 18.3 Sylvia, same as the machine on whihc it works succesfully. Does node have a logging system or anything I can check as it's not throwing any uncaught exceptions?

Thanks

BTW doesn't seem I have the permissions to reopen this so it's staying as closed but I'd like to reopen it
",NA,https://api.github.com/repos/mashlol/notify/issues/32/comments,https://github.com/mashlol/notify/issues/32#issuecomment-403763874,https://api.github.com/repos/mashlol/notify/issues/32
mashlol,notify,338335916,https://api.github.com/repos/mashlol/notify/issues/comments/404051349,404051349,MDEyOklzc3VlQ29tbWVudDQwNDA1MTM0OQ==,1039240,2018-07-11T05:41:24Z,2018-07-11T05:41:24Z,OWNER,My guess is it doesn't have permission to write the `.notifyreg` file. Could you try running `notify -r xxx` w/ `sudo`?,NA,https://api.github.com/repos/mashlol/notify/issues/32/comments,https://github.com/mashlol/notify/issues/32#issuecomment-404051349,https://api.github.com/repos/mashlol/notify/issues/32
mashlol,notify,338050533,https://api.github.com/repos/mashlol/notify/issues/comments/402549211,402549211,MDEyOklzc3VlQ29tbWVudDQwMjU0OTIxMQ==,1039240,2018-07-04T19:44:45Z,2018-07-04T19:44:45Z,OWNER,"I like the idea - probably won't have time to add it for a while, but I'll accept a pull request that adds it (as long as it is still backwards compatible)",NA,https://api.github.com/repos/mashlol/notify/issues/31/comments,https://github.com/mashlol/notify/issues/31#issuecomment-402549211,https://api.github.com/repos/mashlol/notify/issues/31
mashlol,notify,338050533,https://api.github.com/repos/mashlol/notify/issues/comments/402845568,402845568,MDEyOklzc3VlQ29tbWVudDQwMjg0NTU2OA==,29665732,2018-07-05T20:38:21Z,2018-07-05T20:38:21Z,NONE,Do you like the proposed format? Or do you have a better idea?,NA,https://api.github.com/repos/mashlol/notify/issues/31/comments,https://github.com/mashlol/notify/issues/31#issuecomment-402845568,https://api.github.com/repos/mashlol/notify/issues/31
mashlol,notify,338050533,https://api.github.com/repos/mashlol/notify/issues/comments/402849738,402849738,MDEyOklzc3VlQ29tbWVudDQwMjg0OTczOA==,1039240,2018-07-05T20:55:41Z,2018-07-05T20:56:30Z,OWNER,"I think -t is already for the text right? Perhaps -k for key?

So `notify -k asdf -t ""Message"" -i ""Title""`?

In long form:

`notify --only-key asdf --text ""Text"" --title ""title""`",NA,https://api.github.com/repos/mashlol/notify/issues/31/comments,https://github.com/mashlol/notify/issues/31#issuecomment-402849738,https://api.github.com/repos/mashlol/notify/issues/31
mashlol,notify,338050533,https://api.github.com/repos/mashlol/notify/issues/comments/402865236,402865236,MDEyOklzc3VlQ29tbWVudDQwMjg2NTIzNg==,29665732,2018-07-05T21:59:18Z,2018-07-05T21:59:18Z,NONE,"Yeah, that looks good. If you wanted to map a name to a key for instance, `android-device-0 -> asdf `, would you still use the `-k` option? Or would there need to be another option for key aliases? ",NA,https://api.github.com/repos/mashlol/notify/issues/31/comments,https://github.com/mashlol/notify/issues/31#issuecomment-402865236,https://api.github.com/repos/mashlol/notify/issues/31
mashlol,notify,338050533,https://api.github.com/repos/mashlol/notify/issues/comments/402926436,402926436,MDEyOklzc3VlQ29tbWVudDQwMjkyNjQzNg==,1039240,2018-07-06T05:02:53Z,2018-07-06T05:02:53Z,OWNER,"Oh right, I guess we need a command to make the mapping? Perhaps

`notify --map-key 'asdf' --map-to 'android'` or `notify -m 'asdf' -o 'android`

And then 'android' or 'asdf' will both work for the `-k` param?",NA,https://api.github.com/repos/mashlol/notify/issues/31/comments,https://github.com/mashlol/notify/issues/31#issuecomment-402926436,https://api.github.com/repos/mashlol/notify/issues/31
mashlol,notify,338050533,https://api.github.com/repos/mashlol/notify/issues/comments/403182949,403182949,MDEyOklzc3VlQ29tbWVudDQwMzE4Mjk0OQ==,29665732,2018-07-07T02:38:03Z,2018-07-07T02:38:03Z,NONE,"Yeah, I think that would make sense to the user. Just have it be specified in the read me.",NA,https://api.github.com/repos/mashlol/notify/issues/31/comments,https://github.com/mashlol/notify/issues/31#issuecomment-403182949,https://api.github.com/repos/mashlol/notify/issues/31
mashlol,notify,338050533,https://api.github.com/repos/mashlol/notify/issues/comments/403183060,403183060,MDEyOklzc3VlQ29tbWVudDQwMzE4MzA2MA==,29665732,2018-07-07T02:39:51Z,2018-07-07T02:39:51Z,NONE,"Should there be a way to assign the mapping when registering the key?
`notify -r asdf -o 'android'`",NA,https://api.github.com/repos/mashlol/notify/issues/31/comments,https://github.com/mashlol/notify/issues/31#issuecomment-403183060,https://api.github.com/repos/mashlol/notify/issues/31
mashlol,notify,338050533,https://api.github.com/repos/mashlol/notify/issues/comments/403183227,403183227,MDEyOklzc3VlQ29tbWVudDQwMzE4MzIyNw==,1039240,2018-07-07T02:43:32Z,2018-07-07T02:43:32Z,OWNER,Oh yeah we can reuse the -r for mapping. So you could use `notify -r asdf -o 'android'` on an existing key or to register a new one.,NA,https://api.github.com/repos/mashlol/notify/issues/31/comments,https://github.com/mashlol/notify/issues/31#issuecomment-403183227,https://api.github.com/repos/mashlol/notify/issues/31
mashlol,notify,338050533,https://api.github.com/repos/mashlol/notify/issues/comments/403309945,403309945,MDEyOklzc3VlQ29tbWVudDQwMzMwOTk0NQ==,29665732,2018-07-08T19:17:03Z,2018-07-08T19:17:03Z,NONE,"Sweet! I like that. So:
```
notify --register asdf --map-to 'android'
notify -r asdf -o 'android'
```",NA,https://api.github.com/repos/mashlol/notify/issues/31/comments,https://github.com/mashlol/notify/issues/31#issuecomment-403309945,https://api.github.com/repos/mashlol/notify/issues/31
mashlol,notify,335158492,https://api.github.com/repos/mashlol/notify/issues/comments/414640323,414640323,MDEyOklzc3VlQ29tbWVudDQxNDY0MDMyMw==,3299515,2018-08-21T11:19:42Z,2018-08-21T11:19:42Z,NONE,"looks like there's bash version of this script: https://github.com/mashlol/notify/blob/master/sh/notify.sh

  curl   ""https://us-central1-notify-b7652.cloudfunctions.net/sendNotification?to=${KEY}&text=${TEXT}"" 

",NA,https://api.github.com/repos/mashlol/notify/issues/30/comments,https://github.com/mashlol/notify/issues/30#issuecomment-414640323,https://api.github.com/repos/mashlol/notify/issues/30
mashlol,notify,335158492,https://api.github.com/repos/mashlol/notify/issues/comments/415025203,415025203,MDEyOklzc3VlQ29tbWVudDQxNTAyNTIwMw==,34858013,2018-08-22T13:07:28Z,2018-08-22T13:07:28Z,NONE,"Thank you! ðŸ˜Š , that is a valid solution for my case. ",NA,https://api.github.com/repos/mashlol/notify/issues/30/comments,https://github.com/mashlol/notify/issues/30#issuecomment-415025203,https://api.github.com/repos/mashlol/notify/issues/30
mashlol,notify,321644780,https://api.github.com/repos/mashlol/notify/issues/comments/388542386,388542386,MDEyOklzc3VlQ29tbWVudDM4ODU0MjM4Ng==,1039240,2018-05-12T09:22:02Z,2018-05-12T09:22:02Z,OWNER,There should be sound/vibration now if you update to the latest android version & npm package.,NA,https://api.github.com/repos/mashlol/notify/issues/29/comments,https://github.com/mashlol/notify/issues/29#issuecomment-388542386,https://api.github.com/repos/mashlol/notify/issues/29
mashlol,notify,295909213,https://api.github.com/repos/mashlol/notify/issues/comments/388542522,388542522,MDEyOklzc3VlQ29tbWVudDM4ODU0MjUyMg==,1039240,2018-05-12T09:23:52Z,2018-05-12T09:23:52Z,OWNER,The link doesn't work for me?,NA,https://api.github.com/repos/mashlol/notify/issues/28/comments,https://github.com/mashlol/notify/pull/28#issuecomment-388542522,https://api.github.com/repos/mashlol/notify/issues/28
mashlol,notify,295909213,https://api.github.com/repos/mashlol/notify/issues/comments/388558231,388558231,MDEyOklzc3VlQ29tbWVudDM4ODU1ODIzMQ==,705213,2018-05-12T14:16:47Z,2018-05-12T14:16:47Z,NONE,"Yes! I was waiting for your approval before uploading the app on F-Droid, maybe you want to do it yourself to keep control on the release of new versions?",NA,https://api.github.com/repos/mashlol/notify/issues/28/comments,https://github.com/mashlol/notify/pull/28#issuecomment-388558231,https://api.github.com/repos/mashlol/notify/issues/28
mashlol,notify,295909213,https://api.github.com/repos/mashlol/notify/issues/comments/388579761,388579761,MDEyOklzc3VlQ29tbWVudDM4ODU3OTc2MQ==,1039240,2018-05-12T20:08:27Z,2018-05-12T20:08:27Z,OWNER,Sure I can upload it.,NA,https://api.github.com/repos/mashlol/notify/issues/28/comments,https://github.com/mashlol/notify/pull/28#issuecomment-388579761,https://api.github.com/repos/mashlol/notify/issues/28
mashlol,notify,295909213,https://api.github.com/repos/mashlol/notify/issues/comments/476873166,476873166,MDEyOklzc3VlQ29tbWVudDQ3Njg3MzE2Ng==,5254422,2019-03-26T22:11:23Z,2019-03-26T22:11:23Z,NONE,Can't find it on f-droid. What happened?,NA,https://api.github.com/repos/mashlol/notify/issues/28/comments,https://github.com/mashlol/notify/pull/28#issuecomment-476873166,https://api.github.com/repos/mashlol/notify/issues/28
mashlol,notify,295909213,https://api.github.com/repos/mashlol/notify/issues/comments/478195424,478195424,MDEyOklzc3VlQ29tbWVudDQ3ODE5NTQyNA==,1039240,2019-03-30T01:51:25Z,2019-03-30T01:51:25Z,OWNER,Sorry I never ended up getting around to this - happy if someone else does it though,NA,https://api.github.com/repos/mashlol/notify/issues/28/comments,https://github.com/mashlol/notify/pull/28#issuecomment-478195424,https://api.github.com/repos/mashlol/notify/issues/28
mashlol,notify,295909213,https://api.github.com/repos/mashlol/notify/issues/comments/478244750,478244750,MDEyOklzc3VlQ29tbWVudDQ3ODI0NDc1MA==,705213,2019-03-30T13:16:07Z,2019-03-30T13:16:07Z,NONE,@mashlol I can do it!,NA,https://api.github.com/repos/mashlol/notify/issues/28/comments,https://github.com/mashlol/notify/pull/28#issuecomment-478244750,https://api.github.com/repos/mashlol/notify/issues/28
mashlol,notify,295909213,https://api.github.com/repos/mashlol/notify/issues/comments/558063967,558063967,MDEyOklzc3VlQ29tbWVudDU1ODA2Mzk2Nw==,20089138,2019-11-25T09:19:33Z,2019-11-25T09:19:33Z,NONE,Where can I get the smartphone app? (neither on Google Play or F-Droid),NA,https://api.github.com/repos/mashlol/notify/issues/28/comments,https://github.com/mashlol/notify/pull/28#issuecomment-558063967,https://api.github.com/repos/mashlol/notify/issues/28
mashlol,notify,269396425,https://api.github.com/repos/mashlol/notify/issues/comments/340650344,340650344,MDEyOklzc3VlQ29tbWVudDM0MDY1MDM0NA==,1039240,2017-10-31T03:34:30Z,2017-10-31T03:34:30Z,OWNER,"Can you remove the iml/metadata type files from the commit?  Also, if you'd like I can give you direct push access, I don't maintain this anymore and it could use some love.",NA,https://api.github.com/repos/mashlol/notify/issues/27/comments,https://github.com/mashlol/notify/pull/27#issuecomment-340650344,https://api.github.com/repos/mashlol/notify/issues/27
mashlol,notify,269396425,https://api.github.com/repos/mashlol/notify/issues/comments/340701634,340701634,MDEyOklzc3VlQ29tbWVudDM0MDcwMTYzNA==,11231563,2017-10-31T09:07:39Z,2017-10-31T09:07:39Z,CONTRIBUTOR,"Removed metadata as requested.
I have no time to maintain this project yet, I started  a new job right now.
When I have more time, I would like to maintain this one.",NA,https://api.github.com/repos/mashlol/notify/issues/27/comments,https://github.com/mashlol/notify/pull/27#issuecomment-340701634,https://api.github.com/repos/mashlol/notify/issues/27
mashlol,notify,269396425,https://api.github.com/repos/mashlol/notify/issues/comments/340979627,340979627,MDEyOklzc3VlQ29tbWVudDM0MDk3OTYyNw==,1039240,2017-11-01T05:10:12Z,2017-11-01T05:10:12Z,OWNER,Thanks for this!,NA,https://api.github.com/repos/mashlol/notify/issues/27/comments,https://github.com/mashlol/notify/pull/27#issuecomment-340979627,https://api.github.com/repos/mashlol/notify/issues/27
mashlol,notify,266558187,https://api.github.com/repos/mashlol/notify/issues/comments/337803807,337803807,MDEyOklzc3VlQ29tbWVudDMzNzgwMzgwNw==,1039240,2017-10-19T05:30:58Z,2017-10-19T05:30:58Z,OWNER,MIT license.  I don't maintain this anymore though so I'd be happy to give write access if anyone wanted to take it over.,NA,https://api.github.com/repos/mashlol/notify/issues/26/comments,https://github.com/mashlol/notify/issues/26#issuecomment-337803807,https://api.github.com/repos/mashlol/notify/issues/26
mashlol,notify,266558187,https://api.github.com/repos/mashlol/notify/issues/comments/337884743,337884743,MDEyOklzc3VlQ29tbWVudDMzNzg4NDc0Mw==,245791,2017-10-19T11:54:18Z,2017-10-19T11:54:18Z,NONE,I can help you. I was talking to @jhonathas and he's also willing to help.,NA,https://api.github.com/repos/mashlol/notify/issues/26/comments,https://github.com/mashlol/notify/issues/26#issuecomment-337884743,https://api.github.com/repos/mashlol/notify/issues/26
mashlol,notify,266558187,https://api.github.com/repos/mashlol/notify/issues/comments/337989931,337989931,MDEyOklzc3VlQ29tbWVudDMzNzk4OTkzMQ==,1039240,2017-10-19T18:05:41Z,2017-10-19T18:05:41Z,OWNER,Gave you both push access,NA,https://api.github.com/repos/mashlol/notify/issues/26/comments,https://github.com/mashlol/notify/issues/26#issuecomment-337989931,https://api.github.com/repos/mashlol/notify/issues/26
mashlol,notify,265918453,https://api.github.com/repos/mashlol/notify/issues/comments/388542666,388542666,MDEyOklzc3VlQ29tbWVudDM4ODU0MjY2Ng==,1039240,2018-05-12T09:27:01Z,2018-05-12T09:27:01Z,OWNER,Done,NA,https://api.github.com/repos/mashlol/notify/issues/25/comments,https://github.com/mashlol/notify/issues/25#issuecomment-388542666,https://api.github.com/repos/mashlol/notify/issues/25
mashlol,notify,247941362,https://api.github.com/repos/mashlol/notify/issues/comments/346289375,346289375,MDEyOklzc3VlQ29tbWVudDM0NjI4OTM3NQ==,1016580,2017-11-22T09:15:08Z,2017-11-22T09:15:08Z,NONE,"This happens to me too.

I'm using Android Oreo, I will rate this app 5 stars if this changes",NA,https://api.github.com/repos/mashlol/notify/issues/24/comments,https://github.com/mashlol/notify/issues/24#issuecomment-346289375,https://api.github.com/repos/mashlol/notify/issues/24
mashlol,notify,247941362,https://api.github.com/repos/mashlol/notify/issues/comments/388542409,388542409,MDEyOklzc3VlQ29tbWVudDM4ODU0MjQwOQ==,1039240,2018-05-12T09:22:25Z,2018-05-12T09:22:25Z,OWNER,"There should be sound/vibration now if you update to the latest android version & npm package.

",NA,https://api.github.com/repos/mashlol/notify/issues/24/comments,https://github.com/mashlol/notify/issues/24#issuecomment-388542409,https://api.github.com/repos/mashlol/notify/issues/24
mashlol,notify,238337136,https://api.github.com/repos/mashlol/notify/issues/comments/310860313,310860313,MDEyOklzc3VlQ29tbWVudDMxMDg2MDMxMw==,1039240,2017-06-24T19:16:04Z,2017-06-24T19:16:04Z,OWNER,Thanks,NA,https://api.github.com/repos/mashlol/notify/issues/23/comments,https://github.com/mashlol/notify/pull/23#issuecomment-310860313,https://api.github.com/repos/mashlol/notify/issues/23
mashlol,notify,235822754,https://api.github.com/repos/mashlol/notify/issues/comments/337648491,337648491,MDEyOklzc3VlQ29tbWVudDMzNzY0ODQ5MQ==,245791,2017-10-18T16:27:25Z,2017-10-18T16:27:25Z,NONE,I'm facing the same issues with sound and vibration for notifications.,NA,https://api.github.com/repos/mashlol/notify/issues/22/comments,https://github.com/mashlol/notify/issues/22#issuecomment-337648491,https://api.github.com/repos/mashlol/notify/issues/22
mashlol,notify,235822754,https://api.github.com/repos/mashlol/notify/issues/comments/343696225,343696225,MDEyOklzc3VlQ29tbWVudDM0MzY5NjIyNQ==,10761510,2017-11-11T21:43:59Z,2017-11-11T21:43:59Z,NONE,"For some reason, I'm also not receiving sounds or vibration upon notification. Hopefully, this will be fixed.",NA,https://api.github.com/repos/mashlol/notify/issues/22/comments,https://github.com/mashlol/notify/issues/22#issuecomment-343696225,https://api.github.com/repos/mashlol/notify/issues/22
mashlol,notify,235822754,https://api.github.com/repos/mashlol/notify/issues/comments/388542498,388542498,MDEyOklzc3VlQ29tbWVudDM4ODU0MjQ5OA==,1039240,2018-05-12T09:23:26Z,2018-05-12T09:23:26Z,OWNER,Audio issues should be fixed.,NA,https://api.github.com/repos/mashlol/notify/issues/22/comments,https://github.com/mashlol/notify/issues/22#issuecomment-388542498,https://api.github.com/repos/mashlol/notify/issues/22
mashlol,notify,219208753,https://api.github.com/repos/mashlol/notify/issues/comments/291717941,291717941,MDEyOklzc3VlQ29tbWVudDI5MTcxNzk0MQ==,1039240,2017-04-05T02:15:15Z,2017-04-05T02:15:15Z,OWNER,Thanks for this!,NA,https://api.github.com/repos/mashlol/notify/issues/21/comments,https://github.com/mashlol/notify/pull/21#issuecomment-291717941,https://api.github.com/repos/mashlol/notify/issues/21
mashlol,notify,218012016,https://api.github.com/repos/mashlol/notify/issues/comments/290263855,290263855,MDEyOklzc3VlQ29tbWVudDI5MDI2Mzg1NQ==,1039240,2017-03-30T00:11:53Z,2017-03-30T00:11:53Z,OWNER,"I think I ran out of Heroku free hours this month, looking into a better solution.",NA,https://api.github.com/repos/mashlol/notify/issues/20/comments,https://github.com/mashlol/notify/issues/20#issuecomment-290263855,https://api.github.com/repos/mashlol/notify/issues/20
mashlol,notify,218012016,https://api.github.com/repos/mashlol/notify/issues/comments/290263902,290263902,MDEyOklzc3VlQ29tbWVudDI5MDI2MzkwMg==,1039240,2017-03-30T00:12:11Z,2017-03-30T00:12:11Z,OWNER,"Firebase has server functions now, might be able to use that",NA,https://api.github.com/repos/mashlol/notify/issues/20/comments,https://github.com/mashlol/notify/issues/20#issuecomment-290263902,https://api.github.com/repos/mashlol/notify/issues/20
mashlol,notify,218012016,https://api.github.com/repos/mashlol/notify/issues/comments/347200152,347200152,MDEyOklzc3VlQ29tbWVudDM0NzIwMDE1Mg==,406449,2017-11-27T14:36:40Z,2017-11-27T14:36:40Z,NONE,"Having the same problem. Or, not same as you seemingly switched to Firebase, but I'm not receiving anything on the phone. Would it make sense to have the app send a response to the server when receiving a notification, which could then be forwarded on to the client, so as to confirm that the message has gone through?",NA,https://api.github.com/repos/mashlol/notify/issues/20/comments,https://github.com/mashlol/notify/issues/20#issuecomment-347200152,https://api.github.com/repos/mashlol/notify/issues/20
mashlol,notify,218012016,https://api.github.com/repos/mashlol/notify/issues/comments/350837445,350837445,MDEyOklzc3VlQ29tbWVudDM1MDgzNzQ0NQ==,19560709,2017-12-11T19:50:13Z,2017-12-11T19:50:13Z,NONE,"Also having this, only thing is checking the packets with wireshark shows them being sent to heroku.

Did you change back from firebase or is the npm repo version outdated?",NA,https://api.github.com/repos/mashlol/notify/issues/20/comments,https://github.com/mashlol/notify/issues/20#issuecomment-350837445,https://api.github.com/repos/mashlol/notify/issues/20
mashlol,notify,218012016,https://api.github.com/repos/mashlol/notify/issues/comments/351272767,351272767,MDEyOklzc3VlQ29tbWVudDM1MTI3Mjc2Nw==,1039240,2017-12-13T03:37:16Z,2017-12-13T03:37:16Z,OWNER,"Using heroku to talk to firebase (since at the time firebase didn't have cloud functions).  I haven't converted over to cloud functions yet, it still uses heroku.  Afaik it should be working.",NA,https://api.github.com/repos/mashlol/notify/issues/20/comments,https://github.com/mashlol/notify/issues/20#issuecomment-351272767,https://api.github.com/repos/mashlol/notify/issues/20
mashlol,notify,218012016,https://api.github.com/repos/mashlol/notify/issues/comments/363240444,363240444,MDEyOklzc3VlQ29tbWVudDM2MzI0MDQ0NA==,10713805,2018-02-05T22:18:07Z,2018-02-05T22:18:07Z,NONE,"same here - since I switched to another custom rom the notify stopped working (i of course changed the ID)

Galaxy S4 stock rom: working
Cyanogenmod rom: working
Omega Rom (Stock, deodexed, rooted): NOT working

all other notifications (whatsapp etc) work fine
",NA,https://api.github.com/repos/mashlol/notify/issues/20/comments,https://github.com/mashlol/notify/issues/20#issuecomment-363240444,https://api.github.com/repos/mashlol/notify/issues/20
mashlol,notify,218012016,https://api.github.com/repos/mashlol/notify/issues/comments/384359630,384359630,MDEyOklzc3VlQ29tbWVudDM4NDM1OTYzMA==,15375692,2018-04-25T16:59:49Z,2018-04-25T16:59:49Z,NONE,"I can not receive any notification. I reinstalled the mobile app to get a new uuid and it didn't worked. :(
Also verified if app has permissions to show notification, it does.
Phone: Huawei P10
Android version: 8.0.0
",NA,https://api.github.com/repos/mashlol/notify/issues/20/comments,https://github.com/mashlol/notify/issues/20#issuecomment-384359630,https://api.github.com/repos/mashlol/notify/issues/20
mashlol,notify,218012016,https://api.github.com/repos/mashlol/notify/issues/comments/388542428,388542428,MDEyOklzc3VlQ29tbWVudDM4ODU0MjQyOA==,1039240,2018-05-12T09:22:41Z,2018-05-12T09:22:41Z,OWNER,"I upgraded to firebase functions, this should never happen again.",NA,https://api.github.com/repos/mashlol/notify/issues/20/comments,https://github.com/mashlol/notify/issues/20#issuecomment-388542428,https://api.github.com/repos/mashlol/notify/issues/20
mashlol,notify,211334865,https://api.github.com/repos/mashlol/notify/issues/comments/290263943,290263943,MDEyOklzc3VlQ29tbWVudDI5MDI2Mzk0Mw==,1039240,2017-03-30T00:12:26Z,2017-03-30T00:12:26Z,OWNER,Do you want to submit a pull request to update this?,NA,https://api.github.com/repos/mashlol/notify/issues/19/comments,https://github.com/mashlol/notify/issues/19#issuecomment-290263943,https://api.github.com/repos/mashlol/notify/issues/19
mashlol,notify,211334865,https://api.github.com/repos/mashlol/notify/issues/comments/290331476,290331476,MDEyOklzc3VlQ29tbWVudDI5MDMzMTQ3Ng==,10251417,2017-03-30T07:53:37Z,2017-03-30T07:53:37Z,CONTRIBUTOR,"Hi,
thanks for picking up this issue. I was not sure if the proposed solution lines up with the project plan, but if you think this modification is acceptable, i will try. I have to admit, i use github via the SVN interface, so it might take a little time to pick up git-style workflow. ",NA,https://api.github.com/repos/mashlol/notify/issues/19/comments,https://github.com/mashlol/notify/issues/19#issuecomment-290331476,https://api.github.com/repos/mashlol/notify/issues/19
mashlol,notify,211334865,https://api.github.com/repos/mashlol/notify/issues/comments/290634209,290634209,MDEyOklzc3VlQ29tbWVudDI5MDYzNDIwOQ==,1039240,2017-03-31T07:06:07Z,2017-03-31T07:06:07Z,OWNER,Yes I think your solution would be fine.,NA,https://api.github.com/repos/mashlol/notify/issues/19/comments,https://github.com/mashlol/notify/issues/19#issuecomment-290634209,https://api.github.com/repos/mashlol/notify/issues/19
mashlol,notify,196598860,https://api.github.com/repos/mashlol/notify/issues/comments/283600719,283600719,MDEyOklzc3VlQ29tbWVudDI4MzYwMDcxOQ==,10251417,2017-03-02T09:23:07Z,2017-03-02T09:23:07Z,CONTRIBUTOR,"Dear Kevin,

thanks for this great app! I used the old version, now installed the new one, but it still gives I or L characters upon the key generation. Do i miss something?",NA,https://api.github.com/repos/mashlol/notify/issues/18/comments,https://github.com/mashlol/notify/issues/18#issuecomment-283600719,https://api.github.com/repos/mashlol/notify/issues/18
mashlol,notify,196598177,https://api.github.com/repos/mashlol/notify/issues/comments/268431241,268431241,MDEyOklzc3VlQ29tbWVudDI2ODQzMTI0MQ==,1039240,2016-12-21T04:13:28Z,2016-12-21T04:13:28Z,OWNER,"This is because your Node.js version doesn't support the => notation.  Try upgrading Node.js for now, for a better solution I'll update so we don't use ES6 stuff anywhere.",NA,https://api.github.com/repos/mashlol/notify/issues/17/comments,https://github.com/mashlol/notify/issues/17#issuecomment-268431241,https://api.github.com/repos/mashlol/notify/issues/17
mashlol,notify,196598177,https://api.github.com/repos/mashlol/notify/issues/comments/304973918,304973918,MDEyOklzc3VlQ29tbWVudDMwNDk3MzkxOA==,29070885,2017-05-30T18:55:02Z,2017-05-30T18:55:02Z,NONE,"I have the same on centos6. 
The nodejs (from epel) is in 0.10.48-3 for el6 and 6.10.2-1 for el7. ",NA,https://api.github.com/repos/mashlol/notify/issues/17/comments,https://github.com/mashlol/notify/issues/17#issuecomment-304973918,https://api.github.com/repos/mashlol/notify/issues/17
mashlol,notify,195326539,https://api.github.com/repos/mashlol/notify/issues/comments/266816469,266816469,MDEyOklzc3VlQ29tbWVudDI2NjgxNjQ2OQ==,6325682,2016-12-13T18:13:17Z,2016-12-13T18:13:17Z,NONE,"Solved, I used old android app",NA,https://api.github.com/repos/mashlol/notify/issues/16/comments,https://github.com/mashlol/notify/issues/16#issuecomment-266816469,https://api.github.com/repos/mashlol/notify/issues/16
mashlol,notify,193028489,https://api.github.com/repos/mashlol/notify/issues/comments/264369825,264369825,MDEyOklzc3VlQ29tbWVudDI2NDM2OTgyNQ==,1039240,2016-12-02T04:18:26Z,2016-12-02T04:18:45Z,OWNER,"Does Arch Linux have env?  Perhaps it does not live in /usr/bin on Arch Linux?

Can you run `which env` and let me know what it prints?",NA,https://api.github.com/repos/mashlol/notify/issues/15/comments,https://github.com/mashlol/notify/issues/15#issuecomment-264369825,https://api.github.com/repos/mashlol/notify/issues/15
mashlol,notify,193028489,https://api.github.com/repos/mashlol/notify/issues/comments/264450847,264450847,MDEyOklzc3VlQ29tbWVudDI2NDQ1MDg0Nw==,244857,2016-12-02T13:11:49Z,2016-12-02T13:12:03Z,NONE,"$ which env
/usr/bin/env
",NA,https://api.github.com/repos/mashlol/notify/issues/15/comments,https://github.com/mashlol/notify/issues/15#issuecomment-264450847,https://api.github.com/repos/mashlol/notify/issues/15
mashlol,notify,193028489,https://api.github.com/repos/mashlol/notify/issues/comments/264614301,264614301,MDEyOklzc3VlQ29tbWVudDI2NDYxNDMwMQ==,1039240,2016-12-03T03:53:34Z,2016-12-03T03:53:34Z,OWNER,Oh probably the \r is screwing it up...,NA,https://api.github.com/repos/mashlol/notify/issues/15/comments,https://github.com/mashlol/notify/issues/15#issuecomment-264614301,https://api.github.com/repos/mashlol/notify/issues/15
mashlol,notify,193028489,https://api.github.com/repos/mashlol/notify/issues/comments/264653700,264653700,MDEyOklzc3VlQ29tbWVudDI2NDY1MzcwMA==,7643263,2016-12-03T17:41:36Z,2016-12-03T17:41:36Z,NONE,"Same here on Ubuntu (and Raspbian on a PI):

root@server:~# notify
/usr/bin/env: 'node\r': No such file or directory

I can see Windows symbols like `^M` at the end of each line, when I use `vim -b`. Is this an issue?

root@server:~# which env
/usr/bin/env

root@server:~# uname -a
Linux d6bc404348d5 4.4.0-51-generic #72-Ubuntu SMP Thu Nov 24 18:29:54 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux",NA,https://api.github.com/repos/mashlol/notify/issues/15/comments,https://github.com/mashlol/notify/issues/15#issuecomment-264653700,https://api.github.com/repos/mashlol/notify/issues/15
mashlol,notify,193028489,https://api.github.com/repos/mashlol/notify/issues/comments/264662256,264662256,MDEyOklzc3VlQ29tbWVudDI2NDY2MjI1Ng==,1039240,2016-12-03T20:05:36Z,2016-12-03T20:05:50Z,OWNER,Should be fixed in 0.1.3 now.  Just run `npm install notify-cli` again to get the latest version.,NA,https://api.github.com/repos/mashlol/notify/issues/15/comments,https://github.com/mashlol/notify/issues/15#issuecomment-264662256,https://api.github.com/repos/mashlol/notify/issues/15
mashlol,notify,188621620,https://api.github.com/repos/mashlol/notify/issues/comments/260153178,260153178,MDEyOklzc3VlQ29tbWVudDI2MDE1MzE3OA==,3390517,2016-11-12T22:36:02Z,2016-11-12T22:36:02Z,CONTRIBUTOR,"Hi Plehr,

Tried a quick and dirty solution #14 which worked out and notified me on both mobiles.
Applied the changes described in #14 and then appended lines into "".notifyreg"" as you mention and it worked. 
",NA,https://api.github.com/repos/mashlol/notify/issues/13/comments,https://github.com/mashlol/notify/issues/13#issuecomment-260153178,https://api.github.com/repos/mashlol/notify/issues/13
mashlol,notify,188621620,https://api.github.com/repos/mashlol/notify/issues/comments/260159326,260159326,MDEyOklzc3VlQ29tbWVudDI2MDE1OTMyNg==,1039240,2016-11-13T01:00:33Z,2016-11-13T01:00:33Z,OWNER,"@giorger  thanks for the quick solution!

@plehr I'm going to be updating with Firebase soon since Parse is shutting down, and in the rewrite I'll add better support for this.
",NA,https://api.github.com/repos/mashlol/notify/issues/13/comments,https://github.com/mashlol/notify/issues/13#issuecomment-260159326,https://api.github.com/repos/mashlol/notify/issues/13
mashlol,notify,188621620,https://api.github.com/repos/mashlol/notify/issues/comments/260185086,260185086,MDEyOklzc3VlQ29tbWVudDI2MDE4NTA4Ng==,9111687,2016-11-13T13:05:50Z,2016-11-13T13:05:50Z,NONE,"@giorger @mashlol Thank you for your efforts
",NA,https://api.github.com/repos/mashlol/notify/issues/13/comments,https://github.com/mashlol/notify/issues/13#issuecomment-260185086,https://api.github.com/repos/mashlol/notify/issues/13
mashlol,notify,188621620,https://api.github.com/repos/mashlol/notify/issues/comments/262440308,262440308,MDEyOklzc3VlQ29tbWVudDI2MjQ0MDMwOA==,1039240,2016-11-23T06:05:05Z,2016-11-23T06:12:23Z,OWNER,"Pushed the Firebase update (and also added official support for this feature) to npm, waiting on the Android app to be uploaded.  It should be available [here](https://play.google.com/store/apps/details?id=com.kevinbedi.notify) when it is approved.  Don't upgrade before installing the new app or you won't be able to send any pushes at all.  You could also manually download the APK from [here](https://github.com/mashlol/notify/releases).",NA,https://api.github.com/repos/mashlol/notify/issues/13/comments,https://github.com/mashlol/notify/issues/13#issuecomment-262440308,https://api.github.com/repos/mashlol/notify/issues/13
mashlol,notify,139141074,https://api.github.com/repos/mashlol/notify/issues/comments/193553912,193553912,MDEyOklzc3VlQ29tbWVudDE5MzU1MzkxMg==,1039240,2016-03-08T02:02:26Z,2016-03-08T02:02:26Z,OWNER,"I'll have to investigate other hosting options, since I'd rather not pay for any servers for this.  I don't have any immediate plans though.
",NA,https://api.github.com/repos/mashlol/notify/issues/12/comments,https://github.com/mashlol/notify/issues/12#issuecomment-193553912,https://api.github.com/repos/mashlol/notify/issues/12
mashlol,notify,139141074,https://api.github.com/repos/mashlol/notify/issues/comments/262440340,262440340,MDEyOklzc3VlQ29tbWVudDI2MjQ0MDM0MA==,1039240,2016-11-23T06:05:29Z,2016-11-23T06:12:37Z,OWNER,"Added support for Firebase and pushed new version to npm, still waiting on the Android app to be approved.  It should be available [here](https://play.google.com/store/apps/details?id=com.kevinbedi.notify) when it is approved.  Don't upgrade before installing the new app or you won't be able to send any pushes at all.  You could also manually download the APK from [here](https://github.com/mashlol/notify/releases).",NA,https://api.github.com/repos/mashlol/notify/issues/12/comments,https://github.com/mashlol/notify/issues/12#issuecomment-262440340,https://api.github.com/repos/mashlol/notify/issues/12
mashlol,notify,139141074,https://api.github.com/repos/mashlol/notify/issues/comments/263097264,263097264,MDEyOklzc3VlQ29tbWVudDI2MzA5NzI2NA==,181772,2016-11-27T01:52:06Z,2016-11-27T01:52:06Z,NONE,"Nice, thanks for doing this!",NA,https://api.github.com/repos/mashlol/notify/issues/12/comments,https://github.com/mashlol/notify/issues/12#issuecomment-263097264,https://api.github.com/repos/mashlol/notify/issues/12
mashlol,notify,124377281,https://api.github.com/repos/mashlol/notify/issues/comments/168056946,168056946,MDEyOklzc3VlQ29tbWVudDE2ODA1Njk0Ng==,1039240,2015-12-30T19:05:01Z,2015-12-30T19:05:01Z,OWNER,"The messages are delivered using [Parse](https://www.parse.com/), which uses Google Play Services in their Android SDK, and also uses GCM to deliver messages.  I believe Parse does support their own fallback message delivery mechanism but I haven't tested it.
",NA,https://api.github.com/repos/mashlol/notify/issues/11/comments,https://github.com/mashlol/notify/issues/11#issuecomment-168056946,https://api.github.com/repos/mashlol/notify/issues/11
mashlol,notify,120508116,https://api.github.com/repos/mashlol/notify/issues/comments/162718735,162718735,MDEyOklzc3VlQ29tbWVudDE2MjcxODczNQ==,1039240,2015-12-08T00:53:29Z,2015-12-08T00:53:29Z,OWNER,"https://github.com/mashlol/notify/releases/tag/v0.0.1
",NA,https://api.github.com/repos/mashlol/notify/issues/9/comments,https://github.com/mashlol/notify/issues/9#issuecomment-162718735,https://api.github.com/repos/mashlol/notify/issues/9
mashlol,notify,120274271,https://api.github.com/repos/mashlol/notify/issues/comments/161856496,161856496,MDEyOklzc3VlQ29tbWVudDE2MTg1NjQ5Ng==,1039240,2015-12-04T02:43:17Z,2015-12-04T02:43:17Z,OWNER,"It uses parse, which in turn uses GCM.  Not sure why you wanting an APK is dependent on whether or not it uses GCM (don't want it going through Google's servers?), but you can build your own APK from the source code which is published at this repo, or download it from the play store.
",NA,https://api.github.com/repos/mashlol/notify/issues/8/comments,https://github.com/mashlol/notify/issues/8#issuecomment-161856496,https://api.github.com/repos/mashlol/notify/issues/8
mashlol,notify,120274271,https://api.github.com/repos/mashlol/notify/issues/comments/161856775,161856775,MDEyOklzc3VlQ29tbWVudDE2MTg1Njc3NQ==,3650670,2015-12-04T02:45:39Z,2015-12-04T02:45:39Z,NONE,"I don't have google services on my phone, if this didn't use them, it'd be easier to install if you provided an APK. Given that you are using GCM to push the notifications I can't use it anyway
",NA,https://api.github.com/repos/mashlol/notify/issues/8/comments,https://github.com/mashlol/notify/issues/8#issuecomment-161856775,https://api.github.com/repos/mashlol/notify/issues/8
mashlol,notify,120274271,https://api.github.com/repos/mashlol/notify/issues/comments/161857112,161857112,MDEyOklzc3VlQ29tbWVudDE2MTg1NzExMg==,1039240,2015-12-04T02:48:40Z,2015-12-04T02:48:40Z,OWNER,"Hmm, I believe it will work regardless David, Parse has some logic which should fallback to some other service if GCM is not usable (Parse works on iOS, Amazon devices, etc).  According to https://github.com/ParsePlatform/Parse-SDK-Android/issues/35, it will fall back to a special Parse notification service if GCM is not available for use.  It might be worth a try!
",NA,https://api.github.com/repos/mashlol/notify/issues/8/comments,https://github.com/mashlol/notify/issues/8#issuecomment-161857112,https://api.github.com/repos/mashlol/notify/issues/8
mashlol,notify,120274271,https://api.github.com/repos/mashlol/notify/issues/comments/161857192,161857192,MDEyOklzc3VlQ29tbWVudDE2MTg1NzE5Mg==,3650670,2015-12-04T02:49:33Z,2015-12-04T02:49:33Z,NONE,"Awesome, I thought you'd use it for the push notifications. I'll look into this, thanks
",NA,https://api.github.com/repos/mashlol/notify/issues/8/comments,https://github.com/mashlol/notify/issues/8#issuecomment-161857192,https://api.github.com/repos/mashlol/notify/issues/8
mashlol,notify,120233065,https://api.github.com/repos/mashlol/notify/issues/comments/161856591,161856591,MDEyOklzc3VlQ29tbWVudDE2MTg1NjU5MQ==,1039240,2015-12-04T02:43:58Z,2015-12-04T02:43:58Z,OWNER,"Thanks for the request! Could you clarify what the features might be of this desktop app?  Would it just be a giant text field where you could enter the notification message, and then a button to send it?  What's the use-case for that?
",NA,https://api.github.com/repos/mashlol/notify/issues/7/comments,https://github.com/mashlol/notify/issues/7#issuecomment-161856591,https://api.github.com/repos/mashlol/notify/issues/7
mashlol,notify,120233065,https://api.github.com/repos/mashlol/notify/issues/comments/161983169,161983169,MDEyOklzc3VlQ29tbWVudDE2MTk4MzE2OQ==,58234,2015-12-04T14:43:20Z,2015-12-04T14:43:20Z,NONE,"No, I was thinking more along the lines of receiving alerts on my desktop, not sending them 
",NA,https://api.github.com/repos/mashlol/notify/issues/7/comments,https://github.com/mashlol/notify/issues/7#issuecomment-161983169,https://api.github.com/repos/mashlol/notify/issues/7
mashlol,notify,120233065,https://api.github.com/repos/mashlol/notify/issues/comments/162158257,162158257,MDEyOklzc3VlQ29tbWVudDE2MjE1ODI1Nw==,1039240,2015-12-05T07:34:54Z,2015-12-05T07:34:54Z,OWNER,"Ah.  I don't think a desktop app would really be required for that, we could just use something like https://github.com/mikaelbr/node-notifier to push OS-level notifications at the same time as the cloud-based notifications go out.
",NA,https://api.github.com/repos/mashlol/notify/issues/7/comments,https://github.com/mashlol/notify/issues/7#issuecomment-162158257,https://api.github.com/repos/mashlol/notify/issues/7
mashlol,notify,120233065,https://api.github.com/repos/mashlol/notify/issues/comments/388563916,388563916,MDEyOklzc3VlQ29tbWVudDM4ODU2MzkxNg==,705213,2018-05-12T15:47:49Z,2018-05-12T15:47:49Z,NONE,I could be pretty interest in helping develop this Desktop version! ðŸ˜„ ,NA,https://api.github.com/repos/mashlol/notify/issues/7/comments,https://github.com/mashlol/notify/issues/7#issuecomment-388563916,https://api.github.com/repos/mashlol/notify/issues/7
mashlol,notify,120233065,https://api.github.com/repos/mashlol/notify/issues/comments/388579713,388579713,MDEyOklzc3VlQ29tbWVudDM4ODU3OTcxMw==,1039240,2018-05-12T20:07:32Z,2018-05-12T20:07:32Z,OWNER,"> I could be pretty interest in helping develop this Desktop version! 

Sure, happy to have your help. I was thinking of making a site that uses web push notifications and having that be the ""register"" part. Firebase can already push notifications to web. So then it would work on any platform (mac, linux, windows, etc)",NA,https://api.github.com/repos/mashlol/notify/issues/7/comments,https://github.com/mashlol/notify/issues/7#issuecomment-388579713,https://api.github.com/repos/mashlol/notify/issues/7
mashlol,notify,120233065,https://api.github.com/repos/mashlol/notify/issues/comments/430093053,430093053,MDEyOklzc3VlQ29tbWVudDQzMDA5MzA1Mw==,1309820,2018-10-16T04:01:23Z,2018-10-16T04:02:54Z,NONE,"I was looking for this also, which is how I ended up finding your app. Ideally the desktop app would receive notifications the same way the phone would, so that I can see the notification on my phone, or whichever computer I'm actually sitting at. In other words I'm looking for more than just local notification. Currently I use `ssh ... notify-send` which only works within my local network. 

I spent about an hour trying to connect to the firebase REST API with no luck :/",NA,https://api.github.com/repos/mashlol/notify/issues/7/comments,https://github.com/mashlol/notify/issues/7#issuecomment-430093053,https://api.github.com/repos/mashlol/notify/issues/7
mashlol,notify,120202018,https://api.github.com/repos/mashlol/notify/issues/comments/161764319,161764319,MDEyOklzc3VlQ29tbWVudDE2MTc2NDMxOQ==,1545807,2015-12-03T19:54:03Z,2015-12-03T19:54:03Z,NONE,"Reinstalled android app, works now.
",NA,https://api.github.com/repos/mashlol/notify/issues/6/comments,https://github.com/mashlol/notify/issues/6#issuecomment-161764319,https://api.github.com/repos/mashlol/notify/issues/6
mashlol,notify,120189960,https://api.github.com/repos/mashlol/notify/issues/comments/161672347,161672347,MDEyOklzc3VlQ29tbWVudDE2MTY3MjM0Nw==,1039240,2015-12-03T15:18:51Z,2015-12-03T15:18:51Z,OWNER,"Added -t or --text so you can specify a custom message:

`notify --text ""My custom message""`
",NA,https://api.github.com/repos/mashlol/notify/issues/5/comments,https://github.com/mashlol/notify/issues/5#issuecomment-161672347,https://api.github.com/repos/mashlol/notify/issues/5
mashlol,notify,120189960,https://api.github.com/repos/mashlol/notify/issues/comments/161680999,161680999,MDEyOklzc3VlQ29tbWVudDE2MTY4MDk5OQ==,975962,2015-12-03T15:42:40Z,2015-12-03T15:42:40Z,NONE,"Awesome, thanks!

On 3 December 2015 at 16:18, Kevin Bedi notifications@github.com wrote:

> Closed #5 https://github.com/mashlol/notify/issues/5.
> 
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/mashlol/notify/issues/5#event-481532715.
",NA,https://api.github.com/repos/mashlol/notify/issues/5/comments,https://github.com/mashlol/notify/issues/5#issuecomment-161680999,https://api.github.com/repos/mashlol/notify/issues/5
mashlol,notify,120189960,https://api.github.com/repos/mashlol/notify/issues/comments/161681930,161681930,MDEyOklzc3VlQ29tbWVudDE2MTY4MTkzMA==,975962,2015-12-03T15:45:30Z,2015-12-03T15:45:30Z,NONE,"Sorry, but did this get pushed somewhere because I just reinstalled and my `notify` command claims `-t` is not an option.
",NA,https://api.github.com/repos/mashlol/notify/issues/5/comments,https://github.com/mashlol/notify/issues/5#issuecomment-161681930,https://api.github.com/repos/mashlol/notify/issues/5
mashlol,notify,120189960,https://api.github.com/repos/mashlol/notify/issues/comments/161690367,161690367,MDEyOklzc3VlQ29tbWVudDE2MTY5MDM2Nw==,975962,2015-12-03T16:02:13Z,2015-12-03T16:02:13Z,NONE,"Working now, nevermind.
",NA,https://api.github.com/repos/mashlol/notify/issues/5/comments,https://github.com/mashlol/notify/issues/5#issuecomment-161690367,https://api.github.com/repos/mashlol/notify/issues/5
mashlol,notify,120116573,https://api.github.com/repos/mashlol/notify/issues/comments/161672505,161672505,MDEyOklzc3VlQ29tbWVudDE2MTY3MjUwNQ==,1039240,2015-12-03T15:19:25Z,2015-12-03T15:19:25Z,OWNER,"I don't see why a proxy would make a difference, as long as you still have internet access.  Does your proxy block access to Parse (this is the backend I am using)?
",NA,https://api.github.com/repos/mashlol/notify/issues/4/comments,https://github.com/mashlol/notify/issues/4#issuecomment-161672505,https://api.github.com/repos/mashlol/notify/issues/4
mashlol,notify,120116573,https://api.github.com/repos/mashlol/notify/issues/comments/161675582,161675582,MDEyOklzc3VlQ29tbWVudDE2MTY3NTU4Mg==,1039240,2015-12-03T15:27:21Z,2015-12-03T15:27:21Z,OWNER,"I guess a better question is whether or not your proxy blocks access to anything
",NA,https://api.github.com/repos/mashlol/notify/issues/4/comments,https://github.com/mashlol/notify/issues/4#issuecomment-161675582,https://api.github.com/repos/mashlol/notify/issues/4
mashlol,notify,120116573,https://api.github.com/repos/mashlol/notify/issues/comments/165780842,165780842,MDEyOklzc3VlQ29tbWVudDE2NTc4MDg0Mg==,535748,2015-12-18T13:40:51Z,2015-12-18T13:40:51Z,NONE,"Well AFAIK any software running from terminal should use the proxy variables set in the term and sometimes different names are used so. Many software that use terminal specify the name of the variable where the proxy must be stored.
",NA,https://api.github.com/repos/mashlol/notify/issues/4/comments,https://github.com/mashlol/notify/issues/4#issuecomment-165780842,https://api.github.com/repos/mashlol/notify/issues/4
mashlol,notify,120116573,https://api.github.com/repos/mashlol/notify/issues/comments/466995787,466995787,MDEyOklzc3VlQ29tbWVudDQ2Njk5NTc4Nw==,7062410,2019-02-25T12:37:21Z,2019-02-25T12:39:13Z,NONE,"Hi, I just install the notify and trying to use it by a proxy and I get the same behave mentioned by @bodypheo . 
The server used don't have direct access to the internet, only by proxy.
I'm using squid as proxy and monitoring it, no connection becomes from the server where I run the **notify**. 
I'm setting the **http_proxy** and **https_proxy** variables, which I used to install the notify with npm command successfully. 

I also tried set the npm proxy configuration : **npm config set proxy** and **https-proxy** , but the notify still not trying to use the proxy configuration.
Appear this setting works only for manage use. 

@mashlol , I would to ask to reopen this issue for proxy support implementation.
",NA,https://api.github.com/repos/mashlol/notify/issues/4/comments,https://github.com/mashlol/notify/issues/4#issuecomment-466995787,https://api.github.com/repos/mashlol/notify/issues/4
mashlol,notify,120101896,https://api.github.com/repos/mashlol/notify/issues/comments/161556975,161556975,MDEyOklzc3VlQ29tbWVudDE2MTU1Njk3NQ==,1545807,2015-12-03T09:01:28Z,2015-12-03T09:47:03Z,NONE,"[notify] Encountered an error: { code: 141, message: 'Error 112: Channel name must start with a letter: <key>' }

My key starts with a 1.

Getting the same error.
",NA,https://api.github.com/repos/mashlol/notify/issues/3/comments,https://github.com/mashlol/notify/issues/3#issuecomment-161556975,https://api.github.com/repos/mashlol/notify/issues/3
mashlol,notify,120101896,https://api.github.com/repos/mashlol/notify/issues/comments/161672599,161672599,MDEyOklzc3VlQ29tbWVudDE2MTY3MjU5OQ==,1039240,2015-12-03T15:19:43Z,2015-12-03T15:19:43Z,OWNER,"Thanks for reporting guys, this should be fixed now if you update to 0.0.3 using npm.
",NA,https://api.github.com/repos/mashlol/notify/issues/3/comments,https://github.com/mashlol/notify/issues/3#issuecomment-161672599,https://api.github.com/repos/mashlol/notify/issues/3
mashlol,notify,120101896,https://api.github.com/repos/mashlol/notify/issues/comments/161898687,161898687,MDEyOklzc3VlQ29tbWVudDE2MTg5ODY4Nw==,1087345,2015-12-04T07:33:20Z,2015-12-04T07:33:20Z,NONE,"I can confirm that this works
",NA,https://api.github.com/repos/mashlol/notify/issues/3/comments,https://github.com/mashlol/notify/issues/3#issuecomment-161898687,https://api.github.com/repos/mashlol/notify/issues/3
mashlol,notify,120092423,https://api.github.com/repos/mashlol/notify/issues/comments/161511984,161511984,MDEyOklzc3VlQ29tbWVudDE2MTUxMTk4NA==,1039240,2015-12-03T04:34:50Z,2015-12-03T04:34:50Z,OWNER,"These are the public keys
",NA,https://api.github.com/repos/mashlol/notify/issues/2/comments,https://github.com/mashlol/notify/issues/2#issuecomment-161511984,https://api.github.com/repos/mashlol/notify/issues/2
mashlol,notify,120078862,https://api.github.com/repos/mashlol/notify/issues/comments/161495060,161495060,MDEyOklzc3VlQ29tbWVudDE2MTQ5NTA2MA==,1039240,2015-12-03T02:13:57Z,2015-12-03T02:13:57Z,OWNER,"Ah whoops, we really want semi-colon then I believe.
",NA,https://api.github.com/repos/mashlol/notify/issues/1/comments,https://github.com/mashlol/notify/issues/1#issuecomment-161495060,https://api.github.com/repos/mashlol/notify/issues/1
mashlol,notify,120078862,https://api.github.com/repos/mashlol/notify/issues/comments/161505806,161505806,MDEyOklzc3VlQ29tbWVudDE2MTUwNTgwNg==,1039240,2015-12-03T03:32:57Z,2015-12-03T03:32:57Z,OWNER,"Fixed everywhere except the Android app, will get to that later.
",NA,https://api.github.com/repos/mashlol/notify/issues/1/comments,https://github.com/mashlol/notify/issues/1#issuecomment-161505806,https://api.github.com/repos/mashlol/notify/issues/1
mpark,variant,793645232,https://api.github.com/repos/mpark/variant/issues/comments/767228562,767228562,MDEyOklzc3VlQ29tbWVudDc2NzIyODU2Mg==,60368867,2021-01-26T01:48:45Z,2021-01-26T01:48:45Z,NONE,"I am also looking at this with @azukaitis.  For some reason icpc on MacOSX is incorrectly detecting `__has_builtin(__type_pack_element))` as true, and then its frontend can't actually handle it as of intel 19.1.3.  On Linux with an intel 19.0.4 that was built against GCC 7.4.0 where `__has_builtin` was not defined, this all works fine.

Replacing the check for type_pack_element to guard against intel is the minimal fix to get it running with intel 19 on Linux and Macs:

```cpp
#if defined(__has_builtin(__type_pack_element)) && !(defined(__ICC)) 
#define MPARK_TYPE_PACK_ELEMENT
#endif
```
Maybe it makes sense to just disable `__has_builtin` for `__ICC`, but that throws a warning.  I am not that familiar with how Intel depends on the underlying compiler used to build it, but I am guessing it is somehow forwarding along to the clang preprocessor and getting mistaken that it has `__type_pack_element` support.  

We will try to check with the Intel compiler team first to see if there is something we can do to fix this with compiler flags (it won't let me just undefine __has_builtin from the command line), but otherwise it would be of interest to add something like the above to support older versions of Intel on Apple, and potentially with versions of GCC that have __has_builtin defined.",NA,https://api.github.com/repos/mpark/variant/issues/77/comments,https://github.com/mpark/variant/issues/77#issuecomment-767228562,https://api.github.com/repos/mpark/variant/issues/77
mpark,variant,793645232,https://api.github.com/repos/mpark/variant/issues/comments/767734510,767734510,MDEyOklzc3VlQ29tbWVudDc2NzczNDUxMA==,15693223,2021-01-26T18:20:28Z,2021-01-26T18:20:28Z,NONE,Confirmed what @sbolding-LANL found with the fix and it had nothing to do with __clang__.,NA,https://api.github.com/repos/mpark/variant/issues/77/comments,https://github.com/mpark/variant/issues/77#issuecomment-767734510,https://api.github.com/repos/mpark/variant/issues/77
mpark,variant,793645232,https://api.github.com/repos/mpark/variant/issues/comments/777659253,777659253,MDEyOklzc3VlQ29tbWVudDc3NzY1OTI1Mw==,60368867,2021-02-11T17:25:46Z,2021-02-11T17:25:46Z,NONE,"Heard back from Intel, and they are looking into it.  This issue still exists with their new OneAPI compilers on MacOS.",NA,https://api.github.com/repos/mpark/variant/issues/77/comments,https://github.com/mpark/variant/issues/77#issuecomment-777659253,https://api.github.com/repos/mpark/variant/issues/77
mpark,variant,731896891,https://api.github.com/repos/mpark/variant/issues/comments/732510661,732510661,MDEyOklzc3VlQ29tbWVudDczMjUxMDY2MQ==,1353258,2020-11-24T00:47:11Z,2020-11-24T00:47:11Z,CONTRIBUTOR,@mpark friendly ping :),NA,https://api.github.com/repos/mpark/variant/issues/76/comments,https://github.com/mpark/variant/pull/76#issuecomment-732510661,https://api.github.com/repos/mpark/variant/issues/76
mpark,variant,603650678,https://api.github.com/repos/mpark/variant/issues/comments/622239731,622239731,MDEyOklzc3VlQ29tbWVudDYyMjIzOTczMQ==,1923871,2020-05-01T04:16:05Z,2020-05-01T04:19:37Z,NONE,"It looks like the bool conversion logic in overload_leaf is disabled for gcc < 5.0 (I'm guessing because the narrowing conversion check doesn't work).

```
    template <typename Arg, std::size_t I, typename T>
    struct overload_leaf<
        Arg,
        I,
        T,
        true
#if defined(__clang__) || !defined(__GNUC__) || __GNUC__ >= 5
        ,
        lib::enable_if_t<
            std::is_same<lib::remove_cvref_t<T>, bool>::value
                ? std::is_same<lib::remove_cvref_t<Arg>, bool>::value
                : is_non_narrowing_convertible<Arg, T>::value>
#endif
        > {
      using impl = lib::size_constant<I> (*)(T);
      operator impl() const { return nullptr; };
    };
```
But could this be changed to 
```
template <typename Arg, std::size_t I, typename T>
struct overload_leaf<Arg,
                     I,
                     T,
                     true,
                     enable_if_t<std::is_same<remove_cvref_t<T>, bool>::value
                                     ? std::is_same<remove_cvref_t<Arg>, bool>::value
                                     :
#if defined(__clang__) || !defined(__GNUC__) || __GNUC__ >= 5
                                     is_non_narrowing_convertible<Arg, T>::value
#else
                                     std::is_convertible<Arg, T>::value
#endif
                                 >>
{
  using impl = size_constant<I> (*)(T);
  operator impl() const { return nullptr; };
};
```
that would be closer to the correct std::variant behavior",NA,https://api.github.com/repos/mpark/variant/issues/75/comments,https://github.com/mpark/variant/issues/75#issuecomment-622239731,https://api.github.com/repos/mpark/variant/issues/75
mpark,variant,572908400,https://api.github.com/repos/mpark/variant/issues/comments/592703334,592703334,MDEyOklzc3VlQ29tbWVudDU5MjcwMzMzNA==,671827,2020-02-28T20:00:38Z,2020-02-28T20:00:38Z,NONE,"I just learned about `valueless_by_exception`:

https://en.cppreference.com/w/cpp/utility/variant/valueless_by_exception

so I assume this logic is to implicitly deal with that case â€” if so, we can close this issue.",NA,https://api.github.com/repos/mpark/variant/issues/74/comments,https://github.com/mpark/variant/issues/74#issuecomment-592703334,https://api.github.com/repos/mpark/variant/issues/74
mpark,variant,572908400,https://api.github.com/repos/mpark/variant/issues/comments/592724303,592724303,MDEyOklzc3VlQ29tbWVudDU5MjcyNDMwMw==,671827,2020-02-28T20:56:24Z,2020-02-28T20:57:22Z,NONE,"If it is possible to detect that these two cases from the spec are `noexcept` for all the types in the `variant` (maybe using the `noexcept()` operator â€” this would only work for C++17, since the `noexcept` specification is not part of the function type until then), we could probably elide the `bad_variant_access` logic in this case.

- (guaranteed) an exception is thrown during the move initialization of the contained value from the temporary in copy assignment
- (guaranteed) an exception is thrown during the move initialization of the contained value during move assignment",NA,https://api.github.com/repos/mpark/variant/issues/74/comments,https://github.com/mpark/variant/issues/74#issuecomment-592724303,https://api.github.com/repos/mpark/variant/issues/74
mpark,variant,572908400,https://api.github.com/repos/mpark/variant/issues/comments/779996793,779996793,MDEyOklzc3VlQ29tbWVudDc3OTk5Njc5Mw==,507164,2021-02-16T17:30:11Z,2021-02-16T17:30:59Z,NONE,"I believe the problem lies in the fact that any type that has a conversion operator to one of the types the variant holds can throw an exception during the conversion, and since that's something that happens independently from the visitation, I can't see how `visit` would possibly detect that.

One such examples is given by [cppreference](https://en.cppreference.com/w/cpp/utility/variant/valueless_by_exception) itself:

```
struct S {
    operator int() { throw 42; }
};
std::variant<float, int> v{12.f}; // OK
v.emplace<1>(S()); // v may be valueless
```
",NA,https://api.github.com/repos/mpark/variant/issues/74/comments,https://github.com/mpark/variant/issues/74#issuecomment-779996793,https://api.github.com/repos/mpark/variant/issues/74
mpark,variant,572908400,https://api.github.com/repos/mpark/variant/issues/comments/780069408,780069408,MDEyOklzc3VlQ29tbWVudDc4MDA2OTQwOA==,671827,2021-02-16T19:34:51Z,2021-02-16T19:34:51Z,NONE,"Could we only do this optimization when exceptions are disabled?

On Tue, Feb 16, 2021, 10:30 Fabio <notifications@github.com> wrote:

> I believe the problem lies in the fact that any type that has a conversion
> operator to one of the types the variant holds can throw an exception
> during the conversion, and since that's something that happens
> independently from the variant, I can't see how the variant would possibly
> detect that.
>
> One such examples is given by cppreference
> <https://en.cppreference.com/w/cpp/utility/variant/valueless_by_exception>
> itself:
>
> struct S {
>     operator int() { throw 42; }
> };
> std::variant<float, int> v{12.f}; // OK
> v.emplace<1>(S()); // v may be valueless
>
> â€”
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/mpark/variant/issues/74#issuecomment-779996793>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAFEAU2YFUCSCZGKK5OPLV3S7KTTFANCNFSM4K5V4TEQ>
> .
>
",NA,https://api.github.com/repos/mpark/variant/issues/74/comments,https://github.com/mpark/variant/issues/74#issuecomment-780069408,https://api.github.com/repos/mpark/variant/issues/74
mpark,variant,572908400,https://api.github.com/repos/mpark/variant/issues/comments/780726433,780726433,MDEyOklzc3VlQ29tbWVudDc4MDcyNjQzMw==,507164,2021-02-17T17:38:22Z,2021-02-17T17:38:50Z,NONE,"> Could we only do this optimization when exceptions are disabled?

Even though they're disabled in the current compilation unit, they might not haven been disabled in the compilation unit that handed you the variant you want to visit, so I guess that's not a check that can be performed.

**However**, a recent Twitter discussion brought to my attention an optimization that has been performed in libstdc++: trivially copyable types aren't able to now put the variant in a valueless state. If mpark's `variant` did the same thing, then mpark's `visit` could effectively perform the optimization you suggested, but only for those variants that are made of only trivially copyable types.

Here's the discussion: https://twitter.com/BarryRevzin/status/1361826113543110657",NA,https://api.github.com/repos/mpark/variant/issues/74/comments,https://github.com/mpark/variant/issues/74#issuecomment-780726433,https://api.github.com/repos/mpark/variant/issues/74
mpark,variant,572908400,https://api.github.com/repos/mpark/variant/issues/comments/781607283,781607283,MDEyOklzc3VlQ29tbWVudDc4MTYwNzI4Mw==,671827,2021-02-18T20:13:52Z,2021-02-18T20:13:52Z,NONE,"I mean, if -fno-exceptions is set in the compilation unit which #includes
variant.hpp, it's not possible for variant.hpp to *throw* the
bad_variant_access() exception, right? But it still tries to do so:

https://godbolt.org/z/PfPdhn
",NA,https://api.github.com/repos/mpark/variant/issues/74/comments,https://github.com/mpark/variant/issues/74#issuecomment-781607283,https://api.github.com/repos/mpark/variant/issues/74
mpark,variant,572908400,https://api.github.com/repos/mpark/variant/issues/comments/781614271,781614271,MDEyOklzc3VlQ29tbWVudDc4MTYxNDI3MQ==,671827,2021-02-18T20:26:56Z,2021-02-18T20:26:56Z,NONE,"I guess what I'm saying is, if I know in my particular environment that
it's not possible for the variant to be valueless, I expect an exhaustive
std::visit to be equivalent to a switch on an enum (possibly with a
__builtin_unreached() or equivalent afterwards).

On Thu, Feb 18, 2021 at 1:13 PM Ben Hamilton <bgertzfield@gmail.com> wrote:

> I mean, if -fno-exceptions is set in the compilation unit which #includes
> variant.hpp, it's not possible for variant.hpp to *throw* the
> bad_variant_access() exception, right? But it still tries to do so:
>
> https://godbolt.org/z/PfPdhn
>
",NA,https://api.github.com/repos/mpark/variant/issues/74/comments,https://github.com/mpark/variant/issues/74#issuecomment-781614271,https://api.github.com/repos/mpark/variant/issues/74
mpark,variant,572908400,https://api.github.com/repos/mpark/variant/issues/comments/781639041,781639041,MDEyOklzc3VlQ29tbWVudDc4MTYzOTA0MQ==,671827,2021-02-18T21:14:45Z,2021-02-18T21:14:45Z,NONE,"Huh! I just noticed that somewhere between clang 8 and clang 9, the
std::variant implementation got a whole lot more optimized, and it somehow
elides all the logic for std::bad_variant_access, even if I don't pass
-fno-exceptions:

https://godbolt.org/z/8bT6Ed

So, I guess we can just use std::variant and be happy now. :)

On Thu, Feb 18, 2021 at 1:26 PM Ben Hamilton <bgertzfield@gmail.com> wrote:

> I guess what I'm saying is, if I know in my particular environment that
> it's not possible for the variant to be valueless, I expect an exhaustive
> std::visit to be equivalent to a switch on an enum (possibly with a
> __builtin_unreached() or equivalent afterwards).
>
> On Thu, Feb 18, 2021 at 1:13 PM Ben Hamilton <bgertzfield@gmail.com>
> wrote:
>
>> I mean, if -fno-exceptions is set in the compilation unit which #includes
>> variant.hpp, it's not possible for variant.hpp to *throw* the
>> bad_variant_access() exception, right? But it still tries to do so:
>>
>> https://godbolt.org/z/PfPdhn
>>
>
",NA,https://api.github.com/repos/mpark/variant/issues/74/comments,https://github.com/mpark/variant/issues/74#issuecomment-781639041,https://api.github.com/repos/mpark/variant/issues/74
mpark,variant,544258531,https://api.github.com/repos/mpark/variant/issues/comments/571354347,571354347,MDEyOklzc3VlQ29tbWVudDU3MTM1NDM0Nw==,2275240,2020-01-06T23:03:50Z,2020-01-06T23:03:50Z,OWNER,What the heck...? Not sure why/how this got closed...,NA,https://api.github.com/repos/mpark/variant/issues/73/comments,https://github.com/mpark/variant/pull/73#issuecomment-571354347,https://api.github.com/repos/mpark/variant/issues/73
mpark,variant,544258531,https://api.github.com/repos/mpark/variant/issues/comments/571354571,571354571,MDEyOklzc3VlQ29tbWVudDU3MTM1NDU3MQ==,2275240,2020-01-06T23:04:38Z,2020-01-06T23:04:38Z,OWNER,"Oh~ it's because it's trying to merge into the `dev` branch, and I've been messing with the `dev` branch! @gridley Could you target this for `master` please?",NA,https://api.github.com/repos/mpark/variant/issues/73/comments,https://github.com/mpark/variant/pull/73#issuecomment-571354571,https://api.github.com/repos/mpark/variant/issues/73
mpark,variant,544258531,https://api.github.com/repos/mpark/variant/issues/comments/571365963,571365963,MDEyOklzc3VlQ29tbWVudDU3MTM2NTk2Mw==,18088906,2020-01-06T23:48:50Z,2020-01-06T23:50:21Z,CONTRIBUTOR,"Whoops, on it in a bit! My apologies!",NA,https://api.github.com/repos/mpark/variant/issues/73/comments,https://github.com/mpark/variant/pull/73#issuecomment-571365963,https://api.github.com/repos/mpark/variant/issues/73
mpark,variant,544258531,https://api.github.com/repos/mpark/variant/issues/comments/571402602,571402602,MDEyOklzc3VlQ29tbWVudDU3MTQwMjYwMg==,18088906,2020-01-07T02:20:23Z,2020-01-07T02:20:23Z,CONTRIBUTOR,"@mpark OK, it's master now.",NA,https://api.github.com/repos/mpark/variant/issues/73/comments,https://github.com/mpark/variant/pull/73#issuecomment-571402602,https://api.github.com/repos/mpark/variant/issues/73
mpark,variant,544258531,https://api.github.com/repos/mpark/variant/issues/comments/571413496,571413496,MDEyOklzc3VlQ29tbWVudDU3MTQxMzQ5Ng==,2275240,2020-01-07T03:09:42Z,2020-01-07T03:09:42Z,OWNER,@gridley the changes seem much bigger than it was before. seems like there may have been formatting applied or something?,NA,https://api.github.com/repos/mpark/variant/issues/73/comments,https://github.com/mpark/variant/pull/73#issuecomment-571413496,https://api.github.com/repos/mpark/variant/issues/73
mpark,variant,544258531,https://api.github.com/repos/mpark/variant/issues/comments/571419895,571419895,MDEyOklzc3VlQ29tbWVudDU3MTQxOTg5NQ==,18088906,2020-01-07T03:41:32Z,2020-01-07T03:41:32Z,CONTRIBUTOR,"OK, so, *maybe* now this is ready once tests run.",NA,https://api.github.com/repos/mpark/variant/issues/73/comments,https://github.com/mpark/variant/pull/73#issuecomment-571419895,https://api.github.com/repos/mpark/variant/issues/73
mpark,variant,544258531,https://api.github.com/repos/mpark/variant/issues/comments/571891953,571891953,MDEyOklzc3VlQ29tbWVudDU3MTg5MTk1Mw==,2275240,2020-01-08T05:00:54Z,2020-01-08T05:00:54Z,OWNER,"Okay, I've fixed `master` and re-run the tests for this PR. @ax3l Could you confirm whether this works for you please?",NA,https://api.github.com/repos/mpark/variant/issues/73/comments,https://github.com/mpark/variant/pull/73#issuecomment-571891953,https://api.github.com/repos/mpark/variant/issues/73
mpark,variant,544258531,https://api.github.com/repos/mpark/variant/issues/comments/572259676,572259676,MDEyOklzc3VlQ29tbWVudDU3MjI1OTY3Ng==,1353258,2020-01-08T21:14:19Z,2020-01-08T21:27:43Z,CONTRIBUTOR,"I am trying to confirm this fixes my issue as well, but have troubles since the HPC machine (x86) this was found with changed (broke) its software environment recently and I cannot reproduce the original issue on other machines yet (tried on power). Trying to setup another x86 machine/environment today...",NA,https://api.github.com/repos/mpark/variant/issues/73/comments,https://github.com/mpark/variant/pull/73#issuecomment-572259676,https://api.github.com/repos/mpark/variant/issues/73
mpark,variant,544258531,https://api.github.com/repos/mpark/variant/issues/comments/572292517,572292517,MDEyOklzc3VlQ29tbWVudDU3MjI5MjUxNw==,1353258,2020-01-08T22:39:58Z,2020-01-08T22:39:58Z,CONTRIBUTOR,"Ok, I was now able to spin up another env that can reproduce the problem I reported. Sorry for the delay that this caused.
I can confirm the proposed fix in this PR works for me as well and we can close #71 in favour of it. Thanks a lot!

As a side note: I reported the NVCC compiler frontend issue in #70 to Nvidia too and it will be fixed in the next CUDA release as well (latest today: NVCC 10.2.89).",NA,https://api.github.com/repos/mpark/variant/issues/73/comments,https://github.com/mpark/variant/pull/73#issuecomment-572292517,https://api.github.com/repos/mpark/variant/issues/73
mpark,variant,528439852,https://api.github.com/repos/mpark/variant/issues/comments/558726876,558726876,MDEyOklzc3VlQ29tbWVudDU1ODcyNjg3Ng==,1353258,2019-11-26T17:06:54Z,2019-11-26T17:06:54Z,CONTRIBUTOR,@mpark CI seams to fail in `master` for unrelated reasons,NA,https://api.github.com/repos/mpark/variant/issues/71/comments,https://github.com/mpark/variant/pull/71#issuecomment-558726876,https://api.github.com/repos/mpark/variant/issues/71
mpark,variant,528439852,https://api.github.com/repos/mpark/variant/issues/comments/569981771,569981771,MDEyOklzc3VlQ29tbWVudDU2OTk4MTc3MQ==,18088906,2019-12-31T19:52:52Z,2019-12-31T19:52:52Z,CONTRIBUTOR,"So, I think that #73 may fix this. You should take a look. Simplifying all-not to not-any seems to do the trick.",NA,https://api.github.com/repos/mpark/variant/issues/71/comments,https://github.com/mpark/variant/pull/71#issuecomment-569981771,https://api.github.com/repos/mpark/variant/issues/71
mpark,variant,528439852,https://api.github.com/repos/mpark/variant/issues/comments/571046213,571046213,MDEyOklzc3VlQ29tbWVudDU3MTA0NjIxMw==,2275240,2020-01-06T08:12:34Z,2020-01-06T08:12:34Z,OWNER,"@ax3l sorry for the delay, been busy for a while. do you know if #73 solves the problem?",NA,https://api.github.com/repos/mpark/variant/issues/71/comments,https://github.com/mpark/variant/pull/71#issuecomment-571046213,https://api.github.com/repos/mpark/variant/issues/71
mpark,variant,528439852,https://api.github.com/repos/mpark/variant/issues/comments/571233219,571233219,MDEyOklzc3VlQ29tbWVudDU3MTIzMzIxOQ==,1353258,2020-01-06T17:35:13Z,2020-01-06T17:35:13Z,CONTRIBUTOR,"Thanks for the hint, I'll try to test this these days again.
Is #73 already ready to be tested? CI does not pass on it.",NA,https://api.github.com/repos/mpark/variant/issues/71/comments,https://github.com/mpark/variant/pull/71#issuecomment-571233219,https://api.github.com/repos/mpark/variant/issues/71
mpark,variant,528439852,https://api.github.com/repos/mpark/variant/issues/comments/571236602,571236602,MDEyOklzc3VlQ29tbWVudDU3MTIzNjYwMg==,18088906,2020-01-06T17:44:13Z,2020-01-06T17:44:13Z,CONTRIBUTOR,I was unable to figure out why CI would fail on #73 myself. I didnâ€™t try to fix it since it had been mentioned CI was failing in master anyways.,NA,https://api.github.com/repos/mpark/variant/issues/71/comments,https://github.com/mpark/variant/pull/71#issuecomment-571236602,https://api.github.com/repos/mpark/variant/issues/71
mpark,variant,528439852,https://api.github.com/repos/mpark/variant/issues/comments/571302968,571302968,MDEyOklzc3VlQ29tbWVudDU3MTMwMjk2OA==,2275240,2020-01-06T20:34:15Z,2020-01-06T20:34:21Z,OWNER,"The configs for which the CI is failing is due to libc++ tests failures, from not having implemented https://wg21.link/P0608 yet. I'm working on fixing it.",NA,https://api.github.com/repos/mpark/variant/issues/71/comments,https://github.com/mpark/variant/pull/71#issuecomment-571302968,https://api.github.com/repos/mpark/variant/issues/71
mpark,variant,528439852,https://api.github.com/repos/mpark/variant/issues/comments/572292434,572292434,MDEyOklzc3VlQ29tbWVudDU3MjI5MjQzNA==,1353258,2020-01-08T22:39:46Z,2020-01-08T22:39:46Z,CONTRIBUTOR,"Fixed by #73 as well, closing in favour of it :)
Thanks a lot!",NA,https://api.github.com/repos/mpark/variant/issues/71/comments,https://github.com/mpark/variant/pull/71#issuecomment-572292434,https://api.github.com/repos/mpark/variant/issues/71
mpark,variant,528403847,https://api.github.com/repos/mpark/variant/issues/comments/558394760,558394760,MDEyOklzc3VlQ29tbWVudDU1ODM5NDc2MA==,1353258,2019-11-25T23:54:09Z,2020-01-08T22:02:58Z,CONTRIBUTOR,"Cross-link Nvidia Bug report: 2767419
Update: Confirmed by Nvidia to be fixed for the next CUDA release (latest CUDA release today: 10.2.89).",NA,https://api.github.com/repos/mpark/variant/issues/70/comments,https://github.com/mpark/variant/issues/70#issuecomment-558394760,https://api.github.com/repos/mpark/variant/issues/70
mpark,variant,528403847,https://api.github.com/repos/mpark/variant/issues/comments/558408533,558408533,MDEyOklzc3VlQ29tbWVudDU1ODQwODUzMw==,1353258,2019-11-26T00:51:12Z,2019-11-26T01:50:07Z,CONTRIBUTOR,"The problem is likely in `detail::all` which uses a `constexpr` `for`/`if`...
Passing `--expt-relaxed-constexpr` to `nvcc` does not solve the problem either.

It compiles with `clang++` and `g++` with `-std=c++14` features ...",NA,https://api.github.com/repos/mpark/variant/issues/70/comments,https://github.com/mpark/variant/issues/70#issuecomment-558408533,https://api.github.com/repos/mpark/variant/issues/70
mpark,variant,528403847,https://api.github.com/repos/mpark/variant/issues/comments/558437095,558437095,MDEyOklzc3VlQ29tbWVudDU1ODQzNzA5NQ==,1353258,2019-11-26T03:00:04Z,2019-11-26T09:40:54Z,CONTRIBUTOR,"Another minimal reproducer developed by @maxpkatz :
```C++
#include <initializer_list>namespace detail {    
    bool f(std::initializer_list<bool> y) { return true; }
}  // namespace detail
template <typename... X>
bool g(X &&... x) {
    // breaks:
    return detail::f({!x.valueless_by_exception()...});
    // works only for N>1 parameter packs:
    // return detail::f(std::initializer_list<bool>(!x.valueless_by_exception()...));
    // fails as well:
    // return detail::f(std::initializer_list<bool>{!x.valueless_by_exception()...});
}

int main() {
    return 0;
}
```",NA,https://api.github.com/repos/mpark/variant/issues/70/comments,https://github.com/mpark/variant/issues/70#issuecomment-558437095,https://api.github.com/repos/mpark/variant/issues/70
mpark,variant,528403847,https://api.github.com/repos/mpark/variant/issues/comments/572331652,572331652,MDEyOklzc3VlQ29tbWVudDU3MjMzMTY1Mg==,2275240,2020-01-09T01:02:21Z,2020-01-09T01:02:21Z,OWNER,Fixed in 3c7fc8266bb46046b42c2dc2663f9f505f0cec28,NA,https://api.github.com/repos/mpark/variant/issues/70/comments,https://github.com/mpark/variant/issues/70#issuecomment-572331652,https://api.github.com/repos/mpark/variant/issues/70
mpark,variant,484984686,https://api.github.com/repos/mpark/variant/issues/comments/524703138,524703138,MDEyOklzc3VlQ29tbWVudDUyNDcwMzEzOA==,2275240,2019-08-26T03:37:00Z,2019-08-26T03:37:00Z,OWNER,I think the only thing we could do probably is to `#pragma` disable the warning.,NA,https://api.github.com/repos/mpark/variant/issues/68/comments,https://github.com/mpark/variant/issues/68#issuecomment-524703138,https://api.github.com/repos/mpark/variant/issues/68
mpark,variant,484984686,https://api.github.com/repos/mpark/variant/issues/comments/524808197,524808197,MDEyOklzc3VlQ29tbWVudDUyNDgwODE5Nw==,568063,2019-08-26T10:25:35Z,2019-08-26T10:25:35Z,NONE,"> I think the only thing we could do probably is to `#pragma` disable the warning.

@mpark: yes, I agree.",NA,https://api.github.com/repos/mpark/variant/issues/68/comments,https://github.com/mpark/variant/issues/68#issuecomment-524808197,https://api.github.com/repos/mpark/variant/issues/68
mpark,variant,477652271,https://api.github.com/repos/mpark/variant/issues/comments/518898532,518898532,MDEyOklzc3VlQ29tbWVudDUxODg5ODUzMg==,1353258,2019-08-07T01:06:11Z,2019-08-07T01:06:11Z,CONTRIBUTOR,"@mpark hm, there seam to be some CI issues in `master`.",NA,https://api.github.com/repos/mpark/variant/issues/67/comments,https://github.com/mpark/variant/pull/67#issuecomment-518898532,https://api.github.com/repos/mpark/variant/issues/67
mpark,variant,477652271,https://api.github.com/repos/mpark/variant/issues/comments/518938086,518938086,MDEyOklzc3VlQ29tbWVudDUxODkzODA4Ng==,2275240,2019-08-07T04:47:19Z,2019-08-07T04:47:19Z,OWNER,"Yeah, looks like failing the libc++ tests for variant conversion changes in C++20.",NA,https://api.github.com/repos/mpark/variant/issues/67/comments,https://github.com/mpark/variant/pull/67#issuecomment-518938086,https://api.github.com/repos/mpark/variant/issues/67
mpark,variant,477605793,https://api.github.com/repos/mpark/variant/issues/comments/518863821,518863821,MDEyOklzc3VlQ29tbWVudDUxODg2MzgyMQ==,1353258,2019-08-06T22:14:00Z,2019-08-06T22:14:00Z,CONTRIBUTOR,"@mpark CI seams to be a bit flaky right now. Can you please restart the travis CI runs that ""failed"" (curl timeout) and errored?",NA,https://api.github.com/repos/mpark/variant/issues/66/comments,https://github.com/mpark/variant/pull/66#issuecomment-518863821,https://api.github.com/repos/mpark/variant/issues/66
mpark,variant,477605793,https://api.github.com/repos/mpark/variant/issues/comments/518864076,518864076,MDEyOklzc3VlQ29tbWVudDUxODg2NDA3Ng==,2275240,2019-08-06T22:14:55Z,2019-08-06T22:14:55Z,OWNER,Restarted GCC 5 and 6 builds.,NA,https://api.github.com/repos/mpark/variant/issues/66/comments,https://github.com/mpark/variant/pull/66#issuecomment-518864076,https://api.github.com/repos/mpark/variant/issues/66
mpark,variant,477605793,https://api.github.com/repos/mpark/variant/issues/comments/518871418,518871418,MDEyOklzc3VlQ29tbWVudDUxODg3MTQxOA==,2275240,2019-08-06T22:46:08Z,2019-08-06T22:46:08Z,OWNER,@ax3l: This looks good to me. Do you mind parenthesizing around the `&&` please? I'd rather not depend on the operator precedence here.,NA,https://api.github.com/repos/mpark/variant/issues/66/comments,https://github.com/mpark/variant/pull/66#issuecomment-518871418,https://api.github.com/repos/mpark/variant/issues/66
mpark,variant,477605793,https://api.github.com/repos/mpark/variant/issues/comments/518885571,518885571,MDEyOklzc3VlQ29tbWVudDUxODg4NTU3MQ==,1353258,2019-08-06T23:54:36Z,2019-08-06T23:54:36Z,CONTRIBUTOR,"@mpark Agreed, scares me as well at that position.",NA,https://api.github.com/repos/mpark/variant/issues/66/comments,https://github.com/mpark/variant/pull/66#issuecomment-518885571,https://api.github.com/repos/mpark/variant/issues/66
mpark,variant,458671040,https://api.github.com/repos/mpark/variant/issues/comments/554698337,554698337,MDEyOklzc3VlQ29tbWVudDU1NDY5ODMzNw==,20667153,2019-11-17T03:06:07Z,2019-11-17T03:06:07Z,NONE,"I am getting the same warning on GCC 9 that I don't fully understand the reasoning
```
/root/project/build/include/helics_cxx/helics/external/variant.hpp: In member function 'mpark::variant<Ts>& mpark::variant<Ts>::operator=(const mpark::variant<Ts>&) [with Ts = {double, long int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::complex<double>, std::vector<double, std::allocator<double> >, std::vector<std::complex<double>, std::allocator<std::complex<double> > >, helics::NamedPoint}]':
/root/project/build/include/helics_cxx/helics/external/variant.hpp:2181:9: warning: implicitly-declared 'mpark::detail::impl<double, long int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::complex<double>, std::vector<double, std::allocator<double> >, std::vector<std::complex<double>, std::allocator<std::complex<double> > >, helics::NamedPoint>& mpark::detail::impl<double, long int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::complex<double>, std::vector<double, std::allocator<double> >, std::vector<std::complex<double>, std::allocator<std::complex<double> > >, helics::NamedPoint>::operator=(const mpark::detail::impl<double, long int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::complex<double>, std::vector<double, std::allocator<double> >, std::vector<std::complex<double>, std::allocator<std::complex<double> > >, helics::NamedPoint>&)' is deprecated [-Wdeprecated-copy]
 2181 |   class variant {
      |         ^~~~~~~
/root/project/build/include/helics_cxx/helics/external/variant.hpp:2052:26: note: because 'mpark::detail::impl<double, long int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::complex<double>, std::vector<double, std::allocator<double> >, std::vector<std::complex<double>, std::allocator<std::complex<double> > >, helics::NamedPoint>' has user-provided 'mpark::detail::copy_assignment<mpark::detail::traits<Ts ...>, mpark::detail::Trait::Available>& mpark::detail::copy_assignment<mpark::detail::traits<Ts ...>, mpark::detail::Trait::Available>::operator=(const mpark::detail::copy_assignment<mpark::detail::traits<Ts ...>, mpark::detail::Trait::Available>&) [with Ts = {double, long int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::complex<double>, std::vector<double, std::allocator<double> >, std::vector<std::complex<double>, std::allocator<std::complex<double> > >, helics::NamedPoint}]'
 2052 |         copy_assignment &operator=(const copy_assignment &that) {
      |                          ^~~~~~~~
/root/project/build/include/helics_cxx/helics/external/variant.hpp:2042:5: note: in definition of macro 'MPARK_VARIANT_COPY_ASSIGNMENT'
 2042 |     definition                                                           \
      |     ^~~~~~~~~~
/root/project/src/helics/apps/Player.cpp: In member function 'virtual void helics::apps::Player::loadJsonFile(const string&)':
/root/project/src/helics/apps/Player.cpp:566:36: note: synthesized method 'mpark::variant<Ts>& mpark::variant<Ts>::operator=(const mpark::variant<Ts>&) [with Ts = {double, long int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::complex<double>, std::vector<double, std::allocator<double> >, std::vector<std::complex<double>, std::allocator<std::complex<double> > >, helics::NamedPoint}]' first required here
  566 |             points.back ().value = val;
```
points.back().value and val are both the same variant type.",NA,https://api.github.com/repos/mpark/variant/issues/65/comments,https://github.com/mpark/variant/issues/65#issuecomment-554698337,https://api.github.com/repos/mpark/variant/issues/65
mpark,variant,448489930,https://api.github.com/repos/mpark/variant/issues/comments/496733242,496733242,MDEyOklzc3VlQ29tbWVudDQ5NjczMzI0Mg==,2275240,2019-05-28T23:59:58Z,2019-05-28T23:59:58Z,OWNER,"@sieren: are you saying that their library defines a __macro__ named `F`...? ðŸ˜µ

I guess I'm fine with changing this... let's just call it `impl`, and I'll accept.",NA,https://api.github.com/repos/mpark/variant/issues/64/comments,https://github.com/mpark/variant/pull/64#issuecomment-496733242,https://api.github.com/repos/mpark/variant/issues/64
mpark,variant,448489930,https://api.github.com/repos/mpark/variant/issues/comments/496812687,496812687,MDEyOklzc3VlQ29tbWVudDQ5NjgxMjY4Nw==,5174440,2019-05-29T07:06:03Z,2019-05-29T07:06:03Z,CONTRIBUTOR,"Haha, yeah it's weird ðŸ§
Fixed up the commit :)",NA,https://api.github.com/repos/mpark/variant/issues/64/comments,https://github.com/mpark/variant/pull/64#issuecomment-496812687,https://api.github.com/repos/mpark/variant/issues/64
mpark,variant,425017271,https://api.github.com/repos/mpark/variant/issues/comments/476304091,476304091,MDEyOklzc3VlQ29tbWVudDQ3NjMwNDA5MQ==,1693,2019-03-25T17:38:15Z,2019-03-25T17:38:15Z,NONE,"FWIW, building same example using `boost::variant` works: 
```
#include <boost/variant.hpp>

struct Visit : public boost::static_visitor<std::string> {
  std::string operator()(int i) const {
    return ""int"";
  }
};

void test() {
  boost::variant<int> i(1);
  boost::apply_visitor(Visit{}, i);
}
```",NA,https://api.github.com/repos/mpark/variant/issues/63/comments,https://github.com/mpark/variant/issues/63#issuecomment-476304091,https://api.github.com/repos/mpark/variant/issues/63
mpark,variant,425017271,https://api.github.com/repos/mpark/variant/issues/comments/580236823,580236823,MDEyOklzc3VlQ29tbWVudDU4MDIzNjgyMw==,1289205,2020-01-30T12:47:29Z,2020-01-30T12:47:29Z,NONE,"Still fails with nvcc 10.2.89, but with a different error message:
```
variant.hpp: In function â€˜constexpr decltype(auto) mpark::visit(Visitor&&, Vs&& ...)â€™:
variant.hpp:2674:96: error: parameter packs not expanded with â€˜...â€™:
     return (detail::all({!vs.valueless_by_exception()...})
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                     ^                                                                                                      
variant.hpp:2674:96: note:         â€˜vsâ€™
```

As a workaround, you can replace the implementation of the `visit` function
```cpp
  template <typename Visitor, typename... Vs>
  inline constexpr decltype(auto) visit(Visitor &&visitor, Vs &&... vs) {
    return (detail::all({!vs.valueless_by_exception()...})
                ? (void)0
                : throw_bad_variant_access()),
           detail::visitation::variant::visit_value(
               lib::forward<Visitor>(visitor), lib::forward<Vs>(vs)...);
  }
```
with this:
```cpp
  template <typename Visitor, typename... Vs>
  inline constexpr decltype(auto) visit(Visitor &&visitor, Vs &&... vs) {
    return (detail::all(std::initializer_list<bool>({!vs.valueless_by_exception()...}))
                ? (void)0
                : throw_bad_variant_access()),
           detail::visitation::variant::visit_value(
               lib::forward<Visitor>(visitor), lib::forward<Vs>(vs)...);
  }
```

Note that nvcc has problems with the variadic brace-initializer. I've already reported the problem for nvcc 9.2.88 but it seems that the same bug reappeared...",NA,https://api.github.com/repos/mpark/variant/issues/63/comments,https://github.com/mpark/variant/issues/63#issuecomment-580236823,https://api.github.com/repos/mpark/variant/issues/63
mpark,variant,425017271,https://api.github.com/repos/mpark/variant/issues/comments/587982056,587982056,MDEyOklzc3VlQ29tbWVudDU4Nzk4MjA1Ng==,2100259,2020-02-19T01:03:11Z,2020-02-19T01:03:11Z,NONE,Does #73 fix this?,NA,https://api.github.com/repos/mpark/variant/issues/63/comments,https://github.com/mpark/variant/issues/63#issuecomment-587982056,https://api.github.com/repos/mpark/variant/issues/63
mpark,variant,425017271,https://api.github.com/repos/mpark/variant/issues/comments/589132173,589132173,MDEyOklzc3VlQ29tbWVudDU4OTEzMjE3Mw==,1289205,2020-02-20T15:34:16Z,2020-02-20T15:34:16Z,NONE,"Yes, it seems to work with https://github.com/mpark/variant/blob/d1cdfdd3f2ed80710ba4d671fe6bffaa3e28201a/master/variant.hpp",NA,https://api.github.com/repos/mpark/variant/issues/63/comments,https://github.com/mpark/variant/issues/63#issuecomment-589132173,https://api.github.com/repos/mpark/variant/issues/63
mpark,variant,425017271,https://api.github.com/repos/mpark/variant/issues/comments/654523969,654523969,MDEyOklzc3VlQ29tbWVudDY1NDUyMzk2OQ==,2100259,2020-07-07T00:07:00Z,2020-07-07T00:07:00Z,NONE,"I actually am getting errors of the following form:
```
error: calling a __host__ function(""mpark::detail::copy_assignment< ::mpark::detail::traits<....
```

I am currently able to work around this by adding `__host__ __device__` to `copy_assign` at lines 1420 and 1424 of variant.hpp

I imagine there are better solutions to this, anyone else run into this?",NA,https://api.github.com/repos/mpark/variant/issues/63/comments,https://github.com/mpark/variant/issues/63#issuecomment-654523969,https://api.github.com/repos/mpark/variant/issues/63
mpark,variant,425017271,https://api.github.com/repos/mpark/variant/issues/comments/654632185,654632185,MDEyOklzc3VlQ29tbWVudDY1NDYzMjE4NQ==,1289205,2020-07-07T06:32:42Z,2020-07-07T06:32:42Z,NONE,"@dholladay00 Which version of `variant.hpp` are you using? And please show the full error, there is not even a full signature of the function where it fails...",NA,https://api.github.com/repos/mpark/variant/issues/63/comments,https://github.com/mpark/variant/issues/63#issuecomment-654632185,https://api.github.com/repos/mpark/variant/issues/63
mpark,variant,425017271,https://api.github.com/repos/mpark/variant/issues/comments/654931013,654931013,MDEyOklzc3VlQ29tbWVudDY1NDkzMTAxMw==,2100259,2020-07-07T15:10:28Z,2020-07-07T15:10:28Z,NONE,"I am using the latest master of mpark variant with Cuda 10.1. 

Here is a more detailed version of the error(s):

```
error: calling a __host__ function(""mpark::detail::copy_assignment< ::mpark::detail::traits< ... > , ( ::mpark::detail::Trait)1> ::operator ="") from a __device__ function(""Kokkos::Impl::ParallelFor< ::Kokkos::Impl::ViewCopy< ::Kokkos::View< ... >  *,  ::Kokkos::LayoutLeft,  ::Kokkos::Device< ::Kokkos::Serial,  ::Kokkos::AnonymousSpace> ,  ::Kokkos::MemoryTraits<(unsigned int)0u>  > ,  ::Kokkos::LayoutRight,  ::Kokkos::Cuda, (int)1, long, (bool)0> ,  ::Kokkos::RangePolicy< ::Kokkos::Cuda,  ::Kokkos::IndexType<long>  > ,  ::Kokkos::Cuda> ::operator () const"") is not allowed
error: identifier ""mpark::detail::copy_assignment< ::mpark::detail::traits< ... > , ( ::mpark::detail::Trait)1> ::operator ="" is undefined in device code
```

along with some warnings along these lines:

```
/.../include/mpark/variant.hpp(1052): warning: calling a __host__ function(""mpark::detail::destructor< ::mpark::detail::traits< ... > , ( ::mpark::detail::Trait)1> ::~destructor"") from a __host__ __device__ function(""mpark::detail::destructor< ::mpark::detail::traits< ... > , ( ::mpark::detail::Trait)1> ::~destructor [subobject]"") is not allowed
```",NA,https://api.github.com/repos/mpark/variant/issues/63/comments,https://github.com/mpark/variant/issues/63#issuecomment-654931013,https://api.github.com/repos/mpark/variant/issues/63
mpark,variant,421959757,https://api.github.com/repos/mpark/variant/issues/comments/475393884,475393884,MDEyOklzc3VlQ29tbWVudDQ3NTM5Mzg4NA==,2275240,2019-03-21T20:40:53Z,2019-03-21T20:41:44Z,OWNER,"According to https://en.cppreference.com/w/cpp/compiler_support, it seems like `<filesystem>` is available starting GCC 8, and Clang 7. For GCC, you'll have to remove all of the GCC versions < 8 from [`.travis.yml`](https://github.com/GillianGrayson/variant/blob/master/.travis.yml#L26-L59), and remove < C++17 configs from GCC 8 from [`.travis.yml`]( https://github.com/GillianGrayson/variant/blob/master/.travis.yml#L62) as well.",NA,https://api.github.com/repos/mpark/variant/issues/62/comments,https://github.com/mpark/variant/issues/62#issuecomment-475393884,https://api.github.com/repos/mpark/variant/issues/62
mpark,variant,421959757,https://api.github.com/repos/mpark/variant/issues/comments/475398249,475398249,MDEyOklzc3VlQ29tbWVudDQ3NTM5ODI0OQ==,2275240,2019-03-21T20:53:47Z,2019-03-21T20:53:59Z,OWNER,"The Clang configs don't use libc++ today. It simply uses the default libstdc++. As such, you might need to add `g++-8` as a dependency in your Clang configs. I hope that helps!",NA,https://api.github.com/repos/mpark/variant/issues/62/comments,https://github.com/mpark/variant/issues/62#issuecomment-475398249,https://api.github.com/repos/mpark/variant/issues/62
mpark,variant,420595019,https://api.github.com/repos/mpark/variant/issues/comments/473104400,473104400,MDEyOklzc3VlQ29tbWVudDQ3MzEwNDQwMA==,2275240,2019-03-14T23:32:47Z,2019-03-14T23:32:47Z,OWNER,"My observation is that GCC for example does not optimize as well as Clang does for the recursive `if`-`else` chain. I had observed this via the chain of ternary operators approach in #56, but here's a concrete example with your code: https://godbolt.org/z/SsbjYZ",NA,https://api.github.com/repos/mpark/variant/issues/61/comments,https://github.com/mpark/variant/issues/61#issuecomment-473104400,https://api.github.com/repos/mpark/variant/issues/61
mpark,variant,419164841,https://api.github.com/repos/mpark/variant/issues/comments/471343788,471343788,MDEyOklzc3VlQ29tbWVudDQ3MTM0Mzc4OA==,2275240,2019-03-10T20:58:03Z,2019-03-10T20:58:03Z,OWNER,Oh no! That's embarrassing. Thanks for pointing this out ðŸ˜•,NA,https://api.github.com/repos/mpark/variant/issues/60/comments,https://github.com/mpark/variant/issues/60#issuecomment-471343788,https://api.github.com/repos/mpark/variant/issues/60
mpark,variant,419164841,https://api.github.com/repos/mpark/variant/issues/comments/512969794,512969794,MDEyOklzc3VlQ29tbWVudDUxMjk2OTc5NA==,1353258,2019-07-18T20:13:08Z,2019-07-18T20:13:17Z,CONTRIBUTOR,@mpark are you planning a 1.4.1 release (with bumped version)? :),NA,https://api.github.com/repos/mpark/variant/issues/60/comments,https://github.com/mpark/variant/issues/60#issuecomment-512969794,https://api.github.com/repos/mpark/variant/issues/60
mpark,variant,414002971,https://api.github.com/repos/mpark/variant/issues/comments/478891782,478891782,MDEyOklzc3VlQ29tbWVudDQ3ODg5MTc4Mg==,43199315,2019-04-02T08:13:10Z,2019-04-02T08:13:10Z,NONE,"> Because even the trivial move has observable side-effect (the address).

Could you elaborate on that? Obersvable to whom?",NA,https://api.github.com/repos/mpark/variant/issues/59/comments,https://github.com/mpark/variant/issues/59#issuecomment-478891782,https://api.github.com/repos/mpark/variant/issues/59
mpark,variant,414002971,https://api.github.com/repos/mpark/variant/issues/comments/479350341,479350341,MDEyOklzc3VlQ29tbWVudDQ3OTM1MDM0MQ==,204273,2019-04-03T05:54:47Z,2019-04-03T05:54:47Z,NONE,See https://wandbox.org/permlink/XHwzxHs2lzbi3J3k here (credit to arthur-odwyer for example). What's observable is the difference between the pointer during construction and anytime after the emplace.,NA,https://api.github.com/repos/mpark/variant/issues/59/comments,https://github.com/mpark/variant/issues/59#issuecomment-479350341,https://api.github.com/repos/mpark/variant/issues/59
mpark,variant,414002971,https://api.github.com/repos/mpark/variant/issues/comments/479385564,479385564,MDEyOklzc3VlQ29tbWVudDQ3OTM4NTU2NA==,43199315,2019-04-03T08:10:36Z,2019-04-03T08:10:36Z,NONE,"Isn't that an issue with the type having an incorrect copy/move constructor? If my type holds a reference to itself, then that reference should definetly be updated during copy or the special member functions should be deleted. 

I'm not a language laywer, so I don't know what is and what is not permissible according to the standard, but this is one of the cases, where it is imho much better to provide an optimization for the large majority of users instead of supporting some broken type.",NA,https://api.github.com/repos/mpark/variant/issues/59/comments,https://github.com/mpark/variant/issues/59#issuecomment-479385564,https://api.github.com/repos/mpark/variant/issues/59
mpark,variant,414002971,https://api.github.com/repos/mpark/variant/issues/comments/479768000,479768000,MDEyOklzc3VlQ29tbWVudDQ3OTc2ODAwMA==,204273,2019-04-04T06:30:14Z,2019-04-04T06:30:14Z,NONE,"I agree it is an obvious stupid type. I asked about that on cpplang.slack (Feb 24 on #general). The conversation (answer by Arthur):
>> Does that mean the libstdc++ optimization is non conforming? Above you wrote ""may be non conforming"". Why ""may""?
>
> Well, I think it is, but I don't quite remember and don't want to imply that LWG definitely thought it was. My recollection is ""yeah it is non conforming but such pathological types are stupid and if the vendor doesn't want to conform in this case I don't think anyone will lose sleep over it.""

So it depends on how strict one wants to be with conformance. I think my suggestion allows an optimization which is as good as the GCC one. And it is fully conforming as far as I can see and I also don't see a disadvantage. So I think it is the slightly better option. I agree thought that either would be a good option because I also agree it isn't really worth worrying too much about such a pathological type.",NA,https://api.github.com/repos/mpark/variant/issues/59/comments,https://github.com/mpark/variant/issues/59#issuecomment-479768000,https://api.github.com/repos/mpark/variant/issues/59
mpark,variant,407323961,https://api.github.com/repos/mpark/variant/issues/comments/461099223,461099223,MDEyOklzc3VlQ29tbWVudDQ2MTA5OTIyMw==,5955594,2019-02-06T16:55:43Z,2019-02-06T16:55:43Z,NONE,"Ha tbh I thought the lib was already doing this. As a user, I would like to see this as I claim that the majority of variants will have fewer than 255 elements.  

Regards, 
Saad
 Sent from my iPhone (excuse typos)

> On 6 Feb 2019, at 4:37 pm, Tristan Brindle <notifications@github.com> wrote:
> 
> Previously, the index type of the variant was hard-coded to unsigned int. This patch instead selects the index type as the smallest unsigned integer type which can accommodate the number of alternatives, plus one for the valueless_by_exception state.
> 
> For example, in the common case of fewer than 255 alternatives, the index type is selected to be unsigned char; between 256 and 65535 alternatives, unsigned short is used; otherwise (in theory at least) unsigned int is used.
> 
> This results in a smaller variant (and better cache usage etc) whenever max(sizeof(Ts)...) < sizeof(unsigned).
> 
> Before:
> 
> sizeof(variant<monostate, bool, unsigned char, char, signed char>) == 8
> 
> After:
> 
> sizeof(variant<monostate, bool, unsigned char, char, signed char>) == 2
> 
> You can view, comment on, or merge this pull request online at:
> 
>   https://github.com/mpark/variant/pull/58
> 
> Commit Summary
> 
> Select the smallest possible index type
> File Changes
> 
> M include/mpark/variant.hpp (21)
> Patch Links:
> 
> https://github.com/mpark/variant/pull/58.patch
> https://github.com/mpark/variant/pull/58.diff
> â€”
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub, or mute the thread.
",NA,https://api.github.com/repos/mpark/variant/issues/58/comments,https://github.com/mpark/variant/pull/58#issuecomment-461099223,https://api.github.com/repos/mpark/variant/issues/58
mpark,variant,407323961,https://api.github.com/repos/mpark/variant/issues/comments/467084203,467084203,MDEyOklzc3VlQ29tbWVudDQ2NzA4NDIwMw==,2275240,2019-02-25T16:43:35Z,2019-02-25T16:43:35Z,OWNER,Thanks for the patch @tcbrindle! I had helped review this change for libc++'s variant but forgot to implement it here ðŸ˜…,NA,https://api.github.com/repos/mpark/variant/issues/58/comments,https://github.com/mpark/variant/pull/58#issuecomment-467084203,https://api.github.com/repos/mpark/variant/issues/58
mpark,variant,407323961,https://api.github.com/repos/mpark/variant/issues/comments/498522671,498522671,MDEyOklzc3VlQ29tbWVudDQ5ODUyMjY3MQ==,20667153,2019-06-04T05:06:07Z,2019-06-04T05:06:07Z,NONE,"This change fails to compile on MSVC when some other windows headers are included.  The max symbol is unfortunately defined as a macro, this also interferes with std::max.  Short of globally defining -DNOMINMAX which can cause some other issues.  The solution is to wrap the max call here in parenthesis
```cpp
using index_t = typename std::conditional<
            sizeof...(Ts) < (std::numeric_limits<unsigned char>::max)(),
            unsigned char,
            typename std::conditional<
                sizeof...(Ts) < (std::numeric_limits<unsigned short>::max)(),
                unsigned short,
                unsigned int>::type
            >::type;
```

then this compiles fine on MSVC.  I would submit a PR but It is a somewhat annoying process to get permission to do that at my organization.  ",NA,https://api.github.com/repos/mpark/variant/issues/58/comments,https://github.com/mpark/variant/pull/58#issuecomment-498522671,https://api.github.com/repos/mpark/variant/issues/58
mpark,variant,407323961,https://api.github.com/repos/mpark/variant/issues/comments/498940399,498940399,MDEyOklzc3VlQ29tbWVudDQ5ODk0MDM5OQ==,2275240,2019-06-05T05:08:03Z,2019-06-05T05:08:03Z,OWNER,Thanks. Fixed in 2744d0ad,NA,https://api.github.com/repos/mpark/variant/issues/58/comments,https://github.com/mpark/variant/pull/58#issuecomment-498940399,https://api.github.com/repos/mpark/variant/issues/58
mpark,variant,397640041,https://api.github.com/repos/mpark/variant/issues/comments/453008601,453008601,MDEyOklzc3VlQ29tbWVudDQ1MzAwODYwMQ==,2275240,2019-01-10T08:20:21Z,2019-01-10T08:20:45Z,OWNER,"Hi @nightlark, thanks for the patch!

Do you have a minimal repro of this by any chance along with the error message?

The previous fix regarding `MPARK_CPP14_CONSTEXPR` was kind of a hack that I didn't really didn't dig too deeply into because the macro didn't have significant impact. At this point however, I've implemented a more efficient visitation mechanism that relies on that macro #56.

I think I'd like to dig a bit deeper into the issue here. The [latest `master` build](https://ci.appveyor.com/project/mpark/variant/builds/21488450/job/0lby6soe7wiuq5jj?fullLog=true) succeeded with VS 2017 15.9, so it seems like there's at least a missing test case...

cc @mgieseki",NA,https://api.github.com/repos/mpark/variant/issues/57/comments,https://github.com/mpark/variant/pull/57#issuecomment-453008601,https://api.github.com/repos/mpark/variant/issues/57
mpark,variant,397640041,https://api.github.com/repos/mpark/variant/issues/comments/453127564,453127564,MDEyOklzc3VlQ29tbWVudDQ1MzEyNzU2NA==,3620467,2019-01-10T15:06:57Z,2019-01-10T15:09:19Z,NONE,"I can confirm the compilation failure with the latest variant sources from master (using VS 2017, version 15.9.5). For example, the following code fails to compile with the messages appended below (sorry, I've only the German locale installed at the moment). 

```cpp
#include <mpark/variant.hpp>
#include <memory>
#include <vector>

struct S {
  S (int n) : var(n) {}
  mpark::variant<int, std::unique_ptr<int>> var;
};

int main () {
  std::vector<S> vec;
  vec.emplace_back(0);
}
```
Error output:
```
..\libs\variant\include\mpark/variant.hpp(931): error C2280: ""std::unique_ptr<int,std::default_delete<_Ty>>::unique_ptr(const std::unique_ptr<_Ty,std::default_delete<_Ty>> &)"" : Es wurde versucht, auf eine gelÃ¶schte Funktion zu verweisen
        with
        [
            _Ty=int
        ]
C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\memory(2337): note: Siehe Deklaration von ""std::unique_ptr<int,std::default_delete<_Ty>>::unique_ptr""
        with
        [
            _Ty=int
        ]
C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\memory(2337): note: ""std::unique_ptr<int,std::default_delete<_Ty>>::unique_ptr(const std::unique_ptr<_Ty,std::default_delete<_Ty>> &)"": Die Funktion wurde explizit gelÃ¶scht.
        with
        [
            _Ty=int
        ]
..\libs\variant\include\mpark/variant.hpp(1120): note: Siehe Verweis auf die Instanziierung der gerade kompilierten Funktions-Vorlage ""mpark::detail::alt<1,T>::alt<T>(mpark::in_place_t,T &&)"".
        with
        [
            T=std::unique_ptr<int,std::default_delete<int>>
        ]
..\libs\variant\include\mpark/variant.hpp(1120): note: Siehe Verweis auf die Instanziierung der gerade kompilierten Funktions-Vorlage ""mpark::detail::alt<1,T>::alt<T>(mpark::in_place_t,T &&)"".
        with
        [
            T=std::unique_ptr<int,std::default_delete<int>>
        ]
..\libs\variant\include\mpark/variant.hpp(1133): note: Siehe Verweis auf die Instanziierung der gerade kompilierten Funktions-Vorlage ""T &mpark::detail::constructor<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<_Ty>>>>::construct_alt<1,T,const T>(mpark::detail::alt<1,T> &,const T &&)"".
        with
        [
            T=std::unique_ptr<int,std::default_delete<int>>,
            _Ty=int
        ]
..\libs\variant\include\mpark/variant.hpp(1132): note: Siehe Verweis auf die Instanziierung der gerade kompilierten Funktions-Vorlage ""T &mpark::detail::constructor<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<_Ty>>>>::construct_alt<1,T,const T>(mpark::detail::alt<1,T> &,const T &&)"".
        with
        [
            T=std::unique_ptr<int,std::default_delete<int>>,
            _Ty=int
        ]
c:\users\martin\develop\cpp\dvisvgm\libs\variant\include\mpark\lib.hpp(230): note: Siehe Verweis auf die Instanziierung der gerade kompilierten Funktions-Vorlage ""void mpark::detail::constructor<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<_Ty>>>>::generic_construct::<lambda_a372bf6c60ba10b92805b4aebac9455e>::operator ()<mpark::detail::alt<1,T>,const mpark::detail::alt<1,T>>(mpark::detail::alt<1,T> &,const mpark::detail::alt<1,T> &&) const"".
        with
        [
            _Ty=int,
            T=std::unique_ptr<int,std::default_delete<int>>
        ]
..\libs\variant\include\mpark/variant.hpp(618): note: Siehe Verweis auf die Instanziierung der gerade kompilierten Funktions-Vorlage ""R mpark::detail::visitation::base::dispatcher<true,R>::dispatch_case<1,T,mpark::detail::base<mpark::detail::Trait::Available,int,std::unique_ptr<int,std::default_delete<_Ty>>>&,mpark::detail::base<mpark::detail::Trait::Available,int,std::unique_ptr<_Ty,std::default_delete<_Ty>>>>(F &&,mpark::detail::base<mpark::detail::Trait::Available,int,std::unique_ptr<_Ty,std::default_delete<_Ty>>> &,mpark::detail::base<mpark::detail::Trait::Available,int,std::unique_ptr<_Ty,std::default_delete<_Ty>>> &&)"".
        with
        [
            R=void,
            T=mpark::detail::constructor<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<int>>>>::generic_construct::<lambda_a372bf6c60ba10b92805b4aebac9455e>,
            _Ty=int,
            F=mpark::detail::constructor<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<int>>>>::generic_construct::<lambda_a372bf6c60ba10b92805b4aebac9455e>
        ]
..\libs\variant\include\mpark/variant.hpp(618): note: Siehe Verweis auf die Instanziierung der gerade kompilierten Funktions-Vorlage ""R mpark::detail::visitation::base::dispatcher<true,R>::dispatch_case<1,T,mpark::detail::base<mpark::detail::Trait::Available,int,std::unique_ptr<int,std::default_delete<_Ty>>>&,mpark::detail::base<mpark::detail::Trait::Available,int,std::unique_ptr<_Ty,std::default_delete<_Ty>>>>(F &&,mpark::detail::base<mpark::detail::Trait::Available,int,std::unique_ptr<_Ty,std::default_delete<_Ty>>> &,mpark::detail::base<mpark::detail::Trait::Available,int,std::unique_ptr<_Ty,std::default_delete<_Ty>>> &&)"".
        with
        [
            R=void,
            T=mpark::detail::constructor<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<int>>>>::generic_construct::<lambda_a372bf6c60ba10b92805b4aebac9455e>,
            _Ty=int,
            F=mpark::detail::constructor<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<int>>>>::generic_construct::<lambda_a372bf6c60ba10b92805b4aebac9455e>
        ]
..\libs\variant\include\mpark/variant.hpp(837): note: Siehe Verweis auf die Instanziierung der gerade kompilierten Funktions-Vorlage ""R mpark::detail::visitation::base::dispatcher<true,R>::dispatch_at<0,T,mpark::detail::base<mpark::detail::Trait::Available,int,std::unique_ptr<int,std::default_delete<_Ty>>>&,mpark::detail::base<mpark::detail::Trait::Available,int,std::unique_ptr<_Ty,std::default_delete<_Ty>>>>(size_t,F &&,V,mpark::detail::base<mpark::detail::Trait::Available,int,std::unique_ptr<_Ty,std::default_delete<_Ty>>> &&)"".
        with
        [
            R=void,
            T=mpark::detail::constructor<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<int>>>>::generic_construct::<lambda_a372bf6c60ba10b92805b4aebac9455e>,
            _Ty=int,
            F=mpark::detail::constructor<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<int>>>>::generic_construct::<lambda_a372bf6c60ba10b92805b4aebac9455e>,
            V=mpark::detail::base<mpark::detail::Trait::Available,int,std::unique_ptr<int,std::default_delete<int>>> &
        ]
..\libs\variant\include\mpark/variant.hpp(829): note: Siehe Verweis auf die Instanziierung der gerade kompilierten Funktions-Vorlage ""R mpark::detail::visitation::base::dispatcher<true,R>::dispatch_at<0,T,mpark::detail::base<mpark::detail::Trait::Available,int,std::unique_ptr<int,std::default_delete<_Ty>>>&,mpark::detail::base<mpark::detail::Trait::Available,int,std::unique_ptr<_Ty,std::default_delete<_Ty>>>>(size_t,F &&,V,mpark::detail::base<mpark::detail::Trait::Available,int,std::unique_ptr<_Ty,std::default_delete<_Ty>>> &&)"".
        with
        [
            R=void,
            T=mpark::detail::constructor<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<int>>>>::generic_construct::<lambda_a372bf6c60ba10b92805b4aebac9455e>,
            _Ty=int,
            F=mpark::detail::constructor<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<int>>>>::generic_construct::<lambda_a372bf6c60ba10b92805b4aebac9455e>,
            V=mpark::detail::base<mpark::detail::Trait::Available,int,std::unique_ptr<int,std::default_delete<int>>> &
        ]
..\libs\variant\include\mpark/variant.hpp(1140): note: Siehe Verweis auf die Instanziierung der gerade kompilierten Funktions-Vorlage ""R mpark::detail::visitation::alt::visit_alt_at<mpark::detail::constructor<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<_Ty>>>>::generic_construct::<lambda_a372bf6c60ba10b92805b4aebac9455e>,mpark::detail::constructor<mpark::detail::traits<int,std::unique_ptr<_Ty,std::default_delete<_Ty>>>>&,T>(size_t,Visitor &&,mpark::detail::constructor<mpark::detail::traits<int,std::unique_ptr<_Ty,std::default_delete<_Ty>>>> &,T &&)"".
        with
        [
            R=void,
            _Ty=int,
            T=mpark::detail::move_constructor<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<int>>>,mpark::detail::Trait::Available>,
            Visitor=mpark::detail::constructor<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<int>>>>::generic_construct::<lambda_a372bf6c60ba10b92805b4aebac9455e>
        ]
..\libs\variant\include\mpark/variant.hpp(1176): note: Siehe Verweis auf die Instanziierung der gerade kompilierten Funktions-Vorlage ""void mpark::detail::constructor<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<_Ty>>>>::generic_construct<mpark::detail::move_constructor<mpark::detail::traits<int,std::unique_ptr<_Ty,std::default_delete<_Ty>>>,mpark::detail::Trait::Available>>(mpark::detail::constructor<mpark::detail::traits<int,std::unique_ptr<_Ty,std::default_delete<_Ty>>>> &,Rhs &&)"".
        with
        [
            _Ty=int,
            Rhs=mpark::detail::move_constructor<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<int>>>,mpark::detail::Trait::Available>
        ]
..\libs\variant\include\mpark/variant.hpp(1170): note: Siehe Verweis auf die Instanziierung der gerade kompilierten Funktions-Vorlage ""void mpark::detail::constructor<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<_Ty>>>>::generic_construct<mpark::detail::move_constructor<mpark::detail::traits<int,std::unique_ptr<_Ty,std::default_delete<_Ty>>>,mpark::detail::Trait::Available>>(mpark::detail::constructor<mpark::detail::traits<int,std::unique_ptr<_Ty,std::default_delete<_Ty>>>> &,Rhs &&)"".
        with
        [
            _Ty=int,
            Rhs=mpark::detail::move_constructor<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<int>>>,mpark::detail::Trait::Available>
        ]
..\libs\variant\include\mpark/variant.hpp(1176): note: Bei der Kompilierung von Klasse Vorlage der mpark::detail::move_constructor<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<_Ty>>>,mpark::detail::Trait::Available>::move_constructor(mpark::detail::move_constructor<mpark::detail::traits<int,std::unique_ptr<_Ty,std::default_delete<_Ty>>>,mpark::detail::Trait::Available> &&) noexcept-Memberfunktion
        with
        [
            _Ty=int
        ]
varianttest.cpp(15): note: Siehe Verweis auf die Instanziierung der gerade kompilierten Funktions-Vorlage ""mpark::detail::move_constructor<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<_Ty>>>,mpark::detail::Trait::Available>::move_constructor(mpark::detail::move_constructor<mpark::detail::traits<int,std::unique_ptr<_Ty,std::default_delete<_Ty>>>,mpark::detail::Trait::Available> &&) noexcept"".
        with
        [
            _Ty=int
        ]
..\libs\variant\include\mpark/variant.hpp(1217): note: Siehe Verweis auf die Klasse Vorlage-Instanziierung ""mpark::detail::move_constructor<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<_Ty>>>,mpark::detail::Trait::Available>"", die kompiliert wird.
        with
        [
            _Ty=int
        ]
..\libs\variant\include\mpark/variant.hpp(1222): note: Siehe Verweis auf die Klasse Vorlage-Instanziierung ""mpark::detail::copy_constructor<Traits,mpark::detail::Trait::Unavailable>"", die kompiliert wird.
        with
        [
            Traits=mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<int>>>
        ]
..\libs\variant\include\mpark/variant.hpp(1336): note: Siehe Verweis auf die Klasse Vorlage-Instanziierung ""mpark::detail::assignment<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<_Ty>>>>"", die kompiliert wird.
        with
        [
            _Ty=int
        ]
..\libs\variant\include\mpark/variant.hpp(1377): note: Siehe Verweis auf die Klasse Vorlage-Instanziierung ""mpark::detail::move_assignment<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<_Ty>>>,mpark::detail::Trait::Available>"", die kompiliert wird.
        with
        [
            _Ty=int
        ]
..\libs\variant\include\mpark/variant.hpp(1382): note: Siehe Verweis auf die Klasse Vorlage-Instanziierung ""mpark::detail::copy_assignment<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<_Ty>>>,mpark::detail::Trait::Unavailable>"", die kompiliert wird.
        with
        [
            _Ty=int
        ]
..\libs\variant\include\mpark/variant.hpp(1683): note: Siehe Verweis auf die Klasse Vorlage-Instanziierung ""mpark::detail::impl<int,std::unique_ptr<int,std::default_delete<_Ty>>>"", die kompiliert wird.
        with
        [
            _Ty=int
        ]
varianttest.cpp(7): note: Siehe Verweis auf die Klasse Vorlage-Instanziierung ""mpark::variant<int,std::unique_ptr<int,std::default_delete<_Ty>>>"", die kompiliert wird.
        with
        [
            _Ty=int
        ]
```",NA,https://api.github.com/repos/mpark/variant/issues/57/comments,https://github.com/mpark/variant/pull/57#issuecomment-453127564,https://api.github.com/repos/mpark/variant/issues/57
mpark,variant,397640041,https://api.github.com/repos/mpark/variant/issues/comments/453230359,453230359,MDEyOklzc3VlQ29tbWVudDQ1MzIzMDM1OQ==,3969255,2019-01-10T19:50:56Z,2019-01-10T23:00:18Z,NONE,"For me, this is enough to reproduce it using the single-header variant (version 15.9.5):
```
#include ""variant.hpp""

int main() {
	mpark::variant<int, std::unique_ptr<int>> var;
}
```

With these errors:
```
variant-msvc2017-bug-repro\variant.hpp(947): error C2131: expression did not evaluate to a constant
variant-msvc2017-bug-repro\variant.hpp(947): note: failure was caused by non-constant arguments or reference to a non-constant symbol
variant-msvc2017-bug-repro\variant.hpp(947): note: see usage of '$S4'
variant-msvc2017-bug-repro\variant.hpp(1948): note: see reference to class template instantiation 'mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<_Ty>>>' being compiled
        with
        [
            _Ty=int
        ]
variant-msvc2017-bug-repro\variant.hpp(2252): note: see reference to class template instantiation 'mpark::detail::impl<int,std::unique_ptr<int,std::default_delete<_Ty>>>' being compiled
        with
        [
            _Ty=int
        ]
variant-msvc2017-bug-repro\test.cpp(4): note: see reference to class template instantiation 'mpark::variant<int,std::unique_ptr<int,std::default_delete<_Ty>>>' being compiled
        with
        [
            _Ty=int
        ]
variant-msvc2017-bug-repro\variant.hpp(975): error C2131: expression did not evaluate to a constant
c:\program files (x86)\microsoft visual studio\2017\community\vc\tools\msvc\14.16.27023\include\initializer_list(42): note: failure was caused by non-constant arguments or reference to a non-constant symbol
c:\program files (x86)\microsoft visual studio\2017\community\vc\tools\msvc\14.16.27023\include\initializer_list(42): note: see usage of '$S3'
variant-msvc2017-bug-repro\variant.hpp(980): error C2131: expression did not evaluate to a constant
c:\program files (x86)\microsoft visual studio\2017\community\vc\tools\msvc\14.16.27023\include\initializer_list(42): note: failure was caused by non-constant arguments or reference to a non-constant symbol
c:\program files (x86)\microsoft visual studio\2017\community\vc\tools\msvc\14.16.27023\include\initializer_list(42): note: see usage of '$S5'
variant-msvc2017-bug-repro\variant.hpp(985): error C2131: expression did not evaluate to a constant
variant-msvc2017-bug-repro\variant.hpp(985): note: failure was caused by a read of an uninitialized symbol
variant-msvc2017-bug-repro\variant.hpp(985): note: see usage of 'copy_constructible_trait'
variant-msvc2017-bug-repro\variant.hpp(991): error C2131: expression did not evaluate to a constant
variant-msvc2017-bug-repro\variant.hpp(991): note: failure was caused by a read of an uninitialized symbol
variant-msvc2017-bug-repro\variant.hpp(991): note: see usage of 'move_constructible_trait'
variant-msvc2017-bug-repro\variant.hpp(997): error C2131: expression did not evaluate to a constant
c:\program files (x86)\microsoft visual studio\2017\community\vc\tools\msvc\14.16.27023\include\initializer_list(42): note: failure was caused by non-constant arguments or reference to a non-constant symbol
c:\program files (x86)\microsoft visual studio\2017\community\vc\tools\msvc\14.16.27023\include\initializer_list(42): note: see usage of '$S7'
variant-msvc2017-bug-repro\variant.hpp(1948): error C2975: 'unnamed-parameter': invalid template argument for 'mpark::detail::copy_assignment', expected compile-time constant expression
variant-msvc2017-bug-repro\variant.hpp(1910): note: see declaration of 'unnamed-parameter'
variant-msvc2017-bug-repro\variant.hpp(1948): error C2440: 'specialization': cannot convert from 'int' to 'mpark::detail::Trait'
variant-msvc2017-bug-repro\variant.hpp(1948): note: Conversion to enumeration type requires an explicit cast (static_cast, C-style cast or function-style cast)
variant-msvc2017-bug-repro\variant.hpp(1932): error C2975: 'unnamed-parameter': invalid template argument for 'mpark::detail::move_assignment', expected compile-time constant expression
variant-msvc2017-bug-repro\variant.hpp(1870): note: see declaration of 'unnamed-parameter'
variant-msvc2017-bug-repro\variant.hpp(1948): note: see reference to class template instantiation 'mpark::detail::copy_assignment<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<_Ty>>>,0>' being compiled
        with
        [
            _Ty=int
        ]
variant-msvc2017-bug-repro\variant.hpp(1932): error C2440: 'specialization': cannot convert from 'int' to 'mpark::detail::Trait'
variant-msvc2017-bug-repro\variant.hpp(1932): note: Conversion to enumeration type requires an explicit cast (static_cast, C-style cast or function-style cast)
variant-msvc2017-bug-repro\variant.hpp(1785): error C2975: 'unnamed-parameter': invalid template argument for 'mpark::detail::copy_constructor', expected compile-time constant expression
variant-msvc2017-bug-repro\variant.hpp(1747): note: see declaration of 'unnamed-parameter'
variant-msvc2017-bug-repro\variant.hpp(1892): note: see reference to class template instantiation 'mpark::detail::assignment<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<_Ty>>>>' being compiled
        with
        [
            _Ty=int
        ]
variant-msvc2017-bug-repro\variant.hpp(1932): note: see reference to class template instantiation 'mpark::detail::move_assignment<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<_Ty>>>,0>' being compiled
        with
        [
            _Ty=int
        ]
variant-msvc2017-bug-repro\variant.hpp(1785): error C2440: 'specialization': cannot convert from 'int' to 'mpark::detail::Trait'
variant-msvc2017-bug-repro\variant.hpp(1785): note: Conversion to enumeration type requires an explicit cast (static_cast, C-style cast or function-style cast)
variant-msvc2017-bug-repro\variant.hpp(1769): error C2975: 'unnamed-parameter': invalid template argument for 'mpark::detail::move_constructor', expected compile-time constant expression
variant-msvc2017-bug-repro\variant.hpp(1709): note: see declaration of 'unnamed-parameter'
variant-msvc2017-bug-repro\variant.hpp(1785): note: see reference to class template instantiation 'mpark::detail::copy_constructor<Traits,0>' being compiled
        with
        [
            Traits=mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<int>>>
        ]
variant-msvc2017-bug-repro\variant.hpp(1769): error C2440: 'specialization': cannot convert from 'int' to 'mpark::detail::Trait'
variant-msvc2017-bug-repro\variant.hpp(1769): note: Conversion to enumeration type requires an explicit cast (static_cast, C-style cast or function-style cast)
variant-msvc2017-bug-repro\variant.hpp(1662): error C2975: 'unnamed-parameter': invalid template argument for 'mpark::detail::destructor', expected compile-time constant expression
variant-msvc2017-bug-repro\variant.hpp(1614): note: see declaration of 'unnamed-parameter'
variant-msvc2017-bug-repro\variant.hpp(1731): note: see reference to class template instantiation 'mpark::detail::constructor<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<_Ty>>>>' being compiled
        with
        [
            _Ty=int
        ]
variant-msvc2017-bug-repro\variant.hpp(1769): note: see reference to class template instantiation 'mpark::detail::move_constructor<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<_Ty>>>,0>' being compiled
        with
        [
            _Ty=int
        ]
variant-msvc2017-bug-repro\variant.hpp(1662): error C2440: 'specialization': cannot convert from 'int' to 'mpark::detail::Trait'
variant-msvc2017-bug-repro\variant.hpp(1662): note: Conversion to enumeration type requires an explicit cast (static_cast, C-style cast or function-style cast)
variant-msvc2017-bug-repro\variant.hpp(1542): warning C4624: 'mpark::detail::recursive_union<mpark::detail::Trait::TriviallyAvailable,1,std::unique_ptr<int,std::default_delete<_Ty>>>': destructor was implicitly defined as deleted
        with
        [
            _Ty=int
        ]
variant-msvc2017-bug-repro\variant.hpp(1542): note: see reference to class template instantiation 'mpark::detail::recursive_union<mpark::detail::Trait::TriviallyAvailable,1,std::unique_ptr<int,std::default_delete<_Ty>>>' being compiled
        with
        [
            _Ty=int
        ]
variant-msvc2017-bug-repro\variant.hpp(1586): note: see reference to class template instantiation 'mpark::detail::recursive_union<mpark::detail::Trait::TriviallyAvailable,0,int,std::unique_ptr<int,std::default_delete<_Ty>>>' being compiled
        with
        [
            _Ty=int
        ]
variant-msvc2017-bug-repro\variant.hpp(1642): note: see reference to class template instantiation 'mpark::detail::base<mpark::detail::Trait::TriviallyAvailable,int,std::unique_ptr<int,std::default_delete<_Ty>>>' being compiled
        with
        [
            _Ty=int
        ]
variant-msvc2017-bug-repro\variant.hpp(1662): note: see reference to class template instantiation 'mpark::detail::destructor<Traits,0>' being compiled
        with
        [
            Traits=mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<int>>>
        ]
variant-msvc2017-bug-repro\variant.hpp(1542): warning C4624: 'mpark::detail::recursive_union<mpark::detail::Trait::TriviallyAvailable,0,int,std::unique_ptr<int,std::default_delete<_Ty>>>': destructor was implicitly defined as deleted
        with
        [
            _Ty=int
        ]
variant-msvc2017-bug-repro\variant.hpp(1591): warning C4624: 'mpark::detail::base<mpark::detail::Trait::TriviallyAvailable,int,std::unique_ptr<int,std::default_delete<_Ty>>>': destructor was implicitly defined as deleted
        with
        [
            _Ty=int
        ]
variant-msvc2017-bug-repro\variant.hpp(1642): warning C4624: 'mpark::detail::destructor<Traits,0>': destructor was implicitly defined as deleted
        with
        [
            Traits=mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<int>>>
        ]
variant-msvc2017-bug-repro\variant.hpp(1663): error C2975: 'unnamed-parameter': invalid template argument for 'mpark::detail::destructor', expected compile-time constant expression
variant-msvc2017-bug-repro\variant.hpp(1614): note: see declaration of 'unnamed-parameter'
variant-msvc2017-bug-repro\variant.hpp(1663): error C2440: 'specialization': cannot convert from 'int' to 'mpark::detail::Trait'
variant-msvc2017-bug-repro\variant.hpp(1663): note: Conversion to enumeration type requires an explicit cast (static_cast, C-style cast or function-style cast)
variant-msvc2017-bug-repro\variant.hpp(1707): warning C4624: 'mpark::detail::constructor<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<_Ty>>>>': destructor was implicitly defined as deleted
        with
        [
            _Ty=int
        ]
variant-msvc2017-bug-repro\variant.hpp(1731): warning C4624: 'mpark::detail::move_constructor<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<_Ty>>>,0>': destructor was implicitly defined as deleted
        with
        [
            _Ty=int
        ]
variant-msvc2017-bug-repro\variant.hpp(1769): warning C4624: 'mpark::detail::copy_constructor<Traits,0>': destructor was implicitly defined as deleted
        with
        [
            Traits=mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<int>>>
        ]
variant-msvc2017-bug-repro\variant.hpp(1786): error C2975: 'unnamed-parameter': invalid template argument for 'mpark::detail::copy_constructor', expected compile-time constant expression
variant-msvc2017-bug-repro\variant.hpp(1747): note: see declaration of 'unnamed-parameter'
variant-msvc2017-bug-repro\variant.hpp(1786): error C2440: 'specialization': cannot convert from 'int' to 'mpark::detail::Trait'
variant-msvc2017-bug-repro\variant.hpp(1786): note: Conversion to enumeration type requires an explicit cast (static_cast, C-style cast or function-style cast)
variant-msvc2017-bug-repro\variant.hpp(1868): warning C4624: 'mpark::detail::assignment<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<_Ty>>>>': destructor was implicitly defined as deleted
        with
        [
            _Ty=int
        ]
variant-msvc2017-bug-repro\variant.hpp(1892): warning C4624: 'mpark::detail::move_assignment<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<_Ty>>>,0>': destructor was implicitly defined as deleted
        with
        [
            _Ty=int
        ]
variant-msvc2017-bug-repro\variant.hpp(1932): warning C4624: 'mpark::detail::copy_assignment<mpark::detail::traits<int,std::unique_ptr<int,std::default_delete<_Ty>>>,0>': destructor was implicitly defined as deleted
        with
        [
            _Ty=int
        ]
variant-msvc2017-bug-repro\variant.hpp(1949): error C2975: 'unnamed-parameter': invalid template argument for 'mpark::detail::copy_assignment', expected compile-time constant expression
variant-msvc2017-bug-repro\variant.hpp(1910): note: see declaration of 'unnamed-parameter'
variant-msvc2017-bug-repro\variant.hpp(1949): error C2440: 'specialization': cannot convert from 'int' to 'mpark::detail::Trait'
variant-msvc2017-bug-repro\variant.hpp(1949): note: Conversion to enumeration type requires an explicit cast (static_cast, C-style cast or function-style cast)
variant-msvc2017-bug-repro\variant.hpp(2024): warning C4624: 'mpark::detail::impl<int,std::unique_ptr<int,std::default_delete<_Ty>>>': destructor was implicitly defined as deleted
        with
        [
            _Ty=int
        ]
variant-msvc2017-bug-repro\variant.hpp(2256): warning C4624: 'mpark::variant<int,std::unique_ptr<int,std::default_delete<_Ty>>>': destructor was implicitly defined as deleted
        with
        [
            _Ty=int
        ]
```
",NA,https://api.github.com/repos/mpark/variant/issues/57/comments,https://github.com/mpark/variant/pull/57#issuecomment-453230359,https://api.github.com/repos/mpark/variant/issues/57
mpark,variant,397640041,https://api.github.com/repos/mpark/variant/issues/comments/453231461,453231461,MDEyOklzc3VlQ29tbWVudDQ1MzIzMTQ2MQ==,3969255,2019-01-10T19:54:17Z,2019-01-10T19:54:44Z,NONE,"I also tested with VS2019 preview 1.1 (version 16.0.0 preview 1.1, MSC_VER == 1920), and the same set of errors occur.",NA,https://api.github.com/repos/mpark/variant/issues/57/comments,https://github.com/mpark/variant/pull/57#issuecomment-453231461,https://api.github.com/repos/mpark/variant/issues/57
mpark,variant,397640041,https://api.github.com/repos/mpark/variant/issues/comments/453237606,453237606,MDEyOklzc3VlQ29tbWVudDQ1MzIzNzYwNg==,3620467,2019-01-10T20:14:37Z,2019-01-10T20:14:37Z,NONE,"> For me, this is enough to reproduce it using the single-header variant (version 15.9.5):
> 
> ```
> #include ""variant.hpp""
> 
> int main() {
> 	mpark::variant<int, std::unique_ptr<int>> var;
> }
> ```

Strange. For me, this example compiles without errors...",NA,https://api.github.com/repos/mpark/variant/issues/57/comments,https://github.com/mpark/variant/pull/57#issuecomment-453237606,https://api.github.com/repos/mpark/variant/issues/57
mpark,variant,397640041,https://api.github.com/repos/mpark/variant/issues/comments/453275108,453275108,MDEyOklzc3VlQ29tbWVudDQ1MzI3NTEwOA==,3969255,2019-01-10T22:13:51Z,2019-01-10T22:13:51Z,NONE,"It is odd -- I have a new laptop with a fresh install of Visual Studio 2017 and 2019 preview, no settings changed from the defaults.",NA,https://api.github.com/repos/mpark/variant/issues/57/comments,https://github.com/mpark/variant/pull/57#issuecomment-453275108,https://api.github.com/repos/mpark/variant/issues/57
mpark,variant,397640041,https://api.github.com/repos/mpark/variant/issues/comments/453284228,453284228,MDEyOklzc3VlQ29tbWVudDQ1MzI4NDIyOA==,3620467,2019-01-10T22:43:40Z,2019-01-10T22:43:40Z,NONE,"Does your program actually include the latest single-header file from the master branch or is it perhaps an older version? At first glance, the error messages and line numbers don't seem to match the statements in [variant.hpp](https://github.com/mpark/variant/blob/single-header/master/variant.hpp) at the corresponding positions.",NA,https://api.github.com/repos/mpark/variant/issues/57/comments,https://github.com/mpark/variant/pull/57#issuecomment-453284228,https://api.github.com/repos/mpark/variant/issues/57
mpark,variant,397640041,https://api.github.com/repos/mpark/variant/issues/comments/453290838,453290838,MDEyOklzc3VlQ29tbWVudDQ1MzI5MDgzOA==,3969255,2019-01-10T23:02:07Z,2019-01-10T23:04:32Z,NONE,"I'll update it with the log from the latest variant.hpp. I realized I might have been using an older version (of variant.hpp) before posting the log, so I updated variant.hpp and still got the same set of errors so I just posted the old log -- which is why the line numbers are off.",NA,https://api.github.com/repos/mpark/variant/issues/57/comments,https://github.com/mpark/variant/pull/57#issuecomment-453290838,https://api.github.com/repos/mpark/variant/issues/57
mpark,variant,397640041,https://api.github.com/repos/mpark/variant/issues/comments/453709467,453709467,MDEyOklzc3VlQ29tbWVudDQ1MzcwOTQ2Nw==,2275240,2019-01-12T01:51:30Z,2019-01-12T01:51:51Z,OWNER,"On `master`, with `Microsoft (R) C/C++ Optimizing Compiler Version 19.16.27026.1 for x86`:

> For me, this is enough to reproduce it using the single-header variant (version 15.9.5):
> 
> #include ""variant.hpp""
> 
> ```
> int main() {
>   mpark::variant<int, std::unique_ptr<int>> var;
> }
> ```

This compiles fine for me.

---

```
#include <mpark/variant.hpp>
#include <memory>
#include <vector>

struct S {
  S (int n) : var(n) {}
  mpark::variant<int, std::unique_ptr<int>> var;
};

int main () {
  std::vector<S> vec;
  vec.emplace_back(0);
}
```

This does not. But this doesn't pass even after applying this patch ðŸ˜•",NA,https://api.github.com/repos/mpark/variant/issues/57/comments,https://github.com/mpark/variant/pull/57#issuecomment-453709467,https://api.github.com/repos/mpark/variant/issues/57
mpark,variant,397640041,https://api.github.com/repos/mpark/variant/issues/comments/453724027,453724027,MDEyOklzc3VlQ29tbWVudDQ1MzcyNDAyNw==,3969255,2019-01-12T06:29:54Z,2019-01-12T06:29:54Z,NONE,"I should probably get the 19.x version number of the compiler for comparison, since it seems odd that you arenâ€™t getting errors â€” maybe a different optimization level flag? I guess the thing to do would be to try reproducing the errors in a VM, since each of us seems to be seeing different behavior.",NA,https://api.github.com/repos/mpark/variant/issues/57/comments,https://github.com/mpark/variant/pull/57#issuecomment-453724027,https://api.github.com/repos/mpark/variant/issues/57
mpark,variant,397640041,https://api.github.com/repos/mpark/variant/issues/comments/453729112,453729112,MDEyOklzc3VlQ29tbWVudDQ1MzcyOTExMg==,3620467,2019-01-12T08:12:04Z,2019-01-12T08:22:04Z,NONE,"I can confirm @mpark's summary. It's the exact same behavior on my machine. Obviously, my example triggers an issue that's unrelated to this PR. It was introduced by commit 62b41fea0bcb7e32f28ab312b362b17c11b884e3. 

BTW, my MSVC version is 19.16.27026.1 and I call `cl` on the command line without any options except `/I` to specify the include path.
",NA,https://api.github.com/repos/mpark/variant/issues/57/comments,https://github.com/mpark/variant/pull/57#issuecomment-453729112,https://api.github.com/repos/mpark/variant/issues/57
mpark,variant,397640041,https://api.github.com/repos/mpark/variant/issues/comments/454007173,454007173,MDEyOklzc3VlQ29tbWVudDQ1NDAwNzE3Mw==,2275240,2019-01-14T13:38:10Z,2019-01-14T13:38:48Z,OWNER,"I've added @mgieseki's example as a test case in b68278366135ef69cc0614d7a76e4b761cc31c8b, fixed the issue that it was running into in 8cab6363d4580c9c1990a083514189916a6be125, and addressed the root cause of the `MPARK_CPP14_CONSTEXPR` related issue in 19.15 in 657110d5ea11f344d29bce1d9609c7d76696cb64.

Please let me know if there are remaining problems!",NA,https://api.github.com/repos/mpark/variant/issues/57/comments,https://github.com/mpark/variant/pull/57#issuecomment-454007173,https://api.github.com/repos/mpark/variant/issues/57
mpark,variant,397640041,https://api.github.com/repos/mpark/variant/issues/comments/454086530,454086530,MDEyOklzc3VlQ29tbWVudDQ1NDA4NjUzMA==,3969255,2019-01-14T17:19:07Z,2019-01-14T17:19:07Z,NONE,"Awesome! Thanks @mpark, with those changes it now compiles with MSVC 2017 and 2019 preview. Seems like compiler bugs are a bit of a theme with the MSVC compilers.",NA,https://api.github.com/repos/mpark/variant/issues/57/comments,https://github.com/mpark/variant/pull/57#issuecomment-454086530,https://api.github.com/repos/mpark/variant/issues/57
mpark,variant,386114126,https://api.github.com/repos/mpark/variant/issues/comments/451269501,451269501,MDEyOklzc3VlQ29tbWVudDQ1MTI2OTUwMQ==,2275240,2019-01-03T20:37:17Z,2019-01-03T20:43:47Z,OWNER,"The current C++11 implementation using a chain of ternary operators gets optimized nicely under __Clang__ >= 4.0, but does not work well for any __GCC__.

__Clang 3.9__: https://travis-ci.org/mpark/variant/jobs/474776453
```
5: Test command: /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark ""--benchmark_color=true""
5: Test timeout computed to be: 10000000
5: 2019-01-03 10:15:54
5: Running /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark
5: Run on (2 X 2300 MHz CPU s)
5: CPU Caches:
5:   L1 Data 32K (x1)
5:   L1 Instruction 32K (x1)
5:   L2 Unified 256K (x1)
5:   L3 Unified 46080K (x1)
5: Load Average: 1.30, 1.00, 0.46
5: ---------------------------------------------------
5: Benchmark            Time           CPU Iterations
5: ---------------------------------------------------
5: visit1<1>         2801 ns       2801 ns     249965
5: visit1<2>         9519 ns       9519 ns      73370
5: visit1<3>        11382 ns      11382 ns      61655
5: visit1<5>        11409 ns      11409 ns      61178
5: visit1<8>        10320 ns      10320 ns      67497
5: visit1<15>       10483 ns      10483 ns      66962
5: visit1<32>       12492 ns      12492 ns      55995
5: visit1<65>       11254 ns      11253 ns      62465
5: visit1<128>      10673 ns      10673 ns      65601
5: 
1/2 Test #5: execute.visit.1.mpark ............   Passed    7.60 sec
test 6
    Start 6: execute.visit.1.mpark-dev
6: Test command: /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark-dev ""--benchmark_color=true""
6: Test timeout computed to be: 10000000
6: 2019-01-03 10:16:02
6: Running /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark-dev
6: Run on (2 X 2300 MHz CPU s)
6: CPU Caches:
6:   L1 Data 32K (x1)
6:   L1 Instruction 32K (x1)
6:   L2 Unified 256K (x1)
6:   L3 Unified 46080K (x1)
6: Load Average: 1.25, 1.00, 0.47
6: ---------------------------------------------------
6: Benchmark            Time           CPU Iterations
6: ---------------------------------------------------
6: visit1<1>         2843 ns       2842 ns     248350
6: visit1<2>         4347 ns       4347 ns     161527
6: visit1<3>         5755 ns       5755 ns     121301
6: visit1<5>         7205 ns       7205 ns      96040
6: visit1<8>         7921 ns       7921 ns      89186
6: visit1<15>       11592 ns      11592 ns      60326
6: visit1<32>       19671 ns      19609 ns      36296
6: visit1<65>       40742 ns      40741 ns      17328
6: visit1<128>      72147 ns      72147 ns       9872
```
__Clang 4.0__: https://travis-ci.org/mpark/variant/jobs/474776455
```
5: Test command: /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark ""--benchmark_color=true""
5: Test timeout computed to be: 10000000
5: 2019-01-03 10:18:31
5: Running /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark
5: Run on (2 X 2300 MHz CPU s)
5: CPU Caches:
5:   L1 Data 32K (x1)
5:   L1 Instruction 32K (x1)
5:   L2 Unified 256K (x1)
5:   L3 Unified 46080K (x1)
5: Load Average: 1.22, 1.08, 0.52
5: ---------------------------------------------------
5: Benchmark            Time           CPU Iterations
5: ---------------------------------------------------
5: visit1<1>         2868 ns       2868 ns     241164
5: visit1<2>        10869 ns      10869 ns      64441
5: visit1<3>        10247 ns      10247 ns      69007
5: visit1<5>        11963 ns      11963 ns      58924
5: visit1<8>        11442 ns      11442 ns      60372
5: visit1<15>       10584 ns      10583 ns      64373
5: visit1<32>       10750 ns      10750 ns      66075
5: visit1<65>       12604 ns      12604 ns      56448
5: visit1<128>      19759 ns      19759 ns      34593
5: 
1/2 Test #5: execute.visit.1.mpark ............   Passed    7.72 sec
test 6
    Start 6: execute.visit.1.mpark-dev
6: Test command: /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark-dev ""--benchmark_color=true""
6: Test timeout computed to be: 10000000
6: 2019-01-03 10:18:38
6: Running /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark-dev
6: Run on (2 X 2300 MHz CPU s)
6: CPU Caches:
6:   L1 Data 32K (x1)
6:   L1 Instruction 32K (x1)
6:   L2 Unified 256K (x1)
6:   L3 Unified 46080K (x1)
6: Load Average: 1.21, 1.07, 0.52
6: ---------------------------------------------------
6: Benchmark            Time           CPU Iterations
6: ---------------------------------------------------
6: visit1<1>         2896 ns       2896 ns     242707
6: visit1<2>         4413 ns       4412 ns     159670
6: visit1<3>         3852 ns       3851 ns     179790
6: visit1<5>         2977 ns       2977 ns     235361
6: visit1<8>         2954 ns       2954 ns     234164
6: visit1<15>        3022 ns       3021 ns     232849
6: visit1<32>        3833 ns       3833 ns     181819
6: visit1<65>        4925 ns       4924 ns     143037
6: visit1<128>       3833 ns       3833 ns     180677
```

__GCC 4.9__: https://travis-ci.org/mpark/variant/jobs/474776437
```
5: Test command: /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark ""--benchmark_color=true""
5: Test timeout computed to be: 10000000
5: 2019-01-03 09:56:50
5: Running /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark
5: Run on (2 X 2300 MHz CPU s)
5: CPU Caches:
5:   L1 Data 32K (x1)
5:   L1 Instruction 32K (x1)
5:   L2 Unified 256K (x1)
5:   L3 Unified 46080K (x1)
5: Load Average: 1.09, 0.87, 0.41
5: ---------------------------------------------------
5: Benchmark            Time           CPU Iterations
5: ---------------------------------------------------
5: visit1<1>         3790 ns       3789 ns     183728
5: visit1<2>        11373 ns      11373 ns      61747
5: visit1<3>        12018 ns      12018 ns      58302
5: visit1<5>        10370 ns      10370 ns      68172
5: visit1<8>        10569 ns      10569 ns      65518
5: visit1<15>       11400 ns      11400 ns      61299
5: visit1<32>       12453 ns      12436 ns      55540
5: visit1<65>       10548 ns      10545 ns      66160
5: visit1<128>      10768 ns      10767 ns      65310
5: 
1/2 Test #5: execute.visit.1.mpark ............   Passed    7.73 sec
test 6
    Start 6: execute.visit.1.mpark-dev
6: Test command: /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark-dev ""--benchmark_color=true""
6: Test timeout computed to be: 10000000
6: 2019-01-03 09:56:58
6: Running /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark-dev
6: Run on (2 X 2300 MHz CPU s)
6: CPU Caches:
6:   L1 Data 32K (x1)
6:   L1 Instruction 32K (x1)
6:   L2 Unified 256K (x1)
6:   L3 Unified 46080K (x1)
6: Load Average: 1.08, 0.87, 0.42
6: ---------------------------------------------------
6: Benchmark            Time           CPU Iterations
6: ---------------------------------------------------
6: visit1<1>         6873 ns       6873 ns     101490
6: visit1<2>         4174 ns       4174 ns     168032
6: visit1<3>         6520 ns       6520 ns     107322
6: visit1<5>         8068 ns       8068 ns      86855
6: visit1<8>         9532 ns       9532 ns      74410
6: visit1<15>       11307 ns      11307 ns      61789
6: visit1<32>       20581 ns      20581 ns      34392
6: visit1<65>       38014 ns      38015 ns      18733
6: visit1<128>      69736 ns      69735 ns       9869
```
__GCC 8__: https://travis-ci.org/mpark/variant/jobs/474776448
```
5: Test command: /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark ""--benchmark_color=true""
5: Test timeout computed to be: 10000000
5: 2019-01-03 10:09:28
5: Running /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark
5: Run on (2 X 2300 MHz CPU s)
5: CPU Caches:
5:   L1 Data 32K (x1)
5:   L1 Instruction 32K (x1)
5:   L2 Unified 256K (x1)
5:   L3 Unified 46080K (x1)
5: Load Average: 1.15, 1.04, 0.50
5: ---------------------------------------------------
5: Benchmark            Time           CPU Iterations
5: ---------------------------------------------------
5: visit1<1>         3805 ns       3805 ns     182961
5: visit1<2>        10565 ns      10565 ns      66086
5: visit1<3>        11357 ns      11357 ns      61588
5: visit1<5>        11736 ns      11736 ns      59659
5: visit1<8>        12278 ns      12278 ns      57865
5: visit1<15>       12271 ns      12270 ns      57095
5: visit1<32>       11354 ns      11354 ns      61476
5: visit1<65>       12317 ns      12316 ns      56952
5: visit1<128>      10754 ns      10754 ns      64382
5: 
1/2 Test #5: execute.visit.1.mpark ............   Passed    7.76 sec
test 6
    Start 6: execute.visit.1.mpark-dev
6: Test command: /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark-dev ""--benchmark_color=true""
6: Test timeout computed to be: 10000000
6: 2019-01-03 10:09:36
6: Running /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark-dev
6: Run on (2 X 2300 MHz CPU s)
6: CPU Caches:
6:   L1 Data 32K (x1)
6:   L1 Instruction 32K (x1)
6:   L2 Unified 256K (x1)
6:   L3 Unified 46080K (x1)
6: Load Average: 1.14, 1.04, 0.50
6: ---------------------------------------------------
6: Benchmark            Time           CPU Iterations
6: ---------------------------------------------------
6: visit1<1>         5757 ns       5757 ns     121563
6: visit1<2>         4699 ns       4699 ns     149398
6: visit1<3>         6353 ns       6353 ns     109737
6: visit1<5>         7424 ns       7424 ns      93428
6: visit1<8>         8623 ns       8623 ns      81464
6: visit1<15>       13569 ns      13568 ns      50929
6: visit1<32>       20769 ns      20769 ns      33469
6: visit1<65>       38117 ns      38116 ns      18302
6: visit1<128>      69499 ns      69499 ns       9904
```",NA,https://api.github.com/repos/mpark/variant/issues/56/comments,https://github.com/mpark/variant/pull/56#issuecomment-451269501,https://api.github.com/repos/mpark/variant/issues/56
mpark,variant,386114126,https://api.github.com/repos/mpark/variant/issues/comments/451276180,451276180,MDEyOklzc3VlQ29tbWVudDQ1MTI3NjE4MA==,2275240,2019-01-03T21:02:50Z,2019-01-04T06:24:05Z,OWNER,"The current C++14/17 implementation of recursive `switch` works well for all of __Clang__, but has some issues under GCC.
__Clang 7__: https://travis-ci.org/mpark/variant/jobs/474776465
```
5: Test command: /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark ""--benchmark_color=true""
5: Test timeout computed to be: 10000000
5: 2019-01-03 10:31:18
5: Running /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark
5: Run on (2 X 2500 MHz CPU s)
5: CPU Caches:
5:   L1 Data 32K (x1)
5:   L1 Instruction 32K (x1)
5:   L2 Unified 256K (x1)
5:   L3 Unified 30720K (x1)
5: Load Average: 1.32, 1.07, 0.51
5: ---------------------------------------------------
5: Benchmark            Time           CPU Iterations
5: ---------------------------------------------------
5: visit1<1>         2864 ns       2862 ns     239638
5: visit1<2>        11464 ns      11455 ns      61048
5: visit1<3>        10244 ns      10239 ns      68836
5: visit1<5>         9989 ns       9981 ns      70500
5: visit1<8>        11452 ns      11444 ns      60595
5: visit1<15>       12330 ns      12321 ns      56714
5: visit1<32>       10617 ns      10605 ns      66692
5: visit1<65>       10673 ns      10667 ns      65805
5: visit1<128>      14185 ns      14174 ns      48735
5: 
1/2 Test #5: execute.visit.1.mpark ............   Passed    7.64 sec
test 6
    Start 6: execute.visit.1.mpark-dev
6: Test command: /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark-dev ""--benchmark_color=true""
6: Test timeout computed to be: 10000000
6: 2019-01-03 10:31:25
6: Running /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark-dev
6: Run on (2 X 2500 MHz CPU s)
6: CPU Caches:
6:   L1 Data 32K (x1)
6:   L1 Instruction 32K (x1)
6:   L2 Unified 256K (x1)
6:   L3 Unified 30720K (x1)
6: Load Average: 1.30, 1.07, 0.52
6: ---------------------------------------------------
6: Benchmark            Time           CPU Iterations
6: ---------------------------------------------------
6: visit1<1>         2850 ns       2846 ns     245817
6: visit1<2>         3436 ns       3434 ns     201585
6: visit1<3>         2940 ns       2939 ns     236889
6: visit1<5>         3838 ns       3836 ns     182712
6: visit1<8>         2973 ns       2972 ns     236992
6: visit1<15>        2957 ns       2956 ns     237050
6: visit1<32>        2951 ns       2949 ns     237763
6: visit1<65>        2981 ns       2980 ns     235619
6: visit1<128>       9646 ns       9641 ns      72540
```

__GCC 8__: https://travis-ci.org/mpark/variant/jobs/474776450
```
5: Test command: /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark ""--benchmark_color=true""
5: Test timeout computed to be: 10000000
5: 2019-01-03 10:11:32
5: Running /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark
5: Run on (2 X 2500 MHz CPU s)
5: CPU Caches:
5:   L1 Data 32K (x1)
5:   L1 Instruction 32K (x1)
5:   L2 Unified 256K (x1)
5:   L3 Unified 30720K (x1)
5: Load Average: 1.14, 0.89, 0.41
5: ---------------------------------------------------
5: Benchmark            Time           CPU Iterations
5: ---------------------------------------------------
5: visit1<1>         2799 ns       2798 ns     250365
5: visit1<2>         9468 ns       9463 ns      73890
5: visit1<3>        11481 ns      11475 ns      61645
5: visit1<5>        11572 ns      11566 ns      60407
5: visit1<8>        11592 ns      11586 ns      60486
5: visit1<15>       11585 ns      11579 ns      58922
5: visit1<32>       10668 ns      10663 ns      65947
5: visit1<65>       10632 ns      10626 ns      64057
5: visit1<128>      11657 ns      11651 ns      54142
5: 
1/2 Test #5: execute.visit.1.mpark ............   Passed    7.54 sec
test 6
    Start 6: execute.visit.1.mpark-dev
6: Test command: /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark-dev ""--benchmark_color=true""
6: Test timeout computed to be: 10000000
6: 2019-01-03 10:11:40
6: Running /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark-dev
6: Run on (2 X 2500 MHz CPU s)
6: CPU Caches:
6:   L1 Data 32K (x1)
6:   L1 Instruction 32K (x1)
6:   L2 Unified 256K (x1)
6:   L3 Unified 30720K (x1)
6: Load Average: 1.11, 0.90, 0.41
6: ---------------------------------------------------
6: Benchmark            Time           CPU Iterations
6: ---------------------------------------------------
6: visit1<1>         2827 ns       2825 ns     248203
6: visit1<2>         4179 ns       4176 ns     166883
6: visit1<3>         2858 ns       2856 ns     241306
6: visit1<5>         2858 ns       2856 ns     241945
6: visit1<8>         3826 ns       3823 ns     181854
6: visit1<15>        2850 ns       2848 ns     244742
6: visit1<32>        3830 ns       3827 ns     182111
6: visit1<65>        6870 ns       6865 ns     102572
6: visit1<128>      12199 ns      12164 ns      58194
```
GCC >= 7 perform well compared to the manual jump table, but it does not
inline/collapse the recursive `switch`. This is reflected in our numbers in
the progression from `visit<32>`, `visit<65>` to `visit<128>`. It can also be
seen concretely in the generated assembly: https://godbolt.org/z/Rfcv9C

__GCC 6__: https://travis-ci.org/mpark/variant/jobs/474776444
```
5: Test command: /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark ""--benchmark_color=true""
5: Test timeout computed to be: 10000000
5: 2019-01-03 10:04:26
5: Running /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark
5: Run on (2 X 2500 MHz CPU s)
5: CPU Caches:
5:   L1 Data 32K (x1)
5:   L1 Instruction 32K (x1)
5:   L2 Unified 256K (x1)
5:   L3 Unified 30720K (x1)
5: Load Average: 1.25, 0.94, 0.43
5: ---------------------------------------------------
5: Benchmark            Time           CPU Iterations
5: ---------------------------------------------------
5: visit1<1>         2807 ns       2805 ns     249207
5: visit1<2>         9514 ns       9514 ns      72992
5: visit1<3>        11995 ns      11995 ns      58128
5: visit1<5>        10296 ns      10296 ns      68163
5: visit1<8>        10375 ns      10375 ns      67930
5: visit1<15>       10541 ns      10541 ns      64657
5: visit1<32>       10666 ns      10666 ns      63728
5: visit1<65>       10516 ns      10516 ns      66144
5: visit1<128>      12532 ns      12532 ns      57244
5: 
1/2 Test #5: execute.visit.1.mpark ............   Passed    7.58 sec
test 6
    Start 6: execute.visit.1.mpark-dev
6: Test command: /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark-dev ""--benchmark_color=true""
6: Test timeout computed to be: 10000000
6: 2019-01-03 10:04:34
6: Running /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark-dev
6: Run on (2 X 2500 MHz CPU s)
6: CPU Caches:
6:   L1 Data 32K (x1)
6:   L1 Instruction 32K (x1)
6:   L2 Unified 256K (x1)
6:   L3 Unified 30720K (x1)
6: Load Average: 1.21, 0.95, 0.44
6: ---------------------------------------------------
6: Benchmark            Time           CPU Iterations
6: ---------------------------------------------------
6: visit1<1>         7626 ns       7615 ns      90840
6: visit1<2>         7466 ns       7462 ns      94088
6: visit1<3>        10818 ns      10818 ns      64757
6: visit1<5>        11162 ns      11160 ns      63228
6: visit1<8>        10955 ns      10953 ns      63846
6: visit1<15>       11100 ns      11098 ns      63626
6: visit1<32>       11341 ns      11338 ns      61223
6: visit1<65>       12561 ns      12561 ns      55673
6: visit1<128>      16291 ns      16290 ns      42286
```
GCC < 7 produces a jump table of size 32 (our `switch` size) even when unneeded.
This can be seen for example, in the generated assembly for `visit<8>`: https://godbolt.org/z/8XKQGD. Although it seems like it's not actually worse than
the manual jump table aside from the `visit<1>` case ðŸ¤”",NA,https://api.github.com/repos/mpark/variant/issues/56/comments,https://github.com/mpark/variant/pull/56#issuecomment-451276180,https://api.github.com/repos/mpark/variant/issues/56
mpark,variant,386114126,https://api.github.com/repos/mpark/variant/issues/comments/451364630,451364630,MDEyOklzc3VlQ29tbWVudDQ1MTM2NDYzMA==,2275240,2019-01-04T06:44:17Z,2019-01-04T06:44:32Z,OWNER,"Follow-up on the current C++14/17 implementation with regards to __GCC__ >= 7.

I was concerned that the non-inlined recursive `switch` would grow faster than
the manual jump table approach, but having tested for `visit<200>` and `visit<250>`,
the manual jump table performance also grows, at approximately the same rate it seems.

__GCC 8__: https://travis-ci.org/mpark/variant/jobs/475164795
```
5: Test command: /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark ""--benchmark_color=true""
5: Test timeout computed to be: 10000000
5: 2019-01-04 05:49:01
5: Running /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark
5: Run on (2 X 2600 MHz CPU s)
5: CPU Caches:
5:   L1 Data 32K (x1)
5:   L1 Instruction 32K (x1)
5:   L2 Unified 256K (x1)
5:   L3 Unified 20480K (x1)
5: Load Average: 1.00, 1.00, 0.70
5: ---------------------------------------------------
5: Benchmark            Time           CPU Iterations
5: ---------------------------------------------------
5: visit1<200>      17584 ns      17574 ns      40196
5: visit1<250>      20115 ns      20105 ns      34640
5: 
1/2 Test #5: execute.visit.1.mpark ............   Passed    1.84 sec
test 6
    Start 6: execute.visit.1.mpark-dev
6: Test command: /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark-dev ""--benchmark_color=true""
6: Test timeout computed to be: 10000000
6: 2019-01-04 05:49:03
6: Running /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark-dev
6: Run on (2 X 2600 MHz CPU s)
6: CPU Caches:
6:   L1 Data 32K (x1)
6:   L1 Instruction 32K (x1)
6:   L2 Unified 256K (x1)
6:   L3 Unified 20480K (x1)
6: Load Average: 1.00, 1.00, 0.70
6: ---------------------------------------------------
6: Benchmark            Time           CPU Iterations
6: ---------------------------------------------------
6: visit1<200>      16236 ns      16227 ns      43488
6: visit1<250>      18135 ns      18126 ns      38061
```

This is also roughly the case for __GCC__ < 7.

__GCC 6__: https://travis-ci.org/mpark/variant/jobs/475164789
```
5: Test command: /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark ""--benchmark_color=true""
5: Test timeout computed to be: 10000000
5: 2019-01-04 05:34:19
5: Running /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark
5: Run on (2 X 2600 MHz CPU s)
5: CPU Caches:
5:   L1 Data 32K (x1)
5:   L1 Instruction 32K (x1)
5:   L2 Unified 256K (x1)
5:   L3 Unified 20480K (x1)
5: Load Average: 1.40, 1.13, 0.78
5: ---------------------------------------------------
5: Benchmark            Time           CPU Iterations
5: ---------------------------------------------------
5: visit1<200>      15825 ns      15825 ns      43707
5: visit1<250>      20641 ns      20641 ns      32384
5: 
1/2 Test #5: execute.visit.1.mpark ............   Passed    1.80 sec
test 6
    Start 6: execute.visit.1.mpark-dev
6: Test command: /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark-dev ""--benchmark_color=true""
6: Test timeout computed to be: 10000000
6: 2019-01-04 05:34:20
6: Running /home/travis/build/mpark/variant/build/visit.1/execute.visit.1.mpark-dev
6: Run on (2 X 2600 MHz CPU s)
6: CPU Caches:
6:   L1 Data 32K (x1)
6:   L1 Instruction 32K (x1)
6:   L2 Unified 256K (x1)
6:   L3 Unified 20480K (x1)
6: Load Average: 1.40, 1.13, 0.78
6: ---------------------------------------------------
6: Benchmark            Time           CPU Iterations
6: ---------------------------------------------------
6: visit1<200>      18900 ns      18889 ns      37390
6: visit1<250>      20114 ns      20104 ns      34828
```

It seems safe to me to proceed with the current implementation, for C++14/17.",NA,https://api.github.com/repos/mpark/variant/issues/56/comments,https://github.com/mpark/variant/pull/56#issuecomment-451364630,https://api.github.com/repos/mpark/variant/issues/56
mpark,variant,386114126,https://api.github.com/repos/mpark/variant/issues/comments/451782160,451782160,MDEyOklzc3VlQ29tbWVudDQ1MTc4MjE2MA==,2275240,2019-01-06T22:43:25Z,2019-01-07T06:50:35Z,OWNER,"I've reverted the C++11 implementation to be the manual jump table approach,
and only updated C++14/17 implementation to be recursive `switch`.

The following is the full matrix of the benchmarks for `visit.1`
https://travis-ci.org/mpark/variant/builds/476107115",NA,https://api.github.com/repos/mpark/variant/issues/56/comments,https://github.com/mpark/variant/pull/56#issuecomment-451782160,https://api.github.com/repos/mpark/variant/issues/56
mpark,variant,371523278,https://api.github.com/repos/mpark/variant/issues/comments/445375930,445375930,MDEyOklzc3VlQ29tbWVudDQ0NTM3NTkzMA==,2397974,2018-12-07T21:47:44Z,2018-12-07T21:47:44Z,NONE,"As far as we are concerned, we have these issues with MSVC 2017 with most of our libraries that rely on `constexpr`. In my opinion, MSVC is not a viable option for C++ at the moment if you rely non-trivial template stuff.",NA,https://api.github.com/repos/mpark/variant/issues/55/comments,https://github.com/mpark/variant/issues/55#issuecomment-445375930,https://api.github.com/repos/mpark/variant/issues/55
mpark,variant,371523278,https://api.github.com/repos/mpark/variant/issues/comments/448623153,448623153,MDEyOklzc3VlQ29tbWVudDQ0ODYyMzE1Mw==,3620467,2018-12-19T14:54:17Z,2018-12-19T14:54:17Z,NONE,This seems to be the same issue as in #48. It's already fixed in the master branch.,NA,https://api.github.com/repos/mpark/variant/issues/55/comments,https://github.com/mpark/variant/issues/55#issuecomment-448623153,https://api.github.com/repos/mpark/variant/issues/55
mpark,variant,371523278,https://api.github.com/repos/mpark/variant/issues/comments/449409031,449409031,MDEyOklzc3VlQ29tbWVudDQ0OTQwOTAzMQ==,2275240,2018-12-21T14:55:16Z,2018-12-21T14:55:16Z,OWNER,"@mgieseki: Thanks!

Closed by #49.",NA,https://api.github.com/repos/mpark/variant/issues/55/comments,https://github.com/mpark/variant/issues/55#issuecomment-449409031,https://api.github.com/repos/mpark/variant/issues/55
mpark,variant,371523278,https://api.github.com/repos/mpark/variant/issues/comments/454006608,454006608,MDEyOklzc3VlQ29tbWVudDQ1NDAwNjYwOA==,2275240,2019-01-14T13:36:13Z,2019-01-14T13:36:13Z,OWNER,Fixed the root cause in 657110d5ea11f344d29bce1d9609c7d76696cb64,NA,https://api.github.com/repos/mpark/variant/issues/55/comments,https://github.com/mpark/variant/issues/55#issuecomment-454006608,https://api.github.com/repos/mpark/variant/issues/55
mpark,variant,366458899,https://api.github.com/repos/mpark/variant/issues/comments/435019627,435019627,MDEyOklzc3VlQ29tbWVudDQzNTAxOTYyNw==,2275240,2018-11-01T12:01:26Z,2018-11-01T12:01:26Z,OWNER,"* https://help.appveyor.com/discussions/problems/4498-powershell-exception-in-test_script-does-not-fail-build
* https://www.appveyor.com/docs/build-configuration/#script-blocks-in-build-configuration",NA,https://api.github.com/repos/mpark/variant/issues/54/comments,https://github.com/mpark/variant/issues/54#issuecomment-435019627,https://api.github.com/repos/mpark/variant/issues/54
mpark,variant,366458899,https://api.github.com/repos/mpark/variant/issues/comments/435277680,435277680,MDEyOklzc3VlQ29tbWVudDQzNTI3NzY4MA==,2275240,2018-11-02T05:45:47Z,2018-11-02T05:45:47Z,OWNER,Fixed in ab86b12bee3cd8895bf7b1f1cea1c1ccca1cfcce,NA,https://api.github.com/repos/mpark/variant/issues/54/comments,https://github.com/mpark/variant/issues/54#issuecomment-435277680,https://api.github.com/repos/mpark/variant/issues/54
mpark,variant,365165545,https://api.github.com/repos/mpark/variant/issues/comments/454051452,454051452,MDEyOklzc3VlQ29tbWVudDQ1NDA1MTQ1Mg==,2275240,2019-01-14T15:45:48Z,2019-01-14T15:45:48Z,OWNER,@mrzv: My guess would be that this is for ICC running on Windows? Is that right?,NA,https://api.github.com/repos/mpark/variant/issues/53/comments,https://github.com/mpark/variant/issues/53#issuecomment-454051452,https://api.github.com/repos/mpark/variant/issues/53
mpark,variant,365165545,https://api.github.com/repos/mpark/variant/issues/comments/454553543,454553543,MDEyOklzc3VlQ29tbWVudDQ1NDU1MzU0Mw==,720816,2019-01-15T21:13:27Z,2019-01-15T21:13:27Z,NONE,"@mpark Nope, Linux.",NA,https://api.github.com/repos/mpark/variant/issues/53/comments,https://github.com/mpark/variant/issues/53#issuecomment-454553543,https://api.github.com/repos/mpark/variant/issues/53
mpark,variant,365165545,https://api.github.com/repos/mpark/variant/issues/comments/454564116,454564116,MDEyOklzc3VlQ29tbWVudDQ1NDU2NDExNg==,2275240,2019-01-15T21:47:15Z,2019-01-15T21:48:20Z,OWNER,"Hm... I'm not able to repro it on [Godbolt](https://godbolt.org/z/KWuBh8) although the exact compiler version you mentioned isn't available on Godbolt. I'm testing with 18.0.0, 19.0.0 and 19.0.1.

Based on the error message you posted, the error seems to be that the conditional define for [`MPARK_RETURN_TYPE_DEDUCTION`](https://github.com/mpark/variant/blob/74cd46550db4c5d81ba6bcb931006df6d2997d7b/include/mpark/config.hpp#L79-L81) is evaluating to `true` when it shouldn't.
The condition is `defined(__cpp_return_type_deduction) || defined(_MSC_VER)`, so unless `__cpp_return_type_deduction` is set incorrectly (I don't think this is the case...) or `_MSC_VER` is set because ICC is pretending to be MSVC.

Could you check if `_MSC_VER` is set under your config for some reason?",NA,https://api.github.com/repos/mpark/variant/issues/53/comments,https://github.com/mpark/variant/issues/53#issuecomment-454564116,https://api.github.com/repos/mpark/variant/issues/53
mpark,variant,365165545,https://api.github.com/repos/mpark/variant/issues/comments/454566436,454566436,MDEyOklzc3VlQ29tbWVudDQ1NDU2NjQzNg==,720816,2019-01-15T21:54:53Z,2019-01-15T21:54:53Z,NONE,"Running the following input through the compiler
```
#ifdef _MSC_VER
#pragma message ""MSC_VER""
#else
#pragma message ""No MSC_VER""
#endif

#ifdef __cpp_return_type_deduction
#pragma message ""__cpp_return_type_deduction""
#else
#pragma message ""No __cpp_return_type_deduction""
#endif
```
prints
```
No MSC_VER
__cpp_return_type_deduction
```",NA,https://api.github.com/repos/mpark/variant/issues/53/comments,https://github.com/mpark/variant/issues/53#issuecomment-454566436,https://api.github.com/repos/mpark/variant/issues/53
mpark,variant,365165545,https://api.github.com/repos/mpark/variant/issues/comments/454567941,454567941,MDEyOklzc3VlQ29tbWVudDQ1NDU2Nzk0MQ==,720816,2019-01-15T21:59:34Z,2019-01-15T21:59:34Z,NONE,"Ah, I figured it out. I've been doing all of this on a super-computer, which uses a module system. The GCC module is not loaded by default, but the Intel compiler relies on the GCC standard library to work correctly. Loading the gcc module fixes the problem. My bad; thanks for looking into it.",NA,https://api.github.com/repos/mpark/variant/issues/53/comments,https://github.com/mpark/variant/issues/53#issuecomment-454567941,https://api.github.com/repos/mpark/variant/issues/53
mpark,variant,365165545,https://api.github.com/repos/mpark/variant/issues/comments/454578200,454578200,MDEyOklzc3VlQ29tbWVudDQ1NDU3ODIwMA==,2275240,2019-01-15T22:34:22Z,2019-01-15T22:34:22Z,OWNER,"Ah.. interesting. So without the GCC module ICC defines `__cpp_return_type_deduction` under C++11 mode ðŸ¤” that's odd. But okay, I'll close this.",NA,https://api.github.com/repos/mpark/variant/issues/53/comments,https://github.com/mpark/variant/issues/53#issuecomment-454578200,https://api.github.com/repos/mpark/variant/issues/53
mpark,variant,365165545,https://api.github.com/repos/mpark/variant/issues/comments/507823332,507823332,MDEyOklzc3VlQ29tbWVudDUwNzgyMzMzMg==,253316,2019-07-02T19:59:51Z,2019-07-02T19:59:51Z,NONE,"@mrzv What supercomputer site is this? They probably need to set `-gcc-name` and such inside of their `icpc.cfg` file which corresponds to the requisite GCC version. This is important, as the Intel compilers definitely have restrictions on which gcc headers they'll work with.

This is unrelated to the issue, of course, but figured I'd leave it out in the open in case it is helpful.",NA,https://api.github.com/repos/mpark/variant/issues/53/comments,https://github.com/mpark/variant/issues/53#issuecomment-507823332,https://api.github.com/repos/mpark/variant/issues/53
mpark,variant,365165545,https://api.github.com/repos/mpark/variant/issues/comments/507906930,507906930,MDEyOklzc3VlQ29tbWVudDUwNzkwNjkzMA==,720816,2019-07-03T01:54:25Z,2019-07-03T01:54:25Z,NONE,@morrisonlevi Thanks for the note. This all took place at [NERSC](https://www.nersc.gov/). I'll pass this information on to their consultants.,NA,https://api.github.com/repos/mpark/variant/issues/53/comments,https://github.com/mpark/variant/issues/53#issuecomment-507906930,https://api.github.com/repos/mpark/variant/issues/53
mpark,variant,365144784,https://api.github.com/repos/mpark/variant/issues/comments/425659753,425659753,MDEyOklzc3VlQ29tbWVudDQyNTY1OTc1Mw==,10457096,2018-09-29T16:58:31Z,2018-09-29T16:58:31Z,NONE,"Oh, also I'm aware you might not want to use if constexpr in the implementation. I just did it this way for now, can easily switch to tag dispatch.

I didn't do the fdiagonal into switch case yet either, that could be interesting as well. AFAICS, the ""visit_at"" line of functions seems to only be used for two variants of same type, visiting at the same index. Seemingly for binary operations like ==? Should probably do that too actually, seems like significantly better assembly when switch case is used: https://gcc.godbolt.org/z/To-fsq. ",NA,https://api.github.com/repos/mpark/variant/issues/52/comments,https://github.com/mpark/variant/pull/52#issuecomment-425659753,https://api.github.com/repos/mpark/variant/issues/52
mpark,variant,365144784,https://api.github.com/repos/mpark/variant/issues/comments/429462227,429462227,MDEyOklzc3VlQ29tbWVudDQyOTQ2MjIyNw==,2275240,2018-10-12T21:06:14Z,2018-10-12T21:06:14Z,OWNER,Is the idea to use a `switch` for only 1 variant upto some fixed-`N` alternatives? Could we at least support it for arbitrary `N` alternatives for 1 variant?,NA,https://api.github.com/repos/mpark/variant/issues/52/comments,https://github.com/mpark/variant/pull/52#issuecomment-429462227,https://api.github.com/repos/mpark/variant/issues/52
mpark,variant,365144784,https://api.github.com/repos/mpark/variant/issues/comments/429464765,429464765,MDEyOklzc3VlQ29tbWVudDQyOTQ2NDc2NQ==,10457096,2018-10-12T21:16:38Z,2018-10-12T21:18:35Z,NONE,"@mpark Well, you can't have a switch with an arbitrary number of cases. If variant has a well defined hard limit to the number of types it can hold, then you could generate up to that point. But if variant doesn't make such an attempt, and it's only limited by e.g. compiler limits on template recursion and such, then there is no way to do the switch case approach for purely arbitrary N types in a variant.

Personally I don't see this as a big issue; we already know this doesn't cover everything and instead is supposed to be an optimization for common cases. Covering up to e.g. 20 types will in practice significantly improve codegen for the overwhelming majority (IMHO) of uses of variant.

Note also that it will also cover built in binary operations (like comparison) for up to N types, despite those involving two variants, because the dispatch only occurs if the index for both variants matches, so we can handle that with switch case quite easily as well (not yet present in implementation, wanted to verify basic approach first).",NA,https://api.github.com/repos/mpark/variant/issues/52/comments,https://github.com/mpark/variant/pull/52#issuecomment-429464765,https://api.github.com/repos/mpark/variant/issues/52
mpark,variant,365144784,https://api.github.com/repos/mpark/variant/issues/comments/429470579,429470579,MDEyOklzc3VlQ29tbWVudDQyOTQ3MDU3OQ==,2275240,2018-10-12T21:41:36Z,2018-10-12T21:41:36Z,OWNER,"I was thinking of something like this, assuming the fixed-`N` is `32`:
```cpp
template <std::size_t B, typename T>
decltype(auto) switch_visit(T&& v) {
  switch (v.index()) {
    case B+0: /* ... */ break;
    case B+1:  /* ... */ break;
    // ...
    case B+31: /* ... */ break;
    // this should be tail-recursive-able:
    default: switch_visit<B+32>(std::forward<T>(v));
  }
}
```",NA,https://api.github.com/repos/mpark/variant/issues/52/comments,https://github.com/mpark/variant/pull/52#issuecomment-429470579,https://api.github.com/repos/mpark/variant/issues/52
mpark,variant,365144784,https://api.github.com/repos/mpark/variant/issues/comments/429473711,429473711,MDEyOklzc3VlQ29tbWVudDQyOTQ3MzcxMQ==,10457096,2018-10-12T21:56:05Z,2018-10-12T21:56:05Z,NONE,"@mpark Very neat! I hadn't considered an approach like that. However, there's a rather significant issue, which I couldn't solve on the user side but could very well be solvable when we are working inside the implementation. The code inside the `/* .. */` is basically going to ask for a reference to the type inside the variant at index I (where I is just a constant matching the case). If you use an index that isn't within the size of the variant, you immediately get a hard compilation error (at least, this is the behavior both if you use the public api with get_if, and I think it's what you see with `get_alt` as well). For that reason, there has to be a separate switch case function for every size of variant. As well, for performance reasons the compiler may be able to generate better code when it has the unreachable pragma.

In order to use this approach, I'd: a) need some way to get a reference to the `I`th type such that a compilation error is not triggered when I is out of bounds, b) we'd need to convince ourselves that this doesn't make the assembly worse in common case. I'm not against covering these two as long as it's reasonably time efficient, but I still feel like there is little disadvantage in only covering up to fixed N.

What do you think?",NA,https://api.github.com/repos/mpark/variant/issues/52/comments,https://github.com/mpark/variant/pull/52#issuecomment-429473711,https://api.github.com/repos/mpark/variant/issues/52
mpark,variant,365144784,https://api.github.com/repos/mpark/variant/issues/comments/429507894,429507894,MDEyOklzc3VlQ29tbWVudDQyOTUwNzg5NA==,2275240,2018-10-13T03:42:15Z,2018-10-13T03:42:38Z,OWNER,"@quicknir: We have the exact same issue for a fixed size `N` though, right? As in, if our `N` is bigger than the `variant_size`, then we're gonna have to not hard-error on `get`. So it seems to me like we need to solve that problem regardless.",NA,https://api.github.com/repos/mpark/variant/issues/52/comments,https://github.com/mpark/variant/pull/52#issuecomment-429507894,https://api.github.com/repos/mpark/variant/issues/52
mpark,variant,365144784,https://api.github.com/repos/mpark/variant/issues/comments/429508062,429508062,MDEyOklzc3VlQ29tbWVudDQyOTUwODA2Mg==,10457096,2018-10-13T03:44:50Z,2018-10-13T03:45:06Z,NONE,"@mpark Well, I actually generate a separate switch case function for each size of variant. That's what the integer template parameter in the function is for. So in the current approach we don't need to change get_alt and such.",NA,https://api.github.com/repos/mpark/variant/issues/52/comments,https://github.com/mpark/variant/pull/52#issuecomment-429508062,https://api.github.com/repos/mpark/variant/issues/52
mpark,variant,365144784,https://api.github.com/repos/mpark/variant/issues/comments/429518766,429518766,MDEyOklzc3VlQ29tbWVudDQyOTUxODc2Ng==,5955594,2018-10-13T07:27:40Z,2018-10-13T07:27:40Z,NONE,"Sorry if unclear, writing on my phone. I haven't seen the code but I'm inferring from what I am reading here. I have two questions. 
1.
If @quicknir has a method of generating any length of switch up to some fixed n, can't this be combined with @mpark 's method of recursing in batches accept that instead of always assuming a fixed size batch, i.e. batches of 32, the next batch size can be calculated exactly based on the number of cases left to be handled. If it's more than n, (e.g. 32 as above), you pick the largest switch function, the one with n cases. But if what we have left is less than n, then we forward to a switch of length exactly equal to the amount left. Again that is assuming I read it correctly that you can generate switches of any length up to n. 

2. 

Alternatively, to prevent taking a reference to the type inside the variant when the switch case is out of bounds, could you not wrap the code for each case inside a templated function, invoke_if_in_bounds? Can't write this on my phone atm but it would be the basic enable_if, if it's out of bounds, you forward to a function that does not run the code which would cause a compile error. 

   

Regards, 
Saad
 Sent from my iPhone (excuse typos)

> On 13 Oct 2018, at 4:44 am, quicknir <notifications@github.com> wrote:
> 
> Well, I actually generate a separate switch case function for each size of variant. That's what the integer template parameter in the function is for. So in the current approach we don't need to change get_alt and such.
> 
> â€”
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub, or mute the thread.
",NA,https://api.github.com/repos/mpark/variant/issues/52/comments,https://github.com/mpark/variant/pull/52#issuecomment-429518766,https://api.github.com/repos/mpark/variant/issues/52
mpark,variant,365144784,https://api.github.com/repos/mpark/variant/issues/comments/429544495,429544495,MDEyOklzc3VlQ29tbWVudDQyOTU0NDQ5NQ==,10457096,2018-10-13T14:00:57Z,2018-10-13T14:00:57Z,NONE,"I like your first idea a lot, I'll try to show a POC godbolt example this weekend.",NA,https://api.github.com/repos/mpark/variant/issues/52/comments,https://github.com/mpark/variant/pull/52#issuecomment-429544495,https://api.github.com/repos/mpark/variant/issues/52
mpark,variant,365144784,https://api.github.com/repos/mpark/variant/issues/comments/429673904,429673904,MDEyOklzc3VlQ29tbWVudDQyOTY3MzkwNA==,2275240,2018-10-14T23:50:30Z,2018-10-15T00:34:13Z,OWNER,"Here's a version that handles arbitrary # of alternatives, with `N=4`, for one variant with 6 alternatives: https://godbolt.org/z/TBd9rr",NA,https://api.github.com/repos/mpark/variant/issues/52/comments,https://github.com/mpark/variant/pull/52#issuecomment-429673904,https://api.github.com/repos/mpark/variant/issues/52
mpark,variant,365144784,https://api.github.com/repos/mpark/variant/issues/comments/429707908,429707908,MDEyOklzc3VlQ29tbWVudDQyOTcwNzkwOA==,2275240,2018-10-15T04:50:35Z,2018-10-15T04:51:16Z,OWNER,"Okay, I've compared the following options to visit a single variant with 7 alternatives.
 - handwritten `switch`
 - recursive `switch_visit` with `N=4`
 - recursive `switch_visit` with `N=8`
 - `mpark::visit`

|                             | Clang 7 | GCC 8.2 |
|-----------------|------|-----|
| `switch`  | [HS-C] | [HS-G] |
| `switch_visit, N=4` | [SV4-C] | [SV4-G] |
| `switch_visit, N=8` | [SV8-C] | [SV8-G] |
| `mpark::visit` | [MV-C] | [MV-G] |

__Results (roughly.. in terms of code size/quality)__:
  - Clang: [HS-C] == [SV4-C] == [SV8-C] < [MV-C]
  - GCC: [HS-G] == [SV8-G] < [SV4-G] < [MV-G]

GCC doesn't seem to know how to unroll the recursive call in case it's needed, but I think that's fine as it's still better than the current manual jump table approach.

[HS-C]: https://godbolt.org/z/SGsCL_/
[HS-G]: https://godbolt.org/z/qzMlIw
[SV4-C]: https://godbolt.org/z/oTr2GN
[SV4-G]: https://godbolt.org/z/OsnwqU
[SV8-C]: https://godbolt.org/z/KQDZWW
[SV8-G]: https://godbolt.org/z/lV7s0z
[MV-C]: https://godbolt.org/z/8gOIwC
[MV-G]: https://godbolt.org/z/DJgLee",NA,https://api.github.com/repos/mpark/variant/issues/52/comments,https://github.com/mpark/variant/pull/52#issuecomment-429707908,https://api.github.com/repos/mpark/variant/issues/52
mpark,variant,365144784,https://api.github.com/repos/mpark/variant/issues/comments/431683114,431683114,MDEyOklzc3VlQ29tbWVudDQzMTY4MzExNA==,2275240,2018-10-21T16:31:12Z,2018-10-21T16:31:12Z,OWNER,@quicknir: have you had time to look into this stuff?,NA,https://api.github.com/repos/mpark/variant/issues/52/comments,https://github.com/mpark/variant/pull/52#issuecomment-431683114,https://api.github.com/repos/mpark/variant/issues/52
mpark,variant,365144784,https://api.github.com/repos/mpark/variant/issues/comments/432079764,432079764,MDEyOklzc3VlQ29tbWVudDQzMjA3OTc2NA==,10457096,2018-10-23T03:45:44Z,2018-10-23T03:45:44Z,NONE,"@mpark Sorry, got a bit busy, will make time this coming weekend.",NA,https://api.github.com/repos/mpark/variant/issues/52/comments,https://github.com/mpark/variant/pull/52#issuecomment-432079764,https://api.github.com/repos/mpark/variant/issues/52
mpark,variant,365144784,https://api.github.com/repos/mpark/variant/issues/comments/433710924,433710924,MDEyOklzc3VlQ29tbWVudDQzMzcxMDkyNA==,2275240,2018-10-28T14:34:06Z,2018-10-28T14:34:06Z,OWNER,"@quicknir: Cool, no problem! I wanted to share my preference in direction so that we can align our expectations.

- I'd like to pursue the full `switch`-based `visit` solution here, and can the existing mechanism.
- As far as I see, the `switch`-based solution is strictly better for single visitation. Even for GCC which didn't seem to do as much when the # of alternatives is greater than `N`.
- We can build the N-ary visitation off of the single visitation from above, and I __think__ we'll still get better code-gen than the existing solution.

The following are some experiments with double visitation:
  - Hand-rolled nested `switch`: https://godbolt.org/z/-dtsaG
  - Templated nested `switch`: https://godbolt.org/z/4EYgTp

The fact that the templated nested `switch` generates the same code as the hand-rolled code makes me optimistic about even N-ary visitation performing better than the manual jump table approach.

Anyway, I'm not sure exactly what stage you're in at the moment, but let me know if you disagree so that we can figure out what we can agree to do here!",NA,https://api.github.com/repos/mpark/variant/issues/52/comments,https://github.com/mpark/variant/pull/52#issuecomment-433710924,https://api.github.com/repos/mpark/variant/issues/52
mpark,variant,365144784,https://api.github.com/repos/mpark/variant/issues/comments/433713159,433713159,MDEyOklzc3VlQ29tbWVudDQzMzcxMzE1OQ==,10457096,2018-10-28T14:59:45Z,2018-10-28T14:59:45Z,NONE,"@mpark I am fine with the recursive switch solution you suggested for single visitation. The only caveat is that I think that N is reasonable (say 10, as opposed to like 2) so that in common cases we still get optimal codegen, from gcc as well. N-ary visitation, makes me nervous; I tend to agree the codegen will probably be better, it's just that implementing it for arbitrary ariness seems like it will be significantly more complicated, and will delay things going through, and for significantly less benefit. Neither of the links obviously scales to arbitrary ariness AFAICS. It's possible that some kind of recursive approach would work (peel off the first variant each time, create a new visitor that hardcodes the position of the first variant and visits the rest normally and pass it down recursively).

Ideally (IMHO), we would start with single visitation + diagonal visitation (which IIUC is only used internally for binary operators like ==), get that to a place we're happy with and merge it, and only then start looking at N-ary visitation. That way, in parallel we can look at pushing this approach into the standard libraries. Ultimately getting single + diagonal visitation into standard libraries ASAP should be the main priority here, I think, because this is going to deliver the vast majority of the value to users. That said, I'm open if we can find a good implementation strategy. At the start I was doubtful about arbitrary N for single visitation and all the smart people on this thread changed my mind rather quickly :-).",NA,https://api.github.com/repos/mpark/variant/issues/52/comments,https://github.com/mpark/variant/pull/52#issuecomment-433713159,https://api.github.com/repos/mpark/variant/issues/52
mpark,variant,365144784,https://api.github.com/repos/mpark/variant/issues/comments/433775482,433775482,MDEyOklzc3VlQ29tbWVudDQzMzc3NTQ4Mg==,10457096,2018-10-29T03:08:28Z,2018-10-29T03:08:28Z,NONE,"Okay, updated PR. It should now be doing single variant visitation with arbitrary number of types with recursive switch case. For now I only switch-case on 2 values at a time before recursing, but if you're ok with this approach I can extend to something more reasonable like 10, and also proceed to extend it to binary diagonal visitation. I think we might be pretty close!",NA,https://api.github.com/repos/mpark/variant/issues/52/comments,https://github.com/mpark/variant/pull/52#issuecomment-433775482,https://api.github.com/repos/mpark/variant/issues/52
mpark,variant,365144784,https://api.github.com/repos/mpark/variant/issues/comments/433948310,433948310,MDEyOklzc3VlQ29tbWVudDQzMzk0ODMxMA==,2275240,2018-10-29T15:12:23Z,2018-10-29T15:12:23Z,OWNER,"@quicknir:

> N-ary visitation, makes me nervous; I tend to agree the codegen will probably be better, it's just that implementing it for arbitrary ariness seems like it will be significantly more complicated, and will delay things going through, and for significantly less benefit.

I've put together the following snippet to show that the `N`-ary visitation code is achievable without too much complexity over the previous snippet. I would strongly prefer to replace the existing solution entirely, so that we get strictly better code-gen in general and also to solve problems like #41 for free. I promise it will not delay our progress.. personally I'd be much more excited/engaged if we were to pursue the full solution.

> Neither of the links obviously scales to arbitrary ariness AFAICS.

I was only trying to hint that the generated code for the `N`-ary approach would be optimizable by the compiler with these links.

Anyway, the following is comparison of the options:
 - handwritten (nested) `switch`
 - N-ary `switch_visit` with `N=8`
 - `mpark::visit`

## Single Visitation (7 alternatives):
|                         | Clang 7 | GCC 8.2 |
|---------------|--------|---------|
| `switch`          | [HS1-C] | [HS1-G] |
| `switch_visit`  | [SV1-C] | [SV1-G] |
| `mpark::visit` | [MV1-C] | [MV1-G] |

__Results__:
  - Clang: [HS1-C] == [SV1-C] < [MV1-C]
  - GCC: [HS1-G] ==[SV1-G] < [MV1-G]

[HS1-C]: https://godbolt.org/z/Y0qEQi
[HS1-G]: https://godbolt.org/z/yvHSbF
[SV1-C]: https://godbolt.org/z/v977yw
[SV1-G]: https://godbolt.org/z/wEFCfg
[MV1-C]: https://godbolt.org/z/SqeRNd
[MV1-G]: https://godbolt.org/z/y8-acs

## Double Visitation (5 alternatives):
|                         | Clang 7 | GCC 8.2 |
|---------------|--------|---------|
| `switch`          | [HS2-C] | [HS2-G] |
| `switch_visit`  | [SV2-C] | [SV2-G] |
| `mpark::visit` | [MV2-C] | [MV2-G] |

__Results__:
  - Clang: [HS2-C] == [SV2-C] < [MV2-C]
  - GCC: [HS2-G] ==[SV2-G] < [MV2-G]

[HS2-C]: https://godbolt.org/z/cjrmGO
[HS2-G]: https://godbolt.org/z/-dtsaG
[SV2-C]: https://godbolt.org/z/krMVf_/
[SV2-G]: https://godbolt.org/z/fPhhBk
[MV2-C]: https://godbolt.org/z/GyuxQS
[MV2-G]: https://godbolt.org/z/e1iLlj",NA,https://api.github.com/repos/mpark/variant/issues/52/comments,https://github.com/mpark/variant/pull/52#issuecomment-433948310,https://api.github.com/repos/mpark/variant/issues/52
mpark,variant,365144784,https://api.github.com/repos/mpark/variant/issues/comments/433954128,433954128,MDEyOklzc3VlQ29tbWVudDQzMzk1NDEyOA==,10457096,2018-10-29T15:26:19Z,2018-10-29T15:26:19Z,NONE,"@mpark Sorry, you say ""I put together the following snippet"" but then I'm not sure exactly which snippet you are referring to? My basic idea btw for scaling it to n-ariness is something like this:

```
template <class Visitor, class FirstVariant, class ... Variants>
decltype(auto) switch_visit(Visitor&& visitor, FirstVariant&& first_variant, Variants&& ... variants) {
   switch (first_variant.index()) {
      case(0): return switch_visit(make_recursing_visitor(visitor, *get_if<0>(first_variant)), std::forward<Variants>(variants)...);
      ...
   }
}
```

Basically, this function `make_recursing_visitor` is essentially performing currying for us. There's a lot of details to get right here with perfect forwarding, but I think it's basically doable. Is this what you had in mind for a completely generic solution?",NA,https://api.github.com/repos/mpark/variant/issues/52/comments,https://github.com/mpark/variant/pull/52#issuecomment-433954128,https://api.github.com/repos/mpark/variant/issues/52
mpark,variant,365144784,https://api.github.com/repos/mpark/variant/issues/comments/433964384,433964384,MDEyOklzc3VlQ29tbWVudDQzMzk2NDM4NA==,2275240,2018-10-29T15:51:49Z,2018-10-29T15:54:59Z,OWNER,"@quicknir: Sorry about that. I was referring to the `SV` links in the tables above.

The approach you mention of retrieving the alternative per variant and passing it down
the recursive calls induced push/pop calls from my experiments. It seems better to actually
forward the variants through while building up the indices, and retrieve all of them at the end
in the final dispatch. The following is the full solution inlined:

```cpp
template <bool, typename F, typename... Vs>
struct Switch;

template <typename F, typename... Vs>
struct Switch<false, F, Vs...> {
  template <std::size_t = 0, std::size_t... Is, typename... Vs_>
  static void switch_(std::index_sequence<Is...>, F&&, Vs&&..., Vs_&&...) { __builtin_unreachable(); }
};

template <typename F, typename... Vs>
struct Switch<true, F, Vs...> {
  template <std::size_t B = 0, std::size_t... Is>
  static void switch_(std::index_sequence<Is...>, F&& f, Vs&&... vs) {
    static_assert(B == 0);
    std::invoke(std::forward<F>(f), *mpark::get_if<Is>(&vs)...);
  }

  template <std::size_t B = 0, std::size_t... Is, typename V_, typename... Vs_>
  static void switch_(std::index_sequence<Is...>, F&& f, Vs&&... vs, V_&& v_, Vs_&&... vs_) {
    static constexpr std::size_t size = mpark::variant_size_v<std::decay_t<V_>>;
    switch (v_.index()) {
      case B+0: Switch<B+0 < size, F, Vs...>::switch_(std::index_sequence<Is..., B+0>{}, std::forward<F>(f), std::forward<Vs>(vs)..., std::forward<Vs_>(vs_)...); break;
      case B+1: Switch<B+1 < size, F, Vs...>::switch_(std::index_sequence<Is..., B+1>{}, std::forward<F>(f), std::forward<Vs>(vs)..., std::forward<Vs_>(vs_)...); break;
      case B+2: Switch<B+2 < size, F, Vs...>::switch_(std::index_sequence<Is..., B+2>{}, std::forward<F>(f), std::forward<Vs>(vs)..., std::forward<Vs_>(vs_)...); break;
      case B+3: Switch<B+3 < size, F, Vs...>::switch_(std::index_sequence<Is..., B+3>{}, std::forward<F>(f), std::forward<Vs>(vs)..., std::forward<Vs_>(vs_)...); break;
      case B+4: Switch<B+4 < size, F, Vs...>::switch_(std::index_sequence<Is..., B+4>{}, std::forward<F>(f), std::forward<Vs>(vs)..., std::forward<Vs_>(vs_)...); break;
      case B+5: Switch<B+5 < size, F, Vs...>::switch_(std::index_sequence<Is..., B+5>{}, std::forward<F>(f), std::forward<Vs>(vs)..., std::forward<Vs_>(vs_)...); break;
      case B+6: Switch<B+6 < size, F, Vs...>::switch_(std::index_sequence<Is..., B+6>{}, std::forward<F>(f), std::forward<Vs>(vs)..., std::forward<Vs_>(vs_)...); break;
      case B+7: Switch<B+7 < size, F, Vs...>::switch_(std::index_sequence<Is..., B+7>{}, std::forward<F>(f), std::forward<Vs>(vs)..., std::forward<Vs_>(vs_)...); break;
      default: Switch<B+8 < size, F, Vs...>::template switch_<B+8>(std::index_sequence<Is...>{}, std::forward<F>(f), std::forward<Vs>(vs)..., std::forward<V_>(v_), std::forward<Vs_>(vs_)...); break;
    }
  }
};

template <typename F, typename... Vs>
void switch_visit(F&& f, Vs&&... vs) {
  Switch<true, F, Vs...>::switch_(std::index_sequence<>{}, std::forward<F>(f), std::forward<Vs>(vs)..., std::forward<Vs>(vs)...);
}
```",NA,https://api.github.com/repos/mpark/variant/issues/52/comments,https://github.com/mpark/variant/pull/52#issuecomment-433964384,https://api.github.com/repos/mpark/variant/issues/52
mpark,variant,365144784,https://api.github.com/repos/mpark/variant/issues/comments/433966178,433966178,MDEyOklzc3VlQ29tbWVudDQzMzk2NjE3OA==,10457096,2018-10-29T15:56:17Z,2018-10-29T15:56:17Z,NONE,"@mpark That seems reasonable. Okay, I will put together arbitrary visitation including diagonal visitation in the PR by the end of next weekend. ",NA,https://api.github.com/repos/mpark/variant/issues/52/comments,https://github.com/mpark/variant/pull/52#issuecomment-433966178,https://api.github.com/repos/mpark/variant/issues/52
mpark,variant,365144784,https://api.github.com/repos/mpark/variant/issues/comments/433966589,433966589,MDEyOklzc3VlQ29tbWVudDQzMzk2NjU4OQ==,2275240,2018-10-29T15:57:17Z,2018-10-29T15:57:17Z,OWNER,"I'm not sure if we even need a diagonal special case anymore with this approach actually ðŸ¤”
but I haven't tested that yet.",NA,https://api.github.com/repos/mpark/variant/issues/52/comments,https://github.com/mpark/variant/pull/52#issuecomment-433966589,https://api.github.com/repos/mpark/variant/issues/52
mpark,variant,365144784,https://api.github.com/repos/mpark/variant/issues/comments/433974110,433974110,MDEyOklzc3VlQ29tbWVudDQzMzk3NDExMA==,10457096,2018-10-29T16:16:38Z,2018-10-29T16:18:58Z,NONE,"@mpark Maybe it's not conclusive evidence but this snippet leads me to believe that you still need a diagonal special case to be optimal: https://godbolt.org/z/2Xek12. You can see that when I mark the off-diagonal case no inline, code is still generated for it, even though because of the early exit, it's not possible to reach it. This in turn isn't really surprising because the whole switch operation doesn't get fully inlined into `f`; this means that the whole visit code does not actually have knowledge of the early exit and therefore still has to generate branches etc concerning itself with the off diagonal case.

Note: I think the early exit is actually necessary; not here necessarily for equality but e.g. for `operator<` you can't recover the indices from the types in general, so you'll have to have the branch before the visitation, and only do visitation when indices are equal, but then you'll still have branches entering the non-diagonal cases even though they are unreachable, which is sub-optimal.",NA,https://api.github.com/repos/mpark/variant/issues/52/comments,https://github.com/mpark/variant/pull/52#issuecomment-433974110,https://api.github.com/repos/mpark/variant/issues/52
mpark,variant,365144784,https://api.github.com/repos/mpark/variant/issues/comments/434016529,434016529,MDEyOklzc3VlQ29tbWVudDQzNDAxNjUyOQ==,2275240,2018-10-29T18:02:41Z,2018-10-29T18:02:41Z,OWNER,"I'm not understanding what the purpose of the no-`inline` is... could you explain? I assume we wouldn't mark it `inline`, and also tell the compiler that it's unreachable: https://godbolt.org/z/N-ezqd

Anyway, I'm totally fine with implementing the diagonal case. It should be pretty simple, as it'll essentially be the single visitation code, taking arbitrary # of variants and retrieving the `I`th alternative for all of them.",NA,https://api.github.com/repos/mpark/variant/issues/52/comments,https://github.com/mpark/variant/pull/52#issuecomment-434016529,https://api.github.com/repos/mpark/variant/issues/52
mpark,variant,365144784,https://api.github.com/repos/mpark/variant/issues/comments/434034368,434034368,MDEyOklzc3VlQ29tbWVudDQzNDAzNDM2OA==,10457096,2018-10-29T18:49:49Z,2018-10-29T18:49:49Z,NONE,"@mpark Cool, I think I will just implement diagonal side-by-side with the snippet above and look at the assembly. I also realized some other improvements in your approach compared to my PR :-). Feeding the B+K < size into the template parameter is clever for sure. I'll try to put it all together by the end of the coming weekend like I said and hopefully we can move along.",NA,https://api.github.com/repos/mpark/variant/issues/52/comments,https://github.com/mpark/variant/pull/52#issuecomment-434034368,https://api.github.com/repos/mpark/variant/issues/52
mpark,variant,365144784,https://api.github.com/repos/mpark/variant/issues/comments/435639242,435639242,MDEyOklzc3VlQ29tbWVudDQzNTYzOTI0Mg==,10457096,2018-11-04T03:32:53Z,2018-11-04T03:32:53Z,NONE,"@mpark Ok tired for today but made some good progress. I was able to take your snippet and resolve both the return type issue, and get rid of double-passing all the variants. The build and unit tests all passed for me; currently it's using recursive switch-case for all non-diagonal visitation. So we're in good shape! I'll look more at diagonal visitation and make a decision there, and add commits taking care of that and removing the dead code.

For now, the number of cases before recursion is just 2; I lowered it to reduce the size of errors while working :-). We can discuss the correct number at the end; because of the macro I use it's very very easy to change the number of cases.",NA,https://api.github.com/repos/mpark/variant/issues/52/comments,https://github.com/mpark/variant/pull/52#issuecomment-435639242,https://api.github.com/repos/mpark/variant/issues/52
mpark,variant,365144784,https://api.github.com/repos/mpark/variant/issues/comments/437118024,437118024,MDEyOklzc3VlQ29tbWVudDQzNzExODAyNA==,2275240,2018-11-08T19:04:44Z,2018-11-08T19:15:17Z,OWNER,"@quicknir: are you sure the tests pass...? It doesn't seem to compile for me ðŸ˜•
```
../include/mpark/variant.hpp:511:38: error: constexpr variable 'size' must be initialized by a constant expression
        constexpr std::size_t size = v_.size();
                                     ^~~~~~~~~
```",NA,https://api.github.com/repos/mpark/variant/issues/52/comments,https://github.com/mpark/variant/pull/52#issuecomment-437118024,https://api.github.com/repos/mpark/variant/issues/52
mpark,variant,365144784,https://api.github.com/repos/mpark/variant/issues/comments/437121133,437121133,MDEyOklzc3VlQ29tbWVudDQzNzEyMTEzMw==,10457096,2018-11-08T19:14:33Z,2018-11-08T19:14:33Z,NONE,"@mpark I only compiled locally with 17, so perhaps that is the issue? `v_.size()` is actually calling a static constexpr member of `v_`'s class, so it should still compile even if `v` itself is not a constexpr expression. That seems like a compiler bug?

It's not infinite recursion because what happen is that `V_` gets added to the list of types that the struct is templated. Basically, the list of types in `Switch` are the types that have not yet been recursed through; that's why the initial call doesn't give it any types at all. Each recurse a type is added to that list. The termination of the recursion happens when `Switch` is templated on the full type list, at that point the `switch_` overload with no extra arguments gets selected.",NA,https://api.github.com/repos/mpark/variant/issues/52/comments,https://github.com/mpark/variant/pull/52#issuecomment-437121133,https://api.github.com/repos/mpark/variant/issues/52
mpark,variant,365144784,https://api.github.com/repos/mpark/variant/issues/comments/437225495,437225495,MDEyOklzc3VlQ29tbWVudDQzNzIyNTQ5NQ==,10457096,2018-11-09T02:12:18Z,2018-11-09T02:12:18Z,NONE,"Okay, so I think I've convinced myself that a separate diagonal implementation is not necessary. Here was my quick implementation of diagonal visitation: https://godbolt.org/z/JHkwE7. It doesn't seem to generate markedly better code, though it's hard to say. The current approach actually tends to encourage more inlining because there's more complex templating; with the diagonal visitor it didn't tend to inline so much.

At any rate I think it's close enough we can justify doing the simple thing and just not having a diagonal implementation.",NA,https://api.github.com/repos/mpark/variant/issues/52/comments,https://github.com/mpark/variant/pull/52#issuecomment-437225495,https://api.github.com/repos/mpark/variant/issues/52
mpark,variant,365144784,https://api.github.com/repos/mpark/variant/issues/comments/452646168,452646168,MDEyOklzc3VlQ29tbWVudDQ1MjY0NjE2OA==,2275240,2019-01-09T10:22:41Z,2019-01-09T10:22:41Z,OWNER,Closed by #56,NA,https://api.github.com/repos/mpark/variant/issues/52/comments,https://github.com/mpark/variant/pull/52#issuecomment-452646168,https://api.github.com/repos/mpark/variant/issues/52
mpark,variant,364794315,https://api.github.com/repos/mpark/variant/issues/comments/425560191,425560191,MDEyOklzc3VlQ29tbWVudDQyNTU2MDE5MQ==,2275240,2018-09-28T20:43:12Z,2018-09-28T20:43:12Z,OWNER,"Hey @quicknir, thanks for bringing this up! As far as I know, Abseil `variant` performs this optimization and it seems pretty clear that it does help. I haven't been able to make time to implement it myself, but I would love to see it implemented! We should be able to measure at least the resulting binary size within my `variant` benchmark framework: https://mpark.github.io/variant

Are you considering implementing this?",NA,https://api.github.com/repos/mpark/variant/issues/51/comments,https://github.com/mpark/variant/issues/51#issuecomment-425560191,https://api.github.com/repos/mpark/variant/issues/51
mpark,variant,364794315,https://api.github.com/repos/mpark/variant/issues/comments/425567585,425567585,MDEyOklzc3VlQ29tbWVudDQyNTU2NzU4NQ==,10457096,2018-09-28T21:12:34Z,2018-09-28T21:12:34Z,NONE,"Yes definitely. I'll try to look at abseil code in godbolt too, just to get a sense and make sure I end up with equally good assembly. Maybe it's easier if I start with trying to do a PR here, then looking at standard libraries? ",NA,https://api.github.com/repos/mpark/variant/issues/51/comments,https://github.com/mpark/variant/issues/51#issuecomment-425567585,https://api.github.com/repos/mpark/variant/issues/51
mpark,variant,364794315,https://api.github.com/repos/mpark/variant/issues/comments/429003573,429003573,MDEyOklzc3VlQ29tbWVudDQyOTAwMzU3Mw==,10457096,2018-10-11T15:38:28Z,2018-10-11T15:38:28Z,NONE,"@mpark Hey, I opened a PR a while ago (https://github.com/mpark/variant/pull/52), just pinging to draw attention.",NA,https://api.github.com/repos/mpark/variant/issues/51/comments,https://github.com/mpark/variant/issues/51#issuecomment-429003573,https://api.github.com/repos/mpark/variant/issues/51
mpark,variant,364794315,https://api.github.com/repos/mpark/variant/issues/comments/429023211,429023211,MDEyOklzc3VlQ29tbWVudDQyOTAyMzIxMQ==,2275240,2018-10-11T16:27:26Z,2018-10-11T16:27:26Z,OWNER,Thanks for the ping! Looking...,NA,https://api.github.com/repos/mpark/variant/issues/51/comments,https://github.com/mpark/variant/issues/51#issuecomment-429023211,https://api.github.com/repos/mpark/variant/issues/51
mpark,variant,364794315,https://api.github.com/repos/mpark/variant/issues/comments/452646288,452646288,MDEyOklzc3VlQ29tbWVudDQ1MjY0NjI4OA==,2275240,2019-01-09T10:23:05Z,2019-01-09T10:23:05Z,OWNER,Closed by #52 and #56 ,NA,https://api.github.com/repos/mpark/variant/issues/51/comments,https://github.com/mpark/variant/issues/51#issuecomment-452646288,https://api.github.com/repos/mpark/variant/issues/51
mpark,variant,354913635,https://api.github.com/repos/mpark/variant/issues/comments/417044425,417044425,MDEyOklzc3VlQ29tbWVudDQxNzA0NDQyNQ==,493463,2018-08-29T17:50:48Z,2018-08-29T17:50:48Z,CONTRIBUTOR,"Hm, the CI failures seem spurious... is that normal?",NA,https://api.github.com/repos/mpark/variant/issues/49/comments,https://github.com/mpark/variant/pull/49#issuecomment-417044425,https://api.github.com/repos/mpark/variant/issues/49
mpark,variant,354913635,https://api.github.com/repos/mpark/variant/issues/comments/417154608,417154608,MDEyOklzc3VlQ29tbWVudDQxNzE1NDYwOA==,2275240,2018-08-30T00:54:53Z,2018-08-30T00:54:53Z,OWNER,Hm.. I need to take a look at the Travis CI. It seems to have been failing with a minor version update or something. Thanks for the patch!,NA,https://api.github.com/repos/mpark/variant/issues/49/comments,https://github.com/mpark/variant/pull/49#issuecomment-417154608,https://api.github.com/repos/mpark/variant/issues/49
mpark,variant,354913635,https://api.github.com/repos/mpark/variant/issues/comments/423820265,423820265,MDEyOklzc3VlQ29tbWVudDQyMzgyMDI2NQ==,4967343,2018-09-23T14:23:41Z,2018-09-23T14:23:41Z,NONE,"Hi! It would be great to see an update on this PR - I'm also affected by this, and more people in #48, basically everyone that has updated to VS 15.8.",NA,https://api.github.com/repos/mpark/variant/issues/49/comments,https://github.com/mpark/variant/pull/49#issuecomment-423820265,https://api.github.com/repos/mpark/variant/issues/49
mpark,variant,354913635,https://api.github.com/repos/mpark/variant/issues/comments/426408146,426408146,MDEyOklzc3VlQ29tbWVudDQyNjQwODE0Ng==,4967343,2018-10-02T19:54:38Z,2018-10-02T19:54:38Z,NONE,"By the way I think a slight improvement to this PR could be made: MSVC 19.11 up to 19.14 actually worked, it was only 19.15 where this broke. So I think `MPARK_CPP14_CONSTEXPR` could still be enabled, except for 19.15. But not sure it's worth it, since usually people should just be on the latest minor VS version anyway.",NA,https://api.github.com/repos/mpark/variant/issues/49/comments,https://github.com/mpark/variant/pull/49#issuecomment-426408146,https://api.github.com/repos/mpark/variant/issues/49
mpark,variant,354913635,https://api.github.com/repos/mpark/variant/issues/comments/454006443,454006443,MDEyOklzc3VlQ29tbWVudDQ1NDAwNjQ0Mw==,2275240,2019-01-14T13:35:40Z,2019-01-14T13:35:40Z,OWNER,Fixed the root cause in 657110d5ea11f344d29bce1d9609c7d76696cb64,NA,https://api.github.com/repos/mpark/variant/issues/49/comments,https://github.com/mpark/variant/pull/49#issuecomment-454006443,https://api.github.com/repos/mpark/variant/issues/49
mpark,variant,350801444,https://api.github.com/repos/mpark/variant/issues/comments/413565607,413565607,MDEyOklzc3VlQ29tbWVudDQxMzU2NTYwNw==,123798,2018-08-16T14:29:31Z,2018-08-16T14:29:31Z,NONE,Probably a compiler bug. Someone already reported a [similar problem](https://developercommunity.visualstudio.com/content/problem/275141/c2131-expression-did-not-evaluate-to-a-constant-fo.html).,NA,https://api.github.com/repos/mpark/variant/issues/48/comments,https://github.com/mpark/variant/issues/48#issuecomment-413565607,https://api.github.com/repos/mpark/variant/issues/48
mpark,variant,350801444,https://api.github.com/repos/mpark/variant/issues/comments/415943280,415943280,MDEyOklzc3VlQ29tbWVudDQxNTk0MzI4MA==,795851,2018-08-25T06:21:21Z,2018-08-25T06:21:21Z,NONE,I can confirm this. I'm receiving the same error with Visual Studio Community 2017 15.8.1,NA,https://api.github.com/repos/mpark/variant/issues/48/comments,https://github.com/mpark/variant/issues/48#issuecomment-415943280,https://api.github.com/repos/mpark/variant/issues/48
mpark,variant,350801444,https://api.github.com/repos/mpark/variant/issues/comments/454006523,454006523,MDEyOklzc3VlQ29tbWVudDQ1NDAwNjUyMw==,2275240,2019-01-14T13:35:56Z,2019-01-14T13:35:56Z,OWNER,Fixed the root cause in 657110d5ea11f344d29bce1d9609c7d76696cb64,NA,https://api.github.com/repos/mpark/variant/issues/48/comments,https://github.com/mpark/variant/issues/48#issuecomment-454006523,https://api.github.com/repos/mpark/variant/issues/48
mpark,variant,350801444,https://api.github.com/repos/mpark/variant/issues/comments/503986581,503986581,MDEyOklzc3VlQ29tbWVudDUwMzk4NjU4MQ==,47866000,2019-06-20T11:18:50Z,2019-06-20T11:18:50Z,NONE,So... not a compiler bug?,NA,https://api.github.com/repos/mpark/variant/issues/48/comments,https://github.com/mpark/variant/issues/48#issuecomment-503986581,https://api.github.com/repos/mpark/variant/issues/48
mpark,variant,350801444,https://api.github.com/repos/mpark/variant/issues/comments/554437786,554437786,MDEyOklzc3VlQ29tbWVudDU1NDQzNzc4Ng==,5039800,2019-11-15T16:47:54Z,2019-11-15T16:47:54Z,NONE,"> So... not a compiler bug?

Read the commit message.",NA,https://api.github.com/repos/mpark/variant/issues/48/comments,https://github.com/mpark/variant/issues/48#issuecomment-554437786,https://api.github.com/repos/mpark/variant/issues/48
mpark,variant,333722269,https://api.github.com/repos/mpark/variant/issues/comments/454051098,454051098,MDEyOklzc3VlQ29tbWVudDQ1NDA1MTA5OA==,2275240,2019-01-14T15:44:51Z,2019-01-14T15:44:51Z,OWNER,@yujin-lai: Is this still an issue?,NA,https://api.github.com/repos/mpark/variant/issues/47/comments,https://github.com/mpark/variant/issues/47#issuecomment-454051098,https://api.github.com/repos/mpark/variant/issues/47
mpark,variant,333722269,https://api.github.com/repos/mpark/variant/issues/comments/454109628,454109628,MDEyOklzc3VlQ29tbWVudDQ1NDEwOTYyOA==,40398186,2019-01-14T18:27:56Z,2019-01-14T18:27:56Z,NONE,"Sorry, I don't know now. I have switched to another variant because of the issue.
",NA,https://api.github.com/repos/mpark/variant/issues/47/comments,https://github.com/mpark/variant/issues/47#issuecomment-454109628,https://api.github.com/repos/mpark/variant/issues/47
mpark,variant,333722269,https://api.github.com/repos/mpark/variant/issues/comments/454159644,454159644,MDEyOklzc3VlQ29tbWVudDQ1NDE1OTY0NA==,2275240,2019-01-14T21:00:43Z,2019-01-14T21:01:00Z,OWNER,"Ah okay. Sorry I couldn't address the issue for you earlier, but I'm glad you found something that works for you.",NA,https://api.github.com/repos/mpark/variant/issues/47/comments,https://github.com/mpark/variant/issues/47#issuecomment-454159644,https://api.github.com/repos/mpark/variant/issues/47
mpark,variant,332924953,https://api.github.com/repos/mpark/variant/issues/comments/398230855,398230855,MDEyOklzc3VlQ29tbWVudDM5ODIzMDg1NQ==,2275240,2018-06-18T23:53:27Z,2018-06-18T23:53:27Z,OWNER,"Haha, while Googling around for this issue, I found https://github.com/akrzemi1/Optional/issues/57 along with the 2 links that Andrzej referred to. It seems like you're planning to figure out what to do from UE side of things? Seems like there's not much I can do from my end, aside from say, ignoring those warnings myself.",NA,https://api.github.com/repos/mpark/variant/issues/46/comments,https://github.com/mpark/variant/issues/46#issuecomment-398230855,https://api.github.com/repos/mpark/variant/issues/46
mpark,variant,310146872,https://api.github.com/repos/mpark/variant/issues/comments/377820280,377820280,MDEyOklzc3VlQ29tbWVudDM3NzgyMDI4MA==,11347482,2018-04-01T22:04:48Z,2018-04-01T22:04:48Z,NONE,"Traced it down a bit, seems to be a consequence of this GCC bug:
https://gcc.gnu.org/bugzilla/show_bug.cgi?id=80654
So probably nothing this code can do about it. (Well, not easily; apparently GCC's std::variant doesn't hit the bug, but its implementation is probably too different.)",NA,https://api.github.com/repos/mpark/variant/issues/45/comments,https://github.com/mpark/variant/issues/45#issuecomment-377820280,https://api.github.com/repos/mpark/variant/issues/45
mpark,variant,309527212,https://api.github.com/repos/mpark/variant/issues/comments/377075343,377075343,MDEyOklzc3VlQ29tbWVudDM3NzA3NTM0Mw==,2275240,2018-03-28T23:49:30Z,2018-03-28T23:49:37Z,OWNER,"In order for this technique to work, you need `if constexpr`, which you don't have prior to C++17.",NA,https://api.github.com/repos/mpark/variant/issues/44/comments,https://github.com/mpark/variant/issues/44#issuecomment-377075343,https://api.github.com/repos/mpark/variant/issues/44
mpark,variant,309527212,https://api.github.com/repos/mpark/variant/issues/comments/377215264,377215264,MDEyOklzc3VlQ29tbWVudDM3NzIxNTI2NA==,7946730,2018-03-29T12:10:14Z,2018-03-29T12:10:14Z,NONE,"@mpark , thank you for you time.

So without VS 2017 I'm stuck using `functor` ? But If I use lambda, it will be **WITHOUT** types validation at compile time ?",NA,https://api.github.com/repos/mpark/variant/issues/44/comments,https://github.com/mpark/variant/issues/44#issuecomment-377215264,https://api.github.com/repos/mpark/variant/issues/44
mpark,variant,309527212,https://api.github.com/repos/mpark/variant/issues/comments/377282135,377282135,MDEyOklzc3VlQ29tbWVudDM3NzI4MjEzNQ==,5955594,2018-03-29T15:55:09Z,2018-03-29T15:55:09Z,NONE,"For C++14, You have a few options.  If you want to pattern match directly on types, you can create a class that does the type dispatch (at compile time of course).  So usage is like this:


    mpark::visit(overloaded([](int a) { cout << a / 2 << endl; }, 
                            [](string& a) { cout << a + ""\n""
                                                 << endl; },
                            [](auto&) {
                                cout << â€œCatch all\n"" << endl;
                            }), someVariant);


Simply remove the catch all case i.e. the [] (auto&) and you will get a compile time error if you donâ€™t have  a function for every type of the variant.  A basic implementation of the overloaded class is below, itâ€™s got some issues as I just hammered it out.  For example it will only accept R-values meaning that you cannot pass in a variable that is assigned to the lambda, you must declare the lambda inline instead as shown above.

You can also make another class something like staticIf that would allow you to have more complex pattern matches than just type matching.  This would allow conditions like std::is_base_of to be tested at compile time.  I havenâ€™t included it here but lmk me if you are interested.  It would be something like
staticIf<condition>(lambda)
    .otherwiseIf<condition>(lambda)
    .otherwise(lambda);


Hereâ€™s the overloaded class.

template <class... Fs>
struct OverLoaded;

template <class F1>
struct OverLoaded<F1> : F1 {
    using F1::operator();

    OverLoaded(F1&& head) : F1(std::forward<F1>(head)) {}
};

template <class F1, class... Fs>
struct OverLoaded<F1, Fs...> : F1, OverLoaded<Fs...> {
    using F1::operator();
    using OverLoaded<Fs...>::operator();

    OverLoaded(F1&& head, Fs&&... rest)
        : F1(std::forward<F1>(head)),
          OverLoaded<Fs...>(std::forward<Fs>(rest)...) {}
};

template <class... Fs>
OverLoaded<Fs...> overloaded(Fs&&... fs) {
    return OverLoaded<Fs...>(std::forward<Fs>(fs)...);
}
  







> On 29 Mar 2018, at 13:10, Martin Beaudet <notifications@github.com> wrote:
> 
> @mpark <https://github.com/mpark> , thank you for you time.
> 
> So without VS 2017 I'm stuck using functor ? But If I use lambda, it will be WITHOUT types validation at compile time ?
> 
> â€”
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub <https://github.com/mpark/variant/issues/44#issuecomment-377215264>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AFrgCpag4Pw_79QxLXBfQ-EGl0jkIpT5ks5tjM8mgaJpZM4S_WGi>.
> 

",NA,https://api.github.com/repos/mpark/variant/issues/44/comments,https://github.com/mpark/variant/issues/44#issuecomment-377282135,https://api.github.com/repos/mpark/variant/issues/44
mpark,variant,309527212,https://api.github.com/repos/mpark/variant/issues/comments/377333877,377333877,MDEyOklzc3VlQ29tbWVudDM3NzMzMzg3Nw==,7946730,2018-03-29T18:43:51Z,2018-03-29T18:43:51Z,NONE,"@SaadAttieh thank you.

I tried with

```c++
template <class... Fs>
struct OverLoaded;

template <class F1>
struct OverLoaded<F1> : F1 {
    using F1::operator();

    OverLoaded(F1&& head) : F1(std::forward<F1>(head)) {}
};

template <class F1, class... Fs>
struct OverLoaded<F1, Fs...> : F1, OverLoaded<Fs...> {
    using F1::operator();
    using OverLoaded<Fs...>::operator();

    OverLoaded(F1&& head, Fs&&... rest)
        : F1(std::forward<F1>(head)),
        OverLoaded<Fs...>(std::forward<Fs>(rest)...) {}
};

template <class... Fs>
OverLoaded<Fs...> overloaded(Fs&&... fs) {
    return OverLoaded<Fs...>(std::forward<Fs>(fs)...);
}
typedef mpark::variant<
    int8_t, uint8_t,
    double,
    std::string
> VariantValue;

VariantValue v((double)2.0);
```

Doing this compile fine and work great.
```c++
mpark::visit(overloaded(
    [](int a) { std::cout << a / 2 << std::endl; },
    [](std::string& a) { std::cout << a + ""\n"" << std::endl; }//,
    [](auto&) { std::cout << ""Catch all\n"" << std::endl; }
), v);
```

But this compile fine too but I'm getting a warning instead of a compilation error.
```c++
mpark::visit(overloaded(
    [](int a) { std::cout << a / 2 << std::endl; },
    [](std::string& a) { std::cout << a + ""\n"" << std::endl; }//,
    //[](auto&) { std::cout << ""Catch all\n"" << std::endl; }
), v);
```

The warning
```
1>d:\xxx\include\mpark\lib.hpp(237): warning C4244: 'argument': conversion from 'double' to 'int', possible loss of data
1>  d:\xxx\include\mpark\variant.hpp(684): note: see reference to function template instantiation 'void mpark::lib::cpp17::invoke<_Ty,T&>(F &&,T &)' being compiled
1>          with
1>          [
1>              _Ty=OverLoaded<main::<lambda_31e307beed92b69987871195c711937f>,main::<lambda_d72897b211ce01db93a67d525626e679>>,
1>              T=double,
1>              F=OverLoaded<main::<lambda_31e307beed92b69987871195c711937f>,main::<lambda_d72897b211ce01db93a67d525626e679>>
1>          ]
...
``` ",NA,https://api.github.com/repos/mpark/variant/issues/44/comments,https://github.com/mpark/variant/issues/44#issuecomment-377333877,https://api.github.com/repos/mpark/variant/issues/44
mpark,variant,309527212,https://api.github.com/repos/mpark/variant/issues/comments/377419044,377419044,MDEyOklzc3VlQ29tbWVudDM3NzQxOTA0NA==,5955594,2018-03-30T01:24:03Z,2018-03-30T01:24:03Z,NONE,"I believe this is the beautiful part of C++ that inherits a lot of C behaviour :).  It annoyingly allows implicit conversions (all be it with a warning) from int to unsigned int.  

If you are not aware doing something like 
int a;
unsigned int b = a;
tells the compiler to create a temporary unsigned int value, convert that int to the unsigned value and then store that temporary value in b.

In your specific case, the simplest fix is to make it take the int by reference.  This is because an int reference cannot bind to a temporary value.   Hence  no implicit conversion.

> mpark::visit(overloaded(
>     [](int& a) { std::cout << a / 2 << std::endl; },
>     [](std::string& a) { std::cout << a + ""\n"" << std::endl; }//,
>     [](auto&) { std::cout << ""Catch all\n"" << std::endl; }
> ), v);


now this should invoke the catch all case.  And if you remove that catch all case you will get a compile time error.  Just be careful though, if one day you try a const reference so 
[] (const int& a)
then this unfortunately will still allow the implicit conversion as const reference is able to bind to a temporary.

This is a general issue with having multiple number types in a variant.  As I said, the alternative is using the static if I previously demonstrated, it is closest to the if constexpr () mechanism described by @mpark.  Itâ€™s just a bit messy so have chosen not to try producing an implementation.  If you need it I will resurrect it from an old project.  Itâ€™s functionality remember is something like

//method of removing references and const, etc from types

    typename T > using BaseType =
        typename std::remove_cv<typename std::remove_reference<T>::type>::type;

    mpark::visit([] (auto& a) {
        return staticIf < std::is_same < int,
               BaseType<decltype(a)>([](auto& a) { /* a is int*/; })
                       .otherwiseIf < std::is_same < double,
               BaseType<decltype(a)>([](auto& a) { /* a is double*/ })(a);
    ), variant);
 
 
 
  

 
> On 29 Mar 2018, at 19:43, Martin Beaudet <notifications@github.com> wrote:
> 
> @SaadAttieh <https://github.com/SaadAttieh> thank you.
> 
> I tried with
> 
> template <class... Fs>
> struct OverLoaded;
> 
> template <class F1>
> struct OverLoaded<F1> : F1 {
>     using F1::operator();
> 
>     OverLoaded(F1&& head) : F1(std::forward<F1>(head)) {}
> };
> 
> template <class F1, class... Fs>
> struct OverLoaded<F1, Fs...> : F1, OverLoaded<Fs...> {
>     using F1::operator();
>     using OverLoaded<Fs...>::operator();
> 
>     OverLoaded(F1&& head, Fs&&... rest)
>         : F1(std::forward<F1>(head)),
>         OverLoaded<Fs...>(std::forward<Fs>(rest)...) {}
> };
> 
> template <class... Fs>
> OverLoaded<Fs...> overloaded(Fs&&... fs) {
>     return OverLoaded<Fs...>(std::forward<Fs>(fs)...);
> }
> typedef mpark::variant<
>     int8_t, uint8_t,
>     double,
>     std::string
> > VariantValue;
> 
> VariantValue v((double)2.0);
> Doing this compile fine and work great.
> 
> mpark::visit(overloaded(
>     [](int a) { std::cout << a / 2 << std::endl; },
>     [](std::string& a) { std::cout << a + ""\n"" << std::endl; }//,
>     [](auto&) { std::cout << ""Catch all\n"" << std::endl; }
> ), v);
> But this compile fine too but I'm getting a warning instead of a compilation error.
> 
> mpark::visit(overloaded(
>     [](int a) { std::cout << a / 2 << std::endl; },
>     [](std::string& a) { std::cout << a + ""\n"" << std::endl; }//,
>     //[](auto&) { std::cout << ""Catch all\n"" << std::endl; }
> ), v);
> The warning
> 
> 1>d:\xxx\include\mpark\lib.hpp(237): warning C4244: 'argument': conversion from 'double' to 'int', possible loss of data
> 1>  d:\xxx\include\mpark\variant.hpp(684): note: see reference to function template instantiation 'void mpark::lib::cpp17::invoke<_Ty,T&>(F &&,T &)' being compiled
> 1>          with
> 1>          [
> 1>              _Ty=OverLoaded<main::<lambda_31e307beed92b69987871195c711937f>,main::<lambda_d72897b211ce01db93a67d525626e679>>,
> 1>              T=double,
> 1>              F=OverLoaded<main::<lambda_31e307beed92b69987871195c711937f>,main::<lambda_d72897b211ce01db93a67d525626e679>>
> 1>          ]
> ...
> â€”
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub <https://github.com/mpark/variant/issues/44#issuecomment-377333877>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AFrgCs5gTK56Bvv9YjNu4dQlooucSJB6ks5tjStogaJpZM4S_WGi>.
> 

",NA,https://api.github.com/repos/mpark/variant/issues/44/comments,https://github.com/mpark/variant/issues/44#issuecomment-377419044,https://api.github.com/repos/mpark/variant/issues/44
mpark,variant,309527212,https://api.github.com/repos/mpark/variant/issues/comments/377419361,377419361,MDEyOklzc3VlQ29tbWVudDM3NzQxOTM2MQ==,5955594,2018-03-30T01:26:29Z,2018-03-30T01:26:29Z,NONE,"p.s. in my first lines I  meant
unsigned int b = a. means create a temporary unsigned int value, convert the value of the signed int to unsignedâ€¦

 

> On 30 Mar 2018, at 02:23, Saad Attieh <saad_attieh@me.com> wrote:
> 
> I believe this is the beautiful part of C++ that inherits a lot of C behaviour :).  It annoyingly allows implicit conversions (all be it with a warning) from int to unsigned int.  
> 
> If you are not aware doing something like 
> int a;
> unsigned int b = a;
> tells the compiler to create a temporary unsigned int value, convert that int to the unsigned value and then store that temporary value in b.
> 
> In your specific case, the simplest fix is to make it take the int by reference.  This is because an int reference cannot bind to a temporary value.   Hence  no implicit conversion.
> 
>> mpark::visit(overloaded(
>>     [](int& a) { std::cout << a / 2 << std::endl; },
>>     [](std::string& a) { std::cout << a + ""\n"" << std::endl; }//,
>>     [](auto&) { std::cout << ""Catch all\n"" << std::endl; }
>> ), v);
> 
> 
> now this should invoke the catch all case.  And if you remove that catch all case you will get a compile time error.  Just be careful though, if one day you try a const reference so 
> [] (const int& a)
> then this unfortunately will still allow the implicit conversion as const reference is able to bind to a temporary.
> 
> This is a general issue with having multiple number types in a variant.  As I said, the alternative is using the static if I previously demonstrated, it is closest to the if constexpr () mechanism described by @mpark.  Itâ€™s just a bit messy so have chosen not to try producing an implementation.  If you need it I will resurrect it from an old project.  Itâ€™s functionality remember is something like
> 
> //method of removing references and const, etc from types
> 
>     typename T > using BaseType =
>         typename std::remove_cv<typename std::remove_reference<T>::type>::type;
> 
>     mpark::visit([] (auto& a) {
>         return staticIf < std::is_same < int,
>                BaseType<decltype(a)>([](auto& a) { /* a is int*/; })
>                        .otherwiseIf < std::is_same < double,
>                BaseType<decltype(a)>([](auto& a) { /* a is double*/ })(a);
>     ), variant);
>  
>  
>  
>   
> 
>  
>> On 29 Mar 2018, at 19:43, Martin Beaudet <notifications@github.com <mailto:notifications@github.com>> wrote:
>> 
>> @SaadAttieh <https://github.com/SaadAttieh> thank you.
>> 
>> I tried with
>> 
>> template <class... Fs>
>> struct OverLoaded;
>> 
>> template <class F1>
>> struct OverLoaded<F1> : F1 {
>>     using F1::operator();
>> 
>>     OverLoaded(F1&& head) : F1(std::forward<F1>(head)) {}
>> };
>> 
>> template <class F1, class... Fs>
>> struct OverLoaded<F1, Fs...> : F1, OverLoaded<Fs...> {
>>     using F1::operator();
>>     using OverLoaded<Fs...>::operator();
>> 
>>     OverLoaded(F1&& head, Fs&&... rest)
>>         : F1(std::forward<F1>(head)),
>>         OverLoaded<Fs...>(std::forward<Fs>(rest)...) {}
>> };
>> 
>> template <class... Fs>
>> OverLoaded<Fs...> overloaded(Fs&&... fs) {
>>     return OverLoaded<Fs...>(std::forward<Fs>(fs)...);
>> }
>> typedef mpark::variant<
>>     int8_t, uint8_t,
>>     double,
>>     std::string
>> > VariantValue;
>> 
>> VariantValue v((double)2.0);
>> Doing this compile fine and work great.
>> 
>> mpark::visit(overloaded(
>>     [](int a) { std::cout << a / 2 << std::endl; },
>>     [](std::string& a) { std::cout << a + ""\n"" << std::endl; }//,
>>     [](auto&) { std::cout << ""Catch all\n"" << std::endl; }
>> ), v);
>> But this compile fine too but I'm getting a warning instead of a compilation error.
>> 
>> mpark::visit(overloaded(
>>     [](int a) { std::cout << a / 2 << std::endl; },
>>     [](std::string& a) { std::cout << a + ""\n"" << std::endl; }//,
>>     //[](auto&) { std::cout << ""Catch all\n"" << std::endl; }
>> ), v);
>> The warning
>> 
>> 1>d:\xxx\include\mpark\lib.hpp(237): warning C4244: 'argument': conversion from 'double' to 'int', possible loss of data
>> 1>  d:\xxx\include\mpark\variant.hpp(684): note: see reference to function template instantiation 'void mpark::lib::cpp17::invoke<_Ty,T&>(F &&,T &)' being compiled
>> 1>          with
>> 1>          [
>> 1>              _Ty=OverLoaded<main::<lambda_31e307beed92b69987871195c711937f>,main::<lambda_d72897b211ce01db93a67d525626e679>>,
>> 1>              T=double,
>> 1>              F=OverLoaded<main::<lambda_31e307beed92b69987871195c711937f>,main::<lambda_d72897b211ce01db93a67d525626e679>>
>> 1>          ]
>> ...
>> â€”
>> You are receiving this because you were mentioned.
>> Reply to this email directly, view it on GitHub <https://github.com/mpark/variant/issues/44#issuecomment-377333877>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AFrgCs5gTK56Bvv9YjNu4dQlooucSJB6ks5tjStogaJpZM4S_WGi>.
>> 
> 

",NA,https://api.github.com/repos/mpark/variant/issues/44/comments,https://github.com/mpark/variant/issues/44#issuecomment-377419361,https://api.github.com/repos/mpark/variant/issues/44
mpark,variant,309527212,https://api.github.com/repos/mpark/variant/issues/comments/377510002,377510002,MDEyOklzc3VlQ29tbWVudDM3NzUxMDAwMg==,7946730,2018-03-30T11:59:25Z,2018-03-30T11:59:25Z,NONE,"Initially the variant I'm using it's this one
```c++
typedef mpark::variant<
    bool,
    int8_t, uint8_t,
    int16_t, uint16_t,
    int32_t, uint32_t,
    int64_t, uint64_t,
    float, double,
    std::string
> VariantValue;
```

I ask my question without putting all types in the variant because I though the 3 initial one was enough to expose my problem. 

If I'm using **reference** instead of value on Visual Studio 2015 Update 3, I'm still not getting compilation error when commenting the **catch all** instruction.

```c++

VariantValue v((double)2.0);

mpark::visit(overloaded(
    [](const uint8_t& a) { std::cout << a / 2 << std::endl; },
    [](const std::string& a) { std::cout << a + ""\n"" << std::endl; }//,
    //[](auto&) { std::cout << ""Catch all\n"" << std::endl; }
), v);

// I'm still getting the same warning about 
// Warning C4244	'argument': conversion from 'double' to 'const uint8_t', possible loss of data
```

I think I will have to go with the functor and forget about lambda until I get compliant C++17 compiler. As I'm using an old version of [Yocto](https://www.yoctoproject.org/) and I cannot upgrade to a newer version easily.
",NA,https://api.github.com/repos/mpark/variant/issues/44/comments,https://github.com/mpark/variant/issues/44#issuecomment-377510002,https://api.github.com/repos/mpark/variant/issues/44
mpark,variant,309527212,https://api.github.com/repos/mpark/variant/issues/comments/377512261,377512261,MDEyOklzc3VlQ29tbWVudDM3NzUxMjI2MQ==,5955594,2018-03-30T12:15:10Z,2018-03-30T12:15:10Z,NONE,"
Iâ€™m sorry, not sure why, I have less experience with that platform.  On clang at the moment I get 
g8afbcf97:~ SaadAttieh$ g++ -std=c++14 -Wextra -Wall test.cpp
In file included from test.cpp:3:
./variant/include/mpark/variant.hpp:674:11: error: 
      static_assert failed ""`mpark::visit` requires the visitor to be
      exhaustive.""
My code:

template <class... Fs>
struct OverLoaded;

template <class F1>
struct OverLoaded<F1> : F1 {
    using F1::operator();

    OverLoaded(F1&& head) : F1(std::forward<F1>(head)) {}
};

template <class F1, class... Fs>
struct OverLoaded<F1, Fs...> : F1, OverLoaded<Fs...> {
    using F1::operator();
    using OverLoaded<Fs...>::operator();

    OverLoaded(F1&& head, Fs&&... rest)
        : F1(std::forward<F1>(head)),
          OverLoaded<Fs...>(std::forward<Fs>(rest)...) {}
};

template <class... Fs>
OverLoaded<Fs...> overloaded(Fs&&... fs) {
    return OverLoaded<Fs...>(std::forward<Fs>(fs)...);
}

typedef mpark::variant<bool, int8_t, uint8_t, int16_t, uint16_t, int32_t,
                       uint32_t, int64_t, uint64_t, float, double, std::string>
    VariantValue;

int main() {
    VariantValue a = 1.0;
    mpark::visit(overloaded([](int& a) { std::cout << a / 2 << std::endl; },
                            [](std::string& a) {
                                std::cout << (a + ""\n"") << std::endl;
                            }),
                 a);
}
and I get:




> On 30 Mar 2018, at 12:59, Martin Beaudet <notifications@github.com> wrote:
> 
> Initially the variant I'm using it's this one
> 
> typedef mpark::variant<
>     bool,
>     int8_t, uint8_t,
>     int16_t, uint16_t,
>     int32_t, uint32_t,
>     int64_t, uint64_t,
>     float, double,
>     std::string
> > VariantValue;
> I ask my question without putting all types in the variant because I though the 3 initial one was enough to expose my problem.
> 
> If I'm using reference instead of value on Visual Studio 2015 Update 3, I'm still not getting compilation error when commenting the catch all instruction.
> 
> VariantValue v((double)2.0);
> 
> mpark::visit(overloaded(
>     [](const uint8_t& a) { std::cout << a / 2 << std::endl; },
>     [](const std::string& a) { std::cout << a + ""\n"" << std::endl; }//,
>     //[](auto&) { std::cout << ""Catch all\n"" << std::endl; }
> ), v);
> 
> // I'm still getting the same warning about 
> // Warning C4244	'argument': conversion from 'double' to 'const uint8_t', possible loss of data
> I think I will have to go with the functor and forget about lambda until I get compliant C++17 compiler. As I'm using an old version of Yocto <https://www.yoctoproject.org/> and I cannot upgrade to a newer version easily.
> 
> â€”
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub <https://github.com/mpark/variant/issues/44#issuecomment-377510002>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AFrgCjRvj43yy4CZGCbLynzQ4EGSbl83ks5tjh4egaJpZM4S_WGi>.
> 

",NA,https://api.github.com/repos/mpark/variant/issues/44/comments,https://github.com/mpark/variant/issues/44#issuecomment-377512261,https://api.github.com/repos/mpark/variant/issues/44
mpark,variant,309527212,https://api.github.com/repos/mpark/variant/issues/comments/377632127,377632127,MDEyOklzc3VlQ29tbWVudDM3NzYzMjEyNw==,2275240,2018-03-30T21:54:55Z,2018-03-30T21:56:19Z,OWNER,"@SaadAttieh: Thanks for helping out with this!

@purell: @SaadAttieh's solution isn't working for you because you're taking `const int&`
rather than `int&`. `const int&` will take a `double` whereas `int&` would not. But this
solution only works if you're always visiting l-value variants, and you're forced to forgo
`const` correctness.

If you really want to handle the exact types without implicit conversions, you probably
want a different abstraction than `visit`. For example, you could introduce
a `type_switch` like this:

```cpp
  mpark::variant<int, double> v = 1.1, w = 101;

  // single visitation  
  type_switch(v)(
    [](type<int>, auto&& x) { std::cout << ""int: "" << x << '\n'; },
    [](type<double>, auto&& x) { std::cout << ""double: "" << x << '\n'; });

  // double visitation
  type_switch(v, w)(
    [](type<int, int>, auto&& x, auto&& y) {
      std::cout << ""int, int: "" << x << ' ' << y << '\n';
    },
    [](type<double, int>, auto&& x, auto&& y) {
      std::cout << ""double, int: "" << x << ' ' << y << '\n';
    },
    [](type<int, double>, auto&& x, auto&& y) {
      std::cout << ""int, double: "" << x << ' ' << y << '\n';
    },
    [](type<double, double>, auto&& x, auto&& y) {
      std::cout << ""double, double: "" << x << ' ' << y << '\n';
    });
```
Given `template <typename... Ts> struct type {};` and an implementation
of `overload`, `type_switch` can be implemented like this:
```cpp
template <typename... Vs>
auto type_switch(Vs&&... vs) {  // take the variants
  return [&](auto&&... fs) {  // take the cases
    mpark::visit([&](auto&&... xs) {  // visit the variants, get the values
      return overload(std::forward<decltype(fs)>(fs)...)(  // overload the cases
        type<std::decay_t<decltype(xs)>...>{},  // perform tag-dispatching with `type`
        std::forward<decltype(xs)>(xs)...);  // forward the values
    }, std::forward<Vs>(vs)...);
  };
}
```
[Example on Wandbox](https://wandbox.org/permlink/XmCW6PBBSeQkKHtE)",NA,https://api.github.com/repos/mpark/variant/issues/44/comments,https://github.com/mpark/variant/issues/44#issuecomment-377632127,https://api.github.com/repos/mpark/variant/issues/44
mpark,variant,305615927,https://api.github.com/repos/mpark/variant/issues/comments/373429218,373429218,MDEyOklzc3VlQ29tbWVudDM3MzQyOTIxOA==,885054,2018-03-15T16:02:46Z,2018-03-15T16:02:46Z,NONE,Also see the original issue in `xtl`: https://github.com/QuantStack/xtl/issues/71,NA,https://api.github.com/repos/mpark/variant/issues/43/comments,https://github.com/mpark/variant/issues/43#issuecomment-373429218,https://api.github.com/repos/mpark/variant/issues/43
mpark,variant,305615927,https://api.github.com/repos/mpark/variant/issues/comments/373988826,373988826,MDEyOklzc3VlQ29tbWVudDM3Mzk4ODgyNg==,2397974,2018-03-18T10:56:19Z,2018-03-18T10:57:36Z,NONE,Upstream bug report on GCC issue tracker by @mpolacek https://gcc.gnu.org/bugzilla/show_bug.cgi?id=84785,NA,https://api.github.com/repos/mpark/variant/issues/43/comments,https://github.com/mpark/variant/issues/43#issuecomment-373988826,https://api.github.com/repos/mpark/variant/issues/43
mpark,variant,305615927,https://api.github.com/repos/mpark/variant/issues/comments/454041539,454041539,MDEyOklzc3VlQ29tbWVudDQ1NDA0MTUzOQ==,2275240,2019-01-14T15:20:12Z,2019-01-14T15:20:12Z,OWNER,"Tested successfully on 7.3.0 and 7.4.0 https://godbolt.org/z/Ofp6st, I'm not sure what the status is with 7.3.1 in specific, but since it's a compiler bug, I'll close here.",NA,https://api.github.com/repos/mpark/variant/issues/43/comments,https://github.com/mpark/variant/issues/43#issuecomment-454041539,https://api.github.com/repos/mpark/variant/issues/43
mpark,variant,300478684,https://api.github.com/repos/mpark/variant/issues/comments/368741104,368741104,MDEyOklzc3VlQ29tbWVudDM2ODc0MTEwNA==,1326315,2018-02-27T04:09:15Z,2018-02-27T04:46:53Z,NONE,"I can also get rid of the warning by adding `noexcept(noexcept(__VA_ARGS__))` to [`DECLTYPE_AUTO_RETURN`](https://github.com/mpark/variant/blob/8406c3c76cf4ee67afe36e7ef0e761c7c31cea5c/v1.3.0/variant.hpp#L786) used by [`value_visitor::operator()`](https://github.com/mpark/variant/blob/8406c3c76cf4ee67afe36e7ef0e761c7c31cea5c/v1.3.0/variant.hpp#L1247) and just `noexcept` to [`visitation::base::at`](https://github.com/mpark/variant/blob/8406c3c76cf4ee67afe36e7ef0e761c7c31cea5c/v1.3.0/variant.hpp#L1048).

That seems to work for me. Throwing from my lambda without `noexcept` on it propagates the exception while throwing with `noexcept` on the lambda calls terminate like I would expect. But I'm not sure if that's sufficient so someone else will need to look into this. Hopefully my investigation helps though :smiley: .",NA,https://api.github.com/repos/mpark/variant/issues/42/comments,https://github.com/mpark/variant/issues/42#issuecomment-368741104,https://api.github.com/repos/mpark/variant/issues/42
mpark,variant,300478684,https://api.github.com/repos/mpark/variant/issues/comments/368948337,368948337,MDEyOklzc3VlQ29tbWVudDM2ODk0ODMzNw==,1326315,2018-02-27T16:57:42Z,2018-02-27T16:57:42Z,NONE,"Alright, upon further investigation, I appear to only be getting this with gcc 5.4.1 and 6.3.0. It seems 7.2.0 doesn't generate this warning.",NA,https://api.github.com/repos/mpark/variant/issues/42/comments,https://github.com/mpark/variant/issues/42#issuecomment-368948337,https://api.github.com/repos/mpark/variant/issues/42
mpark,variant,300478684,https://api.github.com/repos/mpark/variant/issues/comments/369082890,369082890,MDEyOklzc3VlQ29tbWVudDM2OTA4Mjg5MA==,2275240,2018-02-28T00:59:56Z,2018-02-28T00:59:56Z,OWNER,Thanks for the report! I'll try to get around to looking at this before the weekend.,NA,https://api.github.com/repos/mpark/variant/issues/42/comments,https://github.com/mpark/variant/issues/42#issuecomment-369082890,https://api.github.com/repos/mpark/variant/issues/42
mpark,variant,300478684,https://api.github.com/repos/mpark/variant/issues/comments/372013117,372013117,MDEyOklzc3VlQ29tbWVudDM3MjAxMzExNw==,2275240,2018-03-10T08:29:06Z,2018-03-10T08:29:06Z,OWNER,"Fixed by: bc5d147d59e9132b54837cb226321ebb08c2e9b9 , 2d4503d88f97d880e2817095b27dc3e66646b577, 7d9454be276ad69e407db6e51aab98c995a8cc00, f6638fd18ca6badb421efdbb5442f140d1f76f66",NA,https://api.github.com/repos/mpark/variant/issues/42/comments,https://github.com/mpark/variant/issues/42#issuecomment-372013117,https://api.github.com/repos/mpark/variant/issues/42
mpark,variant,294411045,https://api.github.com/repos/mpark/variant/issues/comments/369083034,369083034,MDEyOklzc3VlQ29tbWVudDM2OTA4MzAzNA==,2275240,2018-02-28T01:00:31Z,2018-02-28T01:00:31Z,OWNER,Interesting... thanks for the report. Looks scary! I'll try to get around to looking at this soon!,NA,https://api.github.com/repos/mpark/variant/issues/41/comments,https://github.com/mpark/variant/issues/41#issuecomment-369083034,https://api.github.com/repos/mpark/variant/issues/41
mpark,variant,294411045,https://api.github.com/repos/mpark/variant/issues/comments/377608418,377608418,MDEyOklzc3VlQ29tbWVudDM3NzYwODQxOA==,7946730,2018-03-30T19:55:37Z,2018-03-30T20:24:06Z,NONE,"I'm getting the same problem too. This is annoying as I'm stuck. Before using `mpark::variant` I was using [strict-variant](https://github.com/cbeck88/strict-variant) but I was getting error for nested variant, so I switch to mpark but again I'm stuck.

I'm also using **Visual Studio 2015 Update 3**.",NA,https://api.github.com/repos/mpark/variant/issues/41/comments,https://github.com/mpark/variant/issues/41#issuecomment-377608418,https://api.github.com/repos/mpark/variant/issues/41
mpark,variant,294411045,https://api.github.com/repos/mpark/variant/issues/comments/377640143,377640143,MDEyOklzc3VlQ29tbWVudDM3NzY0MDE0Mw==,2275240,2018-03-30T22:46:39Z,2018-03-30T22:46:39Z,OWNER,"Took some time to look into this today. It seems like the offending commit is 4bfed80, where I made the jump tables to be global variables. It looks like the jump tables aren't initialized while the `instance` global is being created. Looks like v1.2.2 works, since it doesn't have this commit.

Will need to think about how to fix this...",NA,https://api.github.com/repos/mpark/variant/issues/41/comments,https://github.com/mpark/variant/issues/41#issuecomment-377640143,https://api.github.com/repos/mpark/variant/issues/41
mpark,variant,294411045,https://api.github.com/repos/mpark/variant/issues/comments/377645315,377645315,MDEyOklzc3VlQ29tbWVudDM3NzY0NTMxNQ==,7946730,2018-03-30T23:28:00Z,2018-03-30T23:42:52Z,NONE,"@mpark Thank you so much for your time. I appreciate it. I'm on a big project and being stuck each time I try a variant lib is not a good thing :(

1. Why it is working with GCC ?

2. By curiosity, is this problem present on VS 2017 ?

3. If I downgrade to v1.2.2, what will I loose ? Will I get other compilation error ? Degrade the performance, missing functionality ?

4. How many time do you estimate to fix this problem on the new version ?",NA,https://api.github.com/repos/mpark/variant/issues/41/comments,https://github.com/mpark/variant/issues/41#issuecomment-377645315,https://api.github.com/repos/mpark/variant/issues/41
mpark,variant,294411045,https://api.github.com/repos/mpark/variant/issues/comments/377648319,377648319,MDEyOklzc3VlQ29tbWVudDM3NzY0ODMxOQ==,2275240,2018-03-30T23:54:54Z,2018-03-30T23:54:54Z,OWNER,"Reduced test case:
```
struct Foo {
  Foo() { v.emplace<0>(1); }
  mpark::variant<int, std::vector<int>> v;
} foo;
```",NA,https://api.github.com/repos/mpark/variant/issues/41/comments,https://github.com/mpark/variant/issues/41#issuecomment-377648319,https://api.github.com/repos/mpark/variant/issues/41
mpark,variant,294411045,https://api.github.com/repos/mpark/variant/issues/comments/377648790,377648790,MDEyOklzc3VlQ29tbWVudDM3NzY0ODc5MA==,2275240,2018-03-30T23:59:22Z,2018-03-31T00:51:05Z,OWNER,"@purell:
1. I'm not sure yet. The initialization order rules are quite complicated, and i don't know if MSVC is actually doing anything wrong.
2. I don't know. I don't have that environment set up currently. __UPDATE__: @K-ballo says that it's not an issue on VS2017.
3. Refer to the changes described in https://github.com/mpark/variant/releases/tag/v1.3.0
4. Probably not for a few weeks. I'm currently on vacation, and I made some time tonight but I can't promise anything for the rest.",NA,https://api.github.com/repos/mpark/variant/issues/41/comments,https://github.com/mpark/variant/issues/41#issuecomment-377648790,https://api.github.com/repos/mpark/variant/issues/41
mpark,variant,294411045,https://api.github.com/repos/mpark/variant/issues/comments/390318531,390318531,MDEyOklzc3VlQ29tbWVudDM5MDMxODUzMQ==,9640520,2018-05-18T20:08:56Z,2018-05-18T23:02:26Z,NONE,"I'm getting this error as well with 1.3.0 variant.hpp:

```
 	0000000000000000()	Unknown
 	tiff.exe!mpark::detail::visitation::alt::visit_alt_at<<lambda_a58e77b7b349863bb1db735b5431a7ff>,mpark::detail::assignment<mpark::detail::traits<VTYPES > > & __ptr64,mpark::detail::move_assignment<mpark::detail::traits<VTYPES >,1> >(unsigned __int64 index, mpark::detail::assignment<mpark::detail::traits<VTYPES > >::generic_assign::__l11::<lambda_a58e77b7b349863bb1db735b5431a7ff> && visitor, mpark::detail::assignment<mpark::detail::traits<VTYPES > > & <vs_0>, mpark::detail::move_assignment<mpark::detail::traits<VTYPES >,1> && <vs_1>) Line 1203	C++
 	tiff.exe!mpark::detail::assignment<mpark::detail::traits<VTYPES > >::generic_assign<mpark::detail::move_assignment<mpark::detail::traits<VTYPES >,1> >(mpark::detail::move_assignment<mpark::detail::traits<VTYPES >,1> && that) Line 1673	C++
 	tiff.exe!mpark::detail::move_assignment<mpark::detail::traits<VTYPES >,1>::operator=(mpark::detail::move_assignment<mpark::detail::traits<VTYPES >,1> && that) Line 1700	C++
 	tiff.exe!mpark::detail::copy_assignment<mpark::detail::traits<VTYPES >,1>::operator=(mpark::detail::copy_assignment<mpark::detail::traits<VTYPES >,1> && __that) Line 618	C++
 	[External Code]	
 	tiff.exe!mpark::variant<VTYPES >::operator=(mpark::variant<VTYPES > && __that) Line 618	C++
        tiff.exe!ome::files::VariantPixelBuffer::createBuffer(const boost::detail::multi_array::extent_gen<9> & range, ome::xml::model::enums::PixelType pixeltype, const boost::general_storage_order<9> & storage) Line 323	C++
 	tiff.exe!ome::files::VariantPixelBuffer::VariantPixelBuffer() Line 126	C++
 	tiff.exe!TIFFVariantTest::getPNGData(unsigned int xsize, unsigned int ysize, ome::xml::model::enums::PixelType pixeltype, ome::files::tiff::PlanarConfiguration planarconfig) Line 979	C++
 	tiff.exe!`anonymous namespace'::pixel_tests() Line 1717	C++
 	tiff.exe!`dynamic initializer for 'pixel_params''() Line 1754	C++
 	[External Code]	
```

with a simple assignment [here](https://github.com/rleigh-codelibre/ome-files-cpp/blob/mpark-variant/lib/ome/files/VariantPixelBuffer.h#L266).  I can try to reduce it, but it looks the same as the above.  Works fine on MacOS X and Linux, fails with VS2015.

`VTYPES` are `std::shared_ptr<ome::files::PixelBuffer<signed char> >,std::shared_ptr<ome::files::PixelBuffer<short> >,std::shared_ptr<ome::files::PixelBuffer<int> >,std::shared_ptr<ome::files::PixelBuffer<unsigned char> >,std::shared_ptr<ome::files::PixelBuffer<unsigned short> >,std::shared_ptr<ome::files::PixelBuffer<unsigned int> >,std::shared_ptr<ome::files::PixelBuffer<bool> >,std::shared_ptr<ome::files::PixelBuffer<float> >,std::shared_ptr<ome::files::PixelBuffer<double> >,std::shared_ptr<ome::files::PixelBuffer<std::complex<float> > >,std::shared_ptr<ome::files::PixelBuffer<std::complex<double> > >` i.e. a list of shared pointers of different types; while a bit horrible, it's shouldn't be causing problems.

Retesting with 1.2.2, it works correctly and there's no segfault.

Edit: Just to comment on the use of global jump tables, my code *is* constructing and assigning the variant inside a static (googletest test discovery and registration) before `main()` is entered.",NA,https://api.github.com/repos/mpark/variant/issues/41/comments,https://github.com/mpark/variant/issues/41#issuecomment-390318531,https://api.github.com/repos/mpark/variant/issues/41
mpark,variant,294411045,https://api.github.com/repos/mpark/variant/issues/comments/454278646,454278646,MDEyOklzc3VlQ29tbWVudDQ1NDI3ODY0Ng==,2275240,2019-01-15T06:13:22Z,2019-01-15T06:13:22Z,OWNER,Fixed by 1896bd5f7031b4b81abf6c07595848198aec215a,NA,https://api.github.com/repos/mpark/variant/issues/41/comments,https://github.com/mpark/variant/issues/41#issuecomment-454278646,https://api.github.com/repos/mpark/variant/issues/41
mpark,variant,294207277,https://api.github.com/repos/mpark/variant/issues/comments/362924048,362924048,MDEyOklzc3VlQ29tbWVudDM2MjkyNDA0OA==,5955594,2018-02-04T17:30:27Z,2018-02-04T17:30:27Z,NONE,"There are some compile errors in your code.  After  making changes such that it compiles, your issue becomes clear.  Here is some working code with a comment that should show your problem.  
#include <cstdint>
#include <iostream>
#define MPARK_EXCEPTIONS
#include ""mpark/variant.hpp""

using namespace std;  // i know it's bad, it's just for testing

int main() {
  mpark::variant<int64_t, double, bool, string, size_t> v;
  // v is not assigned to size_t here, so you cannot call mpark::get
  v.emplace<size_t>(0);
  //now it is correctly assigned to type size_t
  mpark::get<size_t>(v) = 15;
  return 0;
}
  
If the type of x, where x is the value you are assigning v to is convertible to only one of the types in v, you would be able to simply write, v = x.  However, this is not possible when you have multiple int types in the same variant so I had to use emplace instead, hence being explicit about the type.


> On 4 Feb 2018, at 15:54, Alexandre Plateau <notifications@github.com> wrote:
> 
> Hi,
> 
> I am trying to store size_t as well as int64_t, double, bool, string in a variant. Storing the 4 last types work fine, but I get a bad_variant_access when trying to get/set the size_t with this code :
> 
> #include <iostream>
> #include <cstdint>
> #define MPARK_EXCEPTIONS
> #include <mpark/variant.hpp>
> 
> using namespace std;  // i know it's bad, it's just for testing
> 
> int main()
> {
>     mpark::variant v<int64_t, double, bool, string, size_t>;
>     mpakr::get<size_t>(v) = 15;  // right there
>     return 0;
> }
> â€”
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub <https://github.com/mpark/variant/issues/40>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AFrgCvtjMEFh28Z7iQTwN9tnzHIofVKkks5tRdLLgaJpZM4R4nUe>.
> 

",NA,https://api.github.com/repos/mpark/variant/issues/40/comments,https://github.com/mpark/variant/issues/40#issuecomment-362924048,https://api.github.com/repos/mpark/variant/issues/40
mpark,variant,294207277,https://api.github.com/repos/mpark/variant/issues/comments/362941026,362941026,MDEyOklzc3VlQ29tbWVudDM2Mjk0MTAyNg==,10694085,2018-02-04T21:25:11Z,2018-02-04T21:25:11Z,NONE,"Even this way, I am still getting a `bad_variant_access` :/",NA,https://api.github.com/repos/mpark/variant/issues/40/comments,https://github.com/mpark/variant/issues/40#issuecomment-362941026,https://api.github.com/repos/mpark/variant/issues/40
mpark,variant,294207277,https://api.github.com/repos/mpark/variant/issues/comments/362991360,362991360,MDEyOklzc3VlQ29tbWVudDM2Mjk5MTM2MA==,2275240,2018-02-05T06:15:22Z,2018-02-05T06:15:49Z,OWNER,"@SuperFola: What configuration are you using? @SaadAttieh's snippet works fine for me:
https://wandbox.org/permlink/GFYehONmvuc7cByX",NA,https://api.github.com/repos/mpark/variant/issues/40/comments,https://github.com/mpark/variant/issues/40#issuecomment-362991360,https://api.github.com/repos/mpark/variant/issues/40
mpark,variant,294207277,https://api.github.com/repos/mpark/variant/issues/comments/363139960,363139960,MDEyOklzc3VlQ29tbWVudDM2MzEzOTk2MA==,10694085,2018-02-05T16:33:17Z,2018-02-05T16:33:17Z,NONE,"I am using gnu gcc (mingw 4.8), with `-Wall -std=c++14 -g` on Windows (I want to add that I had to add the define `MPARK_EXCEPTIONS` to be able to see the error, otherwise I would just get a `terminate without an active exception`).
",NA,https://api.github.com/repos/mpark/variant/issues/40/comments,https://github.com/mpark/variant/issues/40#issuecomment-363139960,https://api.github.com/repos/mpark/variant/issues/40
mpark,variant,294207277,https://api.github.com/repos/mpark/variant/issues/comments/364634334,364634334,MDEyOklzc3VlQ29tbWVudDM2NDYzNDMzNA==,2275240,2018-02-10T08:01:25Z,2018-02-10T08:01:35Z,OWNER,@SuperFola: Does this issue only arise with `size_t` in specific..? or are you able to reproduce it with any type?,NA,https://api.github.com/repos/mpark/variant/issues/40/comments,https://github.com/mpark/variant/issues/40#issuecomment-364634334,https://api.github.com/repos/mpark/variant/issues/40
mpark,variant,294207277,https://api.github.com/repos/mpark/variant/issues/comments/364934919,364934919,MDEyOklzc3VlQ29tbWVudDM2NDkzNDkxOQ==,10694085,2018-02-12T14:17:27Z,2018-02-12T14:30:46Z,NONE,"Using size_t or uint32_t seems to be a problem, I can try others things (it seems to be a problem only with unsigned integers type) if you want :)

edit : 
```cpp
#include <iostream>
#include <cstdint>

#define MPARK_EXCEPTIONS
#include ""variant.hpp""

using namespace std;

int main()
{
    mpark::variant<
        int8_t,  int16_t,  int32_t,  int64_t, uint8_t, uint16_t, /*uint32_t,*/ uint64_t, size_t
    > v;

    cout << 1 << flush; mpark::get<int8_t>(v)   = 15;
    cout << 2 << flush; mpark::get<int16_t>(v)  = 15;
    cout << 3 << flush; mpark::get<int32_t>(v)  = 15;
    cout << 4 << flush; mpark::get<int64_t>(v)  = 15;
    cout << 5 << flush; mpark::get<uint8_t>(v)  = 15;
    cout << 6 << flush; mpark::get<uint16_t>(v) = 15;
    // if I add this line, it refuses to compile
    // cout << 7 << flush; mpark::get<uint32_t>(v) = 15;
    cout << 8 << flush; mpark::get<uint64_t>(v) = 15;
    // same here as long as uint32_t is in the variant
    cout << 9 << flush; mpark::get<size_t>(v)   = 15;

    return 0;
}
```
this code compiles but is throwing a bad variant access

```
12terminate called after throwing an instance of 'mpark::bad_variant_access'
  what():  bad_variant_access
```

I tried to comment some of the ""tests"" and it seems to be a problem to have different integers type in the variant : 
```cpp
    mpark::variant<
        int8_t,  int16_t,  int32_t,  int64_t, uint8_t, uint16_t, /*uint32_t,*/ uint64_t /*size_t*/
    > v;

    cout << 1 << flush; mpark::get<int8_t>(v)   = 15;
    // cout << 2 << flush; mpark::get<int16_t>(v)  = 15;
    // cout << 3 << flush; mpark::get<int32_t>(v)  = 15;
    cout << 4 << flush; mpark::get<int64_t>(v)  = 15;
    cout << 5 << flush; mpark::get<uint8_t>(v)  = 15;
    cout << 6 << flush; mpark::get<uint16_t>(v) = 15;
    // if I add this line, it refuses to compile
    // cout << 7 << flush; mpark::get<uint32_t>(v) = 15;
    cout << 8 << flush; mpark::get<uint64_t>(v) = 15;
    // same here as long as uint32_t is in the variant
    //cout << 9 << flush; mpark::get<size_t>(v)   = 15;

    return 0;
```

still throws a bad_variant_access :/",NA,https://api.github.com/repos/mpark/variant/issues/40/comments,https://github.com/mpark/variant/issues/40#issuecomment-364934919,https://api.github.com/repos/mpark/variant/issues/40
mpark,variant,294207277,https://api.github.com/repos/mpark/variant/issues/comments/454051930,454051930,MDEyOklzc3VlQ29tbWVudDQ1NDA1MTkzMA==,2275240,2019-01-14T15:47:05Z,2019-01-14T15:47:05Z,OWNER,"I'm going to close this for now since MinGW is not a distro that I'm officially supporting at this point. If there are enough requests for MinGW support going forward, I'll perhaps invest more time.",NA,https://api.github.com/repos/mpark/variant/issues/40/comments,https://github.com/mpark/variant/issues/40#issuecomment-454051930,https://api.github.com/repos/mpark/variant/issues/40
mpark,variant,291668956,https://api.github.com/repos/mpark/variant/issues/comments/360602482,360602482,MDEyOklzc3VlQ29tbWVudDM2MDYwMjQ4Mg==,2275240,2018-01-25T21:13:42Z,2018-01-25T21:13:42Z,OWNER,"Thanks for the report and feedback!

I currently poke into the `internal_utils.cmake` file only because when I compile with no exceptions and such, as far as I understand gtest needs to be compiled with the same compiler flags as the tests themselves.

I don't think I understand exactly what the `gtest_main` / `gtest` problem is. Could you elaborate?

My plan ultimately is to switch over to use `Catch2` rather than `gtest`.",NA,https://api.github.com/repos/mpark/variant/issues/39/comments,https://github.com/mpark/variant/issues/39#issuecomment-360602482,https://api.github.com/repos/mpark/variant/issues/39
mpark,variant,291668956,https://api.github.com/repos/mpark/variant/issues/comments/360761140,360761140,MDEyOklzc3VlQ29tbWVudDM2MDc2MTE0MA==,1406456,2018-01-26T11:28:38Z,2018-01-26T11:28:38Z,NONE,"About `gtest_main`/`gtest`:

In `googletest` sources [here](https://github.com/google/googletest/blob/release-1.8.0/googletest/CMakeLists.txt#L90):

```
cxx_library(gtest ""${cxx_strict}"" src/gtest-all.cc)
cxx_library(gtest_main ""${cxx_strict}"" src/gtest_main.cc)
```

and watching the sources, `gtest` is the library itself and `gtest_main` is more like the executable (see [source for `gtest_main.cc`](https://github.com/google/googletest/blob/release-1.8.0/googletest/src/gtest_main.cc)).
",NA,https://api.github.com/repos/mpark/variant/issues/39/comments,https://github.com/mpark/variant/issues/39#issuecomment-360761140,https://api.github.com/repos/mpark/variant/issues/39
mpark,variant,291668956,https://api.github.com/repos/mpark/variant/issues/comments/361025540,361025540,MDEyOklzc3VlQ29tbWVudDM2MTAyNTU0MA==,2275240,2018-01-27T23:52:15Z,2018-01-27T23:52:15Z,OWNER,I think I'll ditch gtest all-together shortly. Could you keep the workarounds for now and remove them once I ditch gtest?,NA,https://api.github.com/repos/mpark/variant/issues/39/comments,https://github.com/mpark/variant/issues/39#issuecomment-361025540,https://api.github.com/repos/mpark/variant/issues/39
mpark,variant,291668956,https://api.github.com/repos/mpark/variant/issues/comments/361046310,361046310,MDEyOklzc3VlQ29tbWVudDM2MTA0NjMxMA==,1406456,2018-01-28T08:27:04Z,2018-01-28T08:27:04Z,NONE,"Yes, of course. Just ping me when you release the new version.

Thanks!",NA,https://api.github.com/repos/mpark/variant/issues/39/comments,https://github.com/mpark/variant/issues/39#issuecomment-361046310,https://api.github.com/repos/mpark/variant/issues/39
mpark,variant,291668956,https://api.github.com/repos/mpark/variant/issues/comments/379410757,379410757,MDEyOklzc3VlQ29tbWVudDM3OTQxMDc1Nw==,2275240,2018-04-06T23:32:11Z,2018-04-06T23:32:11Z,OWNER,@jgsogo: Closing for now. Will ping when I get around to ditching gtest.,NA,https://api.github.com/repos/mpark/variant/issues/39/comments,https://github.com/mpark/variant/issues/39#issuecomment-379410757,https://api.github.com/repos/mpark/variant/issues/39
mpark,variant,281075570,https://api.github.com/repos/mpark/variant/issues/comments/350897069,350897069,MDEyOklzc3VlQ29tbWVudDM1MDg5NzA2OQ==,2275240,2017-12-11T23:50:21Z,2017-12-11T23:50:21Z,OWNER,Closed by 2bb35f1996cdb84a35d3587509a52eeb42d14afd,NA,https://api.github.com/repos/mpark/variant/issues/38/comments,https://github.com/mpark/variant/issues/38#issuecomment-350897069,https://api.github.com/repos/mpark/variant/issues/38
mpark,variant,280250069,https://api.github.com/repos/mpark/variant/issues/comments/350417635,350417635,MDEyOklzc3VlQ29tbWVudDM1MDQxNzYzNQ==,82177,2017-12-09T02:35:28Z,2017-12-09T02:35:28Z,NONE,ETA?  Would it help to prod you on this if I told you I just recommended this implementation to some at-risk youths who are forced to use GCC 4.8.3?,NA,https://api.github.com/repos/mpark/variant/issues/37/comments,https://github.com/mpark/variant/issues/37#issuecomment-350417635,https://api.github.com/repos/mpark/variant/issues/37
mpark,variant,280250069,https://api.github.com/repos/mpark/variant/issues/comments/350506012,350506012,MDEyOklzc3VlQ29tbWVudDM1MDUwNjAxMg==,2275240,2017-12-09T21:15:08Z,2017-12-09T21:15:45Z,OWNER,"Hi Zach! I'm planning to work on this next week. Last time I tried this it was having trouble with
the cartesian product generation here: https://github.com/mpark/variant/blob/master/include/mpark/variant.hpp#L574-L575

So I'll have to figure out how to deal with that part.

Thanks for the recommendation!",NA,https://api.github.com/repos/mpark/variant/issues/37/comments,https://github.com/mpark/variant/issues/37#issuecomment-350506012,https://api.github.com/repos/mpark/variant/issues/37
mpark,variant,280250069,https://api.github.com/repos/mpark/variant/issues/comments/350869630,350869630,MDEyOklzc3VlQ29tbWVudDM1MDg2OTYzMA==,2275240,2017-12-11T21:49:39Z,2017-12-11T21:49:39Z,OWNER,"Closed by 12b77f3325b81ba56a3c3f0b88e61b57505b9ff0.

The minimum repro of the problems were https://wandbox.org/permlink/ypmovjwyTaXnSnwy.
In short, `S<alias<pack..., thing>>` drops `thing` and incorrectly passes `alias<pack...>` instead.

This caused the `true` in `all` implementation to be dropped in `bool_sequence<Bs..., true>`,
and the `J` was dropped in `index_sequence<Is..., J>`. Since the issue was due to the fact that
`bool_sequence` and `index_sequence` are aliases, `integer_sequence` is now used directly.",NA,https://api.github.com/repos/mpark/variant/issues/37/comments,https://github.com/mpark/variant/issues/37#issuecomment-350869630,https://api.github.com/repos/mpark/variant/issues/37
mpark,variant,280250069,https://api.github.com/repos/mpark/variant/issues/comments/350893510,350893510,MDEyOklzc3VlQ29tbWVudDM1MDg5MzUxMA==,82177,2017-12-11T23:31:33Z,2017-12-11T23:31:33Z,NONE,Thanks for the quick resolution!,NA,https://api.github.com/repos/mpark/variant/issues/37/comments,https://github.com/mpark/variant/issues/37#issuecomment-350893510,https://api.github.com/repos/mpark/variant/issues/37
mpark,variant,280249634,https://api.github.com/repos/mpark/variant/issues/comments/350948541,350948541,MDEyOklzc3VlQ29tbWVudDM1MDk0ODU0MQ==,2275240,2017-12-12T05:21:56Z,2017-12-12T05:22:32Z,OWNER,"Closed by 4bfed80ab842d84239f8a07b143848abb1f47f91

The following is a before/after for Clang-5.0 for binary size at https://mpark.github.io/variant/
__NOTE__: All other versions of Clang had similar results.

#### Before

![screen shot 2017-12-11 at 5 50 53 pm](https://user-images.githubusercontent.com/2275240/33868674-25c242b8-deb9-11e7-993c-b361f5e9cd38.png)

#### After

<img width=""1423"" alt=""screen shot 2017-12-11 at 9 18 36 pm"" src=""https://user-images.githubusercontent.com/2275240/33868678-2b5b13bc-deb9-11e7-8454-eed368d4ac18.png"">",NA,https://api.github.com/repos/mpark/variant/issues/36/comments,https://github.com/mpark/variant/issues/36#issuecomment-350948541,https://api.github.com/repos/mpark/variant/issues/36
mpark,variant,279409394,https://api.github.com/repos/mpark/variant/issues/comments/350047085,350047085,MDEyOklzc3VlQ29tbWVudDM1MDA0NzA4NQ==,2275240,2017-12-07T18:02:35Z,2017-12-07T18:02:35Z,OWNER,Thanks for the report! I'll look into this today.,NA,https://api.github.com/repos/mpark/variant/issues/35/comments,https://github.com/mpark/variant/issues/35#issuecomment-350047085,https://api.github.com/repos/mpark/variant/issues/35
mpark,variant,279409394,https://api.github.com/repos/mpark/variant/issues/comments/350443300,350443300,MDEyOklzc3VlQ29tbWVudDM1MDQ0MzMwMA==,770847,2017-12-09T10:09:23Z,2017-12-09T10:55:13Z,NONE,"I got the same error! Any idea what's causing this?
I tried with Tag v1.2.0 and today's master",NA,https://api.github.com/repos/mpark/variant/issues/35/comments,https://github.com/mpark/variant/issues/35#issuecomment-350443300,https://api.github.com/repos/mpark/variant/issues/35
mpark,variant,279409394,https://api.github.com/repos/mpark/variant/issues/comments/350505905,350505905,MDEyOklzc3VlQ29tbWVudDM1MDUwNTkwNQ==,2275240,2017-12-09T21:13:18Z,2017-12-09T21:13:18Z,OWNER,"It seems like it's having trouble with the cartesian product generation here: https://github.com/mpark/variant/blob/master/include/mpark/variant.hpp#L574-L575

I'll have to play with whether I need to reformulate this for MSVC or if there's some way to coax it to understand it.",NA,https://api.github.com/repos/mpark/variant/issues/35/comments,https://github.com/mpark/variant/issues/35#issuecomment-350505905,https://api.github.com/repos/mpark/variant/issues/35
mpark,variant,279409394,https://api.github.com/repos/mpark/variant/issues/comments/350897088,350897088,MDEyOklzc3VlQ29tbWVudDM1MDg5NzA4OA==,2275240,2017-12-11T23:50:27Z,2017-12-11T23:50:27Z,OWNER,Closed by 2bb35f1,NA,https://api.github.com/repos/mpark/variant/issues/35/comments,https://github.com/mpark/variant/issues/35#issuecomment-350897088,https://api.github.com/repos/mpark/variant/issues/35
mpark,variant,260036134,https://api.github.com/repos/mpark/variant/issues/comments/331742792,331742792,MDEyOklzc3VlQ29tbWVudDMzMTc0Mjc5Mg==,2275240,2017-09-24T21:52:06Z,2017-09-24T21:52:06Z,OWNER,"Thanks for the report! I just tried it out and confirmed the issue. It either looks like my fallback implementation of `is_swappable` is instantiating too eagerly, or I'm checking for `is_swappable` where I shouldn't be. I'm going to be attending CppCon this week, so I'll be sure to fix this after the conference!",NA,https://api.github.com/repos/mpark/variant/issues/34/comments,https://github.com/mpark/variant/issues/34#issuecomment-331742792,https://api.github.com/repos/mpark/variant/issues/34
mpark,variant,260036134,https://api.github.com/repos/mpark/variant/issues/comments/331793699,331793699,MDEyOklzc3VlQ29tbWVudDMzMTc5MzY5OQ==,3620467,2017-09-25T07:01:33Z,2017-09-25T07:01:33Z,NONE,Awesome. Thanks for the confirmation and for having a look into the issue. Have a great time at CppCon.,NA,https://api.github.com/repos/mpark/variant/issues/34/comments,https://github.com/mpark/variant/issues/34#issuecomment-331793699,https://api.github.com/repos/mpark/variant/issues/34
mpark,variant,260036134,https://api.github.com/repos/mpark/variant/issues/comments/344006640,344006640,MDEyOklzc3VlQ29tbWVudDM0NDAwNjY0MA==,2275240,2017-11-13T18:09:10Z,2017-11-13T18:09:10Z,OWNER,"The following commits address this issue:
  - https://github.com/mpark/variant/commit/e567a710ea6429b921fd42fa415d12232eab7e46
  - https://github.com/mpark/variant/commit/9e06a9d6a17fb31f2e4aabd42e59f48bd0ee0ed8
  - https://github.com/mpark/variant/commit/ea542a0fef86fb8c1696238b95bdaddb8ff2337c",NA,https://api.github.com/repos/mpark/variant/issues/34/comments,https://github.com/mpark/variant/issues/34#issuecomment-344006640,https://api.github.com/repos/mpark/variant/issues/34
mpark,variant,260036134,https://api.github.com/repos/mpark/variant/issues/comments/344973697,344973697,MDEyOklzc3VlQ29tbWVudDM0NDk3MzY5Nw==,3620467,2017-11-16T16:15:29Z,2017-11-16T16:15:29Z,NONE,Great! Thanks for fixing this issue. It works flawlessly now.,NA,https://api.github.com/repos/mpark/variant/issues/34/comments,https://github.com/mpark/variant/issues/34#issuecomment-344973697,https://api.github.com/repos/mpark/variant/issues/34
mpark,variant,260036134,https://api.github.com/repos/mpark/variant/issues/comments/426996442,426996442,MDEyOklzc3VlQ29tbWVudDQyNjk5NjQ0Mg==,1409019,2018-10-04T12:16:33Z,2018-10-04T12:16:33Z,NONE,"I'm experiencing something similar with gcc 7, seems to work with 8 https://godbolt.org/z/0fb-bV
(also works if I use standard variant)
Thanks!",NA,https://api.github.com/repos/mpark/variant/issues/34/comments,https://github.com/mpark/variant/issues/34#issuecomment-426996442,https://api.github.com/repos/mpark/variant/issues/34
mpark,variant,260036134,https://api.github.com/repos/mpark/variant/issues/comments/454278167,454278167,MDEyOklzc3VlQ29tbWVudDQ1NDI3ODE2Nw==,2275240,2019-01-15T06:10:14Z,2019-01-15T06:10:14Z,OWNER,The core of the issue here was that my implementation of `invoke` was instantiating too much. This is fixed by 5713032c68c6dad70a6a1d9f72b80c737c249a28,NA,https://api.github.com/repos/mpark/variant/issues/34/comments,https://github.com/mpark/variant/issues/34#issuecomment-454278167,https://api.github.com/repos/mpark/variant/issues/34
mpark,variant,230556315,https://api.github.com/repos/mpark/variant/issues/comments/303815331,303815331,MDEyOklzc3VlQ29tbWVudDMwMzgxNTMzMQ==,2275240,2017-05-24T18:43:16Z,2017-05-24T18:43:16Z,OWNER,Closed by 2f1cf7c18b31895a6f335da372fbd67a57af8e91,NA,https://api.github.com/repos/mpark/variant/issues/32/comments,https://github.com/mpark/variant/issues/32#issuecomment-303815331,https://api.github.com/repos/mpark/variant/issues/32
mpark,variant,230270899,https://api.github.com/repos/mpark/variant/issues/comments/302986244,302986244,MDEyOklzc3VlQ29tbWVudDMwMjk4NjI0NA==,7010116,2017-05-22T02:43:40Z,2017-05-22T02:43:40Z,CONTRIBUTOR,This may have something to do with #29 ,NA,https://api.github.com/repos/mpark/variant/issues/31/comments,https://github.com/mpark/variant/pull/31#issuecomment-302986244,https://api.github.com/repos/mpark/variant/issues/31
mpark,variant,230270899,https://api.github.com/repos/mpark/variant/issues/comments/303149097,303149097,MDEyOklzc3VlQ29tbWVudDMwMzE0OTA5Nw==,2275240,2017-05-22T16:20:36Z,2017-05-22T16:20:36Z,OWNER,Thanks for the patch!,NA,https://api.github.com/repos/mpark/variant/issues/31/comments,https://github.com/mpark/variant/pull/31#issuecomment-303149097,https://api.github.com/repos/mpark/variant/issues/31
mpark,variant,230117392,https://api.github.com/repos/mpark/variant/issues/comments/303139342,303139342,MDEyOklzc3VlQ29tbWVudDMwMzEzOTM0Mg==,2275240,2017-05-22T15:46:08Z,2017-05-22T15:46:08Z,OWNER,Closed by https://github.com/mpark/variant/commit/840f86b1f47275eb03c7f0d068d9eefc1774e3ab,NA,https://api.github.com/repos/mpark/variant/issues/30/comments,https://github.com/mpark/variant/issues/30#issuecomment-303139342,https://api.github.com/repos/mpark/variant/issues/30
mpark,variant,229498300,https://api.github.com/repos/mpark/variant/issues/comments/303006654,303006654,MDEyOklzc3VlQ29tbWVudDMwMzAwNjY1NA==,721225,2017-05-22T06:02:26Z,2017-05-22T06:02:26Z,NONE,why do you believe it is better to have relative headers. I'm all for absolute headers and would like to know why I'm wrong.,NA,https://api.github.com/repos/mpark/variant/issues/29/comments,https://github.com/mpark/variant/issues/29#issuecomment-303006654,https://api.github.com/repos/mpark/variant/issues/29
mpark,variant,229498300,https://api.github.com/repos/mpark/variant/issues/comments/303014056,303014056,MDEyOklzc3VlQ29tbWVudDMwMzAxNDA1Ng==,2275240,2017-05-22T06:52:25Z,2017-05-22T06:52:25Z,OWNER,"@viboes: Is #31 convincing enough to you? /cc @pdimov
But I'm actually leaning towards providing a single header. It seems like it would be the most convenient for people",NA,https://api.github.com/repos/mpark/variant/issues/29/comments,https://github.com/mpark/variant/issues/29#issuecomment-303014056,https://api.github.com/repos/mpark/variant/issues/29
mpark,variant,229498300,https://api.github.com/repos/mpark/variant/issues/comments/303052724,303052724,MDEyOklzc3VlQ29tbWVudDMwMzA1MjcyNA==,3116174,2017-05-22T09:47:32Z,2017-05-22T09:47:32Z,NONE,"#31 basically explains it. The idea is to have it work when `variant.hpp` is included, without the `-I` being necessary.",NA,https://api.github.com/repos/mpark/variant/issues/29/comments,https://github.com/mpark/variant/issues/29#issuecomment-303052724,https://api.github.com/repos/mpark/variant/issues/29
mpark,variant,229225885,https://api.github.com/repos/mpark/variant/issues/comments/303139028,303139028,MDEyOklzc3VlQ29tbWVudDMwMzEzOTAyOA==,2275240,2017-05-22T15:45:04Z,2017-05-22T15:45:04Z,OWNER,Closed by https://github.com/mpark/variant/commit/1f46b783ef8d28d33c5d7605ef83e275a85b574f,NA,https://api.github.com/repos/mpark/variant/issues/28/comments,https://github.com/mpark/variant/issues/28#issuecomment-303139028,https://api.github.com/repos/mpark/variant/issues/28
mpark,variant,223663473,https://api.github.com/repos/mpark/variant/issues/comments/296594135,296594135,MDEyOklzc3VlQ29tbWVudDI5NjU5NDEzNQ==,376842,2017-04-24T09:31:59Z,2017-04-24T09:33:37Z,NONE,"I don't think this is easy to support anyway - even if you change `noexcept(...)` to `true` there are a load of other errors, e.g. using `for` in a `constexpr`, which MSVC 2015 doesn't support (but 2017 does apparently).

I don't understand why MSVC 2015 is listed as compatible in the Readme. Edit: Ah wait that is Visual Studio **using Clang**. Might want to make that clearer/bolder?",NA,https://api.github.com/repos/mpark/variant/issues/27/comments,https://github.com/mpark/variant/issues/27#issuecomment-296594135,https://api.github.com/repos/mpark/variant/issues/27
mpark,variant,223663473,https://api.github.com/repos/mpark/variant/issues/comments/296607662,296607662,MDEyOklzc3VlQ29tbWVudDI5NjYwNzY2Mg==,1965405,2017-04-24T10:20:42Z,2017-04-24T10:24:12Z,NONE,"Maybe then add atleast msvc 2017 cl.exe support ?
But msvc 2017 have support for std::variant from c++17. ",NA,https://api.github.com/repos/mpark/variant/issues/27/comments,https://github.com/mpark/variant/issues/27#issuecomment-296607662,https://api.github.com/repos/mpark/variant/issues/27
mpark,variant,223663473,https://api.github.com/repos/mpark/variant/issues/comments/296614926,296614926,MDEyOklzc3VlQ29tbWVudDI5NjYxNDkyNg==,376842,2017-04-24T10:42:02Z,2017-04-24T10:42:02Z,NONE,MSVC 2017 has built in support for `std::variant` so there's not much point.,NA,https://api.github.com/repos/mpark/variant/issues/27/comments,https://github.com/mpark/variant/issues/27#issuecomment-296614926,https://api.github.com/repos/mpark/variant/issues/27
mpark,variant,223663473,https://api.github.com/repos/mpark/variant/issues/comments/296773834,296773834,MDEyOklzc3VlQ29tbWVudDI5Njc3MzgzNA==,2275240,2017-04-24T18:06:10Z,2017-04-24T18:06:10Z,OWNER,"@Timmmm: Thanks for the suggestion. I've bolded __Clang/LLVM__ for now.
@RedSkotina: Are you able to share what your situation is? For example, are you wanting to use the library in a project that needs to build with MSVC 2015 directly?",NA,https://api.github.com/repos/mpark/variant/issues/27/comments,https://github.com/mpark/variant/issues/27#issuecomment-296773834,https://api.github.com/repos/mpark/variant/issues/27
mpark,variant,223663473,https://api.github.com/repos/mpark/variant/issues/comments/296939123,296939123,MDEyOklzc3VlQ29tbWVudDI5NjkzOTEyMw==,1965405,2017-04-25T07:22:11Z,2017-04-25T07:22:11Z,NONE,i use msvc 2015 update 3 native compiler  for my project and want use variant type. I want upgrade my project it to vs2017 but later. So i choose mpark/variant. But msvc2015 c++14 support too outdated for mpark/variant and i anyway forced upgrade my project to msvc 2017 and use std::variant (with /std:c++latest). So you can just resolve this issue as fixed.,NA,https://api.github.com/repos/mpark/variant/issues/27/comments,https://github.com/mpark/variant/issues/27#issuecomment-296939123,https://api.github.com/repos/mpark/variant/issues/27
mpark,variant,223663473,https://api.github.com/repos/mpark/variant/issues/comments/297043443,297043443,MDEyOklzc3VlQ29tbWVudDI5NzA0MzQ0Mw==,2275240,2017-04-25T14:13:42Z,2017-04-25T14:13:42Z,OWNER,"@RedSkotina: Ah, okay. Thanks for sharing! I'll keep it open for now.",NA,https://api.github.com/repos/mpark/variant/issues/27/comments,https://github.com/mpark/variant/issues/27#issuecomment-297043443,https://api.github.com/repos/mpark/variant/issues/27
mpark,variant,223663473,https://api.github.com/repos/mpark/variant/issues/comments/299692156,299692156,MDEyOklzc3VlQ29tbWVudDI5OTY5MjE1Ng==,13639229,2017-05-07T09:10:59Z,2017-05-07T09:10:59Z,NONE,"I would really love to see the support for MSVC implemented (at least for 2017).
We have a fairly large cross-platform project with a dozen developers, and I can't just enable `/std:c++latest` without someone occasionally tripping on a C++17 feature not supported on our non-Windows platforms.",NA,https://api.github.com/repos/mpark/variant/issues/27/comments,https://github.com/mpark/variant/issues/27#issuecomment-299692156,https://api.github.com/repos/mpark/variant/issues/27
mpark,variant,223663473,https://api.github.com/repos/mpark/variant/issues/comments/299712028,299712028,MDEyOklzc3VlQ29tbWVudDI5OTcxMjAyOA==,376842,2017-05-07T15:01:46Z,2017-05-07T15:01:46Z,NONE,"@emptyVoid, MSVC 2017 has official support for `std::variant`.",NA,https://api.github.com/repos/mpark/variant/issues/27/comments,https://github.com/mpark/variant/issues/27#issuecomment-299712028,https://api.github.com/repos/mpark/variant/issues/27
mpark,variant,223663473,https://api.github.com/repos/mpark/variant/issues/comments/300274085,300274085,MDEyOklzc3VlQ29tbWVudDMwMDI3NDA4NQ==,2275240,2017-05-09T19:21:24Z,2017-05-09T19:21:24Z,OWNER,"@Timmmm: @emptyVoid is saying `/std:c++latest` can't be enabled just yet though. which I assume is needed to get access to `std::variant`.
@emptyVoid: I've been working on C++11 support in https://github.com/mpark/variant/tree/c%2B%2B11. I'll see how far that'll get me into MSVC. Stay tuned!",NA,https://api.github.com/repos/mpark/variant/issues/27/comments,https://github.com/mpark/variant/issues/27#issuecomment-300274085,https://api.github.com/repos/mpark/variant/issues/27
mpark,variant,223663473,https://api.github.com/repos/mpark/variant/issues/comments/300478616,300478616,MDEyOklzc3VlQ29tbWVudDMwMDQ3ODYxNg==,13639229,2017-05-10T13:17:19Z,2017-05-10T13:17:19Z,NONE,"Great, thanks!
I really hope MSVC would be able to compile C++11 at least with their lack of proper expression SFINAE.",NA,https://api.github.com/repos/mpark/variant/issues/27/comments,https://github.com/mpark/variant/issues/27#issuecomment-300478616,https://api.github.com/repos/mpark/variant/issues/27
mpark,variant,223663473,https://api.github.com/repos/mpark/variant/issues/comments/301055090,301055090,MDEyOklzc3VlQ29tbWVudDMwMTA1NTA5MA==,2275240,2017-05-12T11:42:19Z,2017-05-12T11:42:19Z,OWNER,MSVC 2015 CL and 2017 CL with `/std:c++14` are now both supported!,NA,https://api.github.com/repos/mpark/variant/issues/27/comments,https://github.com/mpark/variant/issues/27#issuecomment-301055090,https://api.github.com/repos/mpark/variant/issues/27
mpark,variant,223663473,https://api.github.com/repos/mpark/variant/issues/comments/330936574,330936574,MDEyOklzc3VlQ29tbWVudDMzMDkzNjU3NA==,1146528,2017-09-20T18:16:42Z,2017-09-20T18:16:42Z,NONE,"@mpark sorry for replying in a closed issue, but i see that you provide a check _MSC_FULL_VER < 190024215 
any reason for that version particular ? my _MSC_FULL_VER is defined to 190024210 and I can't update (for intentional reason) to that version. I tried compiling it anyway using 190024210 and it seems to be working fine. ",NA,https://api.github.com/repos/mpark/variant/issues/27/comments,https://github.com/mpark/variant/issues/27#issuecomment-330936574,https://api.github.com/repos/mpark/variant/issues/27
mpark,variant,223663473,https://api.github.com/repos/mpark/variant/issues/comments/331743011,331743011,MDEyOklzc3VlQ29tbWVudDMzMTc0MzAxMQ==,2275240,2017-09-24T21:56:35Z,2017-09-24T21:56:35Z,OWNER,@bysreg: Thanks for reporting this! No particular reason for `190024215`. I did some vague research to figure out which version works and doesn't and had determined that one to be the latest one I could test. I've simply changed lowered the value in the latest commit: baaa75a702171596725ea0ec80c5698222c8377c,NA,https://api.github.com/repos/mpark/variant/issues/27/comments,https://github.com/mpark/variant/issues/27#issuecomment-331743011,https://api.github.com/repos/mpark/variant/issues/27
mpark,variant,223343693,https://api.github.com/repos/mpark/variant/issues/comments/301055167,301055167,MDEyOklzc3VlQ29tbWVudDMwMTA1NTE2Nw==,2275240,2017-05-12T11:42:44Z,2017-05-12T11:42:44Z,OWNER,C++11 support is complete.,NA,https://api.github.com/repos/mpark/variant/issues/26/comments,https://github.com/mpark/variant/issues/26#issuecomment-301055167,https://api.github.com/repos/mpark/variant/issues/26
mpark,variant,223312031,https://api.github.com/repos/mpark/variant/issues/comments/296171021,296171021,MDEyOklzc3VlQ29tbWVudDI5NjE3MTAyMQ==,721225,2017-04-21T11:52:27Z,2017-04-21T11:52:27Z,NONE,"+1
If I had a variant that can ensure the non-empty  guaranties I would try to implement  std::expected wrapping a stdxx::variant. 

Note that std::expected could ensure the non-empty  guaranties as the type E must be nothrow as used to report errors. ",NA,https://api.github.com/repos/mpark/variant/issues/25/comments,https://github.com/mpark/variant/issues/25#issuecomment-296171021,https://api.github.com/repos/mpark/variant/issues/25
mpark,variant,223312031,https://api.github.com/repos/mpark/variant/issues/comments/296181590,296181590,MDEyOklzc3VlQ29tbWVudDI5NjE4MTU5MA==,2275240,2017-04-21T12:48:45Z,2017-04-21T12:48:45Z,OWNER,"It's not clear to me how this ticket would help with the implementation of `expected`. Even if `E` is required to be nothrow move, `T` doesn't have that requirement right? So it seems to me like it wouldn't help in the general case? or does `expected` have `optional`-like behavior where it falls back to the default-constructed error state or something?",NA,https://api.github.com/repos/mpark/variant/issues/25/comments,https://github.com/mpark/variant/issues/25#issuecomment-296181590,https://api.github.com/repos/mpark/variant/issues/25
mpark,variant,223312031,https://api.github.com/repos/mpark/variant/issues/comments/296185393,296185393,MDEyOklzc3VlQ29tbWVudDI5NjE4NTM5Mw==,721225,2017-04-21T13:06:37Z,2017-04-21T13:06:37Z,NONE,"Hrr, maybe you are right and I was thinking on P0110R0.
No, neither std::expected nor std::optional fallbacks, at least I don't remember of that.
",NA,https://api.github.com/repos/mpark/variant/issues/25/comments,https://github.com/mpark/variant/issues/25#issuecomment-296185393,https://api.github.com/repos/mpark/variant/issues/25
mpark,variant,212287108,https://api.github.com/repos/mpark/variant/issues/comments/284636106,284636106,MDEyOklzc3VlQ29tbWVudDI4NDYzNjEwNg==,721225,2017-03-07T06:42:04Z,2017-03-07T06:42:04Z,NONE,Could you give more context so that we know what lhs and rhs are?,NA,https://api.github.com/repos/mpark/variant/issues/23/comments,https://github.com/mpark/variant/issues/23#issuecomment-284636106,https://api.github.com/repos/mpark/variant/issues/23
mpark,variant,212287108,https://api.github.com/repos/mpark/variant/issues/comments/284834263,284834263,MDEyOklzc3VlQ29tbWVudDI4NDgzNDI2Mw==,98601,2017-03-07T19:35:57Z,2017-03-07T19:35:57Z,NONE,"Sure --  as far as I can tell (not super familiar with the implementation), this warning will occur regardless of the alternative types of the actual `variant` in use, because the `lhs` and `rhs` involved here are internal to the implementation. The error location is [here](https://github.com/mpark/variant/blob/6bae888c3a8eb19a3bbec76b1453e682439a2eac/include/mpark/variant.hpp#L754), so:

* `lhs` is `mpark::detail::constructor<Traits>` and `lhs.index_` is [`unsigned int index_` from `base`](https://github.com/mpark/variant/blob/6bae888c3a8eb19a3bbec76b1453e682439a2eac/include/mpark/variant.hpp#L673)
* the type of `rhs` is a templated type, but I'm guessing it's also some instantation of `base`, meaning `rhs.index()` is [`base::index()`](https://github.com/mpark/variant/blob/6bae888c3a8eb19a3bbec76b1453e682439a2eac/include/mpark/variant.hpp#L660), returning `std::size_t`.

",NA,https://api.github.com/repos/mpark/variant/issues/23/comments,https://github.com/mpark/variant/issues/23#issuecomment-284834263,https://api.github.com/repos/mpark/variant/issues/23
mpark,variant,212287108,https://api.github.com/repos/mpark/variant/issues/comments/288552431,288552431,MDEyOklzc3VlQ29tbWVudDI4ODU1MjQzMQ==,2275240,2017-03-22T21:56:32Z,2017-03-22T21:56:52Z,OWNER,"Thanks for reporting this @jfirebaugh!
I've fixed this in https://github.com/mpark/variant/commit/d8fd0fbba727f0f2f757a05ae7a86e14c3b5982c",NA,https://api.github.com/repos/mpark/variant/issues/23/comments,https://github.com/mpark/variant/issues/23#issuecomment-288552431,https://api.github.com/repos/mpark/variant/issues/23
mpark,variant,207096169,https://api.github.com/repos/mpark/variant/issues/comments/343926015,343926015,MDEyOklzc3VlQ29tbWVudDM0MzkyNjAxNQ==,2275240,2017-11-13T13:56:54Z,2017-11-13T13:56:54Z,OWNER,https://github.com/mpark/variant/tree/unsafe,NA,https://api.github.com/repos/mpark/variant/issues/21/comments,https://github.com/mpark/variant/issues/21#issuecomment-343926015,https://api.github.com/repos/mpark/variant/issues/21
mpark,variant,199535134,https://api.github.com/repos/mpark/variant/issues/comments/303236850,303236850,MDEyOklzc3VlQ29tbWVudDMwMzIzNjg1MA==,2275240,2017-05-22T22:26:25Z,2017-05-22T22:26:25Z,OWNER,Can't seem to be able to get this to work... doesn't seem worth the trouble.,NA,https://api.github.com/repos/mpark/variant/issues/19/comments,https://github.com/mpark/variant/issues/19#issuecomment-303236850,https://api.github.com/repos/mpark/variant/issues/19
mpark,variant,199533508,https://api.github.com/repos/mpark/variant/issues/comments/273037093,273037093,MDEyOklzc3VlQ29tbWVudDI3MzAzNzA5Mw==,2275240,2017-01-17T06:59:53Z,2017-01-17T06:59:53Z,OWNER,https://github.com/mpark/variant/commit/ac3f4d2aa2166b6563db761f4ec2aab1fbb9f586,NA,https://api.github.com/repos/mpark/variant/issues/17/comments,https://github.com/mpark/variant/issues/17#issuecomment-273037093,https://api.github.com/repos/mpark/variant/issues/17
mpark,variant,199533379,https://api.github.com/repos/mpark/variant/issues/comments/312396407,312396407,MDEyOklzc3VlQ29tbWVudDMxMjM5NjQwNw==,2275240,2017-07-01T00:00:09Z,2017-07-01T00:00:09Z,OWNER,"1b2177310f2da7dbe7987320cba84f1885a8743e
e65a3572884c768ff50049f59d89a3625ffc97fb",NA,https://api.github.com/repos/mpark/variant/issues/16/comments,https://github.com/mpark/variant/issues/16#issuecomment-312396407,https://api.github.com/repos/mpark/variant/issues/16
mpark,variant,197564683,https://api.github.com/repos/mpark/variant/issues/comments/269853043,269853043,MDEyOklzc3VlQ29tbWVudDI2OTg1MzA0Mw==,2275240,2016-12-31T07:00:35Z,2016-12-31T07:00:35Z,OWNER,Addressed by c1157db422a0c3237680099a2d2279ca88b6c1e8,NA,https://api.github.com/repos/mpark/variant/issues/15/comments,https://github.com/mpark/variant/issues/15#issuecomment-269853043,https://api.github.com/repos/mpark/variant/issues/15
mpark,variant,197563836,https://api.github.com/repos/mpark/variant/issues/comments/343925652,343925652,MDEyOklzc3VlQ29tbWVudDM0MzkyNTY1Mg==,2275240,2017-11-13T13:55:43Z,2017-11-13T13:55:43Z,OWNER,https://mpark.github.io/variant/,NA,https://api.github.com/repos/mpark/variant/issues/14/comments,https://github.com/mpark/variant/issues/14#issuecomment-343925652,https://api.github.com/repos/mpark/variant/issues/14
mpark,variant,197563697,https://api.github.com/repos/mpark/variant/issues/comments/269268413,269268413,MDEyOklzc3VlQ29tbWVudDI2OTI2ODQxMw==,2275240,2016-12-27T04:33:15Z,2017-05-21T06:14:50Z,OWNER,"With lack of relaxed constexpr support in < GCC 5, I think I'll hold off on this unless it is requested.
__UPDATE__: I've backported to C++11, which means the relaxed `constexpr` is no longer an issue.",NA,https://api.github.com/repos/mpark/variant/issues/13/comments,https://github.com/mpark/variant/issues/13#issuecomment-269268413,https://api.github.com/repos/mpark/variant/issues/13
mpark,variant,197563697,https://api.github.com/repos/mpark/variant/issues/comments/301043027,301043027,MDEyOklzc3VlQ29tbWVudDMwMTA0MzAyNw==,2275240,2017-05-12T10:33:13Z,2017-05-12T10:33:13Z,OWNER,"Doesn't seem to be worth the pain, at least for now.",NA,https://api.github.com/repos/mpark/variant/issues/13/comments,https://github.com/mpark/variant/issues/13#issuecomment-301043027,https://api.github.com/repos/mpark/variant/issues/13
mpark,variant,197563472,https://api.github.com/repos/mpark/variant/issues/comments/271263087,271263087,MDEyOklzc3VlQ29tbWVudDI3MTI2MzA4Nw==,2275240,2017-01-09T11:31:04Z,2017-01-09T11:31:04Z,OWNER,https://github.com/mpark/variant/commit/756fffc25f5e3317428a50e08e47774da9711e93,NA,https://api.github.com/repos/mpark/variant/issues/12/comments,https://github.com/mpark/variant/issues/12#issuecomment-271263087,https://api.github.com/repos/mpark/variant/issues/12
mpark,variant,189496990,https://api.github.com/repos/mpark/variant/issues/comments/269168610,269168610,MDEyOklzc3VlQ29tbWVudDI2OTE2ODYxMA==,2275240,2016-12-26T06:18:34Z,2016-12-26T06:18:34Z,OWNER,"Hi Erik! Sorry for the delayed response. I just finished implementing the latest specification of `std::variant`, and I've introduced `cpp17::addressof` which tries to be `constexpr` as possible.",NA,https://api.github.com/repos/mpark/variant/issues/11/comments,https://github.com/mpark/variant/issues/11#issuecomment-269168610,https://api.github.com/repos/mpark/variant/issues/11
mpark,variant,178508078,https://api.github.com/repos/mpark/variant/issues/comments/269168704,269168704,MDEyOklzc3VlQ29tbWVudDI2OTE2ODcwNA==,2275240,2016-12-26T06:19:15Z,2016-12-26T06:19:15Z,OWNER,"Thanks for the PR! I've completely overhauled the implementation, and have added `variant_size_v` in the process.",NA,https://api.github.com/repos/mpark/variant/issues/10/comments,https://github.com/mpark/variant/pull/10#issuecomment-269168704,https://api.github.com/repos/mpark/variant/issues/10
mpark,variant,178508078,https://api.github.com/repos/mpark/variant/issues/comments/269222537,269222537,MDEyOklzc3VlQ29tbWVudDI2OTIyMjUzNw==,6893883,2016-12-26T15:59:13Z,2016-12-26T15:59:13Z,NONE,Nice! Thanks :),NA,https://api.github.com/repos/mpark/variant/issues/10/comments,https://github.com/mpark/variant/pull/10#issuecomment-269222537,https://api.github.com/repos/mpark/variant/issues/10
mpark,variant,164314623,https://api.github.com/repos/mpark/variant/issues/comments/231121288,231121288,MDEyOklzc3VlQ29tbWVudDIzMTEyMTI4OA==,721225,2016-07-07T15:49:56Z,2016-07-07T15:49:56Z,NONE,"Don't foget http://wiki.edg.com/pub/Wg21oulu/StrawPolls/P0032R3.pdf
",NA,https://api.github.com/repos/mpark/variant/issues/9/comments,https://github.com/mpark/variant/issues/9#issuecomment-231121288,https://api.github.com/repos/mpark/variant/issues/9
mpark,variant,164314623,https://api.github.com/repos/mpark/variant/issues/comments/269168256,269168256,MDEyOklzc3VlQ29tbWVudDI2OTE2ODI1Ng==,2275240,2016-12-26T06:15:34Z,2016-12-26T06:15:34Z,OWNER,"Implemented the most recent version of `std::variant` specification.
re: P0032R3, parts of it were superseded by http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0504r0.html",NA,https://api.github.com/repos/mpark/variant/issues/9/comments,https://github.com/mpark/variant/issues/9#issuecomment-269168256,https://api.github.com/repos/mpark/variant/issues/9
mpark,variant,127473940,https://api.github.com/repos/mpark/variant/issues/comments/301042960,301042960,MDEyOklzc3VlQ29tbWVudDMwMTA0Mjk2MA==,2275240,2017-05-12T10:32:49Z,2017-05-12T10:32:49Z,OWNER,Closing this as `is_trivially_copyable` is no longer used.,NA,https://api.github.com/repos/mpark/variant/issues/8/comments,https://github.com/mpark/variant/issues/8#issuecomment-301042960,https://api.github.com/repos/mpark/variant/issues/8
mpark,variant,114390765,https://api.github.com/repos/mpark/variant/issues/comments/168450544,168450544,MDEyOklzc3VlQ29tbWVudDE2ODQ1MDU0NA==,2275240,2016-01-03T02:29:38Z,2016-01-03T02:29:38Z,OWNER,"https://github.com/mpark/variant/commit/aa77f7e42fe6d8e626a5c19ab7b02cad36b1d0bc
",NA,https://api.github.com/repos/mpark/variant/issues/6/comments,https://github.com/mpark/variant/issues/6#issuecomment-168450544,https://api.github.com/repos/mpark/variant/issues/6
mpark,variant,114390725,https://api.github.com/repos/mpark/variant/issues/comments/167466235,167466235,MDEyOklzc3VlQ29tbWVudDE2NzQ2NjIzNQ==,2275240,2015-12-28T03:12:28Z,2015-12-28T03:12:40Z,OWNER,"Investigated the feasibility of supporting VS 2015 Update 1. I think there are currently too many missing features to justify the energy required to support it. Will re-investigate when update 2 is released.

The following are a few features I'm waiting on.
- Variable templates
- C++14 `constexpr`
- Full expression SFINAE
",NA,https://api.github.com/repos/mpark/variant/issues/5/comments,https://github.com/mpark/variant/issues/5#issuecomment-167466235,https://api.github.com/repos/mpark/variant/issues/5
mpark,variant,114390725,https://api.github.com/repos/mpark/variant/issues/comments/167603196,167603196,MDEyOklzc3VlQ29tbWVudDE2NzYwMzE5Ng==,1539014,2015-12-28T16:57:51Z,2015-12-28T16:57:51Z,NONE,"BTW, VS2015/update1 shipped with Clang/C2. Your existing code might just compile... 

P.S. Unfortunately, given the amount of templated code in the headers, it would probably be an all-or-nothing deal. I.e. no joy in regular VS builds.
",NA,https://api.github.com/repos/mpark/variant/issues/5/comments,https://github.com/mpark/variant/issues/5#issuecomment-167603196,https://api.github.com/repos/mpark/variant/issues/5
mpark,variant,114390725,https://api.github.com/repos/mpark/variant/issues/comments/167615876,167615876,MDEyOklzc3VlQ29tbWVudDE2NzYxNTg3Ng==,2275240,2015-12-28T18:04:07Z,2015-12-28T18:04:07Z,OWNER,"@os12 Yeah, I found some other projects that support VS with Clang/C2. Do you think that would be useful?
",NA,https://api.github.com/repos/mpark/variant/issues/5/comments,https://github.com/mpark/variant/issues/5#issuecomment-167615876,https://api.github.com/repos/mpark/variant/issues/5
mpark,variant,114390725,https://api.github.com/repos/mpark/variant/issues/comments/167624232,167624232,MDEyOklzc3VlQ29tbWVudDE2NzYyNDIzMg==,1539014,2015-12-28T18:37:51Z,2015-12-28T18:37:51Z,NONE,"Well, I want to move my Windows-based builds to Clang/C2 when it matures 
a bit (you know, when it builds Boost). I love the modern frontend and 
VS/intellisense combo... but, who knows how stable it will be.

As for your std::variant implementation - it would be wonderful to be 
able to pick it up. Hopefully the support will require minimal or even 
no work from you...
",NA,https://api.github.com/repos/mpark/variant/issues/5/comments,https://github.com/mpark/variant/issues/5#issuecomment-167624232,https://api.github.com/repos/mpark/variant/issues/5
mpark,variant,114390725,https://api.github.com/repos/mpark/variant/issues/comments/270612086,270612086,MDEyOklzc3VlQ29tbWVudDI3MDYxMjA4Ng==,2275240,2017-01-05T10:26:08Z,2017-01-05T10:26:08Z,OWNER,"I've added support for `VS 2015` with `Clang/LLVM`. That is, via `clang-cl`.
I've also added CI on Windows via AppVeyor: https://ci.appveyor.com/project/mpark/variant",NA,https://api.github.com/repos/mpark/variant/issues/5/comments,https://github.com/mpark/variant/issues/5#issuecomment-270612086,https://api.github.com/repos/mpark/variant/issues/5
mpark,variant,114389753,https://api.github.com/repos/mpark/variant/issues/comments/269167602,269167602,MDEyOklzc3VlQ29tbWVudDI2OTE2NzYwMg==,2275240,2016-12-26T06:10:05Z,2016-12-26T06:10:05Z,OWNER,Allocator support has been removed from `std::variant`.,NA,https://api.github.com/repos/mpark/variant/issues/4/comments,https://github.com/mpark/variant/issues/4#issuecomment-269167602,https://api.github.com/repos/mpark/variant/issues/4
mpark,variant,114389738,https://api.github.com/repos/mpark/variant/issues/comments/168847885,168847885,MDEyOklzc3VlQ29tbWVudDE2ODg0Nzg4NQ==,2275240,2016-01-04T23:52:22Z,2016-01-04T23:52:22Z,OWNER,"Should be SFINAE + `= delete` as per http://cplusplus.github.io/LWG/lwg-defects.html#2367
",NA,https://api.github.com/repos/mpark/variant/issues/3/comments,https://github.com/mpark/variant/issues/3#issuecomment-168847885,https://api.github.com/repos/mpark/variant/issues/3
mpark,variant,114389738,https://api.github.com/repos/mpark/variant/issues/comments/169169268,169169268,MDEyOklzc3VlQ29tbWVudDE2OTE2OTI2OA==,721225,2016-01-05T23:36:35Z,2016-01-05T23:36:35Z,NONE,"I don't see how this issue is related. To which operations are you referring to?
For get<T>, it should be assert as it is done for pair and tuple.
",NA,https://api.github.com/repos/mpark/variant/issues/3/comments,https://github.com/mpark/variant/issues/3#issuecomment-169169268,https://api.github.com/repos/mpark/variant/issues/3
mpark,variant,114389738,https://api.github.com/repos/mpark/variant/issues/comments/169174263,169174263,MDEyOklzc3VlQ29tbWVudDE2OTE3NDI2Mw==,2275240,2016-01-06T00:06:09Z,2016-01-06T00:40:02Z,OWNER,"@viboes: I think you're right. In that comment, I was referring to constructors. That is, `std::is_constructible<variant<int, std::string>, in_place_type_t<foo>>::value` should be `false`, rather than `true` or a hard error. What do you think?
",NA,https://api.github.com/repos/mpark/variant/issues/3/comments,https://github.com/mpark/variant/issues/3#issuecomment-169174263,https://api.github.com/repos/mpark/variant/issues/3
mpark,variant,114389738,https://api.github.com/repos/mpark/variant/issues/comments/169179502,169179502,MDEyOklzc3VlQ29tbWVudDE2OTE3OTUwMg==,721225,2016-01-06T00:40:48Z,2016-01-06T00:40:48Z,NONE,"Ah I see how the issue is related to these constructors. The overloaded constructor should not participate in overload resolution until Foo is one of the types.

I don't see what do you mean by + = delete. There is nothing to delete. SFINAE do everything; isn't it?
",NA,https://api.github.com/repos/mpark/variant/issues/3/comments,https://github.com/mpark/variant/issues/3#issuecomment-169179502,https://api.github.com/repos/mpark/variant/issues/3
mpark,variant,114389738,https://api.github.com/repos/mpark/variant/issues/comments/169181532,169181532,MDEyOklzc3VlQ29tbWVudDE2OTE4MTUzMg==,2275240,2016-01-06T00:52:59Z,2016-01-06T00:52:59Z,OWNER,"I think semantically they would behave the same. It came up because the current proposal is worded as follows:

> **Remarks**: The function shall not participate in overload resolution unless `T` is one of `Types...`. The function shall be `= delete` if there are multiple occurrences of `T` in `Types...`. If `T`â€™s selected constructor is a `constexpr` constructor, this constructor shall be a `constexpr` constructor.

Perhaps we need to revisit the wording and consider the SFINAE wording of ""does not participate in overload resolution"".

In terms of the actual implementation, `= delete` would report different error messages. Whether it's better or not is perhaps subjective.

``` cpp
#include <iostream>
#include <type_traits>

struct Bar {
  Bar() = delete;
};

template <typename T>
struct Foo {
  template <typename U = T,
            std::enable_if_t<std::is_default_constructible<U>::value, int> = 0>
  Foo() {}

  template <typename U = T,
            std::enable_if_t<!std::is_default_constructible<U>::value, int> = 0>
  Foo() = delete;
};

int main() {
  static_assert(!std::is_default_constructible<Bar>::value, """");
  static_assert(!std::is_default_constructible<Foo<Bar>>::value, """");
}
```

This compiles successfully with or without the presence of the deleted constructor. If I were to attempt to construction however, the error messages are different:

With `= delete`, I get:

```
a.cc:22:3: error: call to deleted constructor of 'Foo<Bar>'
  Foo<Bar>{};
  ^       ~~
a.cc:16:3: note: 'Foo<Bar, 0>' has been explicitly marked deleted here
  Foo() = delete;
  ^
1 error generated.
```

whereas without the `= delete`, I get:

```
a.cc:18:3: error: no matching constructor for initialization of 'Foo<Bar>'
  Foo<Bar>{};
  ^       ~~
/usr/local/Cellar/llvm36/3.6.2/lib/llvm-3.6/bin/../include/c++/v1/type_traits:235:78: note:
      candidate template ignored: disabled by 'enable_if' [with U = Bar]
  ...<bool _Bp, class _Tp = void> using enable_if_t = typename enable_if<_Bp, _Tp>::t...
                                                                         ^
a.cc:9:8: note: candidate constructor (the implicit copy constructor) not viable:
      requires 1 argument, but 0 were provided
struct Foo {
       ^
a.cc:9:8: note: candidate constructor (the implicit move constructor) not viable:
      requires 1 argument, but 0 were provided
1 error generated.
```
",NA,https://api.github.com/repos/mpark/variant/issues/3/comments,https://github.com/mpark/variant/issues/3#issuecomment-169181532,https://api.github.com/repos/mpark/variant/issues/3
mpark,variant,114389738,https://api.github.com/repos/mpark/variant/issues/comments/169184280,169184280,MDEyOklzc3VlQ29tbWVudDE2OTE4NDI4MA==,721225,2016-01-06T01:03:01Z,2016-01-06T01:03:01Z,NONE,"I see. I would let this as an implementation detail.
",NA,https://api.github.com/repos/mpark/variant/issues/3/comments,https://github.com/mpark/variant/issues/3#issuecomment-169184280,https://api.github.com/repos/mpark/variant/issues/3
mpark,variant,114389738,https://api.github.com/repos/mpark/variant/issues/comments/170167908,170167908,MDEyOklzc3VlQ29tbWVudDE3MDE2NzkwOA==,2275240,2016-01-09T00:26:05Z,2016-01-09T00:26:12Z,OWNER,"Yep. The updated proposal will use the SFINAE wording for these constructors.
",NA,https://api.github.com/repos/mpark/variant/issues/3/comments,https://github.com/mpark/variant/issues/3#issuecomment-170167908,https://api.github.com/repos/mpark/variant/issues/3
mpark,variant,114389673,https://api.github.com/repos/mpark/variant/issues/comments/168454663,168454663,MDEyOklzc3VlQ29tbWVudDE2ODQ1NDY2Mw==,2275240,2016-01-03T03:02:09Z,2016-01-03T03:02:09Z,OWNER,"1. Construction and assignment from a corrupted `variant` leaves the target `variant` in a corrupted state.
2. Hashing a corrupted `variant` returns `tuple_not_found`.
3. Swapping with corrupted `variant`s simply swaps the states.
4. Relational operators do not throw in the presence of corrupted `variant`s. `v == w` where both `v` and `w` are corrupted returns `true`, `v < w` where both `v` and `w` are corrupted returns `false`. Corrupted `variant`s are considered to be greater than uncorrupted ones based on the index.
",NA,https://api.github.com/repos/mpark/variant/issues/2/comments,https://github.com/mpark/variant/issues/2#issuecomment-168454663,https://api.github.com/repos/mpark/variant/issues/2
mpark,variant,114389673,https://api.github.com/repos/mpark/variant/issues/comments/168613099,168613099,MDEyOklzc3VlQ29tbWVudDE2ODYxMzA5OQ==,2275240,2016-01-04T08:45:43Z,2016-01-04T08:45:43Z,OWNER,"https://github.com/mpark/variant/commit/1e4f17fd2c4564cda26748f0619ba6b272a95342
",NA,https://api.github.com/repos/mpark/variant/issues/2/comments,https://github.com/mpark/variant/issues/2#issuecomment-168613099,https://api.github.com/repos/mpark/variant/issues/2
mpark,variant,113785584,https://api.github.com/repos/mpark/variant/issues/comments/152258071,152258071,MDEyOklzc3VlQ29tbWVudDE1MjI1ODA3MQ==,2275240,2015-10-29T17:30:32Z,2015-10-29T17:30:32Z,OWNER,"Hi @viboes, I had `type_switch` before when I was implementing [P00080R0](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/p0080r0.pdf). Since I'm now implementing the Kona `variant`, I've implemented `visit` instead.
",NA,https://api.github.com/repos/mpark/variant/issues/1/comments,https://github.com/mpark/variant/issues/1#issuecomment-152258071,https://api.github.com/repos/mpark/variant/issues/1
mpark,variant,113785584,https://api.github.com/repos/mpark/variant/issues/comments/152261746,152261746,MDEyOklzc3VlQ29tbWVudDE1MjI2MTc0Ng==,721225,2015-10-29T17:42:56Z,2015-10-29T17:42:56Z,NONE,"Could you point me to your old implementation of type_switch? In which file it was?

Vicente Botet
",NA,https://api.github.com/repos/mpark/variant/issues/1/comments,https://github.com/mpark/variant/issues/1#issuecomment-152261746,https://api.github.com/repos/mpark/variant/issues/1
mpark,variant,113785584,https://api.github.com/repos/mpark/variant/issues/comments/152263291,152263291,MDEyOklzc3VlQ29tbWVudDE1MjI2MzI5MQ==,2275240,2015-10-29T17:48:23Z,2015-10-29T17:48:48Z,OWNER,"The [P0080R0](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/p0080r0.pdf) implementation is at [this commit](https://github.com/mpark/variant/tree/f61d6b2903f73064285fc24686db0282560eea02). `type_switch` in specific is [here](https://github.com/mpark/variant/blob/f61d6b2903f73064285fc24686db0282560eea02/include/mpark/variant/type_switch.hpp).
",NA,https://api.github.com/repos/mpark/variant/issues/1/comments,https://github.com/mpark/variant/issues/1#issuecomment-152263291,https://api.github.com/repos/mpark/variant/issues/1
mpark,variant,113785584,https://api.github.com/repos/mpark/variant/issues/comments/152346125,152346125,MDEyOklzc3VlQ29tbWVudDE1MjM0NjEyNQ==,721225,2015-10-29T22:31:00Z,2015-10-29T22:37:07Z,NONE,"Thanks. I don't see how the customization is done? 
",NA,https://api.github.com/repos/mpark/variant/issues/1/comments,https://github.com/mpark/variant/issues/1#issuecomment-152346125,https://api.github.com/repos/mpark/variant/issues/1
mpark,variant,113785584,https://api.github.com/repos/mpark/variant/issues/comments/152697377,152697377,MDEyOklzc3VlQ29tbWVudDE1MjY5NzM3Nw==,2275240,2015-10-31T05:37:44Z,2015-10-31T05:37:44Z,OWNER,"@viboes: I didn't go as far as to introduce a generalized `type_switch` like your `match` in [P0050R0](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/p0050r0.pdf). My focus was on exploring what the `type_switch` syntax would look like, and which `variant` specific parts will need to be supported.
",NA,https://api.github.com/repos/mpark/variant/issues/1/comments,https://github.com/mpark/variant/issues/1#issuecomment-152697377,https://api.github.com/repos/mpark/variant/issues/1
mpark,variant,113785584,https://api.github.com/repos/mpark/variant/issues/comments/152707592,152707592,MDEyOklzc3VlQ29tbWVudDE1MjcwNzU5Mg==,721225,2015-10-31T07:29:20Z,2015-10-31T07:29:20Z,NONE,"No problem. I was curious to see your implementation.
",NA,https://api.github.com/repos/mpark/variant/issues/1/comments,https://github.com/mpark/variant/issues/1#issuecomment-152707592,https://api.github.com/repos/mpark/variant/issues/1
passy,build-time-tracker-plugin,761814457,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/743497996,743497996,MDEyOklzc3VlQ29tbWVudDc0MzQ5Nzk5Ng==,22418250,2020-12-12T00:21:37Z,2020-12-12T00:21:37Z,NONE,"If this repository is no longer maintained, it should be [archived](https://docs.github.com/en/free-pro-team@latest/github/creating-cloning-and-archiving-repositories/archiving-a-github-repository)",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/102/comments,https://github.com/passy/build-time-tracker-plugin/issues/102#issuecomment-743497996,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/102
passy,build-time-tracker-plugin,761814457,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/743530488,743530488,MDEyOklzc3VlQ29tbWVudDc0MzUzMDQ4OA==,1302775,2020-12-12T00:44:40Z,2020-12-12T00:44:40Z,NONE,"@cdalexndr That's up to the maintainers, and not written in stone. But you can see that there's not been any recent commits.",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/102/comments,https://github.com/passy/build-time-tracker-plugin/issues/102#issuecomment-743530488,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/102
passy,build-time-tracker-plugin,666936298,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/732027154,732027154,MDEyOklzc3VlQ29tbWVudDczMjAyNzE1NA==,39644124,2020-11-23T09:07:55Z,2020-11-23T09:08:12Z,NONE,"Here's a sample using the Kotlin DSL:

```kotlin
plugins {
    id(""net.rdrei.android.buildtimetracker"") version ""0.11.0""
}

// ...

buildtimetracker {
    reporters {
        register(""csv"") {
            options[""output""] = ""build/times.csv""
            options[""append""] = ""true""
            options[""header""] = ""false""
        }

        register(""summary"") {
            options[""ordered""] = ""false""
            options[""threshold""] = ""50""
            options[""barstyle""] = ""unicode""
        }

        register(""csvSummary"") {
            options[""csv""] = ""build/times.csv""
        }
    }
}
```",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/101/comments,https://github.com/passy/build-time-tracker-plugin/issues/101#issuecomment-732027154,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/101
passy,build-time-tracker-plugin,638928880,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/644195901,644195901,MDEyOklzc3VlQ29tbWVudDY0NDE5NTkwMQ==,1370302,2020-06-15T15:13:14Z,2020-06-15T15:13:14Z,NONE,Here is the commit which deprecated it: https://github.com/gradle/gradle/issues/7613,NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/100/comments,https://github.com/passy/build-time-tracker-plugin/issues/100#issuecomment-644195901,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/100
passy,build-time-tracker-plugin,547926176,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/572991031,572991031,MDEyOklzc3VlQ29tbWVudDU3Mjk5MTAzMQ==,9906,2020-01-10T11:03:45Z,2020-01-10T11:03:45Z,OWNER,"Not the build process itself, for sure. It doesn't have the opportunity to do that based on the hooks it uses. However, there is still the remaining issue https://github.com/passy/build-time-tracker-plugin/issues/70 of the logs being ""funbounded"" if you use the append option which can cause the exit times to increase over time.",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/97/comments,https://github.com/passy/build-time-tracker-plugin/issues/97#issuecomment-572991031,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/97
passy,build-time-tracker-plugin,440085179,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/495671771,495671771,MDEyOklzc3VlQ29tbWVudDQ5NTY3MTc3MQ==,6650137,2019-05-24T15:11:29Z,2019-05-24T15:11:29Z,NONE,"I'm researching a complete CI setup that would allow us to gather data about builds on multiple pipelines. I think this example would be very useful @arcadefire. What did you plan as merging strategy for target endpoint? Would you simply upload new file, rewriting contents? I'm concerned about case, where multiple builds pick up historic stats, perform build and append data about current one, overwriting the other. ",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/95/comments,https://github.com/passy/build-time-tracker-plugin/issues/95#issuecomment-495671771,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/95
passy,build-time-tracker-plugin,440085179,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/496517253,496517253,MDEyOklzc3VlQ29tbWVudDQ5NjUxNzI1Mw==,3945083,2019-05-28T13:30:41Z,2019-05-28T13:36:25Z,NONE,"My initial idea was to just send a post request containing the same JSON generated by the `JsonReporter` to a remote endpoint that logs the data on a DB. I haven't thought about a merging strategy.

For now, my simple endpoint would just append the entries in order to create some sort of historical stats table, used to observe the project's overall compilation times trend.

Since a PR says more than a thousand words, I condensed here my proposal: https://github.com/passy/build-time-tracker-plugin/pull/96. ",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/95/comments,https://github.com/passy/build-time-tracker-plugin/issues/95#issuecomment-496517253,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/95
passy,build-time-tracker-plugin,440085179,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/496561772,496561772,MDEyOklzc3VlQ29tbWVudDQ5NjU2MTc3Mg==,6650137,2019-05-28T15:16:53Z,2019-05-28T15:16:53Z,NONE,"Thank you very much for taking the time to compile the results. 
I understand now, because (in your custom reporter) you use `run(List<Timing> timings)`, you don't need to upload aggregated xml report file, but can instead upload `this` build data. Is that correct? I'll use your example to test locally so that I can check how easy it is to visualise/gain insights. 

During my research, I prepared minimal (local) installation for nebula plugin (https://github.com/nebula-plugins/gradle-metrics-plugin) but I found it very hard to query/visualise that data in kibana later. Your approach looks very promising in that regard. ",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/95/comments,https://github.com/passy/build-time-tracker-plugin/issues/95#issuecomment-496561772,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/95
passy,build-time-tracker-plugin,371460302,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/430956644,430956644,MDEyOklzc3VlQ29tbWVudDQzMDk1NjY0NA==,2906988,2018-10-18T10:23:04Z,2018-10-18T10:23:04Z,NONE,"Condition: the configured CSV file doesn't contain rows.

Root cause: `times` Map in `CSVSummaryReporter` is empty.

`times.collect { it.key >= midnight ? it.value : 0 }.sum()`: `sum` returns `null` for empty map/collection
`times.keySet().min()`: `min` returns `null` for empty map/set",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/94/comments,https://github.com/passy/build-time-tracker-plugin/issues/94#issuecomment-430956644,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/94
passy,build-time-tracker-plugin,349489038,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/412471573,412471573,MDEyOklzc3VlQ29tbWVudDQxMjQ3MTU3Mw==,42003708,2018-08-13T10:14:10Z,2018-08-13T10:14:10Z,CONTRIBUTOR,"Hi @passy, it this something you'd be able to look at anytime soon? Thanks",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/91/comments,https://github.com/passy/build-time-tracker-plugin/issues/91#issuecomment-412471573,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/91
passy,build-time-tracker-plugin,349489038,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/412520472,412520472,MDEyOklzc3VlQ29tbWVudDQxMjUyMDQ3Mg==,9906,2018-08-13T13:36:57Z,2018-08-13T13:36:57Z,OWNER,"Hey @prageethintuit, sorry but I won't have time to look into this. Happy to review PRs for this, but I won't be able to explore this myself.",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/91/comments,https://github.com/passy/build-time-tracker-plugin/issues/91#issuecomment-412520472,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/91
passy,build-time-tracker-plugin,349489038,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/412565794,412565794,MDEyOklzc3VlQ29tbWVudDQxMjU2NTc5NA==,230793,2018-08-13T15:49:14Z,2018-08-13T15:49:14Z,NONE,@passy Please see https://github.com/passy/build-time-tracker-plugin/pull/93,NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/91/comments,https://github.com/passy/build-time-tracker-plugin/issues/91#issuecomment-412565794,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/91
passy,build-time-tracker-plugin,349489038,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/412654209,412654209,MDEyOklzc3VlQ29tbWVudDQxMjY1NDIwOQ==,230793,2018-08-13T20:33:30Z,2018-08-13T20:33:30Z,NONE,@passy Thanks for merging the request. Would you be able to release a patch or minor version please? Cheers! :),NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/91/comments,https://github.com/passy/build-time-tracker-plugin/issues/91#issuecomment-412654209,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/91
passy,build-time-tracker-plugin,349489038,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/412978209,412978209,MDEyOklzc3VlQ29tbWVudDQxMjk3ODIwOQ==,9906,2018-08-14T18:52:44Z,2018-08-14T18:52:44Z,OWNER,Sorry this took so long. Gradle threw some fun NPEs in my face when trying to sign the artifacts. `0.11.1` is up including the change.,NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/91/comments,https://github.com/passy/build-time-tracker-plugin/issues/91#issuecomment-412978209,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/91
passy,build-time-tracker-plugin,349489038,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/413146904,413146904,MDEyOklzc3VlQ29tbWVudDQxMzE0NjkwNA==,42003708,2018-08-15T09:41:12Z,2018-08-15T09:41:49Z,CONTRIBUTOR,"@passy `0.11.1` doesn't seem to be up on maven central yet. Please see:
![Maven Version](https://img.shields.io/maven-central/v/net.rdrei.android.buildtimetracker/gradle-plugin.svg?maxAge=2592000)
http://central.maven.org/maven2/net/rdrei/android/buildtimetracker/gradle-plugin/",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/91/comments,https://github.com/passy/build-time-tracker-plugin/issues/91#issuecomment-413146904,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/91
passy,build-time-tracker-plugin,349489038,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/413909963,413909963,MDEyOklzc3VlQ29tbWVudDQxMzkwOTk2Mw==,42003708,2018-08-17T15:56:02Z,2018-08-17T15:56:02Z,CONTRIBUTOR,@passy Would this be something that can be looked at by next week? Cheers!,NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/91/comments,https://github.com/passy/build-time-tracker-plugin/issues/91#issuecomment-413909963,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/91
passy,build-time-tracker-plugin,349489038,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/413915429,413915429,MDEyOklzc3VlQ29tbWVudDQxMzkxNTQyOQ==,9906,2018-08-17T16:15:05Z,2018-08-17T16:15:05Z,OWNER,"@prageethintuit I'll give it one more try to get this release actually out of the staging area in Maven Central. The tooling is so disastrously terrible that I have no idea whether it will be fruitful this time or not. It took me 3 attempts to log in this time to not receive a `400` and another half-dozen to get the staging list to load.

I should probably just migrate this to Bintray to get the release process to work reliably.",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/91/comments,https://github.com/passy/build-time-tracker-plugin/issues/91#issuecomment-413915429,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/91
passy,build-time-tracker-plugin,349489038,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/414281436,414281436,MDEyOklzc3VlQ29tbWVudDQxNDI4MTQzNg==,42003708,2018-08-20T11:15:24Z,2018-08-20T11:15:24Z,CONTRIBUTOR,"@passy Thanks for sorting this out, it seems to have worked ðŸ‘ ",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/91/comments,https://github.com/passy/build-time-tracker-plugin/issues/91#issuecomment-414281436,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/91
passy,build-time-tracker-plugin,310735521,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/378197807,378197807,MDEyOklzc3VlQ29tbWVudDM3ODE5NzgwNw==,9906,2018-04-03T10:05:07Z,2018-04-03T10:05:07Z,OWNER,"Thanks, that seems like a good thing to do!",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/89/comments,https://github.com/passy/build-time-tracker-plugin/pull/89#issuecomment-378197807,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/89
passy,build-time-tracker-plugin,263337110,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/336510329,336510329,MDEyOklzc3VlQ29tbWVudDMzNjUxMDMyOQ==,16724,2017-10-13T17:02:46Z,2017-10-13T17:02:46Z,NONE,Is there an easy way to use the plugin with your change here in our Gradle now?,NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/86/comments,https://github.com/passy/build-time-tracker-plugin/pull/86#issuecomment-336510329,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/86
passy,build-time-tracker-plugin,263337110,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/336570491,336570491,MDEyOklzc3VlQ29tbWVudDMzNjU3MDQ5MQ==,9906,2017-10-13T21:21:12Z,2017-10-13T21:21:12Z,OWNER,Thanks a lot for this!,NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/86/comments,https://github.com/passy/build-time-tracker-plugin/pull/86#issuecomment-336570491,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/86
passy,build-time-tracker-plugin,263337110,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/336571169,336571169,MDEyOklzc3VlQ29tbWVudDMzNjU3MTE2OQ==,5659391,2017-10-13T21:24:36Z,2017-10-13T21:24:36Z,NONE,Many thanks! This will allow us to move to Gradle 4.x,NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/86/comments,https://github.com/passy/build-time-tracker-plugin/pull/86#issuecomment-336571169,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/86
passy,build-time-tracker-plugin,261197796,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/332731954,332731954,MDEyOklzc3VlQ29tbWVudDMzMjczMTk1NA==,2153370,2017-09-28T05:31:21Z,2017-09-28T05:31:21Z,NONE,Fixing #83 ,NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/85/comments,https://github.com/passy/build-time-tracker-plugin/pull/85#issuecomment-332731954,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/85
passy,build-time-tracker-plugin,261197796,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/333602312,333602312,MDEyOklzc3VlQ29tbWVudDMzMzYwMjMxMg==,9906,2017-10-02T17:20:27Z,2017-10-02T17:20:27Z,OWNER,"@vanta Would it be possible to do this in a backwards-compatible fashion?

And in any case, I believe this will break a bunch of tests that rely on Time being mocked. Could you address that?",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/85/comments,https://github.com/passy/build-time-tracker-plugin/pull/85#issuecomment-333602312,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/85
passy,build-time-tracker-plugin,261197796,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/333630967,333630967,MDEyOklzc3VlQ29tbWVudDMzMzYzMDk2Nw==,2153370,2017-10-02T18:57:21Z,2017-10-02T18:57:21Z,NONE,"Will try to fix that.

pon., 2.10.2017, 10:20 uÅ¼ytkownik Pascal Hartig <notifications@github.com>
napisaÅ‚:

> @vanta <https://github.com/vanta> Would it be possible to do this in a
> backwards-compatible fashion?
>
> And in any case, I believe this will break a bunch of tests that rely on
> Time being mocked. Could you address that?
>
> â€”
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/passy/build-time-tracker-plugin/pull/85#issuecomment-333602312>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ACDbmudNqUfzNUD8b1QylLPnygEQzoOOks5soRtbgaJpZM4Pmvuo>
> .
>
-- 
Krzysztof Wolny
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/85/comments,https://github.com/passy/build-time-tracker-plugin/pull/85#issuecomment-333630967,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/85
passy,build-time-tracker-plugin,261197796,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/334645966,334645966,MDEyOklzc3VlQ29tbWVudDMzNDY0NTk2Ng==,4538363,2017-10-06T02:59:52Z,2017-10-06T02:59:52Z,CONTRIBUTOR,"I don't think it is a good idea if we depend on the `org.gradle.internal` package, or the `org.gradle.util` package,
because it is not the public API of gralde, and can be changed anytime.

I think the only thing needed here is the elapsed time, it can be calculated very simply.
I will create another PR for this issue.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/85/comments,https://github.com/passy/build-time-tracker-plugin/pull/85#issuecomment-334645966,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/85
passy,build-time-tracker-plugin,261197796,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/334654951,334654951,MDEyOklzc3VlQ29tbWVudDMzNDY1NDk1MQ==,2153370,2017-10-06T04:26:58Z,2017-10-06T04:26:58Z,NONE,"Actually, that's the best idea to get rid of this Clock/Timer class at all.
Go for it before I'll fix my PR ;-)

czw., 5.10.2017, 19:59 uÅ¼ytkownik æŽ é¢– <notifications@github.com> napisaÅ‚:

> I don't think it is a good idea if we depend on the org.gradle.internal
> package, or the org.gradle.util package,
> because it is not the public API of gralde, and can be changed anytime.
>
> I think the only thing needed here is the elapsed time, it can be
> calculated very simply.
> I will create another PR for this issue.
>
> â€”
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/passy/build-time-tracker-plugin/pull/85#issuecomment-334645966>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ACDbmpUU1SpMlYZalGQHfqoAVmUt2iKcks5spZepgaJpZM4Pmvuo>
> .
>
-- 
Krzysztof Wolny
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/85/comments,https://github.com/passy/build-time-tracker-plugin/pull/85#issuecomment-334654951,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/85
passy,build-time-tracker-plugin,261197796,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/334656385,334656385,MDEyOklzc3VlQ29tbWVudDMzNDY1NjM4NQ==,4538363,2017-10-06T04:38:50Z,2017-10-06T04:38:50Z,CONTRIBUTOR,"> Actually, that's the best idea to get rid of this Clock/Timer class at all.

Agree, I will try if I can fix it.",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/85/comments,https://github.com/passy/build-time-tracker-plugin/pull/85#issuecomment-334656385,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/85
passy,build-time-tracker-plugin,261197796,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/334659327,334659327,MDEyOklzc3VlQ29tbWVudDMzNDY1OTMyNw==,4538363,2017-10-06T05:05:46Z,2017-10-06T05:05:46Z,CONTRIBUTOR,"Done.
I created a new PR here: https://github.com/passy/build-time-tracker-plugin/pull/86
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/85/comments,https://github.com/passy/build-time-tracker-plugin/pull/85#issuecomment-334659327,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/85
passy,build-time-tracker-plugin,260328779,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/331930100,331930100,MDEyOklzc3VlQ29tbWVudDMzMTkzMDEwMA==,9906,2017-09-25T16:06:10Z,2017-09-25T16:06:10Z,OWNER,"Awesome! Thanks a lot, @flozano.",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/84/comments,https://github.com/passy/build-time-tracker-plugin/pull/84#issuecomment-331930100,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/84
passy,build-time-tracker-plugin,259446259,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/331909922,331909922,MDEyOklzc3VlQ29tbWVudDMzMTkwOTkyMg==,1159291,2017-09-25T15:02:21Z,2017-09-25T15:02:21Z,CONTRIBUTOR,"From looking at gradle code, I'm not sure it's wise to depend on that API ... ",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/83/comments,https://github.com/passy/build-time-tracker-plugin/issues/83#issuecomment-331909922,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/83
passy,build-time-tracker-plugin,259446259,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/331926799,331926799,MDEyOklzc3VlQ29tbWVudDMzMTkyNjc5OQ==,9906,2017-09-25T15:55:38Z,2017-09-25T15:55:38Z,OWNER,"I'm a bit swamped at the moment. If someone else wants to fix this, I'm happy to review a PR.",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/83/comments,https://github.com/passy/build-time-tracker-plugin/issues/83#issuecomment-331926799,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/83
passy,build-time-tracker-plugin,259446259,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/331927678,331927678,MDEyOklzc3VlQ29tbWVudDMzMTkyNzY3OA==,1159291,2017-09-25T15:58:29Z,2017-09-25T15:58:29Z,CONTRIBUTOR,https://github.com/passy/build-time-tracker-plugin/pull/84 I think this should fix it,NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/83/comments,https://github.com/passy/build-time-tracker-plugin/issues/83#issuecomment-331927678,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/83
passy,build-time-tracker-plugin,259446259,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/331941945,331941945,MDEyOklzc3VlQ29tbWVudDMzMTk0MTk0NQ==,9906,2017-09-25T16:46:56Z,2017-09-25T16:46:56Z,OWNER,I've published a new snapshot as `0.11.0-SNAPSHOT`. Could someone verify that this works?,NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/83/comments,https://github.com/passy/build-time-tracker-plugin/issues/83#issuecomment-331941945,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/83
passy,build-time-tracker-plugin,259446259,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/332133617,332133617,MDEyOklzc3VlQ29tbWVudDMzMjEzMzYxNw==,6303277,2017-09-26T08:59:32Z,2017-09-26T08:59:32Z,NONE,"Just tried with Gradle 4.2 and 0.11.0-20170925.164507-1 plugin version.
Unfortunately it now fails with
```
groovy.lang.MissingMethodException: No signature of method: org.gradle.util.Clock.getTimeInMs() is applicable for argument types: () values: []
        at net.rdrei.android.buildtimetracker.TimingRecorder.afterExecute(TimingRecorder.groovy:44)
        at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35)
        at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
        at org.gradle.internal.event.DefaultListenerManager$ListenerDetails.dispatch(DefaultListenerManager.java:371)
        at org.gradle.internal.event.DefaultListenerManager$ListenerDetails.dispatch(DefaultListenerManager.java:353)
        at org.gradle.internal.event.AbstractBroadcastDispatch.dispatch(AbstractBroadcastDispatch.java:58)
        at org.gradle.internal.event.DefaultListenerManager$EventBroadcast$ListenerDispatch.dispatch(DefaultListenerManager.java:341)
        at org.gradle.internal.event.DefaultListenerManager$EventBroadcast$ListenerDispatch.dispatch(DefaultListenerManager.java:328)
        at org.gradle.internal.event.AbstractBroadcastDispatch.dispatch(AbstractBroadcastDispatch.java:42)
        at org.gradle.internal.event.BroadcastDispatch$SingletonDispatch.dispatch(BroadcastDispatch.java:230)
        at org.gradle.internal.event.BroadcastDispatch$SingletonDispatch.dispatch(BroadcastDispatch.java:149)
        at org.gradle.internal.event.ListenerBroadcast.dispatch(ListenerBroadcast.java:140)
        at org.gradle.internal.event.ListenerBroadcast.dispatch(ListenerBroadcast.java:37)
        at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93)
        at com.sun.proxy.$Proxy66.afterExecute(Unknown Source)
        at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.run(DefaultTaskGraphExecuter.java:253)
        at org.gradle.internal.progress.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:336)
        at org.gradle.internal.progress.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:328)
        at org.gradle.internal.progress.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:199)
        at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:110)
        at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:241)
        at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:230)
        at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker.processTask(DefaultTaskPlanExecutor.java:123)
        at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker.access$200(DefaultTaskPlanExecutor.java:79)
        at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker$1.execute(DefaultTaskPlanExecutor.java:104)
        at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker$1.execute(DefaultTaskPlanExecutor.java:98)
        at org.gradle.execution.taskgraph.DefaultTaskExecutionPlan.execute(DefaultTaskExecutionPlan.java:625)
        at org.gradle.execution.taskgraph.DefaultTaskExecutionPlan.executeWithTask(DefaultTaskExecutionPlan.java:580)
        at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker.run(DefaultTaskPlanExecutor.java:98)
        at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63)
        at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46)
        at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55)```
Almost there :)

ps: just in case, same plugin version works fine with Gradle 4.1.",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/83/comments,https://github.com/passy/build-time-tracker-plugin/issues/83#issuecomment-332133617,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/83
passy,build-time-tracker-plugin,259446259,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/334501459,334501459,MDEyOklzc3VlQ29tbWVudDMzNDUwMTQ1OQ==,1191797,2017-10-05T15:28:40Z,2017-10-05T15:28:40Z,NONE,"`org.gradle.util` is not part of the public API, please don't use types in that package. If you need a clock, feel free to copy the code.",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/83/comments,https://github.com/passy/build-time-tracker-plugin/issues/83#issuecomment-334501459,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/83
passy,build-time-tracker-plugin,259446259,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/334660020,334660020,MDEyOklzc3VlQ29tbWVudDMzNDY2MDAyMA==,4538363,2017-10-06T05:12:17Z,2017-10-06T05:12:17Z,CONTRIBUTOR,"Hi, @passy :
I created a PR to fix it, and the Unit Test was passed.
Would you please check my PR, and make a new release if my PR is OK?
Sorry, but we are waiting for the fix....
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/83/comments,https://github.com/passy/build-time-tracker-plugin/issues/83#issuecomment-334660020,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/83
passy,build-time-tracker-plugin,259446259,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/335666585,335666585,MDEyOklzc3VlQ29tbWVudDMzNTY2NjU4NQ==,4538363,2017-10-11T02:51:18Z,2017-10-11T02:51:18Z,CONTRIBUTOR,"Hi, @passy  
If you have time, would you please check my PR for this issue: https://github.com/passy/build-time-tracker-plugin/pull/86

Sorry for the trouble, but we are kind of waiting for it.
Thanks very much.",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/83/comments,https://github.com/passy/build-time-tracker-plugin/issues/83#issuecomment-335666585,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/83
passy,build-time-tracker-plugin,259446259,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/336571127,336571127,MDEyOklzc3VlQ29tbWVudDMzNjU3MTEyNw==,9906,2017-10-13T21:24:22Z,2017-10-13T21:24:22Z,OWNER,Sorry for the wait! I've merged it but probably won't get around to releasing a new version until next week. I did publish a new snapshot though. I'd appreciate it if someone could check if it works with Gradle 4.2.,NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/83/comments,https://github.com/passy/build-time-tracker-plugin/issues/83#issuecomment-336571127,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/83
passy,build-time-tracker-plugin,259446259,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/336626868,336626868,MDEyOklzc3VlQ29tbWVudDMzNjYyNjg2OA==,9906,2017-10-14T10:47:12Z,2017-10-14T10:47:12Z,OWNER,"Sorry again, I've only got a couple of minutes on my hands right now, but the version on master right now is still crashing for me:

```
No signature of method: net.rdrei.android.buildtimetracker.util.Clock.Clock() is applicable for argument types: (java.lang.Long) values: [1507977915620]
Possible solutions: sleep(long), collect(), wait(long), each(groovy.lang.Closure), collect(groovy.lang.Closure), sleep(long, groovy.lang.Closure)
```",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/83/comments,https://github.com/passy/build-time-tracker-plugin/issues/83#issuecomment-336626868,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/83
passy,build-time-tracker-plugin,259446259,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/336768270,336768270,MDEyOklzc3VlQ29tbWVudDMzNjc2ODI3MA==,4538363,2017-10-16T03:01:09Z,2017-10-16T03:01:09Z,CONTRIBUTOR,"Hi @passy 
Sorry, that was my bad.
I created a new PR to fix this error: https://github.com/passy/build-time-tracker-plugin/pull/87",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/83/comments,https://github.com/passy/build-time-tracker-plugin/issues/83#issuecomment-336768270,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/83
passy,build-time-tracker-plugin,259446259,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/337217473,337217473,MDEyOklzc3VlQ29tbWVudDMzNzIxNzQ3Mw==,9906,2017-10-17T12:37:00Z,2017-10-17T12:37:00Z,OWNER,Published another snapshot. For me it's looking good now.,NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/83/comments,https://github.com/passy/build-time-tracker-plugin/issues/83#issuecomment-337217473,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/83
passy,build-time-tracker-plugin,259446259,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/337449715,337449715,MDEyOklzc3VlQ29tbWVudDMzNzQ0OTcxNQ==,4538363,2017-10-18T03:25:01Z,2017-10-18T03:25:01Z,CONTRIBUTOR,"Hi @passy 
I also tested the latest `0.11.0-SNAPSHOT`, it works fine.
So maybe you can publish a real release for this fixed issue.

Thanks very much",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/83/comments,https://github.com/passy/build-time-tracker-plugin/issues/83#issuecomment-337449715,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/83
passy,build-time-tracker-plugin,259446259,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/337907705,337907705,MDEyOklzc3VlQ29tbWVudDMzNzkwNzcwNQ==,9906,2017-10-19T13:28:36Z,2017-10-19T13:28:36Z,OWNER,"Awesome, thanks for confirming! I tried to get a release ready last night, but unfortunately the signing task in Gradle broke with a recent GPG upgrade, so I'll have to fix that first. :(",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/83/comments,https://github.com/passy/build-time-tracker-plugin/issues/83#issuecomment-337907705,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/83
passy,build-time-tracker-plugin,259446259,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/338236384,338236384,MDEyOklzc3VlQ29tbWVudDMzODIzNjM4NA==,9906,2017-10-20T15:16:10Z,2017-10-20T15:16:10Z,OWNER,v0.11.0 is now published. Thanks for your help and patience everyone!,NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/83/comments,https://github.com/passy/build-time-tracker-plugin/issues/83#issuecomment-338236384,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/83
passy,build-time-tracker-plugin,253830694,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/325943266,325943266,MDEyOklzc3VlQ29tbWVudDMyNTk0MzI2Ng==,9906,2017-08-30T09:56:29Z,2017-08-30T09:56:29Z,OWNER,I remember taking this naming convention from some other plugin. I'm a bit hesitant as those changes are quite disruptive. Is this breaking anything in particular for you?,NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/82/comments,https://github.com/passy/build-time-tracker-plugin/issues/82#issuecomment-325943266,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/82
passy,build-time-tracker-plugin,253830694,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/326049361,326049361,MDEyOklzc3VlQ29tbWVudDMyNjA0OTM2MQ==,1302775,2017-08-30T16:42:41Z,2017-08-30T16:54:42Z,NONE,"> is this breaking

Matter of fact, it is. We use common `build.gradle` files that are included in various other builds. For backward compatibility, and since `buildscript` section is not included, but plugins are, we try to apply new plugins conditionally after checking the buildscript path. If a plugin is found on the path, we will apply it. The overly generic name of the build time tracker plugin is a problem because many other plugins also have the words `gradle-plugin` in their names, like the `spring-boot-gradle-plugin` for example, so it makes it impossible to determine whether the plugin is present on the path or not. Following is the snippet of code that I'm referring to:

```
def hasBuildTimeTrackerPlugin = project.buildscript.configurations.classpath
  .find { file(it).name.contains(""gradle-plugin"") }
```

For now, I'm using `file(it).absolutePath.contains(""buildtimetracker"")` to get around this problem, but I think pattern matching absolute path is taking it too far.

If the name changes with a new version, people updating their build files to pick up the new version should be able to update a few more characters to pick up the new name. It's not the end of the world.",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/82/comments,https://github.com/passy/build-time-tracker-plugin/issues/82#issuecomment-326049361,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/82
passy,build-time-tracker-plugin,246715093,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/319033135,319033135,MDEyOklzc3VlQ29tbWVudDMxOTAzMzEzNQ==,9906,2017-07-31T10:45:11Z,2017-07-31T10:45:11Z,OWNER,"Okay, CI issue is unrelated to the change. Looks like Java 7 no longer works. I'll look into that separately.",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/81/comments,https://github.com/passy/build-time-tracker-plugin/pull/81#issuecomment-319033135,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/81
passy,build-time-tracker-plugin,236825951,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/309390539,309390539,MDEyOklzc3VlQ29tbWVudDMwOTM5MDUzOQ==,9906,2017-06-19T09:42:11Z,2017-06-19T09:42:11Z,OWNER,@Nirvanchik That's a good point. `-q` should certainly disable the output. Would you be interested in contributing a pull request for this?,NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/80/comments,https://github.com/passy/build-time-tracker-plugin/issues/80#issuecomment-309390539,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/80
passy,build-time-tracker-plugin,236825951,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/311606257,311606257,MDEyOklzc3VlQ29tbWVudDMxMTYwNjI1Nw==,13875458,2017-06-28T09:25:41Z,2017-06-28T09:26:08Z,NONE,"Not sure I'll do this in my primary work time (we already fixed this by stripping extra lines from gradle output), but **I'll try to do in my free time**.",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/80/comments,https://github.com/passy/build-time-tracker-plugin/issues/80#issuecomment-311606257,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/80
passy,build-time-tracker-plugin,204455786,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/277529335,277529335,MDEyOklzc3VlQ29tbWVudDI3NzUyOTMzNQ==,9906,2017-02-05T16:09:23Z,2017-02-05T16:09:23Z,OWNER,Nice one! Thanks for the comprehensive test coverage. :),NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/78/comments,https://github.com/passy/build-time-tracker-plugin/pull/78#issuecomment-277529335,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/78
passy,build-time-tracker-plugin,200802247,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/272921620,272921620,MDEyOklzc3VlQ29tbWVudDI3MjkyMTYyMA==,9906,2017-01-16T17:30:15Z,2017-01-16T17:30:15Z,OWNER,"Love the idea! 

> I don't have much experience in creating gradle plugins [...]

Neither had I before I started this. :)

Your outline sounds good to me. I'm not sure if there's any form of prior art for customizations like this. I would not be surprised if there's a gradle plugin out there that allows configuration in that style that we could take some inspiration from.",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/77/comments,https://github.com/passy/build-time-tracker-plugin/issues/77#issuecomment-272921620,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/77
passy,build-time-tracker-plugin,200021862,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/272015883,272015883,MDEyOklzc3VlQ29tbWVudDI3MjAxNTg4Mw==,9906,2017-01-11T22:28:06Z,2017-01-11T22:28:06Z,OWNER,Awesome! I really appreciate this. I'll try and get a release out quickly.,NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/76/comments,https://github.com/passy/build-time-tracker-plugin/pull/76#issuecomment-272015883,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/76
passy,build-time-tracker-plugin,198733006,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/270444196,270444196,MDEyOklzc3VlQ29tbWVudDI3MDQ0NDE5Ng==,1096485,2017-01-04T18:16:59Z,2017-01-04T18:16:59Z,NONE,"breakes every build

```
...
:app:clean
:clean

BUILD SUCCESSFUL

Total time: 0.893 secs

FAILURE: Build failed with an exception.

* What went wrong:
Failed to notify build listener.
```",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/75/comments,https://github.com/passy/build-time-tracker-plugin/issues/75#issuecomment-270444196,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/75
passy,build-time-tracker-plugin,198733006,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/270493807,270493807,MDEyOklzc3VlQ29tbWVudDI3MDQ5MzgwNw==,24918301,2017-01-04T21:36:50Z,2017-01-04T21:36:50Z,NONE,"I donâ€™t think is worth it to use the gradle internal class anymore.

From: Pascal Welsch [mailto:notifications@github.com]
Sent: Wednesday, January 04, 2017 12:17 PM
To: passy/build-time-tracker-plugin <build-time-tracker-plugin@noreply.github.com>
Cc: VERWERS, GARY <gv8839@att.com>; Author <author@noreply.github.com>
Subject: Re: [passy/build-time-tracker-plugin] TrueTimeProvider has moved as of gradle 3.3 (#75)


breakes every build

...

:app:clean

:clean



BUILD SUCCESSFUL



Total time: 0.893 secs



FAILURE: Build failed with an exception.



* What went wrong:

Failed to notify build listener.

â€”
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub<https://github.com/passy/build-time-tracker-plugin/issues/75#issuecomment-270444196>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AXw5HdXAHPrL_Iav_RuwsnIwsJRP39quks5rO-IbgaJpZM4LauzY>.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/75/comments,https://github.com/passy/build-time-tracker-plugin/issues/75#issuecomment-270493807,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/75
passy,build-time-tracker-plugin,198733006,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/270933938,270933938,MDEyOklzc3VlQ29tbWVudDI3MDkzMzkzOA==,9906,2017-01-06T15:55:29Z,2017-01-06T15:55:29Z,OWNER,"Hey, I appreciate the report. Given the size of the class, I agree that shipping a custom implementation is probably the best option.

I won't have time to look into this for the next weeks or so. If anyone else wants to work on this, please go ahead. :)",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/75/comments,https://github.com/passy/build-time-tracker-plugin/issues/75#issuecomment-270933938,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/75
passy,build-time-tracker-plugin,195093033,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/347527850,347527850,MDEyOklzc3VlQ29tbWVudDM0NzUyNzg1MA==,513206,2017-11-28T13:46:07Z,2017-11-28T13:46:07Z,NONE,@bryanstern this seems pretty useful. Can I ask why this was closed?,NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/74/comments,https://github.com/passy/build-time-tracker-plugin/pull/74#issuecomment-347527850,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/74
passy,build-time-tracker-plugin,195093033,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/347637246,347637246,MDEyOklzc3VlQ29tbWVudDM0NzYzNzI0Ng==,646560,2017-11-28T19:31:01Z,2017-11-28T19:31:01Z,NONE,The project appeared unmaintained at the time and I ended up going in another direction for my project.,NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/74/comments,https://github.com/passy/build-time-tracker-plugin/pull/74#issuecomment-347637246,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/74
passy,build-time-tracker-plugin,195093033,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/347638455,347638455,MDEyOklzc3VlQ29tbWVudDM0NzYzODQ1NQ==,9906,2017-11-28T19:34:49Z,2017-11-28T19:34:49Z,OWNER,"Well, I'm still here. :)",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/74/comments,https://github.com/passy/build-time-tracker-plugin/pull/74#issuecomment-347638455,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/74
passy,build-time-tracker-plugin,195093033,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/348153922,348153922,MDEyOklzc3VlQ29tbWVudDM0ODE1MzkyMg==,513206,2017-11-30T10:54:14Z,2017-11-30T10:54:14Z,NONE,hah. fair enough. Were you happy with your implementation for the most part @bryanstern ? I need something similar.,NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/74/comments,https://github.com/passy/build-time-tracker-plugin/pull/74#issuecomment-348153922,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/74
passy,build-time-tracker-plugin,195093033,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/348157619,348157619,MDEyOklzc3VlQ29tbWVudDM0ODE1NzYxOQ==,9906,2017-11-30T11:09:29Z,2017-11-30T11:09:29Z,OWNER,"Yeah, that looked good to me. :)",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/74/comments,https://github.com/passy/build-time-tracker-plugin/pull/74#issuecomment-348157619,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/74
passy,build-time-tracker-plugin,195093033,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/348526520,348526520,MDEyOklzc3VlQ29tbWVudDM0ODUyNjUyMA==,513206,2017-12-01T15:38:05Z,2017-12-01T15:38:05Z,NONE,"I checked this out and tested it and it seems to work fine for what I am trying to do. My guess is @bryanstern did not want to go through the trouble of getting it into master, but as far as I am concerned, it works. I am going to use my fork for now, but would love to see this or something comparable revived!",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/74/comments,https://github.com/passy/build-time-tracker-plugin/pull/74#issuecomment-348526520,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/74
passy,build-time-tracker-plugin,180536051,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/264737347,264737347,MDEyOklzc3VlQ29tbWVudDI2NDczNzM0Nw==,9906,2016-12-04T22:37:56Z,2016-12-04T22:37:56Z,OWNER,"Thanks for the report, it's not quite clear to me what is wrong. Do you see actual question marks on your screen? Can you give me information about your system and terminal?",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/73/comments,https://github.com/passy/build-time-tracker-plugin/issues/73#issuecomment-264737347,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/73
passy,build-time-tracker-plugin,180536051,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/264768000,264768000,MDEyOklzc3VlQ29tbWVudDI2NDc2ODAwMA==,1739848,2016-12-05T04:54:46Z,2016-12-05T04:54:46Z,NONE,"@passy Yeah. It does not look nice and fancy like your README.me, it shows `?` instead of the ""dark squares"".",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/73/comments,https://github.com/passy/build-time-tracker-plugin/issues/73#issuecomment-264768000,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/73
passy,build-time-tracker-plugin,180536051,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/264785990,264785990,MDEyOklzc3VlQ29tbWVudDI2NDc4NTk5MA==,9906,2016-12-05T07:37:21Z,2016-12-05T07:37:21Z,OWNER,And the second question? :),NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/73/comments,https://github.com/passy/build-time-tracker-plugin/issues/73#issuecomment-264785990,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/73
passy,build-time-tracker-plugin,180536051,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/266269833,266269833,MDEyOklzc3VlQ29tbWVudDI2NjI2OTgzMw==,1739848,2016-12-11T08:29:15Z,2016-12-11T08:29:15Z,NONE,I am using a Mac. Do you think it is a bashrc setting that is causing the UTF characters not showing up correctly?,NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/73/comments,https://github.com/passy/build-time-tracker-plugin/issues/73#issuecomment-266269833,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/73
passy,build-time-tracker-plugin,180536051,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/341306604,341306604,MDEyOklzc3VlQ29tbWVudDM0MTMwNjYwNA==,1739848,2017-11-02T03:18:54Z,2017-11-02T03:18:54Z,NONE,Any update?,NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/73/comments,https://github.com/passy/build-time-tracker-plugin/issues/73#issuecomment-341306604,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/73
passy,build-time-tracker-plugin,180536051,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/341377359,341377359,MDEyOklzc3VlQ29tbWVudDM0MTM3NzM1OQ==,9906,2017-11-02T10:21:24Z,2017-11-02T10:22:33Z,OWNER,"@jaredsburrows Which terminal are you using? I'd like to repro this, but I can't without knowing your system setup.",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/73/comments,https://github.com/passy/build-time-tracker-plugin/issues/73#issuecomment-341377359,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/73
passy,build-time-tracker-plugin,180536051,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/341503598,341503598,MDEyOklzc3VlQ29tbWVudDM0MTUwMzU5OA==,1739848,2017-11-02T17:45:51Z,2017-11-02T17:45:51Z,NONE,@passy Just the normal mac terminal using bash.,NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/73/comments,https://github.com/passy/build-time-tracker-plugin/issues/73#issuecomment-341503598,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/73
passy,build-time-tracker-plugin,180536051,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/341908731,341908731,MDEyOklzc3VlQ29tbWVudDM0MTkwODczMQ==,9906,2017-11-04T16:08:44Z,2017-11-04T16:08:44Z,OWNER,"Tried to repro it with `Terminal.app` in its vanilla config with SF Mono Regular (11pt) and it seems to look okay:

![screenshot 2017-11-04 16 06 54](https://user-images.githubusercontent.com/9906/32407039-6ce7bb5c-c17a-11e7-91c5-788ab074ce78.png)
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/73/comments,https://github.com/passy/build-time-tracker-plugin/issues/73#issuecomment-341908731,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/73
passy,build-time-tracker-plugin,180536051,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/341923283,341923283,MDEyOklzc3VlQ29tbWVudDM0MTkyMzI4Mw==,1739848,2017-11-04T19:23:33Z,2017-11-04T19:23:33Z,NONE,"Im trying to reproduce this now.

I applied the plugin to https://github.com/jaredsburrows/android-gif-example and it didn't print anything out. Does I have to use the extension?",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/73/comments,https://github.com/passy/build-time-tracker-plugin/issues/73#issuecomment-341923283,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/73
passy,build-time-tracker-plugin,180536051,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/342005864,342005864,MDEyOklzc3VlQ29tbWVudDM0MjAwNTg2NA==,9906,2017-11-05T21:09:42Z,2017-11-05T21:09:42Z,OWNER,@jaredsburrows I don't think I follow.,NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/73/comments,https://github.com/passy/build-time-tracker-plugin/issues/73#issuecomment-342005864,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/73
passy,build-time-tracker-plugin,180536051,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/342006113,342006113,MDEyOklzc3VlQ29tbWVudDM0MjAwNjExMw==,1739848,2017-11-05T21:13:03Z,2017-11-05T21:13:03Z,NONE,"@passy I was saying that this plugin does not work without using the extension. Maybe set default values so users to not need to apply/use the extensions block?

Luckily, I was not able to repro this bug anymore:

```
40 actionable tasks: 30 executed, 10 up-to-date
== CSV Build Time Summary ==
Build time today: 0:02.034
Total build time: 0:02.034
(measured since moments ago)
== Build Time Summary ==
â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡ 95% :lintDebug (0:01.930)
```",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/73/comments,https://github.com/passy/build-time-tracker-plugin/issues/73#issuecomment-342006113,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/73
passy,build-time-tracker-plugin,162255920,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/277529383,277529383,MDEyOklzc3VlQ29tbWVudDI3NzUyOTM4Mw==,9906,2017-02-05T16:09:59Z,2017-02-05T16:09:59Z,OWNER,This should be fixed now. Release coming soon.,NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/71/comments,https://github.com/passy/build-time-tracker-plugin/issues/71#issuecomment-277529383,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/71
passy,build-time-tracker-plugin,162251854,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/228530754,228530754,MDEyOklzc3VlQ29tbWVudDIyODUzMDc1NA==,9906,2016-06-25T10:37:13Z,2016-06-25T10:37:13Z,OWNER,"Thanks for opening the issue and the comprehensive explanation, @jensck! I'm also totally going to steal that term funbounded. :)

The same issue came up internally but we decided to defer working on a solution so far. To me, the second risk was actually more important than the first as the whole point of the plugin was to lead to faster builds - while this could lead to the exact opposite.

I'd like to avoid reinventing the wheel here and log truncation and rotation seems like it should be a pretty well researched topic. Do you - or anyone else - have any suggestions on what could be used here?
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/70/comments,https://github.com/passy/build-time-tracker-plugin/issues/70#issuecomment-228530754,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/70
passy,build-time-tracker-plugin,162251854,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/228532881,228532881,MDEyOklzc3VlQ29tbWVudDIyODUzMjg4MQ==,671275,2016-06-25T11:06:26Z,2016-06-25T11:06:26Z,NONE,"Glad you liked ""funbounded"" -- that's a gem from my former colleagues at Code42 which I thought deserved to enter the broader hacker lexicon. :)

As for the solution, some kind of log rollover approach is what came to mind for me.  There might be more sophisticated approaches that are worth exploring if needed, but here's a simple first cut:

Add a config option specifying the _number of days_ worth of events to keep.  While this isn't a perfect solution for predicting log size, it's arguably the most useful, relatable measurement.
- It should have a default which seems reasonable for a moderately complex project on a reasonably recent laptop
- Allow for a value of -1 if people actually _want_ funbounded logs
- when the limit is hit, just erase the events which are past the limit, instead of trying to roll them over into another file or something, but spit out a long message about what's happening

I don't have any specific thoughts about implementation, but the performance of the JVM and the availability of great IO libraries like Okio should make it fairly straightforward?
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/70/comments,https://github.com/passy/build-time-tracker-plugin/issues/70#issuecomment-228532881,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/70
passy,build-time-tracker-plugin,159151742,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/224602263,224602263,MDEyOklzc3VlQ29tbWVudDIyNDYwMjI2Mw==,9906,2016-06-08T14:14:17Z,2016-06-08T14:14:17Z,OWNER,"Hm, thanks for raising this. I'll revert it for now. I'm not sure if there's a way to configure it dynamically in the way you describe.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/66/comments,https://github.com/passy/build-time-tracker-plugin/issues/66#issuecomment-224602263,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/66
passy,build-time-tracker-plugin,159151742,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/224826500,224826500,MDEyOklzc3VlQ29tbWVudDIyNDgyNjUwMA==,1589887,2016-06-09T08:03:42Z,2016-06-09T08:03:42Z,CONTRIBUTOR,"Np, sorry for introducing a bug!
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/66/comments,https://github.com/passy/build-time-tracker-plugin/issues/66#issuecomment-224826500,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/66
passy,build-time-tracker-plugin,159151742,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/224938501,224938501,MDEyOklzc3VlQ29tbWVudDIyNDkzODUwMQ==,9906,2016-06-09T15:49:59Z,2016-06-09T15:49:59Z,OWNER,"Not at all! I appreciate the contribution and gradle's behavior doesn't seem very intuitive to me.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/66/comments,https://github.com/passy/build-time-tracker-plugin/issues/66#issuecomment-224938501,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/66
passy,build-time-tracker-plugin,159151742,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/226206810,226206810,MDEyOklzc3VlQ29tbWVudDIyNjIwNjgxMA==,10127655,2016-06-15T14:35:52Z,2016-06-15T14:35:52Z,NONE,"Can you please revert this PR and update version in maven?
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/66/comments,https://github.com/passy/build-time-tracker-plugin/issues/66#issuecomment-226206810,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/66
passy,build-time-tracker-plugin,159151742,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/226226844,226226844,MDEyOklzc3VlQ29tbWVudDIyNjIyNjg0NA==,9906,2016-06-15T15:37:49Z,2016-06-15T15:37:54Z,OWNER,"@Tagakov Unfortunately I can't. Some of the tests turned out to be non-deterministic and fail randomly. I need to get those fixed first. I recommend using a previous version until then.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/66/comments,https://github.com/passy/build-time-tracker-plugin/issues/66#issuecomment-226226844,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/66
passy,build-time-tracker-plugin,159151742,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/226568607,226568607,MDEyOklzc3VlQ29tbWVudDIyNjU2ODYwNw==,9906,2016-06-16T18:16:25Z,2016-06-16T18:19:43Z,OWNER,"Okay, tests are fixed now. I'll try to cut a new release tonight.

EDIT: Ugh sorry, won't be able to get this done tonight. Fingers crossed for tomorrow.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/66/comments,https://github.com/passy/build-time-tracker-plugin/issues/66#issuecomment-226568607,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/66
passy,build-time-tracker-plugin,159151742,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/229050693,229050693,MDEyOklzc3VlQ29tbWVudDIyOTA1MDY5Mw==,1589887,2016-06-28T13:36:50Z,2016-06-28T13:36:50Z,CONTRIBUTOR,"Looks like this is working now! Cheers
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/66/comments,https://github.com/passy/build-time-tracker-plugin/issues/66#issuecomment-229050693,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/66
passy,build-time-tracker-plugin,156699449,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/221761722,221761722,MDEyOklzc3VlQ29tbWVudDIyMTc2MTcyMg==,9906,2016-05-26T02:28:54Z,2016-05-26T02:28:54Z,OWNER,"I think that's a good way to handle this. Thanks for adding tests and docs, too! :)
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/64/comments,https://github.com/passy/build-time-tracker-plugin/pull/64#issuecomment-221761722,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/64
passy,build-time-tracker-plugin,156537743,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/221329813,221329813,MDEyOklzc3VlQ29tbWVudDIyMTMyOTgxMw==,9906,2016-05-24T16:36:01Z,2016-05-24T16:36:01Z,OWNER,"Cool, thanks! I'll try to get this out as a release soon.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/63/comments,https://github.com/passy/build-time-tracker-plugin/pull/63#issuecomment-221329813,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/63
passy,build-time-tracker-plugin,156537743,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/221503768,221503768,MDEyOklzc3VlQ29tbWVudDIyMTUwMzc2OA==,1589887,2016-05-25T08:14:02Z,2016-05-25T08:14:02Z,CONTRIBUTOR,"No worries! Sorry I didn't do it earlier, I missed that you added the labels to the ticket! Thanks for merging!
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/63/comments,https://github.com/passy/build-time-tracker-plugin/pull/63#issuecomment-221503768,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/63
passy,build-time-tracker-plugin,142134225,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/199469888,199469888,MDEyOklzc3VlQ29tbWVudDE5OTQ2OTg4OA==,9906,2016-03-21T20:40:20Z,2016-03-21T20:40:20Z,OWNER,"That is a great question, actually. Thanks, I'll add a paragraph to the readme.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/60/comments,https://github.com/passy/build-time-tracker-plugin/issues/60#issuecomment-199469888,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/60
passy,build-time-tracker-plugin,141241100,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/197279651,197279651,MDEyOklzc3VlQ29tbWVudDE5NzI3OTY1MQ==,9906,2016-03-16T11:45:18Z,2016-03-16T11:45:18Z,OWNER,"What's the stacktrace? Are there any special permissions on the file?
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/59/comments,https://github.com/passy/build-time-tracker-plugin/issues/59#issuecomment-197279651,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/59
passy,build-time-tracker-plugin,141211181,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/197279046,197279046,MDEyOklzc3VlQ29tbWVudDE5NzI3OTA0Ng==,9906,2016-03-16T11:42:55Z,2016-03-16T11:42:55Z,OWNER,"I'd be happy to have an option to disable this. I don't have time to implement this myself at the moment. But I'm happy to take PRs.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/58/comments,https://github.com/passy/build-time-tracker-plugin/issues/58#issuecomment-197279046,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/58
passy,build-time-tracker-plugin,133045138,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/183026046,183026046,MDEyOklzc3VlQ29tbWVudDE4MzAyNjA0Ng==,9906,2016-02-11T19:34:02Z,2016-02-11T19:34:02Z,OWNER,"> However, if one cleans the project total day resets.

How do you clean the project?
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/56/comments,https://github.com/passy/build-time-tracker-plugin/issues/56#issuecomment-183026046,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/56
passy,build-time-tracker-plugin,133045138,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/183439700,183439700,MDEyOklzc3VlQ29tbWVudDE4MzQzOTcwMA==,4330630,2016-02-12T18:24:08Z,2016-02-12T18:24:08Z,NONE,"Build -> clean project
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/56/comments,https://github.com/passy/build-time-tracker-plugin/issues/56#issuecomment-183439700,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/56
passy,build-time-tracker-plugin,133045138,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/183473662,183473662,MDEyOklzc3VlQ29tbWVudDE4MzQ3MzY2Mg==,9906,2016-02-12T20:18:11Z,2016-02-12T20:18:11Z,OWNER,"Sorry, but this is not enough information to help. You are in complete control of where the build information is stored. It seems like your build config cleans the full `build/` directory. Store the state somewhere else, and you should be fine.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/56/comments,https://github.com/passy/build-time-tracker-plugin/issues/56#issuecomment-183473662,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/56
passy,build-time-tracker-plugin,120086971,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/161583289,161583289,MDEyOklzc3VlQ29tbWVudDE2MTU4MzI4OQ==,9906,2015-12-03T10:22:32Z,2015-12-03T10:22:32Z,OWNER,"This is based on the terminal size. If the lines break, the graph becomes unreadable. For more in-depth analysis we have the CSV export.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/55/comments,https://github.com/passy/build-time-tracker-plugin/issues/55#issuecomment-161583289,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/55
passy,build-time-tracker-plugin,52501453,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/67690196,67690196,MDEyOklzc3VlQ29tbWVudDY3NjkwMTk2,62836,2014-12-19T20:06:48Z,2014-12-19T20:06:48Z,COLLABORATOR,"Yes, I like potentially trying a range of strategies in TerminalInfo.groovy. Good move, JLine has cost more effort than it is worth. Transition it to an advisory role.

#shipit 
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/49/comments,https://github.com/passy/build-time-tracker-plugin/pull/49#issuecomment-67690196,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/49
passy,build-time-tracker-plugin,52501453,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/67691095,67691095,MDEyOklzc3VlQ29tbWVudDY3NjkxMDk1,9906,2014-12-19T20:14:07Z,2014-12-19T20:14:07Z,OWNER,"JLine, consider yourself reorged.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/49/comments,https://github.com/passy/build-time-tracker-plugin/pull/49#issuecomment-67691095,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/49
passy,build-time-tracker-plugin,52223829,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/67308007,67308007,MDEyOklzc3VlQ29tbWVudDY3MzA4MDA3,9906,2014-12-17T11:09:17Z,2014-12-17T11:09:17Z,OWNER,"Thanks for the report. I'm not quite sure how this can happen. It's not coming from using Android Studio 1.0.1 alone, because it's working fine for me.

I reckon this is because of a mis-report coming form the terminal width detection. I guess it would be sensible to set a minimum width and default to 80 otherwise. Could you give me your system information, though? 
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/47/comments,https://github.com/passy/build-time-tracker-plugin/issues/47#issuecomment-67308007,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/47
passy,build-time-tracker-plugin,52223829,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/67324215,67324215,MDEyOklzc3VlQ29tbWVudDY3MzI0MjE1,5583677,2014-12-17T13:50:34Z,2014-12-17T13:50:34Z,NONE,"system information via. dxdiag:

---

## System Information

Time of this report: 12/17/2014, 14:46:58
       Machine name: ASDN4004
   Operating System: Windows 7 Enterprise 64-bit (6.1, Build 7601) Service Pack 1 (7601.win7sp1_ldr.140303-2307)
           Language: German (Regional Setting: German)
System Manufacturer: Dell Inc.
       System Model: Latitude E7440
               BIOS: BIOS Date: 08/28/14 21:14:22 Ver: A11.00 
          Processor: Intel(R) Core(TM) i7-4600U CPU @ 2.10GHz (4 CPUs), ~2.7GHz
             Memory: 16384MB RAM
Available OS Memory: 16290MB RAM
          Page File: 9663MB used, 22913MB available
        Windows Dir: C:\Windows
    DirectX Version: DirectX 11
DX Setup Parameters: Not found
   User DPI Setting: 96 DPI (100 percent)
 System DPI Setting: 120 DPI (125 percent)
    DWM DPI Scaling: Disabled
     DxDiag Version: 6.01.7601.17514 32bit Unicode

---

## Display Devices

```
      Card name: Intel(R) HD Graphics Family
   Manufacturer: Intel Corporation
      Chip type: Intel(R) HD Graphics Family
       DAC type: Internal
     Device Key: Enum\PCI\VEN_8086&DEV_0A16&SUBSYS_05CB1028&REV_0B
 Display Memory: 1696 MB
```

   Dedicated Memory: 64 MB
      Shared Memory: 1632 MB
       Current Mode: 1920 x 1080 (32 bit) (60Hz)
       Monitor Name: PnP-Monitor (Standard)
      Monitor Model: BenQ GL2450H
         Monitor Id: BNQ78A7
        Native Mode: 1920 x 1080(p) (60.000Hz)
        Output Type: Displayport External
        Driver Name: igdumdim64.dll,igd10iumd64.dll,igd10iumd64.dll,igdumdim32,igd10iumd32,igd10iumd32
Driver File Version: 9.18.0010.3220 (English)
     Driver Version: 9.18.10.3220
        DDI Version: 11
       Driver Model: WDDM 1.1
  Driver Attributes: Final Retail
   Driver Date/Size: 10/25/2013 06:37:26, 8665088 bytes
        WHQL Logo'd: n/a
    WHQL Date Stamp: n/a
  Device Identifier: {D7B78E66-4956-11CF-0C7F-C125B6C2C435}
          Vendor ID: 0x8086
          Device ID: 0x0A16
          SubSys ID: 0x05CB1028
        Revision ID: 0x000B
 Driver Strong Name: oem23.inf:IntelGfx.NTamd64.6.1:iHSWM_w7:9.18.10.3220:pci\ven_8086&dev_0a16&subsys_05cb1028
     Rank Of Driver: 00E00001
        Video Accel: ModeMPEG2_A ModeMPEG2_C ModeWMV9_C ModeVC1_C 
   Deinterlace Caps: {BF752EF6-8CC4-457A-BE1B-08BD1CAEEE9F}: Format(In/Out)=(YUY2,YUY2) Frames(Prev/Fwd/Back)=(0,0,1) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_EdgeFiltering 
                     {335AA36E-7884-43A4-9C91-7F87FAF3E37E}: Format(In/Out)=(YUY2,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_BOBVerticalStretch 
                     {5A54A0C9-C7EC-4BD9-8EDE-F3C75DC4393B}: Format(In/Out)=(YUY2,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend 
                     {BF752EF6-8CC4-457A-BE1B-08BD1CAEEE9F}: Format(In/Out)=(UYVY,YUY2) Frames(Prev/Fwd/Back)=(0,0,1) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_EdgeFiltering 
                     {335AA36E-7884-43A4-9C91-7F87FAF3E37E}: Format(In/Out)=(UYVY,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_BOBVerticalStretch 
                     {5A54A0C9-C7EC-4BD9-8EDE-F3C75DC4393B}: Format(In/Out)=(UYVY,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend 
                     {BF752EF6-8CC4-457A-BE1B-08BD1CAEEE9F}: Format(In/Out)=(YV12,YUY2) Frames(Prev/Fwd/Back)=(0,0,1) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_EdgeFiltering 
                     {335AA36E-7884-43A4-9C91-7F87FAF3E37E}: Format(In/Out)=(YV12,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_BOBVerticalStretch 
                     {5A54A0C9-C7EC-4BD9-8EDE-F3C75DC4393B}: Format(In/Out)=(YV12,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend 
                     {BF752EF6-8CC4-457A-BE1B-08BD1CAEEE9F}: Format(In/Out)=(NV12,YUY2) Frames(Prev/Fwd/Back)=(0,0,1) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_EdgeFiltering 
                     {335AA36E-7884-43A4-9C91-7F87FAF3E37E}: Format(In/Out)=(NV12,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_BOBVerticalStretch 
                     {5A54A0C9-C7EC-4BD9-8EDE-F3C75DC4393B}: Format(In/Out)=(NV12,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend 
                     {BF752EF6-8CC4-457A-BE1B-08BD1CAEEE9F}: Format(In/Out)=(IMC1,YUY2) Frames(Prev/Fwd/Back)=(0,0,1) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_EdgeFiltering 
                     {335AA36E-7884-43A4-9C91-7F87FAF3E37E}: Format(In/Out)=(IMC1,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_BOBVerticalStretch 
                     {5A54A0C9-C7EC-4BD9-8EDE-F3C75DC4393B}: Format(In/Out)=(IMC1,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend 
                     {BF752EF6-8CC4-457A-BE1B-08BD1CAEEE9F}: Format(In/Out)=(IMC2,YUY2) Frames(Prev/Fwd/Back)=(0,0,1) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_EdgeFiltering 
                     {335AA36E-7884-43A4-9C91-7F87FAF3E37E}: Format(In/Out)=(IMC2,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_BOBVerticalStretch 
                     {5A54A0C9-C7EC-4BD9-8EDE-F3C75DC4393B}: Format(In/Out)=(IMC2,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend 
                     {BF752EF6-8CC4-457A-BE1B-08BD1CAEEE9F}: Format(In/Out)=(IMC3,YUY2) Frames(Prev/Fwd/Back)=(0,0,1) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_EdgeFiltering 
                     {335AA36E-7884-43A4-9C91-7F87FAF3E37E}: Format(In/Out)=(IMC3,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_BOBVerticalStretch 
                     {5A54A0C9-C7EC-4BD9-8EDE-F3C75DC4393B}: Format(In/Out)=(IMC3,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend 
                     {BF752EF6-8CC4-457A-BE1B-08BD1CAEEE9F}: Format(In/Out)=(IMC4,YUY2) Frames(Prev/Fwd/Back)=(0,0,1) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_EdgeFiltering 
                     {335AA36E-7884-43A4-9C91-7F87FAF3E37E}: Format(In/Out)=(IMC4,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_BOBVerticalStretch 
                     {5A54A0C9-C7EC-4BD9-8EDE-F3C75DC4393B}: Format(In/Out)=(IMC4,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend 
       D3D9 Overlay: Supported
            DXVA-HD: Supported
       DDraw Status: Enabled
         D3D Status: Enabled
         AGP Status: Enabled

```
      Card name: Intel(R) HD Graphics Family
   Manufacturer: Intel Corporation
      Chip type: Intel(R) HD Graphics Family
       DAC type: Internal
     Device Key: Enum\PCI\VEN_8086&DEV_0A16&SUBSYS_05CB1028&REV_0B
 Display Memory: 1696 MB
```

   Dedicated Memory: 64 MB
      Shared Memory: 1632 MB
       Current Mode: 1920 x 1080 (32 bit) (60Hz)
       Monitor Name: PnP-Monitor (Standard)
      Monitor Model: BenQ GL2450H
         Monitor Id: BNQ78A7
        Native Mode: 1920 x 1080(p) (60.000Hz)
        Output Type: Displayport External
        Driver Name: igdumdim64.dll,igd10iumd64.dll,igd10iumd64.dll,igdumdim32,igd10iumd32,igd10iumd32
Driver File Version: 9.18.0010.3220 (English)
     Driver Version: 9.18.10.3220
        DDI Version: 11
       Driver Model: WDDM 1.1
  Driver Attributes: Final Retail
   Driver Date/Size: 10/25/2013 06:37:26, 8665088 bytes
        WHQL Logo'd: n/a
    WHQL Date Stamp: n/a
  Device Identifier: {D7B78E66-4956-11CF-0C7F-C125B6C2C435}
          Vendor ID: 0x8086
          Device ID: 0x0A16
          SubSys ID: 0x05CB1028
        Revision ID: 0x000B
 Driver Strong Name: oem23.inf:IntelGfx.NTamd64.6.1:iHSWM_w7:9.18.10.3220:pci\ven_8086&dev_0a16&subsys_05cb1028
     Rank Of Driver: 00E00001
        Video Accel: ModeMPEG2_A ModeMPEG2_C ModeWMV9_C ModeVC1_C 
   Deinterlace Caps: {BF752EF6-8CC4-457A-BE1B-08BD1CAEEE9F}: Format(In/Out)=(YUY2,YUY2) Frames(Prev/Fwd/Back)=(0,0,1) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_EdgeFiltering 
                     {335AA36E-7884-43A4-9C91-7F87FAF3E37E}: Format(In/Out)=(YUY2,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_BOBVerticalStretch 
                     {5A54A0C9-C7EC-4BD9-8EDE-F3C75DC4393B}: Format(In/Out)=(YUY2,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend 
                     {BF752EF6-8CC4-457A-BE1B-08BD1CAEEE9F}: Format(In/Out)=(UYVY,YUY2) Frames(Prev/Fwd/Back)=(0,0,1) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_EdgeFiltering 
                     {335AA36E-7884-43A4-9C91-7F87FAF3E37E}: Format(In/Out)=(UYVY,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_BOBVerticalStretch 
                     {5A54A0C9-C7EC-4BD9-8EDE-F3C75DC4393B}: Format(In/Out)=(UYVY,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend 
                     {BF752EF6-8CC4-457A-BE1B-08BD1CAEEE9F}: Format(In/Out)=(YV12,YUY2) Frames(Prev/Fwd/Back)=(0,0,1) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_EdgeFiltering 
                     {335AA36E-7884-43A4-9C91-7F87FAF3E37E}: Format(In/Out)=(YV12,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_BOBVerticalStretch 
                     {5A54A0C9-C7EC-4BD9-8EDE-F3C75DC4393B}: Format(In/Out)=(YV12,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend 
                     {BF752EF6-8CC4-457A-BE1B-08BD1CAEEE9F}: Format(In/Out)=(NV12,YUY2) Frames(Prev/Fwd/Back)=(0,0,1) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_EdgeFiltering 
                     {335AA36E-7884-43A4-9C91-7F87FAF3E37E}: Format(In/Out)=(NV12,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_BOBVerticalStretch 
                     {5A54A0C9-C7EC-4BD9-8EDE-F3C75DC4393B}: Format(In/Out)=(NV12,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend 
                     {BF752EF6-8CC4-457A-BE1B-08BD1CAEEE9F}: Format(In/Out)=(IMC1,YUY2) Frames(Prev/Fwd/Back)=(0,0,1) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_EdgeFiltering 
                     {335AA36E-7884-43A4-9C91-7F87FAF3E37E}: Format(In/Out)=(IMC1,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_BOBVerticalStretch 
                     {5A54A0C9-C7EC-4BD9-8EDE-F3C75DC4393B}: Format(In/Out)=(IMC1,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend 
                     {BF752EF6-8CC4-457A-BE1B-08BD1CAEEE9F}: Format(In/Out)=(IMC2,YUY2) Frames(Prev/Fwd/Back)=(0,0,1) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_EdgeFiltering 
                     {335AA36E-7884-43A4-9C91-7F87FAF3E37E}: Format(In/Out)=(IMC2,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_BOBVerticalStretch 
                     {5A54A0C9-C7EC-4BD9-8EDE-F3C75DC4393B}: Format(In/Out)=(IMC2,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend 
                     {BF752EF6-8CC4-457A-BE1B-08BD1CAEEE9F}: Format(In/Out)=(IMC3,YUY2) Frames(Prev/Fwd/Back)=(0,0,1) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_EdgeFiltering 
                     {335AA36E-7884-43A4-9C91-7F87FAF3E37E}: Format(In/Out)=(IMC3,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_BOBVerticalStretch 
                     {5A54A0C9-C7EC-4BD9-8EDE-F3C75DC4393B}: Format(In/Out)=(IMC3,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend 
                     {BF752EF6-8CC4-457A-BE1B-08BD1CAEEE9F}: Format(In/Out)=(IMC4,YUY2) Frames(Prev/Fwd/Back)=(0,0,1) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_EdgeFiltering 
                     {335AA36E-7884-43A4-9C91-7F87FAF3E37E}: Format(In/Out)=(IMC4,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_BOBVerticalStretch 
                     {5A54A0C9-C7EC-4BD9-8EDE-F3C75DC4393B}: Format(In/Out)=(IMC4,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend 
       D3D9 Overlay: Supported
            DXVA-HD: Supported
       DDraw Status: Enabled
         D3D Status: Enabled
         AGP Status: Enabled

---

## Sound Devices

```
        Description: Lautsprecher (2- High Definition Audio-GerÃ¤t)
```

 Default Sound Playback: Yes
 Default Voice Playback: Yes
            Hardware ID: HDAUDIO\FUNC_01&VEN_10EC&DEV_0292&SUBSYS_102805CB&REV_1000
        Manufacturer ID: 1
             Product ID: 65535
                   Type: WDM
            Driver Name: HdAudio.sys
         Driver Version: 6.01.7601.17514 (German)
      Driver Attributes: Final Retail
            WHQL Logo'd: n/a
          Date and Size: 11/21/2010 04:23:47, 350208 bytes
            Other Files: 
        Driver Provider: Microsoft
         HW Accel Level: Basic
              Cap Flags: 0x0
    Min/Max Sample Rate: 0, 0
Static/Strm HW Mix Bufs: 0, 0
 Static/Strm HW 3D Bufs: 0, 0
              HW Memory: 0
       Voice Management: No
 EAX(tm) 2.0 Listen/Src: No, No
   I3DL2(tm) Listen/Src: No, No
Sensaura(tm) ZoomFX(tm): No

---

## Sound Capture Devices

```
        Description: Mikrofon (2- High Definition Audio-GerÃ¤t)
```

  Default Sound Capture: Yes
  Default Voice Capture: Yes
            Driver Name: HdAudio.sys
         Driver Version: 6.01.7601.17514 (German)
      Driver Attributes: Final Retail
          Date and Size: 11/21/2010 04:23:47, 350208 bytes
              Cap Flags: 0x0
           Format Flags: 0x0

---

## DirectInput Devices

```
  Device Name: Maus
     Attached: 1
Controller ID: n/a
```

Vendor/Product ID: n/a
        FF Driver: n/a

```
  Device Name: Tastatur
     Attached: 1
Controller ID: n/a
```

Vendor/Product ID: n/a
        FF Driver: n/a

```
  Device Name: wired keyboard
     Attached: 1
Controller ID: 0x0
```

Vendor/Product ID: 0x046A, 0x0180
        FF Driver: n/a

```
  Device Name: wired keyboard
     Attached: 1
Controller ID: 0x0
```

Vendor/Product ID: 0x046A, 0x0180
        FF Driver: n/a

```
  Device Name: G9x Laser Mouse
     Attached: 1
Controller ID: 0x0
```

Vendor/Product ID: 0x046D, 0xC066
        FF Driver: n/a

```
  Device Name: G9x Laser Mouse
     Attached: 1
Controller ID: 0x0
```

Vendor/Product ID: 0x046D, 0xC066
        FF Driver: n/a

```
  Device Name: G9x Laser Mouse
     Attached: 1
Controller ID: 0x0
```

Vendor/Product ID: 0x046D, 0xC066
        FF Driver: n/a

Poll w/ Interrupt: No

---

## USB Devices
- USB-Root-Hub
  | Vendor/Product ID: 0x8086, 0x9C26
  | Matching Device ID: usb\root_hub20
  | Service: usbhub
  | 
  +-+ Generic USB Hub
  | | Vendor/Product ID: 0x8087, 0x8000
  | | Location: Port_#0001.Hub_#0001
  | | Matching Device ID: usb\class_09
  | | Service: usbhub
  | | 
  | +-+ Generic USB Hub
  | | | Vendor/Product ID: 0x413C, 0x2134
  | | | Location: Port_#0006.Hub_#0002
  | | | Matching Device ID: usb\class_09
  | | | Service: usbhub
  | | | 
  | +-+ Generic USB Hub
  | | | Vendor/Product ID: 0x413C, 0x2513
  | | | Location: Port_#0001.Hub_#0002
  | | | Matching Device ID: usb\class_09
  | | | Service: usbhub

---

## Gameport Devices

---

## PS/2 Devices
- Standardtastatur (PS/2)
  | Matching Device ID: *pnp0303
  | Service: i8042prt
  | 
- HID-Tastatur
  | Vendor/Product ID: 0x046A, 0x0180
  | Matching Device ID: hid_device_system_keyboard
  | Service: kbdhid
  | 
- HID-Tastatur
  | Vendor/Product ID: 0x046D, 0xC066
  | Matching Device ID: hid_device_system_keyboard
  | Service: kbdhid
  | 
- DameWare Virtual Keyboard Emulation for Standard Keyboard
  | Matching Device ID: *dw0000
  | Service: dwvkbd
  | 
- Terminalserver-Tastaturtreiber
  | Matching Device ID: root\rdp_kbd
  | Upper Filters: kbdclass
  | Service: TermDD
  | 
- Dell Touchpad
  | Matching Device ID: acpi\dll05cb
  | Upper Filters: ApfiltrService
  | Service: i8042prt
  | 
- HID-konforme Maus
  | Vendor/Product ID: 0x046D, 0xC066
  | Matching Device ID: hid_device_system_mouse
  | Service: mouhid
  | 
- Terminalserver-Maustreiber
  | Matching Device ID: root\rdp_mou
  | Upper Filters: mouclass
  | Service: TermDD

---

## Disk & DVD/CD-ROM Drives

```
  Drive: C:
```

 Free Space: 122.3 GB
Total Space: 244.2 GB
File System: NTFS
      Model: SAMSUNG SSD SM841 mSATA SCSI Disk Device

---

## System Devices

```
 Name: Intel(R) Management Engine Interface 
```

Device ID: PCI\VEN_8086&DEV_9C3A&SUBSYS_05CB1028&REV_04\3&11583659&0&B0
   Driver: n/a

```
 Name: Intel(R) Mobile Express Chipset SATA RAID Controller
```

Device ID: PCI\VEN_8086&DEV_282A&SUBSYS_05CB1028&REV_04\3&11583659&0&FA
   Driver: n/a

```
 Name: Intel(R) USB 3.0 eXtensible-Hostcontroller
```

Device ID: PCI\VEN_8086&DEV_9C31&SUBSYS_05CB1028&REV_04\3&11583659&0&A0
   Driver: n/a

```
 Name: Intel(R) Ethernet Connection I218-LM
```

Device ID: PCI\VEN_8086&DEV_155A&SUBSYS_05CB1028&REV_04\3&11583659&0&C8
   Driver: n/a

```
 Name: Standard PCI-zu-USB erweiterter Hostcontroller
```

Device ID: PCI\VEN_8086&DEV_9C26&SUBSYS_05CB1028&REV_04\3&11583659&0&E8
   Driver: n/a

```
 Name: Intel(R) HD Graphics Family
```

Device ID: PCI\VEN_8086&DEV_0A16&SUBSYS_05CB1028&REV_0B\3&11583659&0&10
   Driver: n/a

```
 Name: Intel(R) 8 Series SMBus Controller - 9C22
```

Device ID: PCI\VEN_8086&DEV_9C22&SUBSYS_05CB1028&REV_04\3&11583659&0&FB
   Driver: n/a

```
 Name: High Definition Audio-Controller
```

Device ID: PCI\VEN_8086&DEV_0A0C&SUBSYS_05CB1028&REV_0B\3&11583659&0&18
   Driver: n/a

```
 Name: High Definition Audio-Controller
```

Device ID: PCI\VEN_8086&DEV_9C20&SUBSYS_05CB1028&REV_04\3&11583659&0&D8
   Driver: n/a

```
 Name: PCI Standard-Host-CPU-BrÃ¼cke
```

Device ID: PCI\VEN_8086&DEV_0A04&SUBSYS_05CB1028&REV_0B\3&11583659&0&00
   Driver: n/a

```
 Name: PCI Standard-PCI-zu-PCI-BrÃ¼cke
```

Device ID: PCI\VEN_8086&DEV_9C18&SUBSYS_05CB1028&REV_E4\3&11583659&0&E4
   Driver: n/a

```
 Name: Intel(R) Dual Band Wireless-AC 7260
```

Device ID: PCI\VEN_8086&DEV_08B1&SUBSYS_44708086&REV_73\4&232CE1B2&0&00E3
   Driver: n/a

```
 Name: PCI Standard-ISA-BrÃ¼cke
```

Device ID: PCI\VEN_8086&DEV_9C43&SUBSYS_05CB1028&REV_04\3&11583659&0&F8
   Driver: n/a

```
 Name: PCI Standard-PCI-zu-PCI-BrÃ¼cke
```

Device ID: PCI\VEN_8086&DEV_9C16&SUBSYS_05CB1028&REV_E4\3&11583659&0&E3
   Driver: n/a

```
 Name: SDA-Standard konformer SD-Hostcontroller
```

Device ID: PCI\VEN_1217&DEV_8520&SUBSYS_05CB1028&REV_01\4&32C44212&0&00E4
   Driver: n/a

```
 Name: Intel(R) Active Management Technology - SOL (COM3)
```

Device ID: PCI\VEN_8086&DEV_9C3D&SUBSYS_05CB1028&REV_04\3&11583659&0&B3
   Driver: n/a

```
 Name: PCI Standard-PCI-zu-PCI-BrÃ¼cke
```

Device ID: PCI\VEN_8086&DEV_9C10&SUBSYS_05CB1028&REV_E4\3&11583659&0&E0
   Driver: n/a

---

## DirectShow Filters

DirectShow Filters:
WMAudio Decoder DMO,0x00800800,1,1,WMADMOD.DLL,6.01.7601.17514
WMAPro over S/PDIF DMO,0x00600800,1,1,WMADMOD.DLL,6.01.7601.17514
WMSpeech Decoder DMO,0x00600800,1,1,WMSPDMOD.DLL,6.01.7601.17514
MP3 Decoder DMO,0x00600800,1,1,mp3dmod.dll,6.01.7600.16385
Mpeg4s Decoder DMO,0x00800001,1,1,mp4sdecd.dll,6.01.7600.16385
WMV Screen decoder DMO,0x00600800,1,1,wmvsdecd.dll,6.01.7601.17514
WMVideo Decoder DMO,0x00800001,1,1,wmvdecod.dll,6.01.7601.18221
Mpeg43 Decoder DMO,0x00800001,1,1,mp43decd.dll,6.01.7600.16385
Mpeg4 Decoder DMO,0x00800001,1,1,mpg4decd.dll,6.01.7600.16385
DV Muxer,0x00400000,0,0,qdv.dll,6.06.7601.17514
Color Space Converter,0x00400001,1,1,quartz.dll,6.06.7601.17713
WM ASF Reader,0x00400000,0,0,qasf.dll,12.00.7601.17514
Screen Capture filter,0x00200000,0,1,wmpsrcwp.dll,12.00.7601.17514
AVI Splitter,0x00600000,1,1,quartz.dll,6.06.7601.17713
VGA 16 Color Ditherer,0x00400000,1,1,quartz.dll,6.06.7601.17713
SBE2MediaTypeProfile,0x00200000,0,0,sbe.dll,6.06.7601.17528
Microsoft DTV-DVD Video Decoder,0x005fffff,2,4,msmpeg2vdec.dll,12.00.9200.17037
AC3 Parser Filter,0x00600000,1,1,mpg2splt.ax,6.06.7601.17528
StreamBufferSink,0x00200000,0,0,sbe.dll,6.06.7601.17528
MJPEG Decompressor,0x00600000,1,1,quartz.dll,6.06.7601.17713
MPEG-I Stream Splitter,0x00600000,1,2,quartz.dll,6.06.7601.17713
SAMI (CC) Parser,0x00400000,1,1,quartz.dll,6.06.7601.17713
VBI Codec,0x00600000,1,4,VBICodec.ax,6.06.7601.17514
MPEG-2 Splitter,0x005fffff,1,0,mpg2splt.ax,6.06.7601.17528
Closed Captions Analysis Filter,0x00200000,2,5,cca.dll,6.06.7601.17514
SBE2FileScan,0x00200000,0,0,sbe.dll,6.06.7601.17528
Microsoft MPEG-2 Video Encoder,0x00200000,1,1,msmpeg2enc.dll,6.01.7601.17514
Internal Script Command Renderer,0x00800001,1,0,quartz.dll,6.06.7601.17713
MPEG Audio Decoder,0x03680001,1,1,quartz.dll,6.06.7601.17713
DV Splitter,0x00600000,1,2,qdv.dll,6.06.7601.17514
Video Mixing Renderer 9,0x00200000,1,0,quartz.dll,6.06.7601.17713
Microsoft MPEG-2 Encoder,0x00200000,2,1,msmpeg2enc.dll,6.01.7601.17514
ACM Wrapper,0x00600000,1,1,quartz.dll,6.06.7601.17713
Video Renderer,0x00800001,1,0,quartz.dll,6.06.7601.17713
MPEG-2 Video Stream Analyzer,0x00200000,0,0,sbe.dll,6.06.7601.17528
Line 21 Decoder,0x00600000,1,1,qdvd.dll,6.06.7601.18611
Video Port Manager,0x00600000,2,1,quartz.dll,6.06.7601.17713
Video Renderer,0x00400000,1,0,quartz.dll,6.06.7601.17713
VPS Decoder,0x00200000,0,0,WSTPager.ax,6.06.7601.17514
WM ASF Writer,0x00400000,0,0,qasf.dll,12.00.7601.17514
VBI Surface Allocator,0x00600000,1,1,vbisurf.ax,6.01.7601.17514
File writer,0x00200000,1,0,qcap.dll,6.06.7601.17514
iTV Data Sink,0x00600000,1,0,itvdata.dll,6.06.7601.17514
iTV Data Capture filter,0x00600000,1,1,itvdata.dll,6.06.7601.17514
DVD Navigator,0x00200000,0,3,qdvd.dll,6.06.7601.18611
Overlay Mixer2,0x00200000,1,1,qdvd.dll,6.06.7601.18611
AVI Draw,0x00600064,9,1,quartz.dll,6.06.7601.17713
RDP DShow Redirection Filter,0xffffffff,1,0,DShowRdpFilter.dll,
Microsoft MPEG-2 Audio Encoder,0x00200000,1,1,msmpeg2enc.dll,6.01.7601.17514
WST Pager,0x00200000,1,1,WSTPager.ax,6.06.7601.17514
MPEG-2 Demultiplexer,0x00600000,1,1,mpg2splt.ax,6.06.7601.17528
DV Video Decoder,0x00800000,1,1,qdv.dll,6.06.7601.17514
SampleGrabber,0x00200000,1,1,qedit.dll,6.06.7601.18501
Null Renderer,0x00200000,1,0,qedit.dll,6.06.7601.18501
MPEG-2 Sections and Tables,0x005fffff,1,0,Mpeg2Data.ax,6.06.7601.17514
Microsoft AC3 Encoder,0x00200000,1,1,msac3enc.dll,6.01.7601.17514
StreamBufferSource,0x00200000,0,0,sbe.dll,6.06.7601.17528
Smart Tee,0x00200000,1,2,qcap.dll,6.06.7601.17514
Overlay Mixer,0x00200000,0,0,qdvd.dll,6.06.7601.18611
AVI Decompressor,0x00600000,1,1,quartz.dll,6.06.7601.17713
AVI/WAV File Source,0x00400000,0,2,quartz.dll,6.06.7601.17713
Wave Parser,0x00400000,1,1,quartz.dll,6.06.7601.17713
MIDI Parser,0x00400000,1,1,quartz.dll,6.06.7601.17713
Multi-file Parser,0x00400000,1,1,quartz.dll,6.06.7601.17713
File stream renderer,0x00400000,1,1,quartz.dll,6.06.7601.17713
Microsoft DTV-DVD Audio Decoder,0x005fffff,1,1,msmpeg2adec.dll,6.01.7140.0000
StreamBufferSink2,0x00200000,0,0,sbe.dll,6.06.7601.17528
AVI Mux,0x00200000,1,0,qcap.dll,6.06.7601.17514
Line 21 Decoder 2,0x00600002,1,1,quartz.dll,6.06.7601.17713
File Source (Async.),0x00400000,0,1,quartz.dll,6.06.7601.17713
File Source (URL),0x00400000,0,1,quartz.dll,6.06.7601.17713
Infinite Pin Tee Filter,0x00200000,1,1,qcap.dll,6.06.7601.17514
Enhanced Video Renderer,0x00200000,1,0,evr.dll,6.01.7601.17514
BDA MPEG2 Transport Information Filter,0x00200000,2,0,psisrndr.ax,6.06.7601.17669
MPEG Video Decoder,0x40000001,1,1,quartz.dll,6.06.7601.17713

WDM Streaming Tee/Splitter Devices:
Tee/Sink-to-Sink-Konvertierung,0x00200000,1,1,ksproxy.ax,6.01.7601.17514

Video Compressors:
WMVideo8 Encoder DMO,0x00600800,1,1,wmvxencd.dll,6.01.7600.16385
WMVideo9 Encoder DMO,0x00600800,1,1,wmvencod.dll,6.01.7600.16385
MSScreen 9 encoder DMO,0x00600800,1,1,wmvsencd.dll,6.01.7600.16385
DV Video Encoder,0x00200000,0,0,qdv.dll,6.06.7601.17514
MJPEG Compressor,0x00200000,0,0,quartz.dll,6.06.7601.17713
Cinepak Codec von Radius,0x00200000,1,1,qcap.dll,6.06.7601.17514
Intel IYUV Codec,0x00200000,1,1,qcap.dll,6.06.7601.17514
Intel IYUV Codec,0x00200000,1,1,qcap.dll,6.06.7601.17514
Microsoft RLE,0x00200000,1,1,qcap.dll,6.06.7601.17514
Microsoft Video 1,0x00200000,1,1,qcap.dll,6.06.7601.17514

Audio Compressors:
WM Speech Encoder DMO,0x00600800,1,1,WMSPDMOE.DLL,6.01.7600.16385
WMAudio Encoder DMO,0x00600800,1,1,WMADMOE.DLL,6.01.7600.16385
IMA ADPCM,0x00200000,1,1,quartz.dll,6.06.7601.17713
PCM,0x00200000,1,1,quartz.dll,6.06.7601.17713
Microsoft ADPCM,0x00200000,1,1,quartz.dll,6.06.7601.17713
GSM 6.10,0x00200000,1,1,quartz.dll,6.06.7601.17713
CCITT A-Law,0x00200000,1,1,quartz.dll,6.06.7601.17713
CCITT u-Law,0x00200000,1,1,quartz.dll,6.06.7601.17713
MPEG Layer-3,0x00200000,1,1,quartz.dll,6.06.7601.17713

Audio Capture Sources:
Mikrofon (2- High Definition Au,0x00200000,0,0,qcap.dll,6.06.7601.17514

PBDA CP Filters:
PBDA DTFilter,0x00600000,1,1,CPFilters.dll,6.06.7601.17528
PBDA ETFilter,0x00200000,0,0,CPFilters.dll,6.06.7601.17528
PBDA PTFilter,0x00200000,0,0,CPFilters.dll,6.06.7601.17528

Midi Renderers:
Default MidiOut Device,0x00800000,1,0,quartz.dll,6.06.7601.17713
Microsoft GS Wavetable Synth,0x00200000,1,0,quartz.dll,6.06.7601.17713

WDM Streaming Capture Devices:
HD Audio-Mikrofon 2,0x00200000,1,1,ksproxy.ax,6.01.7601.17514
Integrated Webcam,0x00200000,1,1,ksproxy.ax,6.01.7601.17514

WDM Streaming Rendering Devices:
HD Audio-KopfhÃ¶rer/Lautsprecher,0x00200000,1,1,ksproxy.ax,6.01.7601.17514

BDA Network Providers:
Microsoft ATSC Network Provider,0x00200000,0,1,MSDvbNP.ax,6.06.7601.17514
Microsoft DVBC Network Provider,0x00200000,0,1,MSDvbNP.ax,6.06.7601.17514
Microsoft DVBS Network Provider,0x00200000,0,1,MSDvbNP.ax,6.06.7601.17514
Microsoft DVBT Network Provider,0x00200000,0,1,MSDvbNP.ax,6.06.7601.17514
Microsoft Network Provider,0x00200000,0,1,MSNP.ax,6.06.7601.17514

Video Capture Sources:
Integrated Webcam,0x00200000,1,1,ksproxy.ax,6.01.7601.17514

Multi-Instance Capable VBI Codecs:
VBI Codec,0x00600000,1,4,VBICodec.ax,6.06.7601.17514

BDA Transport Information Renderers:
BDA MPEG2 Transport Information Filter,0x00600000,2,0,psisrndr.ax,6.06.7601.17669
MPEG-2 Sections and Tables,0x00600000,1,0,Mpeg2Data.ax,6.06.7601.17514

BDA CP/CA Filters:
Decrypt/Tag,0x00600000,1,1,EncDec.dll,6.06.7601.17708
Encrypt/Tag,0x00200000,0,0,EncDec.dll,6.06.7601.17708
PTFilter,0x00200000,0,0,EncDec.dll,6.06.7601.17708
XDS Codec,0x00200000,0,0,EncDec.dll,6.06.7601.17708

WDM Streaming Communication Transforms:
Tee/Sink-to-Sink-Konvertierung,0x00200000,1,1,ksproxy.ax,6.01.7601.17514

Audio Renderers:
Lautsprecher (2- High Definitio,0x00200000,1,0,quartz.dll,6.06.7601.17713
Default DirectSound Device,0x00800000,1,0,quartz.dll,6.06.7601.17713
Default WaveOut Device,0x00200000,1,0,quartz.dll,6.06.7601.17713
DirectSound: Lautsprecher (2- High Definition Audio-GerÃ¤t),0x00200000,1,0,quartz.dll,6.06.7601.17713

---

## EVR Power Information

Current Setting: {5C67A112-A4C9-483F-B4A7-1D473BECAFDC} (Quality) 
  Quality Flags: 2576
    Enabled:
    Force throttling
    Allow half deinterlace
    Allow scaling
    Decode Power Usage: 100
  Balanced Flags: 1424
    Enabled:
    Force throttling
    Allow batching
    Force half deinterlace
    Force scaling
    Decode Power Usage: 50
  PowerFlags: 1424
    Enabled:
    Force throttling
    Allow batching
    Force half deinterlace
    Force scaling
    Decode Power Usage: 0
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/47/comments,https://github.com/passy/build-time-tracker-plugin/issues/47#issuecomment-67324215,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/47
passy,build-time-tracker-plugin,52223829,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/67330482,67330482,MDEyOklzc3VlQ29tbWVudDY3MzMwNDgy,9906,2014-12-17T14:37:42Z,2014-12-17T14:37:42Z,OWNER,"Okay, thanks. Not surprised that terminals on Windows report rubbish. I'll add a fallback for the case that the value is < 0.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/47/comments,https://github.com/passy/build-time-tracker-plugin/issues/47#issuecomment-67330482,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/47
passy,build-time-tracker-plugin,47153085,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/81704073,81704073,MDEyOklzc3VlQ29tbWVudDgxNzA0MDcz,9906,2015-03-16T14:37:07Z,2015-03-16T14:37:07Z,OWNER,"Lost the screenie, will reopen if I witness it again.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/46/comments,https://github.com/passy/build-time-tracker-plugin/issues/46#issuecomment-81704073,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/46
passy,build-time-tracker-plugin,47153085,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/81714693,81714693,MDEyOklzc3VlQ29tbWVudDgxNzE0Njkz,9906,2015-03-16T14:56:28Z,2015-03-16T14:56:28Z,OWNER,"There we go. 
![screenshot 2015-03-16 14 55 53](https://cloud.githubusercontent.com/assets/9906/6668729/9db28ab8-cbec-11e4-8bcc-726af7649ebe.png)
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/46/comments,https://github.com/passy/build-time-tracker-plugin/issues/46#issuecomment-81714693,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/46
passy,build-time-tracker-plugin,45736217,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/59209231,59209231,MDEyOklzc3VlQ29tbWVudDU5MjA5MjMx,9906,2014-10-15T14:00:12Z,2014-10-15T14:00:12Z,OWNER,"![](http://media.giphy.com/media/a3IWyhkEC0p32/giphy.gif)
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/43/comments,https://github.com/passy/build-time-tracker-plugin/pull/43#issuecomment-59209231,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/43
passy,build-time-tracker-plugin,45634153,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/58892134,58892134,MDEyOklzc3VlQ29tbWVudDU4ODkyMTM0,62836,2014-10-13T13:32:18Z,2014-10-13T13:32:18Z,COLLABORATOR,"Tidy work. #shipit
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/42/comments,https://github.com/passy/build-time-tracker-plugin/pull/42#issuecomment-58892134,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/42
passy,build-time-tracker-plugin,45482146,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/58657581,58657581,MDEyOklzc3VlQ29tbWVudDU4NjU3NTgx,9906,2014-10-10T13:48:24Z,2014-10-10T13:48:24Z,OWNER,"Related to https://github.com/jline/jline2/issues/163

Will downgrade to jline 2.11 for the time being.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/41/comments,https://github.com/passy/build-time-tracker-plugin/issues/41#issuecomment-58657581,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/41
passy,build-time-tracker-plugin,45482146,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/58659684,58659684,MDEyOklzc3VlQ29tbWVudDU4NjU5Njg0,9906,2014-10-10T14:03:11Z,2014-10-10T14:03:11Z,OWNER,"Released as `0.3.2` and `0.4.1` respectively.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/41/comments,https://github.com/passy/build-time-tracker-plugin/issues/41#issuecomment-58659684,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/41
passy,build-time-tracker-plugin,45482146,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/60077630,60077630,MDEyOklzc3VlQ29tbWVudDYwMDc3NjMw,9906,2014-10-22T12:35:58Z,2014-10-22T12:37:54Z,OWNER,"This appears to still be an issue. Investigating whether we have to roll back further wrt to jline.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/41/comments,https://github.com/passy/build-time-tracker-plugin/issues/41#issuecomment-60077630,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/41
passy,build-time-tracker-plugin,45482146,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/60225632,60225632,MDEyOklzc3VlQ29tbWVudDYwMjI1NjMy,9906,2014-10-23T11:33:49Z,2014-10-23T11:33:49Z,OWNER,"Still monitoring, but should be addressed by #44.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/41/comments,https://github.com/passy/build-time-tracker-plugin/issues/41#issuecomment-60225632,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/41
passy,build-time-tracker-plugin,45095098,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/58169538,58169538,MDEyOklzc3VlQ29tbWVudDU4MTY5NTM4,62836,2014-10-07T11:18:27Z,2014-10-07T11:18:27Z,COLLABORATOR,"#shipit
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/38/comments,https://github.com/passy/build-time-tracker-plugin/pull/38#issuecomment-58169538,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/38
passy,build-time-tracker-plugin,45093408,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/338315947,338315947,MDEyOklzc3VlQ29tbWVudDMzODMxNTk0Nw==,2153370,2017-10-20T20:35:53Z,2017-10-20T20:35:53Z,NONE,Any update here?,NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/37/comments,https://github.com/passy/build-time-tracker-plugin/issues/37#issuecomment-338315947,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/37
passy,build-time-tracker-plugin,45093408,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/409318014,409318014,MDEyOklzc3VlQ29tbWVudDQwOTMxODAxNA==,444345,2018-07-31T18:16:15Z,2018-07-31T18:16:15Z,NONE,"Is there any thought on supporting this?  All my projects use the ""new"" style and plugin repository so it makes this plugin unusable since the two cannot be mixed and matched.",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/37/comments,https://github.com/passy/build-time-tracker-plugin/issues/37#issuecomment-409318014,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/37
passy,build-time-tracker-plugin,45093408,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/412233947,412233947,MDEyOklzc3VlQ29tbWVudDQxMjIzMzk0Nw==,9906,2018-08-10T23:52:58Z,2018-08-10T23:52:58Z,OWNER,"Sorry that this took so ridiculously long, but it's finally up on the Gradle plugin repo. 

https://plugins.gradle.org/plugin/net.rdrei.android.buildtimetracker",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/37/comments,https://github.com/passy/build-time-tracker-plugin/issues/37#issuecomment-412233947,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/37
passy,build-time-tracker-plugin,45088483,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/58166754,58166754,MDEyOklzc3VlQ29tbWVudDU4MTY2NzU0,9906,2014-10-07T10:48:35Z,2014-10-07T10:48:35Z,OWNER,"Verified against gradle 2.1 using a new project generated with AS 0.8.11. Works like a charm without deprecation warnings. 
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/35/comments,https://github.com/passy/build-time-tracker-plugin/issues/35#issuecomment-58166754,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/35
passy,build-time-tracker-plugin,45088483,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/58166827,58166827,MDEyOklzc3VlQ29tbWVudDU4MTY2ODI3,9906,2014-10-07T10:49:24Z,2014-10-07T10:49:24Z,OWNER,"Will use this ticket to upgrade the integration tool suite to 2.1 as well.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/35/comments,https://github.com/passy/build-time-tracker-plugin/issues/35#issuecomment-58166827,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/35
passy,build-time-tracker-plugin,43289983,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/56245476,56245476,MDEyOklzc3VlQ29tbWVudDU2MjQ1NDc2,9906,2014-09-19T22:47:18Z,2014-09-19T23:00:03Z,OWNER,"I cheated a bit with the tests for the CPU info. Will do some manual testing for this in a bit.
- [x] OS X
- [ ] Linux
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/34/comments,https://github.com/passy/build-time-tracker-plugin/pull/34#issuecomment-56245476,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/34
passy,build-time-tracker-plugin,43289983,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/56246681,56246681,MDEyOklzc3VlQ29tbWVudDU2MjQ2Njgx,9906,2014-09-19T23:03:25Z,2014-09-19T23:11:26Z,OWNER,"Oh bollocks, the `totalMemory` I used is total crap. It's not what's available but what's in use. Investigating some alternatives.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/34/comments,https://github.com/passy/build-time-tracker-plugin/pull/34#issuecomment-56246681,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/34
passy,build-time-tracker-plugin,43289983,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/56310637,56310637,MDEyOklzc3VlQ29tbWVudDU2MzEwNjM3,62836,2014-09-21T20:00:18Z,2014-09-21T20:00:18Z,COLLABORATOR,"Super feature, looks good. #shipit
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/34/comments,https://github.com/passy/build-time-tracker-plugin/pull/34#issuecomment-56310637,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/34
passy,build-time-tracker-plugin,43289983,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/56400456,56400456,MDEyOklzc3VlQ29tbWVudDU2NDAwNDU2,9906,2014-09-22T16:31:43Z,2014-09-22T16:31:43Z,OWNER,"Thanks for the review, Daithi. I'm gonna merge as soon as Travis is happy.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/34/comments,https://github.com/passy/build-time-tracker-plugin/pull/34#issuecomment-56400456,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/34
passy,build-time-tracker-plugin,40340037,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/52324674,52324674,MDEyOklzc3VlQ29tbWVudDUyMzI0Njc0,9906,2014-08-15T16:06:22Z,2014-08-15T16:06:22Z,OWNER,"Figured it out. I was just stupid and forget the use {} closure. Still not entirely sure though when this is necessary and when it isn't.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/32/comments,https://github.com/passy/build-time-tracker-plugin/pull/32#issuecomment-52324674,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/32
passy,build-time-tracker-plugin,40340037,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/52994381,52994381,MDEyOklzc3VlQ29tbWVudDUyOTk0Mzgx,9906,2014-08-21T22:34:08Z,2014-08-21T22:34:08Z,OWNER,"@daithiocrualaoich Could you have a look at this?
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/32/comments,https://github.com/passy/build-time-tracker-plugin/pull/32#issuecomment-52994381,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/32
passy,build-time-tracker-plugin,40340037,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/53043402,53043402,MDEyOklzc3VlQ29tbWVudDUzMDQzNDAy,62836,2014-08-22T09:57:28Z,2014-08-22T09:57:28Z,COLLABORATOR,"Apologies, thought I'd seen this last week.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/32/comments,https://github.com/passy/build-time-tracker-plugin/pull/32#issuecomment-53043402,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/32
passy,build-time-tracker-plugin,40340037,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/54463482,54463482,MDEyOklzc3VlQ29tbWVudDU0NDYzNDgy,9906,2014-09-04T12:13:39Z,2014-09-04T12:13:39Z,OWNER,"@daithiocrualaoich Time zones utterly confuse me. I _think_ that this approach is right, though. No need to get out of the bed to review this though, I'm probably going to merge this later today. :)
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/32/comments,https://github.com/passy/build-time-tracker-plugin/pull/32#issuecomment-54463482,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/32
passy,build-time-tracker-plugin,40036800,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/51890474,51890474,MDEyOklzc3VlQ29tbWVudDUxODkwNDc0,9906,2014-08-12T09:10:09Z,2014-08-12T09:10:09Z,OWNER,"I squashed another commit into this. I store the CSV data in a different data structure now that would allow grouping/filtering by time stamp.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/31/comments,https://github.com/passy/build-time-tracker-plugin/pull/31#issuecomment-51890474,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/31
passy,build-time-tracker-plugin,40036800,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/51894073,51894073,MDEyOklzc3VlQ29tbWVudDUxODk0MDcz,62836,2014-08-12T09:49:01Z,2014-08-12T09:49:01Z,COLLABORATOR,"#shipit obviously.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/31/comments,https://github.com/passy/build-time-tracker-plugin/pull/31#issuecomment-51894073,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/31
passy,build-time-tracker-plugin,40036800,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/52028067,52028067,MDEyOklzc3VlQ29tbWVudDUyMDI4MDY3,62836,2014-08-13T09:39:17Z,2014-08-13T09:39:17Z,COLLABORATOR,"This patch is top tier engineered. #shipit
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/31/comments,https://github.com/passy/build-time-tracker-plugin/pull/31#issuecomment-52028067,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/31
passy,build-time-tracker-plugin,39960005,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/52122478,52122478,MDEyOklzc3VlQ29tbWVudDUyMTIyNDc4,9906,2014-08-13T22:48:29Z,2014-08-13T22:48:29Z,OWNER,"Done.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/29/comments,https://github.com/passy/build-time-tracker-plugin/issues/29#issuecomment-52122478,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/29
passy,build-time-tracker-plugin,39916297,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/51762454,51762454,MDEyOklzc3VlQ29tbWVudDUxNzYyNDU0,62836,2014-08-11T10:12:17Z,2014-08-11T10:12:17Z,COLLABORATOR,"Tweaks aside, question is whether the CSV summary reporter should also make a CSV reporter for the filename. Thoughts?
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/28/comments,https://github.com/passy/build-time-tracker-plugin/pull/28#issuecomment-51762454,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/28
passy,build-time-tracker-plugin,39916297,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/51784571,51784571,MDEyOklzc3VlQ29tbWVudDUxNzg0NTcx,9906,2014-08-11T14:08:14Z,2014-08-11T14:08:14Z,OWNER,"@daithiocrualaoich I tend to prefer keeping them separate and require the additional setup. I'm not generally opposed to the idea of automatically setting up other reporters, but we need to come up with a good interface for this.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/28/comments,https://github.com/passy/build-time-tracker-plugin/pull/28#issuecomment-51784571,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/28
passy,build-time-tracker-plugin,39916297,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/51786536,51786536,MDEyOklzc3VlQ29tbWVudDUxNzg2NTM2,62836,2014-08-11T14:23:25Z,2014-08-11T14:23:25Z,COLLABORATOR,"@passy My view is that if I've specified that I want a CSVSummaryReporter then I should get a CSVReporter included. I think it is a bad experience if I specify just the CSVSummaryReporter and get nothing because I didn't realise(read the documentation) that I also had to specify a CSVReporter with the same filename.

But I trust your judgement. #shipit
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/28/comments,https://github.com/passy/build-time-tracker-plugin/pull/28#issuecomment-51786536,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/28
passy,build-time-tracker-plugin,39916297,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/51787376,51787376,MDEyOklzc3VlQ29tbWVudDUxNzg3Mzc2,9906,2014-08-11T14:29:57Z,2014-08-11T14:29:57Z,OWNER,"@daithiocrualaoich I'll open a separate issue for that. I agree that it would be better UX if you wouldn't have to do this manually and it's actually explicit if you still have to specify the output.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/28/comments,https://github.com/passy/build-time-tracker-plugin/pull/28#issuecomment-51787376,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/28
passy,build-time-tracker-plugin,39463689,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/51215740,51215740,MDEyOklzc3VlQ29tbWVudDUxMjE1NzQw,9906,2014-08-05T15:41:24Z,2014-08-05T15:41:24Z,OWNER,"@daithiocrualaoich TZ is now fixed to UTC.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/27/comments,https://github.com/passy/build-time-tracker-plugin/pull/27#issuecomment-51215740,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/27
passy,build-time-tracker-plugin,39463689,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/51226268,51226268,MDEyOklzc3VlQ29tbWVudDUxMjI2MjY4,62836,2014-08-05T16:53:01Z,2014-08-05T16:53:01Z,COLLABORATOR,"@passy As it should be! ;) #shipit
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/27/comments,https://github.com/passy/build-time-tracker-plugin/pull/27#issuecomment-51226268,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/27
passy,build-time-tracker-plugin,39347248,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/51006111,51006111,MDEyOklzc3VlQ29tbWVudDUxMDA2MTEx,62836,2014-08-03T23:00:53Z,2014-08-03T23:00:53Z,COLLABORATOR,"Agreed and issue #22 will provide at least one `Timing` to make this case redundant.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/25/comments,https://github.com/passy/build-time-tracker-plugin/pull/25#issuecomment-51006111,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/25
passy,build-time-tracker-plugin,39338099,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/50948567,50948567,MDEyOklzc3VlQ29tbWVudDUwOTQ4NTY3,9906,2014-08-02T00:33:45Z,2014-08-02T00:33:45Z,OWNER,"Oh balls. There's no such thing as too many null checks!
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/24/comments,https://github.com/passy/build-time-tracker-plugin/issues/24#issuecomment-50948567,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/24
passy,build-time-tracker-plugin,39315968,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/50936887,50936887,MDEyOklzc3VlQ29tbWVudDUwOTM2ODg3,62836,2014-08-01T21:28:32Z,2014-08-01T21:28:32Z,COLLABORATOR,"Yes, could maybe highlight task lines that deviate significantly from recent weather too.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/23/comments,https://github.com/passy/build-time-tracker-plugin/issues/23#issuecomment-50936887,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/23
passy,build-time-tracker-plugin,39314567,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/56155222,56155222,MDEyOklzc3VlQ29tbWVudDU2MTU1MjIy,9906,2014-09-19T09:22:18Z,2014-09-19T09:22:18Z,OWNER,"The best way to achieve this would be to compare the age of the process to the sum of all subtask timings. I'm afraid that this could be fairly difficult until Java 8 though when we get a proper Process API. 
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/22/comments,https://github.com/passy/build-time-tracker-plugin/issues/22#issuecomment-56155222,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/22
passy,build-time-tracker-plugin,39314567,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/58638401,58638401,MDEyOklzc3VlQ29tbWVudDU4NjM4NDAx,9906,2014-10-10T10:29:32Z,2014-10-10T10:29:44Z,OWNER,"Good example here where fetching plugins and dependencies take an insane amount of time of which we log nothing:

```
hartig@tw-mbp-phartig ~/P/t/someproject (undefined â–¼ â–² ) $ i@  () $ ./gradlew
Relying on packaging to define the extension of the main artifact has been deprecated and is scheduled to be removed in Gradle 2.0
flavor: custom
flavor: default
flavor: dogfood
flavor: nightly
:help

Welcome to Gradle 1.11.

To run a build, run gradlew <task> ...

To see a list of available tasks, run gradlew tasks

To see a list of command-line options, run gradlew --help

BUILD SUCCESSFUL

Total time: 4 mins 47.611 secs
== CSV Build Time Summary ==
Build time today: 0:00.008
Total build time: 13:52:27
(measured since 2 months ago)
== Build Time Summary ==
```
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/22/comments,https://github.com/passy/build-time-tracker-plugin/issues/22#issuecomment-58638401,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/22
passy,build-time-tracker-plugin,39306604,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/50916507,50916507,MDEyOklzc3VlQ29tbWVudDUwOTE2NTA3,9906,2014-08-01T18:10:20Z,2014-08-01T18:10:20Z,OWNER,"We could generalize this issue a bit. I'd like to collect some other factors that could influence the build time, for example the started task and possibly the parameters it's started with. Using `-a --configure-on-demand --parallel --parallel-threads 8` helped me in some cases. JVM flags, whether a daemon is active and JVM flags might also be interesting.

Could this work as a meta line in CSV?
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/21/comments,https://github.com/passy/build-time-tracker-plugin/issues/21#issuecomment-50916507,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/21
passy,build-time-tracker-plugin,39306604,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/50936696,50936696,MDEyOklzc3VlQ29tbWVudDUwOTM2Njk2,62836,2014-08-01T21:26:16Z,2014-08-01T21:26:16Z,COLLABORATOR,"Yes, good to collect as many features as we think are relevant. Add them on as columns.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/21/comments,https://github.com/passy/build-time-tracker-plugin/issues/21#issuecomment-50936696,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/21
passy,build-time-tracker-plugin,39306404,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/50912631,50912631,MDEyOklzc3VlQ29tbWVudDUwOTEyNjMx,9906,2014-08-01T17:35:23Z,2014-08-01T17:35:23Z,OWNER,"#shipit
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/20/comments,https://github.com/passy/build-time-tracker-plugin/pull/20#issuecomment-50912631,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/20
passy,build-time-tracker-plugin,39284125,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/50885986,50885986,MDEyOklzc3VlQ29tbWVudDUwODg1OTg2,9906,2014-08-01T13:50:18Z,2014-08-01T13:50:18Z,OWNER,"Yess! Thanks, buddy!
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/18/comments,https://github.com/passy/build-time-tracker-plugin/pull/18#issuecomment-50885986,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/18
passy,build-time-tracker-plugin,39282769,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/50875678,50875678,MDEyOklzc3VlQ29tbWVudDUwODc1Njc4,9906,2014-08-01T11:48:31Z,2014-08-01T11:48:31Z,OWNER,"Thanks so much, @sindresorhus!

![](http://media.giphy.com/media/12XWYy7Ur7JeaQ/original.gif)

@daithiocrualaoich WDYT?
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/16/comments,https://github.com/passy/build-time-tracker-plugin/pull/16#issuecomment-50875678,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/16
passy,build-time-tracker-plugin,39282769,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/50875905,50875905,MDEyOklzc3VlQ29tbWVudDUwODc1OTA1,62836,2014-08-01T11:51:54Z,2014-08-01T11:51:54Z,COLLABORATOR,"@sindresorhus I love this! Thank you so much.

#shipit #shipit #shipit
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/16/comments,https://github.com/passy/build-time-tracker-plugin/pull/16#issuecomment-50875905,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/16
passy,build-time-tracker-plugin,39282558,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/50889655,50889655,MDEyOklzc3VlQ29tbWVudDUwODg5NjU1,62836,2014-08-01T14:22:02Z,2014-08-01T14:22:02Z,COLLABORATOR,"#shipit
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/15/comments,https://github.com/passy/build-time-tracker-plugin/pull/15#issuecomment-50889655,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/15
passy,build-time-tracker-plugin,39177410,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/50737069,50737069,MDEyOklzc3VlQ29tbWVudDUwNzM3MDY5,9906,2014-07-31T09:28:49Z,2014-07-31T09:28:49Z,OWNER,"That makes me feel a lot better. I made the same pull requests a few days ago, but it's actually right this way. The last part is just the template that you can use for yourself (and that is used in the README).
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/14/comments,https://github.com/passy/build-time-tracker-plugin/pull/14#issuecomment-50737069,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/14
passy,build-time-tracker-plugin,39177410,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/50737505,50737505,MDEyOklzc3VlQ29tbWVudDUwNzM3NTA1,170270,2014-07-31T09:33:42Z,2014-07-31T09:33:42Z,CONTRIBUTOR,"I know. I'm just :trollface: you :p
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/14/comments,https://github.com/passy/build-time-tracker-plugin/pull/14#issuecomment-50737505,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/14
passy,build-time-tracker-plugin,39177410,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/50741963,50741963,MDEyOklzc3VlQ29tbWVudDUwNzQxOTYz,9906,2014-07-31T10:24:57Z,2014-07-31T10:24:57Z,OWNER,":'(
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/14/comments,https://github.com/passy/build-time-tracker-plugin/pull/14#issuecomment-50741963,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/14
passy,build-time-tracker-plugin,39084639,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/50612092,50612092,MDEyOklzc3VlQ29tbWVudDUwNjEyMDky,9906,2014-07-30T13:14:41Z,2014-07-30T13:14:41Z,OWNER,"Obsolete due to #10
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/9/comments,https://github.com/passy/build-time-tracker-plugin/pull/9#issuecomment-50612092,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/9
passy,build-time-tracker-plugin,39083396,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/50604515,50604515,MDEyOklzc3VlQ29tbWVudDUwNjA0NTE1,62836,2014-07-30T11:50:10Z,2014-07-30T11:50:10Z,COLLABORATOR,"#shipit
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/8/comments,https://github.com/passy/build-time-tracker-plugin/pull/8#issuecomment-50604515,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/8
passy,build-time-tracker-plugin,39054293,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/50591993,50591993,MDEyOklzc3VlQ29tbWVudDUwNTkxOTkz,62836,2014-07-30T09:18:14Z,2014-07-30T09:18:14Z,COLLABORATOR,"Got a couple of more changes to add for this PR.
- Total line should be removed. It's a nuisance in analyzing the CSV.
- Add file append option.
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/5/comments,https://github.com/passy/build-time-tracker-plugin/pull/5#issuecomment-50591993,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/5
passy,build-time-tracker-plugin,39054293,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/50602908,50602908,MDEyOklzc3VlQ29tbWVudDUwNjAyOTA4,9906,2014-07-30T11:29:34Z,2014-07-30T11:29:34Z,OWNER,"#shipit!
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/5/comments,https://github.com/passy/build-time-tracker-plugin/pull/5#issuecomment-50602908,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/5
passy,build-time-tracker-plugin,39054293,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/50602922,50602922,MDEyOklzc3VlQ29tbWVudDUwNjAyOTIy,62836,2014-07-30T11:29:44Z,2014-07-30T11:29:44Z,COLLABORATOR,"#shipit!
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/5/comments,https://github.com/passy/build-time-tracker-plugin/pull/5#issuecomment-50602922,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/5
passy,build-time-tracker-plugin,39019710,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/50554544,50554544,MDEyOklzc3VlQ29tbWVudDUwNTU0NTQ0,9906,2014-07-29T23:40:53Z,2014-07-29T23:40:53Z,OWNER,"![](https://gs1.wac.edgecastcdn.net/8019B6/data.tumblr.com/tumblr_m6jl82KKna1rt8t3no1_400.gif)

Thanks for the review!
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/4/comments,https://github.com/passy/build-time-tracker-plugin/pull/4#issuecomment-50554544,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/4
passy,build-time-tracker-plugin,39019710,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/50564895,50564895,MDEyOklzc3VlQ29tbWVudDUwNTY0ODk1,62836,2014-07-30T01:57:10Z,2014-07-30T01:57:10Z,COLLABORATOR,"#shipit
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/4/comments,https://github.com/passy/build-time-tracker-plugin/pull/4#issuecomment-50564895,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/4
passy,build-time-tracker-plugin,39014099,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/50511545,50511545,MDEyOklzc3VlQ29tbWVudDUwNTExNTQ1,9906,2014-07-29T17:43:42Z,2014-07-29T17:43:42Z,OWNER,"![](https://gs1.wac.edgecastcdn.net/8019B6/data.tumblr.com/4eecf57ab9316133765cd110cf98c449/tumblr_mzh7k4zgpK1qdlh1io1_r1_250.gif)
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/3/comments,https://github.com/passy/build-time-tracker-plugin/pull/3#issuecomment-50511545,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/3
passy,build-time-tracker-plugin,38998436,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/comments/50489513,50489513,MDEyOklzc3VlQ29tbWVudDUwNDg5NTEz,9906,2014-07-29T15:09:56Z,2014-07-29T15:09:56Z,OWNER,"![](http://now-here-this.timeout.com/wp-content/uploads/2014/01/Benedict-Cumberbatch-saying-thank-you-as-Sherlock-GIF.gif)
",NA,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/2/comments,https://github.com/passy/build-time-tracker-plugin/pull/2#issuecomment-50489513,https://api.github.com/repos/passy/build-time-tracker-plugin/issues/2
nsomar,OAStackView,202262309,https://api.github.com/repos/nsomar/OAStackView/issues/comments/274430632,274430632,MDEyOklzc3VlQ29tbWVudDI3NDQzMDYzMg==,1461052,2017-01-23T08:52:07Z,2017-01-23T08:52:07Z,OWNER,Looking Cool!,NA,https://api.github.com/repos/nsomar/OAStackView/issues/101/comments,https://github.com/nsomar/OAStackView/pull/101#issuecomment-274430632,https://api.github.com/repos/nsomar/OAStackView/issues/101
nsomar,OAStackView,201929422,https://api.github.com/repos/nsomar/OAStackView/issues/comments/274200456,274200456,MDEyOklzc3VlQ29tbWVudDI3NDIwMDQ1Ng==,310894,2017-01-20T22:46:57Z,2017-01-20T22:46:57Z,CONTRIBUTOR,"While PR #60 lets you use `UIStackView` in IB files, what I think PR #87 is after is to let you use `OAStackViewProxy` in IB files and have it inherit from the right thing (`OAStackView` or `UIStackView`, depending on the OS).

There's a simpler way to get at this functionality.  We can just have `OAStackViewProxy` inherit from `UIStackView`.  Since PR #60 makes `UIStackView` inherit from `OAStackView` on pre-IOS7, this will work out.",NA,https://api.github.com/repos/nsomar/OAStackView/issues/100/comments,https://github.com/nsomar/OAStackView/issues/100#issuecomment-274200456,https://api.github.com/repos/nsomar/OAStackView/issues/100
nsomar,OAStackView,201929422,https://api.github.com/repos/nsomar/OAStackView/issues/comments/772079049,772079049,MDEyOklzc3VlQ29tbWVudDc3MjA3OTA0OQ==,46784000,2021-02-02T23:11:37Z,2021-02-02T23:11:37Z,NONE,I imagine this is no longer an issue with #101 merged?,NA,https://api.github.com/repos/nsomar/OAStackView/issues/100/comments,https://github.com/nsomar/OAStackView/issues/100#issuecomment-772079049,https://api.github.com/repos/nsomar/OAStackView/issues/100
nsomar,OAStackView,201650667,https://api.github.com/repos/nsomar/OAStackView/issues/comments/273781615,273781615,MDEyOklzc3VlQ29tbWVudDI3Mzc4MTYxNQ==,1461052,2017-01-19T13:54:25Z,2017-01-19T13:54:25Z,OWNER,Great! ðŸ‘ ,NA,https://api.github.com/repos/nsomar/OAStackView/issues/99/comments,https://github.com/nsomar/OAStackView/pull/99#issuecomment-273781615,https://api.github.com/repos/nsomar/OAStackView/issues/99
nsomar,OAStackView,181870480,https://api.github.com/repos/nsomar/OAStackView/issues/comments/252470242,252470242,MDEyOklzc3VlQ29tbWVudDI1MjQ3MDI0Mg==,5897680,2016-10-09T07:29:44Z,2016-10-09T07:29:44Z,NONE,"Some work needs to be done to get travis build pass, I'll just leave it as is since my change is really trivial.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/96/comments,https://github.com/nsomar/OAStackView/pull/96#issuecomment-252470242,https://api.github.com/repos/nsomar/OAStackView/issues/96
nsomar,OAStackView,181870480,https://api.github.com/repos/nsomar/OAStackView/issues/comments/259441576,259441576,MDEyOklzc3VlQ29tbWVudDI1OTQ0MTU3Ng==,1461052,2016-11-09T15:27:28Z,2016-11-09T15:27:28Z,OWNER,"@axl411 Thanks for the change. and sorry for being unresponsive :)
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/96/comments,https://github.com/nsomar/OAStackView/pull/96#issuecomment-259441576,https://api.github.com/repos/nsomar/OAStackView/issues/96
nsomar,OAStackView,171706935,https://api.github.com/repos/nsomar/OAStackView/issues/comments/276495725,276495725,MDEyOklzc3VlQ29tbWVudDI3NjQ5NTcyNQ==,183774,2017-01-31T21:19:08Z,2017-01-31T21:19:08Z,NONE,Is there anything I can do to help get this PR merged?,NA,https://api.github.com/repos/nsomar/OAStackView/issues/94/comments,https://github.com/nsomar/OAStackView/pull/94#issuecomment-276495725,https://api.github.com/repos/nsomar/OAStackView/issues/94
nsomar,OAStackView,171706935,https://api.github.com/repos/nsomar/OAStackView/issues/comments/276613066,276613066,MDEyOklzc3VlQ29tbWVudDI3NjYxMzA2Ng==,1461052,2017-02-01T09:38:38Z,2017-02-01T09:38:38Z,OWNER,"@samsymons Hey, thanks for the interest.
Do you think you can get the travis build green again :) that would be super helpful!",NA,https://api.github.com/repos/nsomar/OAStackView/issues/94/comments,https://github.com/nsomar/OAStackView/pull/94#issuecomment-276613066,https://api.github.com/repos/nsomar/OAStackView/issues/94
nsomar,OAStackView,171706935,https://api.github.com/repos/nsomar/OAStackView/issues/comments/276675183,276675183,MDEyOklzc3VlQ29tbWVudDI3NjY3NTE4Mw==,183774,2017-02-01T14:45:53Z,2017-02-01T14:45:53Z,NONE,"@oarrabi Absolutely, I'll take a look into that â€”Â thanks! ðŸ˜„ ",NA,https://api.github.com/repos/nsomar/OAStackView/issues/94/comments,https://github.com/nsomar/OAStackView/pull/94#issuecomment-276675183,https://api.github.com/repos/nsomar/OAStackView/issues/94
nsomar,OAStackView,171706935,https://api.github.com/repos/nsomar/OAStackView/issues/comments/281238811,281238811,MDEyOklzc3VlQ29tbWVudDI4MTIzODgxMQ==,183774,2017-02-21T03:56:22Z,2017-02-21T03:56:22Z,NONE,"I have not yet had time to fix up the CI issues with this PR but hope to look into that this week; sorry about that!

I also found an edge case with this fix, where views added multiple times to the stack view will not get properly deallocated after this fix in `dealloc`:

```
[self removeObserverForViews:self.subviews];
```

In this case, I had a label being added to the stack view multiple times, but that line above will only remove the observer for _subviews_. I fixed this crash in our project by updating the fix to remove the observer for all arranged subviews, not just `self.subviews`.

It seems that `addArrangedSubview:` is [assuming that it is always getting a new item](https://github.com/oarrabi/OAStackView/blob/master/Pod/Classes/OAStackView.m#L248), so you can have a situation where the same view is added multiple times. Does it makes sense to have `addArrangedSubview:` check whether the newly added subview already exists in the arranged subviews array before adding it?",NA,https://api.github.com/repos/nsomar/OAStackView/issues/94/comments,https://github.com/nsomar/OAStackView/pull/94#issuecomment-281238811,https://api.github.com/repos/nsomar/OAStackView/issues/94
nsomar,OAStackView,165724706,https://api.github.com/repos/nsomar/OAStackView/issues/comments/245176804,245176804,MDEyOklzc3VlQ29tbWVudDI0NTE3NjgwNA==,10137,2016-09-07T05:10:34Z,2016-09-07T05:10:34Z,NONE,"I have the same issue. Anyone can answer this?
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/92/comments,https://github.com/nsomar/OAStackView/issues/92#issuecomment-245176804,https://api.github.com/repos/nsomar/OAStackView/issues/92
nsomar,OAStackView,165081227,https://api.github.com/repos/nsomar/OAStackView/issues/comments/232080602,232080602,MDEyOklzc3VlQ29tbWVudDIzMjA4MDYwMg==,829783,2016-07-12T15:16:32Z,2016-07-12T15:16:32Z,COLLABORATOR,"@jurassic it happens because `Podfile` was written for old version of cocoapods, after 1.0 is out some changes are [needed](http://blog.cocoapods.org/CocoaPods-1.0-Migration-Guide/). PR is welcome, as usual.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/91/comments,https://github.com/nsomar/OAStackView/issues/91#issuecomment-232080602,https://api.github.com/repos/nsomar/OAStackView/issues/91
nsomar,OAStackView,165081227,https://api.github.com/repos/nsomar/OAStackView/issues/comments/273555022,273555022,MDEyOklzc3VlQ29tbWVudDI3MzU1NTAyMg==,310894,2017-01-18T18:13:57Z,2017-01-18T18:13:57Z,CONTRIBUTOR,I've made PR #99 that fixes this issue.,NA,https://api.github.com/repos/nsomar/OAStackView/issues/91/comments,https://github.com/nsomar/OAStackView/issues/91#issuecomment-273555022,https://api.github.com/repos/nsomar/OAStackView/issues/91
nsomar,OAStackView,165081227,https://api.github.com/repos/nsomar/OAStackView/issues/comments/273781683,273781683,MDEyOklzc3VlQ29tbWVudDI3Mzc4MTY4Mw==,1461052,2017-01-19T13:54:43Z,2017-01-19T13:54:43Z,OWNER,Pr #99 merged!,NA,https://api.github.com/repos/nsomar/OAStackView/issues/91/comments,https://github.com/nsomar/OAStackView/issues/91#issuecomment-273781683,https://api.github.com/repos/nsomar/OAStackView/issues/91
nsomar,OAStackView,154853445,https://api.github.com/repos/nsomar/OAStackView/issues/comments/220953393,220953393,MDEyOklzc3VlQ29tbWVudDIyMDk1MzM5Mw==,7514637,2016-05-23T11:20:05Z,2016-05-23T11:20:05Z,NONE,"I had the same issue only with this pod after updating to cocoapods 1.0. Deleting derived data solved it without changing extensions
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/90/comments,https://github.com/nsomar/OAStackView/issues/90#issuecomment-220953393,https://api.github.com/repos/nsomar/OAStackView/issues/90
nsomar,OAStackView,154853445,https://api.github.com/repos/nsomar/OAStackView/issues/comments/225045304,225045304,MDEyOklzc3VlQ29tbWVudDIyNTA0NTMwNA==,7627,2016-06-09T22:25:22Z,2016-06-09T22:25:22Z,NONE,"Deleting derived data worked for me too.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/90/comments,https://github.com/nsomar/OAStackView/issues/90#issuecomment-225045304,https://api.github.com/repos/nsomar/OAStackView/issues/90
nsomar,OAStackView,153281497,https://api.github.com/repos/nsomar/OAStackView/issues/comments/217279919,217279919,MDEyOklzc3VlQ29tbWVudDIxNzI3OTkxOQ==,1461052,2016-05-05T21:06:12Z,2016-05-05T21:06:12Z,OWNER,"Cool!
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/89/comments,https://github.com/nsomar/OAStackView/pull/89#issuecomment-217279919,https://api.github.com/repos/nsomar/OAStackView/issues/89
nsomar,OAStackView,153276177,https://api.github.com/repos/nsomar/OAStackView/issues/comments/217218864,217218864,MDEyOklzc3VlQ29tbWVudDIxNzIxODg2NA==,2092156,2016-05-05T17:31:40Z,2016-05-05T17:31:40Z,CONTRIBUTOR,"Ah, I forgot this project supports iOS 7. I'm looking in to this now.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/88/comments,https://github.com/nsomar/OAStackView/pull/88#issuecomment-217218864,https://api.github.com/repos/nsomar/OAStackView/issues/88
nsomar,OAStackView,153276177,https://api.github.com/repos/nsomar/OAStackView/issues/comments/217423285,217423285,MDEyOklzc3VlQ29tbWVudDIxNzQyMzI4NQ==,2092156,2016-05-06T12:06:03Z,2016-05-06T12:06:03Z,CONTRIBUTOR,"OK, this is passing and ready for review now, @oarrabi  ðŸ‘
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/88/comments,https://github.com/nsomar/OAStackView/pull/88#issuecomment-217423285,https://api.github.com/repos/nsomar/OAStackView/issues/88
nsomar,OAStackView,153276177,https://api.github.com/repos/nsomar/OAStackView/issues/comments/217435761,217435761,MDEyOklzc3VlQ29tbWVudDIxNzQzNTc2MQ==,1461052,2016-05-06T13:12:01Z,2016-05-06T13:12:01Z,OWNER,"Cool! 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/88/comments,https://github.com/nsomar/OAStackView/pull/88#issuecomment-217435761,https://api.github.com/repos/nsomar/OAStackView/issues/88
nsomar,OAStackView,153276177,https://api.github.com/repos/nsomar/OAStackView/issues/comments/217437130,217437130,MDEyOklzc3VlQ29tbWVudDIxNzQzNzEzMA==,2092156,2016-05-06T13:18:59Z,2016-05-06T13:18:59Z,CONTRIBUTOR,"Thanks! What are your thoughts on cutting a new release, say 1.1.0, with the proxy?
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/88/comments,https://github.com/nsomar/OAStackView/pull/88#issuecomment-217437130,https://api.github.com/repos/nsomar/OAStackView/issues/88
nsomar,OAStackView,153104379,https://api.github.com/repos/nsomar/OAStackView/issues/comments/217101055,217101055,MDEyOklzc3VlQ29tbWVudDIxNzEwMTA1NQ==,1461052,2016-05-05T08:19:26Z,2016-05-05T08:19:26Z,OWNER,"Hey super cool PR thanks. ðŸ‘ 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/87/comments,https://github.com/nsomar/OAStackView/pull/87#issuecomment-217101055,https://api.github.com/repos/nsomar/OAStackView/issues/87
nsomar,OAStackView,153104379,https://api.github.com/repos/nsomar/OAStackView/issues/comments/241356640,241356640,MDEyOklzc3VlQ29tbWVudDI0MTM1NjY0MA==,136644,2016-08-22T09:16:01Z,2016-08-22T09:16:01Z,CONTRIBUTOR,"Wouldn't this compromise iOS 7 compatibility? I mean, CocoaPods requires frameworks and hence iOS 8+ deployment target for dependencies with Swift code.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/87/comments,https://github.com/nsomar/OAStackView/pull/87#issuecomment-241356640,https://api.github.com/repos/nsomar/OAStackView/issues/87
nsomar,OAStackView,139726386,https://api.github.com/repos/nsomar/OAStackView/issues/comments/194584962,194584962,MDEyOklzc3VlQ29tbWVudDE5NDU4NDk2Mg==,86030,2016-03-10T00:18:25Z,2016-03-10T00:18:25Z,CONTRIBUTOR,"More information about the issue can be found here:
https://github.com/CocoaPods/CocoaPods/issues/4420

It seems most libraries removed the angular imports.
Ex. https://github.com/Mantle/Mantle/issues/626
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/85/comments,https://github.com/nsomar/OAStackView/issues/85#issuecomment-194584962,https://api.github.com/repos/nsomar/OAStackView/issues/85
nsomar,OAStackView,139726386,https://api.github.com/repos/nsomar/OAStackView/issues/comments/194717428,194717428,MDEyOklzc3VlQ29tbWVudDE5NDcxNzQyOA==,86030,2016-03-10T07:36:54Z,2016-03-10T07:36:54Z,CONTRIBUTOR,"Following the practice of other open source libraries I fixed the imports in this PR https://github.com/oarrabi/OAStackView/pull/86
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/85/comments,https://github.com/nsomar/OAStackView/issues/85#issuecomment-194717428,https://api.github.com/repos/nsomar/OAStackView/issues/85
nsomar,OAStackView,139726386,https://api.github.com/repos/nsomar/OAStackView/issues/comments/194783466,194783466,MDEyOklzc3VlQ29tbWVudDE5NDc4MzQ2Ng==,829783,2016-03-10T10:34:50Z,2016-03-10T10:34:50Z,COLLABORATOR,"Thanks @intonarumori, looks like it works fine for my obj-c project too.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/85/comments,https://github.com/nsomar/OAStackView/issues/85#issuecomment-194783466,https://api.github.com/repos/nsomar/OAStackView/issues/85
nsomar,OAStackView,136879915,https://api.github.com/repos/nsomar/OAStackView/issues/comments/217656695,217656695,MDEyOklzc3VlQ29tbWVudDIxNzY1NjY5NQ==,106105,2016-05-07T18:06:03Z,2016-05-07T18:06:03Z,NONE,"This only happens if you add the subviews in the wrong order, and they have constraints (in my case in a storyboard). This is user error in my view. I solved my problem by ensuring the stack views were added by the storyboard in the order they should be displayed (if visible).
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/84/comments,https://github.com/nsomar/OAStackView/issues/84#issuecomment-217656695,https://api.github.com/repos/nsomar/OAStackView/issues/84
nsomar,OAStackView,136655577,https://api.github.com/repos/nsomar/OAStackView/issues/comments/189209756,189209756,MDEyOklzc3VlQ29tbWVudDE4OTIwOTc1Ng==,777591,2016-02-26T10:31:57Z,2016-02-26T10:31:57Z,NONE,"Ok, I found a quick fix. It seems that adding these lines:

```
for (UIView *view in self.subviews) {
    [self.mutableArrangedSubviews addObject:view];
}
```

in `initWithCoder:` will fix my issue. So here's my complete implementation of that method:

```
- (instancetype)initWithCoder:(NSCoder *)decoder {
    self = [super initWithCoder:decoder];

    if (self) {
        [self commonInitWithInitalSubviews:@[]];

        if ([NSStringFromClass([self class]) isEqualToString:@""UIStackView""]) {
            self.axis = [decoder decodeIntegerForKey:@""UIStackViewAxis""];
            self.distribution = [decoder decodeIntegerForKey:@""UIStackViewDistribution""];
            self.alignment = [decoder decodeIntegerForKey:@""UIStackViewAlignment""];
            self.spacing = [decoder decodeDoubleForKey:@""UIStackViewSpacing""];
            self.baselineRelativeArrangement = [decoder decodeBoolForKey:@""UIStackViewBaselineRelative""];
            self.layoutMarginsRelativeArrangement = [decoder decodeBoolForKey:@""UIStackViewLayoutMarginsRelative""];
        }

        for (UIView *view in self.subviews) {
            [self.mutableArrangedSubviews addObject:view];
        }

        [self layoutArrangedViews];
    }

    return self;
}
```
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/83/comments,https://github.com/nsomar/OAStackView/issues/83#issuecomment-189209756,https://api.github.com/repos/nsomar/OAStackView/issues/83
nsomar,OAStackView,136655577,https://api.github.com/repos/nsomar/OAStackView/issues/comments/204528234,204528234,MDEyOklzc3VlQ29tbWVudDIwNDUyODIzNA==,829783,2016-04-01T19:13:15Z,2016-04-01T19:13:15Z,COLLABORATOR,"It looks like regression caused by my fix of KVO: https://github.com/oarrabi/OAStackView/commit/fb7d74110aee1de8b6780ddf91428b3eaced1753

:( no idea how to solve your issue correctly at the moment
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/83/comments,https://github.com/nsomar/OAStackView/issues/83#issuecomment-204528234,https://api.github.com/repos/nsomar/OAStackView/issues/83
nsomar,OAStackView,136507156,https://api.github.com/repos/nsomar/OAStackView/issues/comments/189604639,189604639,MDEyOklzc3VlQ29tbWVudDE4OTYwNDYzOQ==,5697471,2016-02-27T08:23:38Z,2016-02-27T08:23:38Z,NONE,"I agree, it is not helpful at all without this information. Also, iOS 9 Stack Views preserve alignment automatically when you drag new elements in. It appears this repo does not have this feature implemented.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/82/comments,https://github.com/nsomar/OAStackView/issues/82#issuecomment-189604639,https://api.github.com/repos/nsomar/OAStackView/issues/82
nsomar,OAStackView,136308488,https://api.github.com/repos/nsomar/OAStackView/issues/comments/225884880,225884880,MDEyOklzc3VlQ29tbWVudDIyNTg4NDg4MA==,815372,2016-06-14T13:44:03Z,2016-06-14T13:44:03Z,NONE,"@AgentFeeble I think your issue might be related to the missing functionality of **layoutMarginsRelativeArrangement**. 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/81/comments,https://github.com/nsomar/OAStackView/issues/81#issuecomment-225884880,https://api.github.com/repos/nsomar/OAStackView/issues/81
nsomar,OAStackView,135164614,https://api.github.com/repos/nsomar/OAStackView/issues/comments/186757739,186757739,MDEyOklzc3VlQ29tbWVudDE4Njc1NzczOQ==,829783,2016-02-21T06:25:43Z,2016-02-21T06:25:43Z,COLLABORATOR,"Hi @GriffinSchneider sorry for confusion. Actually `horizontal` is correct. Even though `UIStackView` does not say it explicitly, `UILayoutConstraintAxisHorizontal` has value of 0 which makes it default. Please see https://github.com/oarrabi/OAStackView/issues/51 for more details
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/80/comments,https://github.com/nsomar/OAStackView/pull/80#issuecomment-186757739,https://api.github.com/repos/nsomar/OAStackView/issues/80
nsomar,OAStackView,130670244,https://api.github.com/repos/nsomar/OAStackView/issues/comments/179162312,179162312,MDEyOklzc3VlQ29tbWVudDE3OTE2MjMxMg==,2689177,2016-02-03T10:55:14Z,2016-02-03T10:55:14Z,NONE,"Try to delete Module at Custom Class for OAStackView in InterfaceBuilder.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/79/comments,https://github.com/nsomar/OAStackView/issues/79#issuecomment-179162312,https://api.github.com/repos/nsomar/OAStackView/issues/79
nsomar,OAStackView,129625357,https://api.github.com/repos/nsomar/OAStackView/issues/comments/226773794,226773794,MDEyOklzc3VlQ29tbWVudDIyNjc3Mzc5NA==,660472,2016-06-17T13:50:45Z,2016-06-17T13:50:45Z,NONE,"I've just found it to be true. 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/78/comments,https://github.com/nsomar/OAStackView/issues/78#issuecomment-226773794,https://api.github.com/repos/nsomar/OAStackView/issues/78
nsomar,OAStackView,129441798,https://api.github.com/repos/nsomar/OAStackView/issues/comments/176163811,176163811,MDEyOklzc3VlQ29tbWVudDE3NjE2MzgxMQ==,829783,2016-01-28T12:47:33Z,2016-01-28T12:47:33Z,COLLABORATOR,"@m1entus JFYI
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/77/comments,https://github.com/nsomar/OAStackView/pull/77#issuecomment-176163811,https://api.github.com/repos/nsomar/OAStackView/issues/77
nsomar,OAStackView,126921178,https://api.github.com/repos/nsomar/OAStackView/issues/comments/172127178,172127178,MDEyOklzc3VlQ29tbWVudDE3MjEyNzE3OA==,1283243,2016-01-15T23:41:19Z,2016-01-15T23:41:19Z,NONE,"@codepleaser Take a look how Cocoapods works: https://cocoapods.org (tab ""GET STARTED"").
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/76/comments,https://github.com/nsomar/OAStackView/issues/76#issuecomment-172127178,https://api.github.com/repos/nsomar/OAStackView/issues/76
nsomar,OAStackView,125750294,https://api.github.com/repos/nsomar/OAStackView/issues/comments/189606797,189606797,MDEyOklzc3VlQ29tbWVudDE4OTYwNjc5Nw==,5697471,2016-02-27T08:43:32Z,2016-02-27T08:43:32Z,NONE,"Can you make make the documentation more thorough for those who want to incorporate this repo into their own projects?
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/74/comments,https://github.com/nsomar/OAStackView/issues/74#issuecomment-189606797,https://api.github.com/repos/nsomar/OAStackView/issues/74
nsomar,OAStackView,125223865,https://api.github.com/repos/nsomar/OAStackView/issues/comments/169728376,169728376,MDEyOklzc3VlQ29tbWVudDE2OTcyODM3Ng==,829783,2016-01-07T16:59:03Z,2016-01-07T16:59:03Z,COLLABORATOR,"Hi @m1entus, there is at least two interesting PRs hanging, I hope me and @oarrabi will review and merge them and release new version after that.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/73/comments,https://github.com/nsomar/OAStackView/issues/73#issuecomment-169728376,https://api.github.com/repos/nsomar/OAStackView/issues/73
nsomar,OAStackView,125223865,https://api.github.com/repos/nsomar/OAStackView/issues/comments/170220000,170220000,MDEyOklzc3VlQ29tbWVudDE3MDIyMDAwMA==,829783,2016-01-09T09:57:11Z,2016-01-09T09:57:19Z,COLLABORATOR,"@m1entus 0.3.0 is live (it does not include your PR, I'm making final preparations to release 1.0.0 version since it would have minor but breaking change https://github.com/oarrabi/OAStackView/issues/51
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/73/comments,https://github.com/nsomar/OAStackView/issues/73#issuecomment-170220000,https://api.github.com/repos/nsomar/OAStackView/issues/73
nsomar,OAStackView,125107922,https://api.github.com/repos/nsomar/OAStackView/issues/comments/173379513,173379513,MDEyOklzc3VlQ29tbWVudDE3MzM3OTUxMw==,35043,2016-01-20T22:16:29Z,2016-01-20T22:16:29Z,NONE,"@zlanchun `OAStackView` orders views in the order they are placed in their superview, not based on x-y coordinates. Make sure your views are ordered correctly in the outline view of interface builder: ![outline view](https://developer.apple.com/library/tvos/documentation/ToolsLanguages/Conceptual/Xcode_Overview/Art/OutlineView_2x.png)

You can drag the subviews in the outline to order them as desired.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/72/comments,https://github.com/nsomar/OAStackView/issues/72#issuecomment-173379513,https://api.github.com/repos/nsomar/OAStackView/issues/72
nsomar,OAStackView,125107922,https://api.github.com/repos/nsomar/OAStackView/issues/comments/189079081,189079081,MDEyOklzc3VlQ29tbWVudDE4OTA3OTA4MQ==,111197,2016-02-26T02:11:20Z,2016-02-26T02:11:47Z,NONE,"I've also seen in Xcode 7.2 at least that the ordering showing in the sidebar isn't always reflective of the order they are added at runtime.  Rearranging the views in the sidebar and then putting them back to the correct order has fixed the problem for me.  I don't know if other versions have this problem or not.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/72/comments,https://github.com/nsomar/OAStackView/issues/72#issuecomment-189079081,https://api.github.com/repos/nsomar/OAStackView/issues/72
nsomar,OAStackView,121685652,https://api.github.com/repos/nsomar/OAStackView/issues/comments/169792576,169792576,MDEyOklzc3VlQ29tbWVudDE2OTc5MjU3Ng==,2101850,2016-01-07T20:12:20Z,2016-01-07T20:13:22Z,CONTRIBUTOR,"I found some problems when testing this in the Example app that should be resolved now.
- The example storyboard had a bunch of constraints set on the buttons in the OAStackView. They were previously removed by the `removeDecendentConstraints` method but obviously caused some problems when I had removed it. I've set 'remove at build time' on these constraints.
- My previous implementation removed the strategies constraints by:

```
[self.distributionStrategy removeAddedConstraints];
[self.alignmentStrategy removeAddedConstraints];
```

This caused a layout to occur after the distributions removal and before the alignment - resulting in constraint warnings. I've therefore exposed the constraints on those objects as a readonly array so that OAStackView can remove all of them without causing a layout before all constraints are removed.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/70/comments,https://github.com/nsomar/OAStackView/pull/70#issuecomment-169792576,https://api.github.com/repos/nsomar/OAStackView/issues/70
nsomar,OAStackView,121685652,https://api.github.com/repos/nsomar/OAStackView/issues/comments/169964946,169964946,MDEyOklzc3VlQ29tbWVudDE2OTk2NDk0Ng==,829783,2016-01-08T11:04:26Z,2016-01-08T11:04:26Z,COLLABORATOR,"@mattiasjahnke thanks for your work! please resolve final comments and I will merge PR
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/70/comments,https://github.com/nsomar/OAStackView/pull/70#issuecomment-169964946,https://api.github.com/repos/nsomar/OAStackView/issues/70
nsomar,OAStackView,121685652,https://api.github.com/repos/nsomar/OAStackView/issues/comments/170025666,170025666,MDEyOklzc3VlQ29tbWVudDE3MDAyNTY2Ng==,2101850,2016-01-08T15:02:48Z,2016-01-08T15:02:48Z,CONTRIBUTOR,"Taken care of ;)
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/70/comments,https://github.com/nsomar/OAStackView/pull/70#issuecomment-170025666,https://api.github.com/repos/nsomar/OAStackView/issues/70
nsomar,OAStackView,121685652,https://api.github.com/repos/nsomar/OAStackView/issues/comments/170050138,170050138,MDEyOklzc3VlQ29tbWVudDE3MDA1MDEzOA==,829783,2016-01-08T16:39:13Z,2016-01-08T16:39:13Z,COLLABORATOR,"Merged, thanks @mattiasjahnke!
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/70/comments,https://github.com/nsomar/OAStackView/pull/70#issuecomment-170050138,https://api.github.com/repos/nsomar/OAStackView/issues/70
nsomar,OAStackView,119357954,https://api.github.com/repos/nsomar/OAStackView/issues/comments/160420966,160420966,MDEyOklzc3VlQ29tbWVudDE2MDQyMDk2Ng==,1461052,2015-11-29T14:56:28Z,2015-11-29T14:56:28Z,OWNER,"What version of Xcode are you using? at the moment since OAStackView is using generics then Xcode 7 is required.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/69/comments,https://github.com/nsomar/OAStackView/issues/69#issuecomment-160420966,https://api.github.com/repos/nsomar/OAStackView/issues/69
nsomar,OAStackView,119357954,https://api.github.com/repos/nsomar/OAStackView/issues/comments/160421378,160421378,MDEyOklzc3VlQ29tbWVudDE2MDQyMTM3OA==,1476754,2015-11-29T15:04:53Z,2015-11-29T15:55:30Z,NONE,"xcode 6.4 can i get old version
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/69/comments,https://github.com/nsomar/OAStackView/issues/69#issuecomment-160421378,https://api.github.com/repos/nsomar/OAStackView/issues/69
nsomar,OAStackView,119357954,https://api.github.com/repos/nsomar/OAStackView/issues/comments/160568087,160568087,MDEyOklzc3VlQ29tbWVudDE2MDU2ODA4Nw==,1476754,2015-11-30T09:20:53Z,2015-11-30T09:20:53Z,NONE,"i fix it by 
pod 'OAStackView', :git => 'https://github.com/oarrabi/OAStackView.git', :commit => 'a49dc63'
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/69/comments,https://github.com/nsomar/OAStackView/issues/69#issuecomment-160568087,https://api.github.com/repos/nsomar/OAStackView/issues/69
nsomar,OAStackView,119163007,https://api.github.com/repos/nsomar/OAStackView/issues/comments/160179294,160179294,MDEyOklzc3VlQ29tbWVudDE2MDE3OTI5NA==,853032,2015-11-27T17:23:27Z,2015-11-27T17:23:27Z,CONTRIBUTOR,"Swift's type system won't let you do this. The compiler can't guarantee that UIStackView and OAStackView are isomorphic, and there's no facility for ensuring it anyway.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/68/comments,https://github.com/nsomar/OAStackView/issues/68#issuecomment-160179294,https://api.github.com/repos/nsomar/OAStackView/issues/68
nsomar,OAStackView,119163007,https://api.github.com/repos/nsomar/OAStackView/issues/comments/273814124,273814124,MDEyOklzc3VlQ29tbWVudDI3MzgxNDEyNA==,310894,2017-01-19T15:55:17Z,2017-01-19T15:55:17Z,CONTRIBUTOR,Is this a dupe of #2?,NA,https://api.github.com/repos/nsomar/OAStackView/issues/68/comments,https://github.com/nsomar/OAStackView/issues/68#issuecomment-273814124,https://api.github.com/repos/nsomar/OAStackView/issues/68
nsomar,OAStackView,118716876,https://api.github.com/repos/nsomar/OAStackView/issues/comments/159434822,159434822,MDEyOklzc3VlQ29tbWVudDE1OTQzNDgyMg==,1461052,2015-11-24T23:08:19Z,2015-11-24T23:08:19Z,OWNER,"Merging now :+1: 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/66/comments,https://github.com/nsomar/OAStackView/pull/66#issuecomment-159434822,https://api.github.com/repos/nsomar/OAStackView/issues/66
nsomar,OAStackView,118716535,https://api.github.com/repos/nsomar/OAStackView/issues/comments/159434506,159434506,MDEyOklzc3VlQ29tbWVudDE1OTQzNDUwNg==,1461052,2015-11-24T23:07:10Z,2015-11-24T23:07:10Z,OWNER,"I agree with your fix, can you please submit a PR with this fix?
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/65/comments,https://github.com/nsomar/OAStackView/issues/65#issuecomment-159434506,https://api.github.com/repos/nsomar/OAStackView/issues/65
nsomar,OAStackView,118716535,https://api.github.com/repos/nsomar/OAStackView/issues/comments/159434707,159434707,MDEyOklzc3VlQ29tbWVudDE1OTQzNDcwNw==,1461052,2015-11-24T23:07:49Z,2015-11-24T23:07:49Z,OWNER,"Sorry I didn't see the PR, I am viewing it now.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/65/comments,https://github.com/nsomar/OAStackView/issues/65#issuecomment-159434707,https://api.github.com/repos/nsomar/OAStackView/issues/65
nsomar,OAStackView,115255882,https://api.github.com/repos/nsomar/OAStackView/issues/comments/159435783,159435783,MDEyOklzc3VlQ29tbWVudDE1OTQzNTc4Mw==,1461052,2015-11-24T23:13:40Z,2015-11-24T23:13:40Z,OWNER,"Sorry for the late reply, I will be trying this out soon. Thanks for the patience :)
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/63/comments,https://github.com/nsomar/OAStackView/issues/63#issuecomment-159435783,https://api.github.com/repos/nsomar/OAStackView/issues/63
nsomar,OAStackView,115255882,https://api.github.com/repos/nsomar/OAStackView/issues/comments/159543202,159543202,MDEyOklzc3VlQ29tbWVudDE1OTU0MzIwMg==,133607,2015-11-25T09:10:26Z,2015-11-25T09:10:26Z,NONE,"No worries, thanks for having a look at it :-)
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/63/comments,https://github.com/nsomar/OAStackView/issues/63#issuecomment-159543202,https://api.github.com/repos/nsomar/OAStackView/issues/63
nsomar,OAStackView,115255882,https://api.github.com/repos/nsomar/OAStackView/issues/comments/263485021,263485021,MDEyOklzc3VlQ29tbWVudDI2MzQ4NTAyMQ==,12591229,2016-11-29T06:07:47Z,2016-11-29T06:07:47Z,NONE,"is this bug fixing in progress or not ? i am in worse condition, i need to use this feature . crossing fingers :)",NA,https://api.github.com/repos/nsomar/OAStackView/issues/63/comments,https://github.com/nsomar/OAStackView/issues/63#issuecomment-263485021,https://api.github.com/repos/nsomar/OAStackView/issues/63
nsomar,OAStackView,111929732,https://api.github.com/repos/nsomar/OAStackView/issues/comments/159433634,159433634,MDEyOklzc3VlQ29tbWVudDE1OTQzMzYzNA==,1461052,2015-11-24T23:03:28Z,2015-11-24T23:03:28Z,OWNER,"@pocketpixels I am able to reproduce this, on a quick glance, I think the issue is with the current implementation of layout guides. I will be spending more time on this over the coming days.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/62/comments,https://github.com/nsomar/OAStackView/issues/62#issuecomment-159433634,https://api.github.com/repos/nsomar/OAStackView/issues/62
nsomar,OAStackView,111808779,https://api.github.com/repos/nsomar/OAStackView/issues/comments/148712313,148712313,MDEyOklzc3VlQ29tbWVudDE0ODcxMjMxMw==,829783,2015-10-16T13:08:59Z,2015-10-16T13:08:59Z,COLLABORATOR,"Can you please attach time profiler screenshot showing what exactly is ""slow""? View inspector screenshot with view hierarchy might also help
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/61/comments,https://github.com/nsomar/OAStackView/issues/61#issuecomment-148712313,https://api.github.com/repos/nsomar/OAStackView/issues/61
nsomar,OAStackView,111808779,https://api.github.com/repos/nsomar/OAStackView/issues/comments/148887912,148887912,MDEyOklzc3VlQ29tbWVudDE0ODg4NzkxMg==,918490,2015-10-17T05:58:29Z,2015-10-17T05:58:29Z,NONE,"Sure.I wonder you could check the following log

```
----------------------
shotworks_for_ph2_by_swift.PushSettingVCL
loadView()
Optional(2015-10-17T14:49:32.906+0900)
----------------------
----------------------
shotworks_for_ph2_by_swift.PushSettingVCL
viewDidLoad()
Optional(2015-10-17T14:49:41.685+0900)
----------------------
```

It took 9 seconds from 'loadview' to 'viewDidLoad'.
I also attached hierarchy.

<img width=""1189"" alt=""2015-10-17 14 53 38"" src=""https://cloud.githubusercontent.com/assets/918490/10557262/79cf1c46-74df-11e5-891f-18414e936a40.png"">
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/61/comments,https://github.com/nsomar/OAStackView/issues/61#issuecomment-148887912,https://api.github.com/repos/nsomar/OAStackView/issues/61
nsomar,OAStackView,111808779,https://api.github.com/repos/nsomar/OAStackView/issues/comments/148896871,148896871,MDEyOklzc3VlQ29tbWVudDE0ODg5Njg3MQ==,918490,2015-10-17T09:07:57Z,2015-10-17T09:07:57Z,NONE,"I have updated program.
I attached hierarchy again.
I am using 3 oastackview.
<img width=""1385"" alt=""2015-10-17 18 06 13"" src=""https://cloud.githubusercontent.com/assets/918490/10557950/fbba9810-74f9-11e5-9a3a-b61f73469fb4.png"">
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/61/comments,https://github.com/nsomar/OAStackView/issues/61#issuecomment-148896871,https://api.github.com/repos/nsomar/OAStackView/issues/61
nsomar,OAStackView,111808779,https://api.github.com/repos/nsomar/OAStackView/issues/comments/149615804,149615804,MDEyOklzc3VlQ29tbWVudDE0OTYxNTgwNA==,1911951,2015-10-20T16:00:29Z,2015-10-20T16:00:29Z,NONE,"Have you profiled it in instruments using the time profiler?
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/61/comments,https://github.com/nsomar/OAStackView/issues/61#issuecomment-149615804,https://api.github.com/repos/nsomar/OAStackView/issues/61
nsomar,OAStackView,111808779,https://api.github.com/repos/nsomar/OAStackView/issues/comments/149721569,149721569,MDEyOklzc3VlQ29tbWVudDE0OTcyMTU2OQ==,918490,2015-10-20T22:31:55Z,2015-10-20T22:31:55Z,NONE,"Oh,I see.I have not used the time profiler at this time.So I will try it. 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/61/comments,https://github.com/nsomar/OAStackView/issues/61#issuecomment-149721569,https://api.github.com/repos/nsomar/OAStackView/issues/61
nsomar,OAStackView,111808779,https://api.github.com/repos/nsomar/OAStackView/issues/comments/159432646,159432646,MDEyOklzc3VlQ29tbWVudDE1OTQzMjY0Ng==,1461052,2015-11-24T22:58:22Z,2015-11-24T22:58:22Z,OWNER,"@shiratsu any update on this?
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/61/comments,https://github.com/nsomar/OAStackView/issues/61#issuecomment-159432646,https://api.github.com/repos/nsomar/OAStackView/issues/61
nsomar,OAStackView,111808779,https://api.github.com/repos/nsomar/OAStackView/issues/comments/159545204,159545204,MDEyOklzc3VlQ29tbWVudDE1OTU0NTIwNA==,918490,2015-11-25T09:19:25Z,2015-11-25T09:19:25Z,NONE,"Sorry,please wait.I will post result within end of this year.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/61/comments,https://github.com/nsomar/OAStackView/issues/61#issuecomment-159545204,https://api.github.com/repos/nsomar/OAStackView/issues/61
nsomar,OAStackView,111808779,https://api.github.com/repos/nsomar/OAStackView/issues/comments/162746866,162746866,MDEyOklzc3VlQ29tbWVudDE2Mjc0Njg2Ng==,918490,2015-12-08T03:16:32Z,2015-12-08T03:16:32Z,NONE,"I'm sorry for very late replying.
I have checked it.
On iOS8,xcode 7.1,Swift 2.1 Custom UIView is very slow.

My custom view and also OAStackView are slow.
'initWithCoder method' is very slow.
I attached capture of time profiler.
![2015-12-08 12 16 05](https://cloud.githubusercontent.com/assets/918490/11646870/7e4e4492-9da5-11e5-808c-b67baf93d84a.png)
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/61/comments,https://github.com/nsomar/OAStackView/issues/61#issuecomment-162746866,https://api.github.com/repos/nsomar/OAStackView/issues/61
nsomar,OAStackView,111808779,https://api.github.com/repos/nsomar/OAStackView/issues/comments/162752777,162752777,MDEyOklzc3VlQ29tbWVudDE2Mjc1Mjc3Nw==,918490,2015-12-08T04:03:17Z,2015-12-08T04:03:17Z,NONE,"I attached another capture.
![2015-12-08 13 02 44](https://cloud.githubusercontent.com/assets/918490/11647483/0a45cafa-9dac-11e5-9d0d-b5696c3bbded.png)
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/61/comments,https://github.com/nsomar/OAStackView/issues/61#issuecomment-162752777,https://api.github.com/repos/nsomar/OAStackView/issues/61
nsomar,OAStackView,111808779,https://api.github.com/repos/nsomar/OAStackView/issues/comments/162788187,162788187,MDEyOklzc3VlQ29tbWVudDE2Mjc4ODE4Nw==,918490,2015-12-08T06:32:29Z,2015-12-08T06:32:29Z,NONE,"I might get answer.
In iOS8-Xcode7,I am using 'HiraginoSans' as custom font.
But this font name is not available on iOS8.
It is from iOS9.
So,My application have taken  a much time for downloading above font.

I'm sorry.Maybe,OAStackView does not have relation about slow loading. 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/61/comments,https://github.com/nsomar/OAStackView/issues/61#issuecomment-162788187,https://api.github.com/repos/nsomar/OAStackView/issues/61
nsomar,OAStackView,111572776,https://api.github.com/repos/nsomar/OAStackView/issues/comments/148324551,148324551,MDEyOklzc3VlQ29tbWVudDE0ODMyNDU1MQ==,1565368,2015-10-15T09:06:15Z,2015-10-15T09:07:24Z,CONTRIBUTOR,"Okay check now @oarrabi @garnett , i ammended latest commit
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/60/comments,https://github.com/nsomar/OAStackView/pull/60#issuecomment-148324551,https://api.github.com/repos/nsomar/OAStackView/issues/60
nsomar,OAStackView,111572776,https://api.github.com/repos/nsomar/OAStackView/issues/comments/148359791,148359791,MDEyOklzc3VlQ29tbWVudDE0ODM1OTc5MQ==,829783,2015-10-15T11:30:04Z,2015-10-15T11:30:04Z,COLLABORATOR,"I've seen this functionality in another implementation of stackView and it is definitely handy, but inline assembler scares me a little bit.
@oarrabi please take a look at this PR
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/60/comments,https://github.com/nsomar/OAStackView/pull/60#issuecomment-148359791,https://api.github.com/repos/nsomar/OAStackView/issues/60
nsomar,OAStackView,111572776,https://api.github.com/repos/nsomar/OAStackView/issues/comments/148494425,148494425,MDEyOklzc3VlQ29tbWVudDE0ODQ5NDQyNQ==,1565368,2015-10-15T19:18:38Z,2015-10-15T19:18:38Z,CONTRIBUTOR,"@garnett actually without asm it also works fine, i removed unnecessary code and added sentinel if someone would like to turn of replacing class in <iOS9.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/60/comments,https://github.com/nsomar/OAStackView/pull/60#issuecomment-148494425,https://api.github.com/repos/nsomar/OAStackView/issues/60
nsomar,OAStackView,111572776,https://api.github.com/repos/nsomar/OAStackView/issues/comments/151532605,151532605,MDEyOklzc3VlQ29tbWVudDE1MTUzMjYwNQ==,1565368,2015-10-27T15:05:04Z,2015-10-27T15:05:04Z,CONTRIBUTOR,"Any progress with that ?
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/60/comments,https://github.com/nsomar/OAStackView/pull/60#issuecomment-151532605,https://api.github.com/repos/nsomar/OAStackView/issues/60
nsomar,OAStackView,111572776,https://api.github.com/repos/nsomar/OAStackView/issues/comments/157990972,157990972,MDEyOklzc3VlQ29tbWVudDE1Nzk5MDk3Mg==,1461052,2015-11-19T08:45:23Z,2015-11-19T08:45:23Z,OWNER,"@m1entus hey, sorry for the super late reply, I will be working on this on the weekend, thanks for the great addition :)
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/60/comments,https://github.com/nsomar/OAStackView/pull/60#issuecomment-157990972,https://api.github.com/repos/nsomar/OAStackView/issues/60
nsomar,OAStackView,111572776,https://api.github.com/repos/nsomar/OAStackView/issues/comments/162483615,162483615,MDEyOklzc3VlQ29tbWVudDE2MjQ4MzYxNQ==,1565368,2015-12-07T11:03:05Z,2015-12-07T11:03:05Z,CONTRIBUTOR,"Any progress ?
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/60/comments,https://github.com/nsomar/OAStackView/pull/60#issuecomment-162483615,https://api.github.com/repos/nsomar/OAStackView/issues/60
nsomar,OAStackView,111572776,https://api.github.com/repos/nsomar/OAStackView/issues/comments/169020704,169020704,MDEyOklzc3VlQ29tbWVudDE2OTAyMDcwNA==,1565368,2016-01-05T14:48:21Z,2016-01-05T14:48:21Z,CONTRIBUTOR,"...
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/60/comments,https://github.com/nsomar/OAStackView/pull/60#issuecomment-169020704,https://api.github.com/repos/nsomar/OAStackView/issues/60
nsomar,OAStackView,111572776,https://api.github.com/repos/nsomar/OAStackView/issues/comments/169021003,169021003,MDEyOklzc3VlQ29tbWVudDE2OTAyMTAwMw==,1461052,2016-01-05T14:49:34Z,2016-01-05T14:49:34Z,OWNER,"Hello @m1entus, I am extremely sorry for the delay, will check this on the weekends. Sorry for the inconvenience :disappointed: 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/60/comments,https://github.com/nsomar/OAStackView/pull/60#issuecomment-169021003,https://api.github.com/repos/nsomar/OAStackView/issues/60
nsomar,OAStackView,111572776,https://api.github.com/repos/nsomar/OAStackView/issues/comments/170051932,170051932,MDEyOklzc3VlQ29tbWVudDE3MDA1MTkzMg==,829783,2016-01-08T16:45:47Z,2016-01-08T16:45:47Z,COLLABORATOR,"@m1entus could you please rebase your work on current head? Sorry for returning so late
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/60/comments,https://github.com/nsomar/OAStackView/pull/60#issuecomment-170051932,https://api.github.com/repos/nsomar/OAStackView/issues/60
nsomar,OAStackView,111572776,https://api.github.com/repos/nsomar/OAStackView/issues/comments/170156581,170156581,MDEyOklzc3VlQ29tbWVudDE3MDE1NjU4MQ==,1565368,2016-01-08T23:33:25Z,2016-01-08T23:33:25Z,CONTRIBUTOR,"Done
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/60/comments,https://github.com/nsomar/OAStackView/pull/60#issuecomment-170156581,https://api.github.com/repos/nsomar/OAStackView/issues/60
nsomar,OAStackView,111572776,https://api.github.com/repos/nsomar/OAStackView/issues/comments/170215124,170215124,MDEyOklzc3VlQ29tbWVudDE3MDIxNTEyNA==,829783,2016-01-09T09:34:50Z,2016-01-09T09:34:50Z,COLLABORATOR,"@m1entus looking at your PR and it is really awesome!
Could you please elaborate what `OAStackViewPatchEntry`` actually is, when is it invoked, is it documented, etc.?

Another thing I see is that there is a plenty of constraint errors in the log (maybe it is the reason I see different behaviour of `OAStackView` and `UIStackView` with the same setup)
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/60/comments,https://github.com/nsomar/OAStackView/pull/60#issuecomment-170215124,https://api.github.com/repos/nsomar/OAStackView/issues/60
nsomar,OAStackView,111572776,https://api.github.com/repos/nsomar/OAStackView/issues/comments/170215561,170215561,MDEyOklzc3VlQ29tbWVudDE3MDIxNTU2MQ==,1565368,2016-01-09T09:40:03Z,2016-01-09T09:40:03Z,CONTRIBUTOR,"OAStackViewPatchEntry is static constructor, it is called automatically. Probably https://github.com/oarrabi/OAStackView/commit/a683019f19680678f5c11a7ba78d198c0c454faa this commit change some storybooad constraint, thats why there are errors on setup. Could you just cherrypick OAStackView.h file, and apply it? It doesn't break anything in library, only replace UIStackView symbol in <=iOS8, and would be useful. 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/60/comments,https://github.com/nsomar/OAStackView/pull/60#issuecomment-170215561,https://api.github.com/repos/nsomar/OAStackView/issues/60
nsomar,OAStackView,111572776,https://api.github.com/repos/nsomar/OAStackView/issues/comments/170218159,170218159,MDEyOklzc3VlQ29tbWVudDE3MDIxODE1OQ==,829783,2016-01-09T09:46:58Z,2016-01-09T09:46:58Z,COLLABORATOR,"@m1entus I've already fixed constraints now everything looks pretty fine. I realised that there is no point to compare stackview in both tabs, because their ""default"" setup is different - one is built as UIStackView in IB and another is just a set of views added to superview, so it is not bothering me anymore.
I will finish reviewing this PR and merge it soon, can't wait to start building stackview in XIBs for iOS8! :bowtie: 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/60/comments,https://github.com/nsomar/OAStackView/pull/60#issuecomment-170218159,https://api.github.com/repos/nsomar/OAStackView/issues/60
nsomar,OAStackView,111572776,https://api.github.com/repos/nsomar/OAStackView/issues/comments/170220742,170220742,MDEyOklzc3VlQ29tbWVudDE3MDIyMDc0Mg==,829783,2016-01-09T10:10:33Z,2016-01-09T10:10:33Z,COLLABORATOR,"Sorry I had no idea what is the best workflow for patching this pr so I create mine based on this one and merged it manually: https://github.com/oarrabi/OAStackView/pull/75

Thanks a lot @m1entus!
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/60/comments,https://github.com/nsomar/OAStackView/pull/60#issuecomment-170220742,https://api.github.com/repos/nsomar/OAStackView/issues/60
nsomar,OAStackView,111572776,https://api.github.com/repos/nsomar/OAStackView/issues/comments/170221025,170221025,MDEyOklzc3VlQ29tbWVudDE3MDIyMTAyNQ==,1565368,2016-01-09T10:16:39Z,2016-01-09T10:17:21Z,CONTRIBUTOR,"Cool thanks :+1: 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/60/comments,https://github.com/nsomar/OAStackView/pull/60#issuecomment-170221025,https://api.github.com/repos/nsomar/OAStackView/issues/60
nsomar,OAStackView,110609323,https://api.github.com/repos/nsomar/OAStackView/issues/comments/148139663,148139663,MDEyOklzc3VlQ29tbWVudDE0ODEzOTY2Mw==,1565368,2015-10-14T18:06:51Z,2015-10-14T19:49:53Z,CONTRIBUTOR,"This was fixed in 0.2.0 !
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/59/comments,https://github.com/nsomar/OAStackView/issues/59#issuecomment-148139663,https://api.github.com/repos/nsomar/OAStackView/issues/59
nsomar,OAStackView,109809043,https://api.github.com/repos/nsomar/OAStackView/issues/comments/145559294,145559294,MDEyOklzc3VlQ29tbWVudDE0NTU1OTI5NA==,1813244,2015-10-05T15:00:59Z,2015-10-05T15:00:59Z,NONE,"This is actually an issue with nested OAStackviews in IB.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/58/comments,https://github.com/nsomar/OAStackView/issues/58#issuecomment-145559294,https://api.github.com/repos/nsomar/OAStackView/issues/58
nsomar,OAStackView,109807660,https://api.github.com/repos/nsomar/OAStackView/issues/comments/145545846,145545846,MDEyOklzc3VlQ29tbWVudDE0NTU0NTg0Ng==,829783,2015-10-05T14:27:22Z,2015-10-05T14:27:22Z,COLLABORATOR,"@oliverfoggin you are right, it is pretty easy addition. Can you make PR please? :smiley: 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/57/comments,https://github.com/nsomar/OAStackView/issues/57#issuecomment-145545846,https://api.github.com/repos/nsomar/OAStackView/issues/57
nsomar,OAStackView,109807660,https://api.github.com/repos/nsomar/OAStackView/issues/comments/157341801,157341801,MDEyOklzc3VlQ29tbWVudDE1NzM0MTgwMQ==,1662306,2015-11-17T11:22:19Z,2015-11-17T11:22:19Z,CONTRIBUTOR,"@oliverfoggin @garnett: I was just trying to use OAStack today and got stuck on this (http://stackoverflow.com/q/33752916/632735?sem=2); what are the acceptable values for the axis?  
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/57/comments,https://github.com/nsomar/OAStackView/issues/57#issuecomment-157341801,https://api.github.com/repos/nsomar/OAStackView/issues/57
nsomar,OAStackView,109807660,https://api.github.com/repos/nsomar/OAStackView/issues/comments/157345956,157345956,MDEyOklzc3VlQ29tbWVudDE1NzM0NTk1Ng==,829783,2015-11-17T11:43:28Z,2015-11-17T11:43:28Z,COLLABORATOR,"@neilbilly since IB does not really support ""enums"" as IBInspectable properties, you have to set 0 or 1 in `axisValue` in IB.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/57/comments,https://github.com/nsomar/OAStackView/issues/57#issuecomment-157345956,https://api.github.com/repos/nsomar/OAStackView/issues/57
nsomar,OAStackView,109807660,https://api.github.com/repos/nsomar/OAStackView/issues/comments/157363899,157363899,MDEyOklzc3VlQ29tbWVudDE1NzM2Mzg5OQ==,1662306,2015-11-17T12:56:11Z,2015-11-17T12:57:03Z,CONTRIBUTOR,"Ah right, got it! Thanks @garnett  :+1: :smile: 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/57/comments,https://github.com/nsomar/OAStackView/issues/57#issuecomment-157363899,https://api.github.com/repos/nsomar/OAStackView/issues/57
nsomar,OAStackView,109807660,https://api.github.com/repos/nsomar/OAStackView/issues/comments/159433864,159433864,MDEyOklzc3VlQ29tbWVudDE1OTQzMzg2NA==,1461052,2015-11-24T23:04:48Z,2015-11-24T23:04:48Z,OWNER,"Will be closing this as there isn't a clear solution for now.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/57/comments,https://github.com/nsomar/OAStackView/issues/57#issuecomment-159433864,https://api.github.com/repos/nsomar/OAStackView/issues/57
nsomar,OAStackView,109565964,https://api.github.com/repos/nsomar/OAStackView/issues/comments/152830596,152830596,MDEyOklzc3VlQ29tbWVudDE1MjgzMDU5Ng==,2394509,2015-11-01T14:36:43Z,2015-11-01T14:36:43Z,CONTRIBUTOR,"@bencallis Done!
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/56/comments,https://github.com/nsomar/OAStackView/pull/56#issuecomment-152830596,https://api.github.com/repos/nsomar/OAStackView/issues/56
nsomar,OAStackView,109565964,https://api.github.com/repos/nsomar/OAStackView/issues/comments/155947641,155947641,MDEyOklzc3VlQ29tbWVudDE1NTk0NzY0MQ==,464574,2015-11-11T23:52:07Z,2015-11-11T23:52:07Z,CONTRIBUTOR,"+1 would love to see this merged.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/56/comments,https://github.com/nsomar/OAStackView/pull/56#issuecomment-155947641,https://api.github.com/repos/nsomar/OAStackView/issues/56
nsomar,OAStackView,109565964,https://api.github.com/repos/nsomar/OAStackView/issues/comments/157991161,157991161,MDEyOklzc3VlQ29tbWVudDE1Nzk5MTE2MQ==,1461052,2015-11-19T08:46:23Z,2015-11-19T08:46:23Z,OWNER,"This looks fantastic, sorry for the late action, I will be merging it this weekend, thanks for the addition :)
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/56/comments,https://github.com/nsomar/OAStackView/pull/56#issuecomment-157991161,https://api.github.com/repos/nsomar/OAStackView/issues/56
nsomar,OAStackView,109565964,https://api.github.com/repos/nsomar/OAStackView/issues/comments/157996274,157996274,MDEyOklzc3VlQ29tbWVudDE1Nzk5NjI3NA==,2394509,2015-11-19T09:12:17Z,2015-11-19T09:12:17Z,CONTRIBUTOR,"Awesome, thanks! :cocktail: 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/56/comments,https://github.com/nsomar/OAStackView/pull/56#issuecomment-157996274,https://api.github.com/repos/nsomar/OAStackView/issues/56
nsomar,OAStackView,109565964,https://api.github.com/repos/nsomar/OAStackView/issues/comments/159411998,159411998,MDEyOklzc3VlQ29tbWVudDE1OTQxMTk5OA==,1461052,2015-11-24T21:35:28Z,2015-11-24T21:35:28Z,OWNER,"@agelber Great addition! I have rebased it and will merge it in a moment.
The only real change that I made was removing the old arrangedView in the class extension since its not used anymore.

Thanks for this PR!
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/56/comments,https://github.com/nsomar/OAStackView/pull/56#issuecomment-159411998,https://api.github.com/repos/nsomar/OAStackView/issues/56
nsomar,OAStackView,109565964,https://api.github.com/repos/nsomar/OAStackView/issues/comments/159422940,159422940,MDEyOklzc3VlQ29tbWVudDE1OTQyMjk0MA==,2394509,2015-11-24T22:13:13Z,2015-11-24T22:13:13Z,CONTRIBUTOR,"Cheers! 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/56/comments,https://github.com/nsomar/OAStackView/pull/56#issuecomment-159422940,https://api.github.com/repos/nsomar/OAStackView/issues/56
nsomar,OAStackView,109565964,https://api.github.com/repos/nsomar/OAStackView/issues/comments/166579224,166579224,MDEyOklzc3VlQ29tbWVudDE2NjU3OTIyNA==,800105,2015-12-22T10:47:10Z,2015-12-22T10:53:49Z,NONE,"Hi guys. Thanks for fixing the problem with the `arrangedSubview` property. Could you please bump the Podspec version so the fix can be accessible via the CocoaPods.
Cheers!
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/56/comments,https://github.com/nsomar/OAStackView/pull/56#issuecomment-166579224,https://api.github.com/repos/nsomar/OAStackView/issues/56
nsomar,OAStackView,108937955,https://api.github.com/repos/nsomar/OAStackView/issues/comments/144313575,144313575,MDEyOklzc3VlQ29tbWVudDE0NDMxMzU3NQ==,829783,2015-09-30T07:32:23Z,2015-09-30T07:32:23Z,COLLABORATOR,"@phatmann can you please explain why it is not possible?
if you add view with `addSubview:` it is not going to be arranged
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/55/comments,https://github.com/nsomar/OAStackView/issues/55#issuecomment-144313575,https://api.github.com/repos/nsomar/OAStackView/issues/55
nsomar,OAStackView,108937955,https://api.github.com/repos/nsomar/OAStackView/issues/comments/144465703,144465703,MDEyOklzc3VlQ29tbWVudDE0NDQ2NTcwMw==,36146,2015-09-30T16:20:41Z,2015-09-30T16:20:41Z,NONE,"Now I see: if I add a subview after initialization is done it will not be laid out. Thanks for the clarification.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/55/comments,https://github.com/nsomar/OAStackView/issues/55#issuecomment-144465703,https://api.github.com/repos/nsomar/OAStackView/issues/55
nsomar,OAStackView,107991414,https://api.github.com/repos/nsomar/OAStackView/issues/comments/142862754,142862754,MDEyOklzc3VlQ29tbWVudDE0Mjg2Mjc1NA==,829783,2015-09-24T09:03:58Z,2015-09-24T09:04:08Z,COLLABORATOR,"thanks @harlanhaskins! can you update your branch with current master please? It should fix travis.
I am not sure however if everybody is using Xcode7 already, maybe we need one more release for this, what do you think?
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/54/comments,https://github.com/nsomar/OAStackView/pull/54#issuecomment-142862754,https://api.github.com/repos/nsomar/OAStackView/issues/54
nsomar,OAStackView,107991414,https://api.github.com/repos/nsomar/OAStackView/issues/comments/159383556,159383556,MDEyOklzc3VlQ29tbWVudDE1OTM4MzU1Ng==,1461052,2015-11-24T19:39:49Z,2015-11-24T19:39:49Z,OWNER,"Hello @harlanhaskins, @garnett I am going to take this PR and merge it manually
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/54/comments,https://github.com/nsomar/OAStackView/pull/54#issuecomment-159383556,https://api.github.com/repos/nsomar/OAStackView/issues/54
nsomar,OAStackView,107991414,https://api.github.com/repos/nsomar/OAStackView/issues/comments/159386497,159386497,MDEyOklzc3VlQ29tbWVudDE1OTM4NjQ5Nw==,1461052,2015-11-24T19:51:19Z,2015-11-24T19:51:19Z,OWNER,"Merged!
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/54/comments,https://github.com/nsomar/OAStackView/pull/54#issuecomment-159386497,https://api.github.com/repos/nsomar/OAStackView/issues/54
nsomar,OAStackView,107991414,https://api.github.com/repos/nsomar/OAStackView/issues/comments/159387170,159387170,MDEyOklzc3VlQ29tbWVudDE1OTM4NzE3MA==,853032,2015-11-24T19:54:14Z,2015-11-24T19:54:14Z,CONTRIBUTOR,"Thanks so much!
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/54/comments,https://github.com/nsomar/OAStackView/pull/54#issuecomment-159387170,https://api.github.com/repos/nsomar/OAStackView/issues/54
nsomar,OAStackView,107826027,https://api.github.com/repos/nsomar/OAStackView/issues/comments/142456866,142456866,MDEyOklzc3VlQ29tbWVudDE0MjQ1Njg2Ng==,627312,2015-09-23T00:00:45Z,2015-09-23T00:00:45Z,NONE,"Oh wait...
pod 'OAStackView', :head
solves the issue :smile: 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/53/comments,https://github.com/nsomar/OAStackView/issues/53#issuecomment-142456866,https://api.github.com/repos/nsomar/OAStackView/issues/53
nsomar,OAStackView,107479489,https://api.github.com/repos/nsomar/OAStackView/issues/comments/141975957,141975957,MDEyOklzc3VlQ29tbWVudDE0MTk3NTk1Nw==,829783,2015-09-21T13:27:20Z,2015-09-21T13:27:20Z,COLLABORATOR,"builds for me without any issues Are you using cocoapods/cartage for integration?
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/52/comments,https://github.com/nsomar/OAStackView/issues/52#issuecomment-141975957,https://api.github.com/repos/nsomar/OAStackView/issues/52
nsomar,OAStackView,107479489,https://api.github.com/repos/nsomar/OAStackView/issues/comments/143145485,143145485,MDEyOklzc3VlQ29tbWVudDE0MzE0NTQ4NQ==,8086633,2015-09-25T07:01:32Z,2015-09-25T07:01:32Z,NONE,"I download your demo.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/52/comments,https://github.com/nsomar/OAStackView/issues/52#issuecomment-143145485,https://api.github.com/repos/nsomar/OAStackView/issues/52
nsomar,OAStackView,107479489,https://api.github.com/repos/nsomar/OAStackView/issues/comments/143160117,143160117,MDEyOklzc3VlQ29tbWVudDE0MzE2MDExNw==,829783,2015-09-25T08:29:57Z,2015-09-25T08:29:57Z,COLLABORATOR,"@wy19901227 it works for me, did you run `pod install` inside example project folder?
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/52/comments,https://github.com/nsomar/OAStackView/issues/52#issuecomment-143160117,https://api.github.com/repos/nsomar/OAStackView/issues/52
nsomar,OAStackView,107479489,https://api.github.com/repos/nsomar/OAStackView/issues/comments/159432270,159432270,MDEyOklzc3VlQ29tbWVudDE1OTQzMjI3MA==,1461052,2015-11-24T22:56:31Z,2015-11-24T22:56:31Z,OWNER,"@wy19901227 I am closing this issue since there have been no feedback from your side.
Please feel free to reopen it if the issue persists.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/52/comments,https://github.com/nsomar/OAStackView/issues/52#issuecomment-159432270,https://api.github.com/repos/nsomar/OAStackView/issues/52
nsomar,OAStackView,107479332,https://api.github.com/repos/nsomar/OAStackView/issues/comments/162400626,162400626,MDEyOklzc3VlQ29tbWVudDE2MjQwMDYyNg==,10137,2015-12-07T03:23:58Z,2015-12-07T03:23:58Z,NONE,"Any update on this?
Still noticing it remains the same.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/51/comments,https://github.com/nsomar/OAStackView/issues/51#issuecomment-162400626,https://api.github.com/repos/nsomar/OAStackView/issues/51
nsomar,OAStackView,107479332,https://api.github.com/repos/nsomar/OAStackView/issues/comments/169729599,169729599,MDEyOklzc3VlQ29tbWVudDE2OTcyOTU5OQ==,829783,2016-01-07T17:01:48Z,2016-01-07T17:01:48Z,COLLABORATOR,"@oarrabi need your input here, issue is really straightforward but will require Major version bump since it will break all the apps
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/51/comments,https://github.com/nsomar/OAStackView/issues/51#issuecomment-169729599,https://api.github.com/repos/nsomar/OAStackView/issues/51
nsomar,OAStackView,107479332,https://api.github.com/repos/nsomar/OAStackView/issues/comments/169729944,169729944,MDEyOklzc3VlQ29tbWVudDE2OTcyOTk0NA==,853032,2016-01-07T17:02:32Z,2016-01-07T17:02:32Z,CONTRIBUTOR,"I really don't like changing the default axis. But we have to.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/51/comments,https://github.com/nsomar/OAStackView/issues/51#issuecomment-169729944,https://api.github.com/repos/nsomar/OAStackView/issues/51
nsomar,OAStackView,107479332,https://api.github.com/repos/nsomar/OAStackView/issues/comments/169730431,169730431,MDEyOklzc3VlQ29tbWVudDE2OTczMDQzMQ==,829783,2016-01-07T17:03:38Z,2016-01-07T17:03:38Z,COLLABORATOR,"@harlanhaskins thanks for your input, looks like good change for 1.0 release! :trollface: 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/51/comments,https://github.com/nsomar/OAStackView/issues/51#issuecomment-169730431,https://api.github.com/repos/nsomar/OAStackView/issues/51
nsomar,OAStackView,107479332,https://api.github.com/repos/nsomar/OAStackView/issues/comments/170221158,170221158,MDEyOklzc3VlQ29tbWVudDE3MDIyMTE1OA==,829783,2016-01-09T10:19:32Z,2016-01-09T10:19:32Z,COLLABORATOR,"Fixed in `1.0.0` version
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/51/comments,https://github.com/nsomar/OAStackView/issues/51#issuecomment-170221158,https://api.github.com/repos/nsomar/OAStackView/issues/51
nsomar,OAStackView,107479332,https://api.github.com/repos/nsomar/OAStackView/issues/comments/186257828,186257828,MDEyOklzc3VlQ29tbWVudDE4NjI1NzgyOA==,5191060,2016-02-19T15:21:20Z,2016-02-19T15:21:20Z,NONE,"As an FYI, the header for OAStackView still says the default is Vertical.

Through me for a loop when I upgraded my pods and none of my views worked anymore.  May want to consider updating that as well.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/51/comments,https://github.com/nsomar/OAStackView/issues/51#issuecomment-186257828,https://api.github.com/repos/nsomar/OAStackView/issues/51
nsomar,OAStackView,107479332,https://api.github.com/repos/nsomar/OAStackView/issues/comments/186757561,186757561,MDEyOklzc3VlQ29tbWVudDE4Njc1NzU2MQ==,829783,2016-02-21T06:24:11Z,2016-02-21T06:24:11Z,COLLABORATOR,"@coridn nice catch, fixed here https://github.com/oarrabi/OAStackView/commit/c606348cbf7f4dc40f87e841462504cc0c4e6cb1
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/51/comments,https://github.com/nsomar/OAStackView/issues/51#issuecomment-186757561,https://api.github.com/repos/nsomar/OAStackView/issues/51
nsomar,OAStackView,106936491,https://api.github.com/repos/nsomar/OAStackView/issues/comments/141016053,141016053,MDEyOklzc3VlQ29tbWVudDE0MTAxNjA1Mw==,829783,2015-09-17T08:59:40Z,2015-09-17T08:59:40Z,COLLABORATOR,"There is `@property(nonatomic) IBInspectable CGFloat spacing;` property for this
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/50/comments,https://github.com/nsomar/OAStackView/issues/50#issuecomment-141016053,https://api.github.com/repos/nsomar/OAStackView/issues/50
nsomar,OAStackView,106936491,https://api.github.com/repos/nsomar/OAStackView/issues/comments/141023858,141023858,MDEyOklzc3VlQ29tbWVudDE0MTAyMzg1OA==,8086633,2015-09-17T09:35:59Z,2015-09-17T09:35:59Z,NONE,"I want to set the first view to the secondView's spacing .not the the stackView's spacing . what I mean can I set the subView's spacing not equal?
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/50/comments,https://github.com/nsomar/OAStackView/issues/50#issuecomment-141023858,https://api.github.com/repos/nsomar/OAStackView/issues/50
nsomar,OAStackView,106936491,https://api.github.com/repos/nsomar/OAStackView/issues/comments/141412416,141412416,MDEyOklzc3VlQ29tbWVudDE0MTQxMjQxNg==,829783,2015-09-18T10:21:32Z,2015-09-18T10:22:10Z,COLLABORATOR,"@wy19901227 it is not possible with `UIStackView` (and `OAStackView`) view itself.
However, you can set spacing to 0 and insert container views with specific height between your views:

| your view |
| --- |
| container with height 10 |
| your second view |
| container with height 20 |

etc.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/50/comments,https://github.com/nsomar/OAStackView/issues/50#issuecomment-141412416,https://api.github.com/repos/nsomar/OAStackView/issues/50
nsomar,OAStackView,106936491,https://api.github.com/repos/nsomar/OAStackView/issues/comments/141424016,141424016,MDEyOklzc3VlQ29tbWVudDE0MTQyNDAxNg==,8086633,2015-09-18T11:34:07Z,2015-09-18T11:34:07Z,NONE,"UIStackview can not .but I think you can make it possible.Do you think it need?

å‘è‡ªæˆ‘çš„ iPhone

> åœ¨ 2015å¹´9æœˆ18æ—¥ï¼Œ18:21ï¼ŒDenis Lebedev notifications@github.com å†™é“ï¼š
> 
> @wy19901227 it is not possible with UIStackView (and OAStackView) view itself.
> However, you can set spacing to 0 and insert container views with specific height between your views:
> 
> |your view|
> 
> container with height 10
> your second view
> container with height 20
> etc.
> 
> â€”
> Reply to this email directly or view it on GitHub.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/50/comments,https://github.com/nsomar/OAStackView/issues/50#issuecomment-141424016,https://api.github.com/repos/nsomar/OAStackView/issues/50
nsomar,OAStackView,106936491,https://api.github.com/repos/nsomar/OAStackView/issues/comments/141426232,141426232,MDEyOklzc3VlQ29tbWVudDE0MTQyNjIzMg==,829783,2015-09-18T11:47:12Z,2015-09-18T11:47:12Z,COLLABORATOR,"Unfortunately no, because the idea behind `OAStackView` is to exactly replicate `UIStackView`
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/50/comments,https://github.com/nsomar/OAStackView/issues/50#issuecomment-141426232,https://api.github.com/repos/nsomar/OAStackView/issues/50
nsomar,OAStackView,106936491,https://api.github.com/repos/nsomar/OAStackView/issues/comments/159433399,159433399,MDEyOklzc3VlQ29tbWVudDE1OTQzMzM5OQ==,1461052,2015-11-24T23:02:15Z,2015-11-24T23:02:15Z,OWNER,"I will be closing this issue as its not compatible with UIStackView. Sorry for that but I don't think we should diverge from UIStackView behaviour.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/50/comments,https://github.com/nsomar/OAStackView/issues/50#issuecomment-159433399,https://api.github.com/repos/nsomar/OAStackView/issues/50
nsomar,OAStackView,106936491,https://api.github.com/repos/nsomar/OAStackView/issues/comments/333025208,333025208,MDEyOklzc3VlQ29tbWVudDMzMzAyNTIwOA==,7535201,2017-09-29T03:57:57Z,2017-09-29T03:57:57Z,NONE,In iOS 11 it is possible to customize the spacing after any of the arranged subviews of a stack view. I think OAStackView can make it possible,NA,https://api.github.com/repos/nsomar/OAStackView/issues/50/comments,https://github.com/nsomar/OAStackView/issues/50#issuecomment-333025208,https://api.github.com/repos/nsomar/OAStackView/issues/50
nsomar,OAStackView,105840505,https://api.github.com/repos/nsomar/OAStackView/issues/comments/140373677,140373677,MDEyOklzc3VlQ29tbWVudDE0MDM3MzY3Nw==,829783,2015-09-15T12:34:45Z,2015-09-15T12:34:45Z,COLLABORATOR,"@oarrabi can you review this PR please? Explanation seems reasonable for me and I've seen something similar in my app
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/49/comments,https://github.com/nsomar/OAStackView/pull/49#issuecomment-140373677,https://api.github.com/repos/nsomar/OAStackView/issues/49
nsomar,OAStackView,105840505,https://api.github.com/repos/nsomar/OAStackView/issues/comments/141012985,141012985,MDEyOklzc3VlQ29tbWVudDE0MTAxMjk4NQ==,829783,2015-09-17T08:54:02Z,2015-09-17T08:54:02Z,COLLABORATOR,"Tests are passing but Travis fails due to unknown reason.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/49/comments,https://github.com/nsomar/OAStackView/pull/49#issuecomment-141012985,https://api.github.com/repos/nsomar/OAStackView/issues/49
nsomar,OAStackView,105310700,https://api.github.com/repos/nsomar/OAStackView/issues/comments/141412060,141412060,MDEyOklzc3VlQ29tbWVudDE0MTQxMjA2MA==,829783,2015-09-18T10:19:02Z,2015-09-18T10:19:02Z,COLLABORATOR,"@skagedal I've tried your snippet and it works for me as expected, can you try it again using HEAD version?
`pod 'OAStackView', :head`
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/48/comments,https://github.com/nsomar/OAStackView/issues/48#issuecomment-141412060,https://api.github.com/repos/nsomar/OAStackView/issues/48
nsomar,OAStackView,105310700,https://api.github.com/repos/nsomar/OAStackView/issues/comments/159433159,159433159,MDEyOklzc3VlQ29tbWVudDE1OTQzMzE1OQ==,1461052,2015-11-24T23:01:07Z,2015-11-24T23:01:07Z,OWNER,"@skagedal  I am closing this issue since there have been no feedback from your side.
Please feel free to reopen it if the issue persists.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/48/comments,https://github.com/nsomar/OAStackView/issues/48#issuecomment-159433159,https://api.github.com/repos/nsomar/OAStackView/issues/48
nsomar,OAStackView,105283504,https://api.github.com/repos/nsomar/OAStackView/issues/comments/139740221,139740221,MDEyOklzc3VlQ29tbWVudDEzOTc0MDIyMQ==,686478,2015-09-12T08:30:05Z,2015-09-12T08:30:05Z,NONE,"I found the answers. It's not the OAStackView's fault. Just a pure autolayout issues.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/47/comments,https://github.com/nsomar/OAStackView/issues/47#issuecomment-139740221,https://api.github.com/repos/nsomar/OAStackView/issues/47
nsomar,OAStackView,105283504,https://api.github.com/repos/nsomar/OAStackView/issues/comments/158422658,158422658,MDEyOklzc3VlQ29tbWVudDE1ODQyMjY1OA==,1223339,2015-11-20T14:52:59Z,2015-11-20T14:52:59Z,NONE,"@j796160836 can you help to explain the reason? I think I got the same issue.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/47/comments,https://github.com/nsomar/OAStackView/issues/47#issuecomment-158422658,https://api.github.com/repos/nsomar/OAStackView/issues/47
nsomar,OAStackView,105283504,https://api.github.com/repos/nsomar/OAStackView/issues/comments/316970728,316970728,MDEyOklzc3VlQ29tbWVudDMxNjk3MDcyOA==,16228277,2017-07-21T10:49:55Z,2017-07-21T10:49:55Z,NONE,"I think, this is Autolayout issues, it is not a big problem, we can be solved by add this line of code inside the viewdidload.

  UserDefaults.standard.setValue(false, forKey:""_UIConstraintBasedLayoutLogUnsatisfiable"")
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/47/comments,https://github.com/nsomar/OAStackView/issues/47#issuecomment-316970728,https://api.github.com/repos/nsomar/OAStackView/issues/47
nsomar,OAStackView,104565110,https://api.github.com/repos/nsomar/OAStackView/issues/comments/137502965,137502965,MDEyOklzc3VlQ29tbWVudDEzNzUwMjk2NQ==,1911951,2015-09-03T16:25:43Z,2015-09-03T16:54:26Z,NONE,"As far as I know this can not be done with IBInspectable.

> You can attach the IBInspectable attribute to any property in a class declaration, class extension, or category for any type thatâ€™s supported by the Interface Builder defined runtime attributes: boolean, integer or floating point number, string, localized string, rectangle, point, size, color, range, and nil.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/46/comments,https://github.com/nsomar/OAStackView/issues/46#issuecomment-137502965,https://api.github.com/repos/nsomar/OAStackView/issues/46
nsomar,OAStackView,104565110,https://api.github.com/repos/nsomar/OAStackView/issues/comments/141018430,141018430,MDEyOklzc3VlQ29tbWVudDE0MTAxODQzMA==,829783,2015-09-17T09:09:51Z,2015-09-17T09:09:51Z,COLLABORATOR,"@bencallis is right, currently it is not possible.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/46/comments,https://github.com/nsomar/OAStackView/issues/46#issuecomment-141018430,https://api.github.com/repos/nsomar/OAStackView/issues/46
nsomar,OAStackView,104348555,https://api.github.com/repos/nsomar/OAStackView/issues/comments/137095070,137095070,MDEyOklzc3VlQ29tbWVudDEzNzA5NTA3MA==,310778,2015-09-02T14:12:04Z,2015-09-02T14:12:04Z,NONE,"if i implement traitCollectionDidChange: in OAStackView and then call layoutArrangedViews it seems to fix the issue
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/45/comments,https://github.com/nsomar/OAStackView/issues/45#issuecomment-137095070,https://api.github.com/repos/nsomar/OAStackView/issues/45
nsomar,OAStackView,104348555,https://api.github.com/repos/nsomar/OAStackView/issues/comments/143055529,143055529,MDEyOklzc3VlQ29tbWVudDE0MzA1NTUyOQ==,829783,2015-09-24T21:28:00Z,2015-09-24T21:28:00Z,COLLABORATOR,"@stallent can you provide small example project for this case, please?
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/45/comments,https://github.com/nsomar/OAStackView/issues/45#issuecomment-143055529,https://api.github.com/repos/nsomar/OAStackView/issues/45
nsomar,OAStackView,104348555,https://api.github.com/repos/nsomar/OAStackView/issues/comments/143248174,143248174,MDEyOklzc3VlQ29tbWVudDE0MzI0ODE3NA==,310778,2015-09-25T15:09:57Z,2015-09-25T15:09:57Z,NONE,"No need. It reproduces in the example project. Simply select the StackView in the main storyboard and add a size class to it (Regular,Regular for example). You will notice the initial rendering is incorrect. Once you click buttons it fixes itself but the issue is layoutArrangedViews never gets called once you add a size class to the view in the nib.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/45/comments,https://github.com/nsomar/OAStackView/issues/45#issuecomment-143248174,https://api.github.com/repos/nsomar/OAStackView/issues/45
nsomar,OAStackView,104348555,https://api.github.com/repos/nsomar/OAStackView/issues/comments/143780006,143780006,MDEyOklzc3VlQ29tbWVudDE0Mzc4MDAwNg==,829783,2015-09-28T15:32:58Z,2015-09-28T15:32:58Z,COLLABORATOR,"@stallent I've added size class and it looks fine for me. Can you please if the issue still exists in 0.2.0 tag?
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/45/comments,https://github.com/nsomar/OAStackView/issues/45#issuecomment-143780006,https://api.github.com/repos/nsomar/OAStackView/issues/45
nsomar,OAStackView,104348555,https://api.github.com/repos/nsomar/OAStackView/issues/comments/143783946,143783946,MDEyOklzc3VlQ29tbWVudDE0Mzc4Mzk0Ng==,310778,2015-09-28T15:43:16Z,2015-09-28T15:43:16Z,NONE,"Just tried. Still there.  Steps to reproduce... Open example.  Select the stackView instance in the storyboard and add this:

![screen shot 2015-09-28 at 10 37 12 am](https://cloud.githubusercontent.com/assets/310778/10140443/1b4ada00-65cd-11e5-80d6-5f2934bfcbb6.png)

Run in simulator and initial rendering becomes this: (which isn't correct)

<img width=""503"" alt=""screen shot 2015-09-28 at 10 36 50 am"" src=""https://cloud.githubusercontent.com/assets/310778/10140459/2ef5808c-65cd-11e5-8cfd-707ff7c15cd8.png"">

As I mentioned initially, the timing of when the view gets sized is different when you add a sizeClass and layoutArrangedViews is getting called too early. 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/45/comments,https://github.com/nsomar/OAStackView/issues/45#issuecomment-143783946,https://api.github.com/repos/nsomar/OAStackView/issues/45
nsomar,OAStackView,104348555,https://api.github.com/repos/nsomar/OAStackView/issues/comments/143792266,143792266,MDEyOklzc3VlQ29tbWVudDE0Mzc5MjI2Ng==,829783,2015-09-28T16:12:22Z,2015-09-28T16:12:22Z,COLLABORATOR,"@stallent thanks a lot for this, now I'm finally managed to reproduce it
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/45/comments,https://github.com/nsomar/OAStackView/issues/45#issuecomment-143792266,https://api.github.com/repos/nsomar/OAStackView/issues/45
nsomar,OAStackView,104348555,https://api.github.com/repos/nsomar/OAStackView/issues/comments/143795427,143795427,MDEyOklzc3VlQ29tbWVudDE0Mzc5NTQyNw==,829783,2015-09-28T16:21:02Z,2015-09-28T16:21:02Z,COLLABORATOR,"@stallent fixed now :+1: 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/45/comments,https://github.com/nsomar/OAStackView/issues/45#issuecomment-143795427,https://api.github.com/repos/nsomar/OAStackView/issues/45
nsomar,OAStackView,104348555,https://api.github.com/repos/nsomar/OAStackView/issues/comments/143803976,143803976,MDEyOklzc3VlQ29tbWVudDE0MzgwMzk3Ng==,310778,2015-09-28T16:57:35Z,2015-09-28T16:57:35Z,NONE,"Thank you!  Looks great!
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/45/comments,https://github.com/nsomar/OAStackView/issues/45#issuecomment-143803976,https://api.github.com/repos/nsomar/OAStackView/issues/45
nsomar,OAStackView,103928771,https://api.github.com/repos/nsomar/OAStackView/issues/comments/136846925,136846925,MDEyOklzc3VlQ29tbWVudDEzNjg0NjkyNQ==,12350117,2015-09-01T20:14:09Z,2015-09-01T20:14:09Z,NONE,"Yes please  provide some example
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/44/comments,https://github.com/nsomar/OAStackView/issues/44#issuecomment-136846925,https://api.github.com/repos/nsomar/OAStackView/issues/44
nsomar,OAStackView,103928771,https://api.github.com/repos/nsomar/OAStackView/issues/comments/137503295,137503295,MDEyOklzc3VlQ29tbWVudDEzNzUwMzI5NQ==,1911951,2015-09-03T16:27:14Z,2015-09-03T16:27:14Z,NONE,"Why not just embedded the StackView in a scrollview (providing you get your constraints correct) it will just work.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/44/comments,https://github.com/nsomar/OAStackView/issues/44#issuecomment-137503295,https://api.github.com/repos/nsomar/OAStackView/issues/44
nsomar,OAStackView,103928771,https://api.github.com/repos/nsomar/OAStackView/issues/comments/141412796,141412796,MDEyOklzc3VlQ29tbWVudDE0MTQxMjc5Ng==,829783,2015-09-18T10:24:01Z,2015-09-18T10:24:01Z,COLLABORATOR,"@Lipe991, @bencallis is right: stackview can be placed in scroll view as any other view with valid intrinsic content size (height in particular). Please refer to apple tech note: https://developer.apple.com/library/ios/technotes/tn2154/_index.html
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/44/comments,https://github.com/nsomar/OAStackView/issues/44#issuecomment-141412796,https://api.github.com/repos/nsomar/OAStackView/issues/44
nsomar,OAStackView,103899547,https://api.github.com/repos/nsomar/OAStackView/issues/comments/159435104,159435104,MDEyOklzc3VlQ29tbWVudDE1OTQzNTEwNA==,1461052,2015-11-24T23:09:57Z,2015-11-24T23:09:57Z,OWNER,"I think https://github.com/oarrabi/OAStackView/pull/56, which was just merged, solves this issue, Will be closing it now.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/43/comments,https://github.com/nsomar/OAStackView/issues/43#issuecomment-159435104,https://api.github.com/repos/nsomar/OAStackView/issues/43
nsomar,OAStackView,102809270,https://api.github.com/repos/nsomar/OAStackView/issues/comments/134373880,134373880,MDEyOklzc3VlQ29tbWVudDEzNDM3Mzg4MA==,11229,2015-08-24T20:42:34Z,2015-08-24T20:42:34Z,NONE,"+1. For now, I've just got my app's Podfile set up like so:
`pod 'OAStackView', :git => 'https://github.com/oarrabi/OAStackView.git'`
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/42/comments,https://github.com/nsomar/OAStackView/issues/42#issuecomment-134373880,https://api.github.com/repos/nsomar/OAStackView/issues/42
nsomar,OAStackView,102809270,https://api.github.com/repos/nsomar/OAStackView/issues/comments/134389436,134389436,MDEyOklzc3VlQ29tbWVudDEzNDM4OTQzNg==,1461052,2015-08-24T21:45:23Z,2015-08-24T21:45:23Z,OWNER,"I am working on this. I will try to update the version in the upcoming days.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/42/comments,https://github.com/nsomar/OAStackView/issues/42#issuecomment-134389436,https://api.github.com/repos/nsomar/OAStackView/issues/42
nsomar,OAStackView,102809270,https://api.github.com/repos/nsomar/OAStackView/issues/comments/134534133,134534133,MDEyOklzc3VlQ29tbWVudDEzNDUzNDEzMw==,1911951,2015-08-25T09:05:35Z,2015-08-25T09:05:35Z,NONE,"Thanks :+1: 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/42/comments,https://github.com/nsomar/OAStackView/issues/42#issuecomment-134534133,https://api.github.com/repos/nsomar/OAStackView/issues/42
nsomar,OAStackView,102809270,https://api.github.com/repos/nsomar/OAStackView/issues/comments/137002877,137002877,MDEyOklzc3VlQ29tbWVudDEzNzAwMjg3Nw==,1911951,2015-09-02T09:44:35Z,2015-09-02T09:44:35Z,NONE,"Any update?
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/42/comments,https://github.com/nsomar/OAStackView/issues/42#issuecomment-137002877,https://api.github.com/repos/nsomar/OAStackView/issues/42
nsomar,OAStackView,102809270,https://api.github.com/repos/nsomar/OAStackView/issues/comments/142556513,142556513,MDEyOklzc3VlQ29tbWVudDE0MjU1NjUxMw==,829783,2015-09-23T10:17:40Z,2015-09-23T10:17:40Z,COLLABORATOR,"as per my comment here: https://github.com/oarrabi/OAStackView/issues/30
new tag is 0.2.0, podspec currently waits to be pushed by maintainer
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/42/comments,https://github.com/nsomar/OAStackView/issues/42#issuecomment-142556513,https://api.github.com/repos/nsomar/OAStackView/issues/42
nsomar,OAStackView,102809270,https://api.github.com/repos/nsomar/OAStackView/issues/comments/144111957,144111957,MDEyOklzc3VlQ29tbWVudDE0NDExMTk1Nw==,36146,2015-09-29T16:28:51Z,2015-09-29T16:28:51Z,NONE,"@mthole, why not just use the `:head` option in your Podfile?

```
pod 'OAStackView', :head
```
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/42/comments,https://github.com/nsomar/OAStackView/issues/42#issuecomment-144111957,https://api.github.com/repos/nsomar/OAStackView/issues/42
nsomar,OAStackView,102809270,https://api.github.com/repos/nsomar/OAStackView/issues/comments/144206873,144206873,MDEyOklzc3VlQ29tbWVudDE0NDIwNjg3Mw==,11229,2015-09-29T22:16:49Z,2015-09-29T22:16:49Z,NONE,"@phatmann I think that'd do the same thing. :+1:
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/42/comments,https://github.com/nsomar/OAStackView/issues/42#issuecomment-144206873,https://api.github.com/repos/nsomar/OAStackView/issues/42
nsomar,OAStackView,102809270,https://api.github.com/repos/nsomar/OAStackView/issues/comments/146579773,146579773,MDEyOklzc3VlQ29tbWVudDE0NjU3OTc3Mw==,873387,2015-10-08T15:30:08Z,2015-10-08T15:30:08Z,NONE,"@phatmann :head not working for me -- installs ""Head based on 0.1.0"" version
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/42/comments,https://github.com/nsomar/OAStackView/issues/42#issuecomment-146579773,https://api.github.com/repos/nsomar/OAStackView/issues/42
nsomar,OAStackView,102809270,https://api.github.com/repos/nsomar/OAStackView/issues/comments/146584733,146584733,MDEyOklzc3VlQ29tbWVudDE0NjU4NDczMw==,36146,2015-10-08T15:47:12Z,2015-10-08T15:47:12Z,NONE,"That is a confusing message. It shows whatever version is in the current Podspec file, which is still ""0.1.0"". But the option is working for you.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/42/comments,https://github.com/nsomar/OAStackView/issues/42#issuecomment-146584733,https://api.github.com/repos/nsomar/OAStackView/issues/42
nsomar,OAStackView,102809270,https://api.github.com/repos/nsomar/OAStackView/issues/comments/147782699,147782699,MDEyOklzc3VlQ29tbWVudDE0Nzc4MjY5OQ==,3384451,2015-10-13T17:19:50Z,2015-10-13T17:19:50Z,NONE,"Any update on when 0.2.0 will be pushed to cocoapods?
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/42/comments,https://github.com/nsomar/OAStackView/issues/42#issuecomment-147782699,https://api.github.com/repos/nsomar/OAStackView/issues/42
nsomar,OAStackView,102809270,https://api.github.com/repos/nsomar/OAStackView/issues/comments/147785007,147785007,MDEyOklzc3VlQ29tbWVudDE0Nzc4NTAwNw==,829783,2015-10-13T17:28:40Z,2015-10-13T17:28:40Z,COLLABORATOR,"@chipsnyder just pushed, please check if 0.2.0 works for you
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/42/comments,https://github.com/nsomar/OAStackView/issues/42#issuecomment-147785007,https://api.github.com/repos/nsomar/OAStackView/issues/42
nsomar,OAStackView,102293069,https://api.github.com/repos/nsomar/OAStackView/issues/comments/133768113,133768113,MDEyOklzc3VlQ29tbWVudDEzMzc2ODExMw==,11229,2015-08-23T00:51:21Z,2015-08-23T00:51:21Z,NONE,"FWIW, https://github.com/oarrabi/OAStackView/pull/17 will hopefully fix this issue.

As a workaround, you may be able to inspect the stack view's `subviews` property to decide which index to insert at. No guarantees on that, though -- I've not tried personally.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/41/comments,https://github.com/nsomar/OAStackView/issues/41#issuecomment-133768113,https://api.github.com/repos/nsomar/OAStackView/issues/41
nsomar,OAStackView,102183493,https://api.github.com/repos/nsomar/OAStackView/issues/comments/133768127,133768127,MDEyOklzc3VlQ29tbWVudDEzMzc2ODEyNw==,11229,2015-08-23T00:51:58Z,2015-08-23T00:51:58Z,NONE,"@calebd Can you confirm that UIStackView on iOS 9 has the behavior your describe/desire?
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/40/comments,https://github.com/nsomar/OAStackView/issues/40#issuecomment-133768127,https://api.github.com/repos/nsomar/OAStackView/issues/40
nsomar,OAStackView,102183493,https://api.github.com/repos/nsomar/OAStackView/issues/comments/134359289,134359289,MDEyOklzc3VlQ29tbWVudDEzNDM1OTI4OQ==,1761121,2015-08-24T20:00:23Z,2015-08-24T20:00:23Z,CONTRIBUTOR,"#42
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/40/comments,https://github.com/nsomar/OAStackView/issues/40#issuecomment-134359289,https://api.github.com/repos/nsomar/OAStackView/issues/40
nsomar,OAStackView,102183493,https://api.github.com/repos/nsomar/OAStackView/issues/comments/134360345,134360345,MDEyOklzc3VlQ29tbWVudDEzNDM2MDM0NQ==,1761121,2015-08-24T20:04:57Z,2015-08-24T20:04:57Z,CONTRIBUTOR,"`UIStackView` does not let center aligned views go wider than the stack view itself. Why would it allow that?

A quick test: Put a multi line label with a lot of text in a width-constrained, center-aligned stack view. Notice that the stack view height grows while restricting the label to its width.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/40/comments,https://github.com/nsomar/OAStackView/issues/40#issuecomment-134360345,https://api.github.com/repos/nsomar/OAStackView/issues/40
nsomar,OAStackView,102183493,https://api.github.com/repos/nsomar/OAStackView/issues/comments/142862132,142862132,MDEyOklzc3VlQ29tbWVudDE0Mjg2MjEzMg==,829783,2015-09-24T09:00:39Z,2015-09-24T09:00:39Z,COLLABORATOR,"@calebd can you please check 0.2.0 tag, it works the same way as OAStackView for me (tried to reproduce your setup:

```
    OAStackView *stackView = [OAStackView new];
    stackView.translatesAutoresizingMaskIntoConstraints = NO;
    [self.view addSubview:stackView];

    UILayoutGuide *margin = self.view.layoutMarginsGuide;
    [stackView.topAnchor constraintEqualToAnchor:margin.topAnchor constant:16.0].active = YES;
    [stackView.leadingAnchor constraintEqualToAnchor:margin.leadingAnchor constant:50].active = YES;
    //[stackView.bottomAnchor constraintEqualToAnchor:margin.bottomAnchor constant:-16.0].active = YES;
    [stackView.trailingAnchor constraintEqualToAnchor:margin.trailingAnchor constant:-50].active = YES;

    stackView.axis = UILayoutConstraintAxisVertical;
    stackView.distribution = UIStackViewDistributionFill;
    stackView.alignment = UIStackViewAlignmentCenter;
    stackView.spacing = 0.0;

    UIView *redView = [UIView new];
    redView.translatesAutoresizingMaskIntoConstraints = NO;
    redView.backgroundColor = [UIColor redColor];
    [redView.heightAnchor constraintEqualToConstant:100].active = YES;
    [redView.widthAnchor constraintEqualToConstant:50].active = YES;

    UILabel *text = [[UILabel alloc] init];
    text.text = @""ofofo ofofo ofofoof ofof ofofo fofo ofofo ofofofo ofofof ofofofo ofofofo ofofofo ofo foofo fo"";
    text.numberOfLines = 0;

    [stackView addArrangedSubview:text];
    [stackView addArrangedSubview:redView];
```
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/40/comments,https://github.com/nsomar/OAStackView/issues/40#issuecomment-142862132,https://api.github.com/repos/nsomar/OAStackView/issues/40
nsomar,OAStackView,102183493,https://api.github.com/repos/nsomar/OAStackView/issues/comments/159433099,159433099,MDEyOklzc3VlQ29tbWVudDE1OTQzMzA5OQ==,1461052,2015-11-24T23:00:47Z,2015-11-24T23:00:47Z,OWNER,"@calebd  I am closing this issue since there have been no feedback from your side.
Please feel free to reopen it if the issue persists.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/40/comments,https://github.com/nsomar/OAStackView/issues/40#issuecomment-159433099,https://api.github.com/repos/nsomar/OAStackView/issues/40
nsomar,OAStackView,102064969,https://api.github.com/repos/nsomar/OAStackView/issues/comments/132979186,132979186,MDEyOklzc3VlQ29tbWVudDEzMjk3OTE4Ng==,2662182,2015-08-20T11:21:58Z,2015-08-20T11:21:58Z,NONE,"I solved this issue. It was not a bug. When adding new UIView to OAStackView or UIStackView you have to specify view's height or width constraint with Auto Layout depending on axis. 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/39/comments,https://github.com/nsomar/OAStackView/issues/39#issuecomment-132979186,https://api.github.com/repos/nsomar/OAStackView/issues/39
nsomar,OAStackView,100609030,https://api.github.com/repos/nsomar/OAStackView/issues/comments/130409100,130409100,MDEyOklzc3VlQ29tbWVudDEzMDQwOTEwMA==,853032,2015-08-12T18:46:56Z,2015-08-12T18:47:14Z,CONTRIBUTOR,"That's a very well-known quirk with UITableViewCell automatic sizing in iOS 8, and not related to OAStackView. http://stackoverflow.com/questions/27996438/jerky-scrolling-after-updating-uitableviewcell-in-place-with-uitableviewautomati
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/38/comments,https://github.com/nsomar/OAStackView/issues/38#issuecomment-130409100,https://api.github.com/repos/nsomar/OAStackView/issues/38
nsomar,OAStackView,100609030,https://api.github.com/repos/nsomar/OAStackView/issues/comments/131869540,131869540,MDEyOklzc3VlQ29tbWVudDEzMTg2OTU0MA==,11229,2015-08-17T15:46:50Z,2015-08-17T15:46:50Z,NONE,"Suggest closing this issue.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/38/comments,https://github.com/nsomar/OAStackView/issues/38#issuecomment-131869540,https://api.github.com/repos/nsomar/OAStackView/issues/38
nsomar,OAStackView,100609030,https://api.github.com/repos/nsomar/OAStackView/issues/comments/131871324,131871324,MDEyOklzc3VlQ29tbWVudDEzMTg3MTMyNA==,1461052,2015-08-17T15:53:25Z,2015-08-17T15:53:25Z,OWNER,"@mthole Yep, I will close it now!
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/38/comments,https://github.com/nsomar/OAStackView/issues/38#issuecomment-131871324,https://api.github.com/repos/nsomar/OAStackView/issues/38
nsomar,OAStackView,100456295,https://api.github.com/repos/nsomar/OAStackView/issues/comments/130154601,130154601,MDEyOklzc3VlQ29tbWVudDEzMDE1NDYwMQ==,853032,2015-08-12T04:11:47Z,2015-08-30T22:52:18Z,CONTRIBUTOR,"You can use layoutMarginsRelativeAlignment. Just set stackView.layoutMarginsRelativeAlignment to true and set `stackView.layoutMargins` to `UIEdgeInsets(top: <value>, left: 0, bottom: <value>, right: 0)`
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/37/comments,https://github.com/nsomar/OAStackView/issues/37#issuecomment-130154601,https://api.github.com/repos/nsomar/OAStackView/issues/37
nsomar,OAStackView,100456295,https://api.github.com/repos/nsomar/OAStackView/issues/comments/136210837,136210837,MDEyOklzc3VlQ29tbWVudDEzNjIxMDgzNw==,163204,2015-08-30T22:34:06Z,2015-08-30T22:34:06Z,NONE,"@harlanhaskins Except `layoutMargins` is iOS8 only.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/37/comments,https://github.com/nsomar/OAStackView/issues/37#issuecomment-136210837,https://api.github.com/repos/nsomar/OAStackView/issues/37
nsomar,OAStackView,100456295,https://api.github.com/repos/nsomar/OAStackView/issues/comments/136212115,136212115,MDEyOklzc3VlQ29tbWVudDEzNjIxMjExNQ==,853032,2015-08-30T22:51:46Z,2015-08-30T22:51:46Z,CONTRIBUTOR,"Not with OAStackView. OAStackView adds the property for iOS 7 compatibility.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/37/comments,https://github.com/nsomar/OAStackView/issues/37#issuecomment-136212115,https://api.github.com/repos/nsomar/OAStackView/issues/37
nsomar,OAStackView,100456295,https://api.github.com/repos/nsomar/OAStackView/issues/comments/136236297,136236297,MDEyOklzc3VlQ29tbWVudDEzNjIzNjI5Nw==,163204,2015-08-31T02:27:20Z,2015-08-31T02:27:20Z,NONE,"@harlanhaskins Ah nice! It isn't in the released version of the pod I've been using, but I see the commits. Thanks.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/37/comments,https://github.com/nsomar/OAStackView/issues/37#issuecomment-136236297,https://api.github.com/repos/nsomar/OAStackView/issues/37
nsomar,OAStackView,100456295,https://api.github.com/repos/nsomar/OAStackView/issues/comments/143055670,143055670,MDEyOklzc3VlQ29tbWVudDE0MzA1NTY3MA==,829783,2015-09-24T21:28:46Z,2015-09-24T21:28:46Z,COLLABORATOR,"@christophercotton try out 0.2.0 tag (still no new cocoapod version though)
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/37/comments,https://github.com/nsomar/OAStackView/issues/37#issuecomment-143055670,https://api.github.com/repos/nsomar/OAStackView/issues/37
nsomar,OAStackView,100432521,https://api.github.com/repos/nsomar/OAStackView/issues/comments/159435695,159435695,MDEyOklzc3VlQ29tbWVudDE1OTQzNTY5NQ==,1461052,2015-11-24T23:13:11Z,2015-11-24T23:13:11Z,OWNER,"I think this is possible, but it would require some work. I will be playing around this in the coming days.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/36/comments,https://github.com/nsomar/OAStackView/issues/36#issuecomment-159435695,https://api.github.com/repos/nsomar/OAStackView/issues/36
nsomar,OAStackView,100105543,https://api.github.com/repos/nsomar/OAStackView/issues/comments/129562036,129562036,MDEyOklzc3VlQ29tbWVudDEyOTU2MjAzNg==,1761121,2015-08-10T18:45:07Z,2015-08-10T18:45:07Z,CONTRIBUTOR,"Can we move the framework target into the main project and make the sample app just link against the framework? People keep committing changes that break the framework.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/35/comments,https://github.com/nsomar/OAStackView/pull/35#issuecomment-129562036,https://api.github.com/repos/nsomar/OAStackView/issues/35
nsomar,OAStackView,100105543,https://api.github.com/repos/nsomar/OAStackView/issues/comments/129936424,129936424,MDEyOklzc3VlQ29tbWVudDEyOTkzNjQyNA==,1461052,2015-08-11T15:44:07Z,2015-08-11T15:44:07Z,OWNER,"@calebd That sound like a good plan, I will open an issue to implement this later.
Will be merging this now! Thanks @yusefnapora!
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/35/comments,https://github.com/nsomar/OAStackView/pull/35#issuecomment-129936424,https://api.github.com/repos/nsomar/OAStackView/issues/35
nsomar,OAStackView,99612716,https://api.github.com/repos/nsomar/OAStackView/issues/comments/129009911,129009911,MDEyOklzc3VlQ29tbWVudDEyOTAwOTkxMQ==,1461052,2015-08-08T16:44:03Z,2015-08-08T16:44:03Z,OWNER,"Hey, I am not aware of such a problem.
Could you please share more info, it would be great if you can supply a very small snippet (or project) that I could run. Thanks
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/34/comments,https://github.com/nsomar/OAStackView/issues/34#issuecomment-129009911,https://api.github.com/repos/nsomar/OAStackView/issues/34
nsomar,OAStackView,99612716,https://api.github.com/repos/nsomar/OAStackView/issues/comments/159434980,159434980,MDEyOklzc3VlQ29tbWVudDE1OTQzNDk4MA==,1461052,2015-11-24T23:09:16Z,2015-11-24T23:09:16Z,OWNER,"@dubiao  I am closing this issue since there have been no feedback from your side.
Please feel free to reopen it if the issue persists.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/34/comments,https://github.com/nsomar/OAStackView/issues/34#issuecomment-159434980,https://api.github.com/repos/nsomar/OAStackView/issues/34
nsomar,OAStackView,99485824,https://api.github.com/repos/nsomar/OAStackView/issues/comments/128450832,128450832,MDEyOklzc3VlQ29tbWVudDEyODQ1MDgzMg==,853032,2015-08-06T17:26:33Z,2015-08-06T17:26:33Z,CONTRIBUTOR,"Can you post a screenshot?
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/33/comments,https://github.com/nsomar/OAStackView/issues/33#issuecomment-128450832,https://api.github.com/repos/nsomar/OAStackView/issues/33
nsomar,OAStackView,99485824,https://api.github.com/repos/nsomar/OAStackView/issues/comments/128450971,128450971,MDEyOklzc3VlQ29tbWVudDEyODQ1MDk3MQ==,853032,2015-08-06T17:27:09Z,2015-08-06T17:27:09Z,CONTRIBUTOR,"With the UIStackView behavior and OAStackView behavior?
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/33/comments,https://github.com/nsomar/OAStackView/issues/33#issuecomment-128450971,https://api.github.com/repos/nsomar/OAStackView/issues/33
nsomar,OAStackView,99485824,https://api.github.com/repos/nsomar/OAStackView/issues/comments/128458589,128458589,MDEyOklzc3VlQ29tbWVudDEyODQ1ODU4OQ==,4520588,2015-08-06T17:55:35Z,2015-08-06T17:55:35Z,NONE,"### UIStackView (iOS 9)

![simulator screen shot aug 6 2015 1 43 11 pm](https://cloud.githubusercontent.com/assets/4520588/9118903/939c3946-3c41-11e5-9391-c730f4ffc5d3.png)

---

### OAStackView (iOS 8.4 & iOS 9)

![ios simulator screen shot aug 6 2015 1 45 40 pm](https://cloud.githubusercontent.com/assets/4520588/9118911/a324836e-3c41-11e5-8171-681f16a395f3.png)

---

I also double checked OAStackView running on iOS9 to make sure it wasn't an iOS8 issue.

In this case I want the label to truncate, but it continues past the edge. I have also come across a similar problem where I allow the label to grow to multiple lines instead of truncating. UIStackView is able to handle this case, but I'm running into the same problem with OAStackView.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/33/comments,https://github.com/nsomar/OAStackView/issues/33#issuecomment-128458589,https://api.github.com/repos/nsomar/OAStackView/issues/33
nsomar,OAStackView,99485824,https://api.github.com/repos/nsomar/OAStackView/issues/comments/128460539,128460539,MDEyOklzc3VlQ29tbWVudDEyODQ2MDUzOQ==,4520588,2015-08-06T18:04:30Z,2015-08-06T18:04:30Z,NONE,"I just realized the title of this issue isn't the best description, but I'm not sure what to call it. Feel free to do something about that. :joy: 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/33/comments,https://github.com/nsomar/OAStackView/issues/33#issuecomment-128460539,https://api.github.com/repos/nsomar/OAStackView/issues/33
nsomar,OAStackView,99485824,https://api.github.com/repos/nsomar/OAStackView/issues/comments/145443442,145443442,MDEyOklzc3VlQ29tbWVudDE0NTQ0MzQ0Mg==,153960,2015-10-05T06:38:26Z,2015-10-05T06:38:26Z,NONE,"Have you found a solution for the issue?
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/33/comments,https://github.com/nsomar/OAStackView/issues/33#issuecomment-145443442,https://api.github.com/repos/nsomar/OAStackView/issues/33
nsomar,OAStackView,99485824,https://api.github.com/repos/nsomar/OAStackView/issues/comments/145781831,145781831,MDEyOklzc3VlQ29tbWVudDE0NTc4MTgzMQ==,829783,2015-10-06T08:29:22Z,2015-10-06T08:29:22Z,COLLABORATOR,"sorry @fabb, not yet. I'm going to work on this as a first thing soon 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/33/comments,https://github.com/nsomar/OAStackView/issues/33#issuecomment-145781831,https://api.github.com/repos/nsomar/OAStackView/issues/33
nsomar,OAStackView,99485824,https://api.github.com/repos/nsomar/OAStackView/issues/comments/145905971,145905971,MDEyOklzc3VlQ29tbWVudDE0NTkwNTk3MQ==,153960,2015-10-06T15:45:49Z,2015-10-06T15:45:49Z,NONE,"I ended up using TZStackView which works as expected.
https://github.com/tomvanzummeren/TZStackView
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/33/comments,https://github.com/nsomar/OAStackView/issues/33#issuecomment-145905971,https://api.github.com/repos/nsomar/OAStackView/issues/33
nsomar,OAStackView,99255134,https://api.github.com/repos/nsomar/OAStackView/issues/comments/128081493,128081493,MDEyOklzc3VlQ29tbWVudDEyODA4MTQ5Mw==,1164565,2015-08-05T17:19:00Z,2015-08-05T17:19:00Z,NONE,"As mentioned in #2 - I preferred to do the UIView approach.

All properties needed to be declared and the setters pass the property to the correct stack view (native or backwards compatible).
The methods declared in the interface didn't need to be implemented, as the class implements `methodSignatureForSelector:` and `forwardInvocation`.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/32/comments,https://github.com/nsomar/OAStackView/pull/32#issuecomment-128081493,https://api.github.com/repos/nsomar/OAStackView/issues/32
nsomar,OAStackView,99255134,https://api.github.com/repos/nsomar/OAStackView/issues/comments/128082649,128082649,MDEyOklzc3VlQ29tbWVudDEyODA4MjY0OQ==,1164565,2015-08-05T17:21:39Z,2015-08-05T17:21:39Z,NONE,"And bear in mind it doesn't support interface builder yet.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/32/comments,https://github.com/nsomar/OAStackView/pull/32#issuecomment-128082649,https://api.github.com/repos/nsomar/OAStackView/issues/32
nsomar,OAStackView,99255134,https://api.github.com/repos/nsomar/OAStackView/issues/comments/129030595,129030595,MDEyOklzc3VlQ29tbWVudDEyOTAzMDU5NQ==,1461052,2015-08-08T18:15:08Z,2015-08-08T18:15:08Z,OWNER,"That is a nice approach, Thanks!
Do you think you can add a simple test (preferably on a new spec file)?
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/32/comments,https://github.com/nsomar/OAStackView/pull/32#issuecomment-129030595,https://api.github.com/repos/nsomar/OAStackView/issues/32
nsomar,OAStackView,99255134,https://api.github.com/repos/nsomar/OAStackView/issues/comments/129189971,129189971,MDEyOklzc3VlQ29tbWVudDEyOTE4OTk3MQ==,1164565,2015-08-09T14:07:20Z,2015-08-09T14:07:20Z,NONE,"Hey Omar! I'll try to add it soon.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/32/comments,https://github.com/nsomar/OAStackView/pull/32#issuecomment-129189971,https://api.github.com/repos/nsomar/OAStackView/issues/32
nsomar,OAStackView,99255134,https://api.github.com/repos/nsomar/OAStackView/issues/comments/129190025,129190025,MDEyOklzc3VlQ29tbWVudDEyOTE5MDAyNQ==,1461052,2015-08-09T14:08:51Z,2015-08-09T14:08:51Z,OWNER,"Please also don't forget to rebase since I fixed the test failing issue in iOS7. Again wonderful addition, I think we can expand on it to have a seamless iOS 9 integration :)
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/32/comments,https://github.com/nsomar/OAStackView/pull/32#issuecomment-129190025,https://api.github.com/repos/nsomar/OAStackView/issues/32
nsomar,OAStackView,99255134,https://api.github.com/repos/nsomar/OAStackView/issues/comments/130870994,130870994,MDEyOklzc3VlQ29tbWVudDEzMDg3MDk5NA==,1461052,2015-08-13T22:49:54Z,2015-08-13T22:49:54Z,OWNER,"Hey, Any updates on this, I can add the tests and rebase in the weekend if you want.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/32/comments,https://github.com/nsomar/OAStackView/pull/32#issuecomment-130870994,https://api.github.com/repos/nsomar/OAStackView/issues/32
nsomar,OAStackView,99255134,https://api.github.com/repos/nsomar/OAStackView/issues/comments/130871988,130871988,MDEyOklzc3VlQ29tbWVudDEzMDg3MTk4OA==,1164565,2015-08-13T22:56:52Z,2015-08-13T22:56:52Z,NONE,"Hey Omar! I tried to rebase a few days ago, but had some problems (did you change the file tree?). If you could do it, that would be awesome. 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/32/comments,https://github.com/nsomar/OAStackView/pull/32#issuecomment-130871988,https://api.github.com/repos/nsomar/OAStackView/issues/32
nsomar,OAStackView,99255134,https://api.github.com/repos/nsomar/OAStackView/issues/comments/130872899,130872899,MDEyOklzc3VlQ29tbWVudDEzMDg3Mjg5OQ==,1461052,2015-08-13T23:03:48Z,2015-08-13T23:03:48Z,OWNER,"Sure I will do that no problem!
Yeah, Sadly I added some files, and moved some code around. Anyways, I will rebase and add some tests. 
I will create a pull request with the tests so that you can give me your opinion on wether they cover all your planned scenarios. I will share them in a new branch prior to merge.
Does that sound good to you?

Thanks for the great addition btw!
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/32/comments,https://github.com/nsomar/OAStackView/pull/32#issuecomment-130872899,https://api.github.com/repos/nsomar/OAStackView/issues/32
nsomar,OAStackView,99255134,https://api.github.com/repos/nsomar/OAStackView/issues/comments/173623534,173623534,MDEyOklzc3VlQ29tbWVudDE3MzYyMzUzNA==,702124,2016-01-21T16:24:04Z,2016-01-21T16:24:04Z,NONE,"@natanrolnik maybe you'll suggest me how to use native NSLayoutAnchor API category if API is available? https://github.com/k06a/MissingAnchors/blob/master/Pod/Classes/ABLayoutAnchor/UIView%2BABLayoutAnchor.m
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/32/comments,https://github.com/nsomar/OAStackView/pull/32#issuecomment-173623534,https://api.github.com/repos/nsomar/OAStackView/issues/32
nsomar,OAStackView,99255134,https://api.github.com/repos/nsomar/OAStackView/issues/comments/173689001,173689001,MDEyOklzc3VlQ29tbWVudDE3MzY4OTAwMQ==,1164565,2016-01-21T19:50:03Z,2016-01-21T19:50:03Z,NONE,"Hm, interesting... I will try to take a look.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/32/comments,https://github.com/nsomar/OAStackView/pull/32#issuecomment-173689001,https://api.github.com/repos/nsomar/OAStackView/issues/32
nsomar,OAStackView,99255134,https://api.github.com/repos/nsomar/OAStackView/issues/comments/220947429,220947429,MDEyOklzc3VlQ29tbWVudDIyMDk0NzQyOQ==,1164565,2016-05-23T10:46:58Z,2016-05-23T10:46:58Z,NONE,"Closing as #87 was merged already
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/32/comments,https://github.com/nsomar/OAStackView/pull/32#issuecomment-220947429,https://api.github.com/repos/nsomar/OAStackView/issues/32
nsomar,OAStackView,97833382,https://api.github.com/repos/nsomar/OAStackView/issues/comments/125796480,125796480,MDEyOklzc3VlQ29tbWVudDEyNTc5NjQ4MA==,1461052,2015-07-29T01:19:33Z,2015-07-29T01:19:33Z,OWNER,"Awesome!! great addition :) 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/31/comments,https://github.com/nsomar/OAStackView/pull/31#issuecomment-125796480,https://api.github.com/repos/nsomar/OAStackView/issues/31
nsomar,OAStackView,97833382,https://api.github.com/repos/nsomar/OAStackView/issues/comments/125796537,125796537,MDEyOklzc3VlQ29tbWVudDEyNTc5NjUzNw==,648369,2015-07-29T01:20:11Z,2015-07-29T01:20:11Z,CONTRIBUTOR,"Glad to help out, hopefully this will help others along too.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/31/comments,https://github.com/nsomar/OAStackView/pull/31#issuecomment-125796537,https://api.github.com/repos/nsomar/OAStackView/issues/31
nsomar,OAStackView,97691497,https://api.github.com/repos/nsomar/OAStackView/issues/comments/125589154,125589154,MDEyOklzc3VlQ29tbWVudDEyNTU4OTE1NA==,853032,2015-07-28T12:45:38Z,2015-07-28T12:45:38Z,CONTRIBUTOR,"You've got a constraint on all sides of the UIScrollView? Make sure your bottom constraint is >=0, because a UIScrollView doesn't have a size that AutoLayout can actually make use of.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/30/comments,https://github.com/nsomar/OAStackView/issues/30#issuecomment-125589154,https://api.github.com/repos/nsomar/OAStackView/issues/30
nsomar,OAStackView,97691497,https://api.github.com/repos/nsomar/OAStackView/issues/comments/125603618,125603618,MDEyOklzc3VlQ29tbWVudDEyNTYwMzYxOA==,5191060,2015-07-28T13:17:29Z,2015-07-28T13:17:29Z,NONE,"@harlanhaskins Doing that causes an ambiguous content height for the scroll view.  I've pasted an image of my constraints.  The height constraint is removed at runtime.  I'm using it to avoid the ambiguous content height warning I still would get by changing the constraint to >= 0.

![screen shot 2015-07-28 at 9 15 07 am](https://cloud.githubusercontent.com/assets/5191060/8932152/2aea59a8-3509-11e5-81c2-99c454825454.png)

I guess I still don't understand how that is the issue when adding a single view to the stackview works, but adding a second view does not.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/30/comments,https://github.com/nsomar/OAStackView/issues/30#issuecomment-125603618,https://api.github.com/repos/nsomar/OAStackView/issues/30
nsomar,OAStackView,97691497,https://api.github.com/repos/nsomar/OAStackView/issues/comments/125604664,125604664,MDEyOklzc3VlQ29tbWVudDEyNTYwNDY2NA==,853032,2015-07-28T13:22:54Z,2015-07-28T13:22:54Z,CONTRIBUTOR,"I'd recommend reading this Apple developer page about using AutoLayout with UIScrollView.

https://developer.apple.com/library/ios/technotes/tn2154/_index.html
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/30/comments,https://github.com/nsomar/OAStackView/issues/30#issuecomment-125604664,https://api.github.com/repos/nsomar/OAStackView/issues/30
nsomar,OAStackView,97691497,https://api.github.com/repos/nsomar/OAStackView/issues/comments/125934632,125934632,MDEyOklzc3VlQ29tbWVudDEyNTkzNDYzMg==,5191060,2015-07-29T12:21:45Z,2015-07-29T12:21:45Z,NONE,"Aside from my apparently scrollview issues (which work regardless), all I'm asking for is an update to the current cocoapods spec.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/30/comments,https://github.com/nsomar/OAStackView/issues/30#issuecomment-125934632,https://api.github.com/repos/nsomar/OAStackView/issues/30
nsomar,OAStackView,97691497,https://api.github.com/repos/nsomar/OAStackView/issues/comments/132337509,132337509,MDEyOklzc3VlQ29tbWVudDEzMjMzNzUwOQ==,11229,2015-08-18T20:15:56Z,2015-08-18T20:15:56Z,NONE,"I ran into this exact same problem today using the Pod-installed v0.1.0 of OAStackView. Updating to master by manually specifying the git repo in my Podfile fixed it.

+1 to publishing a new spec with the latest (cc: @oarrabi)
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/30/comments,https://github.com/nsomar/OAStackView/issues/30#issuecomment-132337509,https://api.github.com/repos/nsomar/OAStackView/issues/30
nsomar,OAStackView,97691497,https://api.github.com/repos/nsomar/OAStackView/issues/comments/142555706,142555706,MDEyOklzc3VlQ29tbWVudDE0MjU1NTcwNg==,829783,2015-09-23T10:12:51Z,2015-09-23T10:12:51Z,COLLABORATOR,"@oarrabi is AFK for a couple of weeks. I've added 0.2.0 tag, so you can refer to it in the meantime.

@oarrabi please add me as a maintainer to pods repo as well when you have a chance:

```
$ pod trunk add-owner OAStackView d2.lebedev@gmail.com
```
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/30/comments,https://github.com/nsomar/OAStackView/issues/30#issuecomment-142555706,https://api.github.com/repos/nsomar/OAStackView/issues/30
nsomar,OAStackView,97691497,https://api.github.com/repos/nsomar/OAStackView/issues/comments/146006165,146006165,MDEyOklzc3VlQ29tbWVudDE0NjAwNjE2NQ==,1461052,2015-10-06T21:23:30Z,2015-10-06T21:23:30Z,OWNER,"I am really sorry for being this late, @garnett I have added you as an owner to the pod.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/30/comments,https://github.com/nsomar/OAStackView/issues/30#issuecomment-146006165,https://api.github.com/repos/nsomar/OAStackView/issues/30
nsomar,OAStackView,97691497,https://api.github.com/repos/nsomar/OAStackView/issues/comments/159432808,159432808,MDEyOklzc3VlQ29tbWVudDE1OTQzMjgwOA==,1461052,2015-11-24T22:59:10Z,2015-11-24T22:59:10Z,OWNER,"Closing this issue since it has been solved
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/30/comments,https://github.com/nsomar/OAStackView/issues/30#issuecomment-159432808,https://api.github.com/repos/nsomar/OAStackView/issues/30
nsomar,OAStackView,97649009,https://api.github.com/repos/nsomar/OAStackView/issues/comments/125507623,125507623,MDEyOklzc3VlQ29tbWVudDEyNTUwNzYyMw==,1461052,2015-07-28T08:48:45Z,2015-07-28T08:48:45Z,OWNER,"Hello, I did not really test it on iOS6, I cannot think of a reason why it shouldnt work. I dont have an iOS 6 simulator running at the moment, It would be amazing if some one tries it on iOS6.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/29/comments,https://github.com/nsomar/OAStackView/issues/29#issuecomment-125507623,https://api.github.com/repos/nsomar/OAStackView/issues/29
nsomar,OAStackView,97649009,https://api.github.com/repos/nsomar/OAStackView/issues/comments/125590151,125590151,MDEyOklzc3VlQ29tbWVudDEyNTU5MDE1MQ==,853032,2015-07-28T12:47:13Z,2015-07-28T12:47:13Z,CONTRIBUTOR,"Probably shouldn't make any changes to make it compatible, if it's not already. iOS 6 is a fraction of a fraction of market share right now.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/29/comments,https://github.com/nsomar/OAStackView/issues/29#issuecomment-125590151,https://api.github.com/repos/nsomar/OAStackView/issues/29
nsomar,OAStackView,97649009,https://api.github.com/repos/nsomar/OAStackView/issues/comments/125774145,125774145,MDEyOklzc3VlQ29tbWVudDEyNTc3NDE0NQ==,1461052,2015-07-28T22:48:58Z,2015-07-28T22:48:58Z,OWNER,"@harlanhaskins I totally agree, I think if OAStackView supports iOS6, then its good, otherwise, I dont think we should make any changes to it
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/29/comments,https://github.com/nsomar/OAStackView/issues/29#issuecomment-125774145,https://api.github.com/repos/nsomar/OAStackView/issues/29
nsomar,OAStackView,97649009,https://api.github.com/repos/nsomar/OAStackView/issues/comments/126143122,126143122,MDEyOklzc3VlQ29tbWVudDEyNjE0MzEyMg==,1461052,2015-07-30T00:49:49Z,2015-07-30T00:49:49Z,OWNER,"I will be closing this issue, since I think we agree on it.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/29/comments,https://github.com/nsomar/OAStackView/issues/29#issuecomment-126143122,https://api.github.com/repos/nsomar/OAStackView/issues/29
nsomar,OAStackView,97649009,https://api.github.com/repos/nsomar/OAStackView/issues/comments/132109661,132109661,MDEyOklzc3VlQ29tbWVudDEzMjEwOTY2MQ==,8086633,2015-08-18T07:52:07Z,2015-08-18T07:53:04Z,NONE,"I have many ios6 iphoneï¼Œthe demo  doesn't wort well .  It will crash when tap some btton
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/29/comments,https://github.com/nsomar/OAStackView/issues/29#issuecomment-132109661,https://api.github.com/repos/nsomar/OAStackView/issues/29
nsomar,OAStackView,96618602,https://api.github.com/repos/nsomar/OAStackView/issues/comments/125776413,125776413,MDEyOklzc3VlQ29tbWVudDEyNTc3NjQxMw==,1461052,2015-07-28T22:56:22Z,2015-07-28T22:56:22Z,OWNER,"Wonderful! :+1: 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/28/comments,https://github.com/nsomar/OAStackView/pull/28#issuecomment-125776413,https://api.github.com/repos/nsomar/OAStackView/issues/28
nsomar,OAStackView,96092679,https://api.github.com/repos/nsomar/OAStackView/issues/comments/124121088,124121088,MDEyOklzc3VlQ29tbWVudDEyNDEyMTA4OA==,853032,2015-07-23T14:16:38Z,2015-07-23T14:16:38Z,CONTRIBUTOR,"I'm using this fork now, and it's very nice and works as intended. :+1:
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/27/comments,https://github.com/nsomar/OAStackView/pull/27#issuecomment-124121088,https://api.github.com/repos/nsomar/OAStackView/issues/27
nsomar,OAStackView,96092679,https://api.github.com/repos/nsomar/OAStackView/issues/comments/125776353,125776353,MDEyOklzc3VlQ29tbWVudDEyNTc3NjM1Mw==,1461052,2015-07-28T22:56:00Z,2015-07-28T22:56:00Z,OWNER,"This looks great!!, Sorry for the later reply, Can you add a couple of test cases to verify the margin alignment? Otherwise I will add them my self in the weekend.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/27/comments,https://github.com/nsomar/OAStackView/pull/27#issuecomment-125776353,https://api.github.com/repos/nsomar/OAStackView/issues/27
nsomar,OAStackView,96092679,https://api.github.com/repos/nsomar/OAStackView/issues/comments/126207011,126207011,MDEyOklzc3VlQ29tbWVudDEyNjIwNzAxMQ==,71881,2015-07-30T07:14:57Z,2015-07-30T07:14:57Z,CONTRIBUTOR,"Yes, Iâ€™ll try to add them in the weekend.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/27/comments,https://github.com/nsomar/OAStackView/pull/27#issuecomment-126207011,https://api.github.com/repos/nsomar/OAStackView/issues/27
nsomar,OAStackView,96092679,https://api.github.com/repos/nsomar/OAStackView/issues/comments/127378844,127378844,MDEyOklzc3VlQ29tbWVudDEyNzM3ODg0NA==,853032,2015-08-03T19:30:17Z,2015-08-03T19:30:17Z,CONTRIBUTOR,"Any updates? I'm anxious to try this fork with the latest updates to master :grin:
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/27/comments,https://github.com/nsomar/OAStackView/pull/27#issuecomment-127378844,https://api.github.com/repos/nsomar/OAStackView/issues/27
nsomar,OAStackView,96092679,https://api.github.com/repos/nsomar/OAStackView/issues/comments/127418513,127418513,MDEyOklzc3VlQ29tbWVudDEyNzQxODUxMw==,1461052,2015-08-03T22:06:24Z,2015-08-03T22:06:24Z,OWNER,"Hey, Me too :D.
I had little time this weekend but I definitely want to merge this asap.
I will try to add some specs tomorrow in case @cabeca cannot add them.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/27/comments,https://github.com/nsomar/OAStackView/pull/27#issuecomment-127418513,https://api.github.com/repos/nsomar/OAStackView/issues/27
nsomar,OAStackView,96092679,https://api.github.com/repos/nsomar/OAStackView/issues/comments/127697269,127697269,MDEyOklzc3VlQ29tbWVudDEyNzY5NzI2OQ==,71881,2015-08-04T18:13:46Z,2015-08-04T18:13:46Z,CONTRIBUTOR,"Sorry about the delay. I added the missing tests to the project.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/27/comments,https://github.com/nsomar/OAStackView/pull/27#issuecomment-127697269,https://api.github.com/repos/nsomar/OAStackView/issues/27
nsomar,OAStackView,96092679,https://api.github.com/repos/nsomar/OAStackView/issues/comments/129028250,129028250,MDEyOklzc3VlQ29tbWVudDEyOTAyODI1MA==,1461052,2015-08-08T18:09:27Z,2015-08-08T18:09:27Z,OWNER,"Great, this addition is super cool!
I will merge this branch by hand since I want to fix the failing test on iOS 7
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/27/comments,https://github.com/nsomar/OAStackView/pull/27#issuecomment-129028250,https://api.github.com/repos/nsomar/OAStackView/issues/27
nsomar,OAStackView,96092679,https://api.github.com/repos/nsomar/OAStackView/issues/comments/129029281,129029281,MDEyOklzc3VlQ29tbWVudDEyOTAyOTI4MQ==,1461052,2015-08-08T18:11:02Z,2015-08-08T18:11:02Z,OWNER,"Merged! Thanks!
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/27/comments,https://github.com/nsomar/OAStackView/pull/27#issuecomment-129029281,https://api.github.com/repos/nsomar/OAStackView/issues/27
nsomar,OAStackView,95300575,https://api.github.com/repos/nsomar/OAStackView/issues/comments/121768295,121768295,MDEyOklzc3VlQ29tbWVudDEyMTc2ODI5NQ==,1461052,2015-07-15T22:35:27Z,2015-07-15T22:35:27Z,OWNER,"Hey, what version of Xcode are you on?
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/26/comments,https://github.com/nsomar/OAStackView/issues/26#issuecomment-121768295,https://api.github.com/repos/nsomar/OAStackView/issues/26
nsomar,OAStackView,95300575,https://api.github.com/repos/nsomar/OAStackView/issues/comments/121858654,121858654,MDEyOklzc3VlQ29tbWVudDEyMTg1ODY1NA==,3435334,2015-07-16T07:27:17Z,2015-07-16T07:27:17Z,NONE,"I am using Xcode 6.2
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/26/comments,https://github.com/nsomar/OAStackView/issues/26#issuecomment-121858654,https://api.github.com/repos/nsomar/OAStackView/issues/26
nsomar,OAStackView,95300575,https://api.github.com/repos/nsomar/OAStackView/issues/comments/122013773,122013773,MDEyOklzc3VlQ29tbWVudDEyMjAxMzc3Mw==,853032,2015-07-16T16:36:03Z,2015-07-16T16:36:03Z,CONTRIBUTOR,"You need to update to Xcode 6.4.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/26/comments,https://github.com/nsomar/OAStackView/issues/26#issuecomment-122013773,https://api.github.com/repos/nsomar/OAStackView/issues/26
nsomar,OAStackView,95300575,https://api.github.com/repos/nsomar/OAStackView/issues/comments/122077145,122077145,MDEyOklzc3VlQ29tbWVudDEyMjA3NzE0NQ==,3435334,2015-07-16T20:16:21Z,2015-07-16T20:16:40Z,NONE,"Yes, its working fine with even Xcode 6.3.1. If possible then please specify minimum Xcode version in README file. It helps others.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/26/comments,https://github.com/nsomar/OAStackView/issues/26#issuecomment-122077145,https://api.github.com/repos/nsomar/OAStackView/issues/26
nsomar,OAStackView,95126832,https://api.github.com/repos/nsomar/OAStackView/issues/comments/124120944,124120944,MDEyOklzc3VlQ29tbWVudDEyNDEyMDk0NA==,853032,2015-07-23T14:16:05Z,2015-07-23T14:16:05Z,CONTRIBUTOR,":+1: 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/25/comments,https://github.com/nsomar/OAStackView/pull/25#issuecomment-124120944,https://api.github.com/repos/nsomar/OAStackView/issues/25
nsomar,OAStackView,95126832,https://api.github.com/repos/nsomar/OAStackView/issues/comments/125775481,125775481,MDEyOklzc3VlQ29tbWVudDEyNTc3NTQ4MQ==,1461052,2015-07-28T22:52:12Z,2015-07-28T22:52:12Z,OWNER,"Great PR :+1: 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/25/comments,https://github.com/nsomar/OAStackView/pull/25#issuecomment-125775481,https://api.github.com/repos/nsomar/OAStackView/issues/25
nsomar,OAStackView,95126832,https://api.github.com/repos/nsomar/OAStackView/issues/comments/125794073,125794073,MDEyOklzc3VlQ29tbWVudDEyNTc5NDA3Mw==,648369,2015-07-29T01:00:27Z,2015-07-29T01:00:27Z,CONTRIBUTOR,"Looks like we need another pass of #24 - the newly created files in this PR were not added to the framework project, so it does not build any longer.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/25/comments,https://github.com/nsomar/OAStackView/pull/25#issuecomment-125794073,https://api.github.com/repos/nsomar/OAStackView/issues/25
nsomar,OAStackView,93917062,https://api.github.com/repos/nsomar/OAStackView/issues/comments/119765509,119765509,MDEyOklzc3VlQ29tbWVudDExOTc2NTUwOQ==,1461052,2015-07-09T00:15:33Z,2015-07-09T00:15:33Z,OWNER,"Great :+1: 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/24/comments,https://github.com/nsomar/OAStackView/pull/24#issuecomment-119765509,https://api.github.com/repos/nsomar/OAStackView/issues/24
nsomar,OAStackView,93917062,https://api.github.com/repos/nsomar/OAStackView/issues/comments/119872629,119872629,MDEyOklzc3VlQ29tbWVudDExOTg3MjYyOQ==,1761121,2015-07-09T08:27:23Z,2015-07-09T08:27:23Z,CONTRIBUTOR,"Thanks for the awesome library!
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/24/comments,https://github.com/nsomar/OAStackView/pull/24#issuecomment-119872629,https://api.github.com/repos/nsomar/OAStackView/issues/24
nsomar,OAStackView,93866383,https://api.github.com/repos/nsomar/OAStackView/issues/comments/125371968,125371968,MDEyOklzc3VlQ29tbWVudDEyNTM3MTk2OA==,1111661,2015-07-27T22:47:49Z,2015-07-27T22:47:49Z,NONE,"I had this problem too. It seems to follow the order of the views listed in the Document Outline sidebar on the left in IB. Rearranging my views there fixed things for me. Your view sticking to the bottom might need to be dragged up to a different location. (Incidentally, if you're not aware already, this ordering in IB is usually used for determining which view sits in front of another view, if two views overlap each other.)

![screen shot 2015-07-27 at 3 36 29 pm](https://cloud.githubusercontent.com/assets/1111661/8919624/d8e2b738-3475-11e5-8d1a-3ad3a7942c6e.png)

Hope that helps.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/23/comments,https://github.com/nsomar/OAStackView/issues/23#issuecomment-125371968,https://api.github.com/repos/nsomar/OAStackView/issues/23
nsomar,OAStackView,93866383,https://api.github.com/repos/nsomar/OAStackView/issues/comments/125774026,125774026,MDEyOklzc3VlQ29tbWVudDEyNTc3NDAyNg==,1461052,2015-07-28T22:48:16Z,2015-07-28T22:48:16Z,OWNER,"@davidkobilnyk that is correct, the order of the views in the IB is actually reflected in the order of the views in self.subviews.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/23/comments,https://github.com/nsomar/OAStackView/issues/23#issuecomment-125774026,https://api.github.com/repos/nsomar/OAStackView/issues/23
nsomar,OAStackView,93866383,https://api.github.com/repos/nsomar/OAStackView/issues/comments/129040289,129040289,MDEyOklzc3VlQ29tbWVudDEyOTA0MDI4OQ==,1461052,2015-08-08T19:36:34Z,2015-08-08T19:36:34Z,OWNER,"I will be closing this issue. Please reopen it if there are still concerns about it.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/23/comments,https://github.com/nsomar/OAStackView/issues/23#issuecomment-129040289,https://api.github.com/repos/nsomar/OAStackView/issues/23
nsomar,OAStackView,93287518,https://api.github.com/repos/nsomar/OAStackView/issues/comments/118957574,118957574,MDEyOklzc3VlQ29tbWVudDExODk1NzU3NA==,1461052,2015-07-06T18:53:46Z,2015-07-06T18:53:46Z,OWNER,"Wow this is very cool, thanks for the help :)
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/22/comments,https://github.com/nsomar/OAStackView/pull/22#issuecomment-118957574,https://api.github.com/repos/nsomar/OAStackView/issues/22
nsomar,OAStackView,92955530,https://api.github.com/repos/nsomar/OAStackView/issues/comments/118570013,118570013,MDEyOklzc3VlQ29tbWVudDExODU3MDAxMw==,1461052,2015-07-05T01:44:34Z,2015-07-05T01:44:34Z,OWNER,"This pull request is amazing, especially adding the needed LayoutGuide. Wonderfull addition :+1: keep them coming :)
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/21/comments,https://github.com/nsomar/OAStackView/pull/21#issuecomment-118570013,https://api.github.com/repos/nsomar/OAStackView/issues/21
nsomar,OAStackView,91825579,https://api.github.com/repos/nsomar/OAStackView/issues/comments/117038348,117038348,MDEyOklzc3VlQ29tbWVudDExNzAzODM0OA==,1461052,2015-06-30T07:34:42Z,2015-06-30T07:34:42Z,OWNER,"Great addition :+1: 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/19/comments,https://github.com/nsomar/OAStackView/pull/19#issuecomment-117038348,https://api.github.com/repos/nsomar/OAStackView/issues/19
nsomar,OAStackView,91036343,https://api.github.com/repos/nsomar/OAStackView/issues/comments/116881848,116881848,MDEyOklzc3VlQ29tbWVudDExNjg4MTg0OA==,648369,2015-06-29T23:54:03Z,2015-06-29T23:54:03Z,CONTRIBUTOR,"Said another way, [Carthage](https://github.com/Carthage/Carthage) support would be great - at present the repo doesn't have an _xcodeproj_ with a framework target to build from.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/18/comments,https://github.com/nsomar/OAStackView/issues/18#issuecomment-116881848,https://api.github.com/repos/nsomar/OAStackView/issues/18
nsomar,OAStackView,91036343,https://api.github.com/repos/nsomar/OAStackView/issues/comments/116882116,116882116,MDEyOklzc3VlQ29tbWVudDExNjg4MjExNg==,648369,2015-06-29T23:55:55Z,2015-06-29T23:55:55Z,CONTRIBUTOR,"And of course I said that before looking at the PRs, @avalanched's #19 adds an _xcodeproj_ with dynamic framework target to do precisely this. :wink:
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/18/comments,https://github.com/nsomar/OAStackView/issues/18#issuecomment-116882116,https://api.github.com/repos/nsomar/OAStackView/issues/18
nsomar,OAStackView,91036343,https://api.github.com/repos/nsomar/OAStackView/issues/comments/117059784,117059784,MDEyOklzc3VlQ29tbWVudDExNzA1OTc4NA==,1461052,2015-06-30T08:47:34Z,2015-06-30T08:47:34Z,OWNER,"@pizthewiz Hey, I just merged pr #19 :+1: 
I will be closing this issue, thanks :)
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/18/comments,https://github.com/nsomar/OAStackView/issues/18#issuecomment-117059784,https://api.github.com/repos/nsomar/OAStackView/issues/18
nsomar,OAStackView,90961571,https://api.github.com/repos/nsomar/OAStackView/issues/comments/115396179,115396179,MDEyOklzc3VlQ29tbWVudDExNTM5NjE3OQ==,134170,2015-06-25T20:56:36Z,2015-06-25T20:56:36Z,CONTRIBUTOR,"I've discovered some bugs after I filed the PR, so it definitely deserves the `[WIP]` annotation. I will push additional changes, but in the meantime it would be great if you think this direction is favorable.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/17/comments,https://github.com/nsomar/OAStackView/pull/17#issuecomment-115396179,https://api.github.com/repos/nsomar/OAStackView/issues/17
nsomar,OAStackView,90961571,https://api.github.com/repos/nsomar/OAStackView/issues/comments/115432425,115432425,MDEyOklzc3VlQ29tbWVudDExNTQzMjQyNQ==,1461052,2015-06-25T23:50:34Z,2015-06-25T23:50:34Z,OWNER,"This looks like a change to the correct direction, working with visible views was not really the optimal solution and I wanted to move away from it. I agree with your decision with going with arranged view storage. 
I am really interested in seeing where this path leads :+1: 
Great work!
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/17/comments,https://github.com/nsomar/OAStackView/pull/17#issuecomment-115432425,https://api.github.com/repos/nsomar/OAStackView/issues/17
nsomar,OAStackView,90961571,https://api.github.com/repos/nsomar/OAStackView/issues/comments/115995812,115995812,MDEyOklzc3VlQ29tbWVudDExNTk5NTgxMg==,134170,2015-06-27T09:22:43Z,2015-06-27T09:22:43Z,CONTRIBUTOR,"I just pushed some more commits to this PR. Not all of them strictly relate to the arrangedSubviews change, but I've been trying to make improvements to `OAStackView` while integrating it into a project at work and some changes now depend on each other. `ffcde3b` for example has more to do with #14 (and #14 should not be merged without it). I'll explain the additional changes:
- `3058581` the subviews of the stack need to shrink if the stack's size is constrained. The intrinsicContentSize based approach did not cover this. Instead, the alignment strategies now add additional constraints to make sure that the stack and subviews grow/shrink together.
- `ffcde3b` this is needed for #14 or else the baseline constraints will not be removed properly. I am not sure about the compiler checks for the newer `NSLayoutAttributes`. Let me know what you think.
- `fc97522` implements the `UIStackView` approach for baseline aligning a stack view itself. I'm not sure if this is as powerful as `UIStackView`, since we're probably missing some private API that will update the baseline alignment when the contents of the stack changes.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/17/comments,https://github.com/nsomar/OAStackView/pull/17#issuecomment-115995812,https://api.github.com/repos/nsomar/OAStackView/issues/17
nsomar,OAStackView,90961571,https://api.github.com/repos/nsomar/OAStackView/issues/comments/132825038,132825038,MDEyOklzc3VlQ29tbWVudDEzMjgyNTAzOA==,11229,2015-08-19T23:28:40Z,2015-08-19T23:28:40Z,NONE,"@Thomvis â€”Â Ping on this one? I just ran into some of the `arrangedSubviews` you described here and am wondering what the state of this is. Thanks!
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/17/comments,https://github.com/nsomar/OAStackView/pull/17#issuecomment-132825038,https://api.github.com/repos/nsomar/OAStackView/issues/17
nsomar,OAStackView,90771145,https://api.github.com/repos/nsomar/OAStackView/issues/comments/115236503,115236503,MDEyOklzc3VlQ29tbWVudDExNTIzNjUwMw==,58052,2015-06-25T12:37:29Z,2015-06-25T12:37:29Z,CONTRIBUTOR,"Fully agree with this.

Additionally, how do you feel about trying to match the constraints generated by `UIStackView` itself ?

I've been poking around in a playground and inspecting the `UIStackView` generated constraints, and they're somewhat different.

It may be interesting to set up tests that:
1. configure a `UIStackView` according to the settings under test
2. configure a `OAStackView` with identical settings
3. assert that the constraints match
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/16/comments,https://github.com/nsomar/OAStackView/issues/16#issuecomment-115236503,https://api.github.com/repos/nsomar/OAStackView/issues/16
nsomar,OAStackView,90771145,https://api.github.com/repos/nsomar/OAStackView/issues/comments/115394962,115394962,MDEyOklzc3VlQ29tbWVudDExNTM5NDk2Mg==,134170,2015-06-25T20:54:49Z,2015-06-25T20:54:49Z,CONTRIBUTOR,"@bartvandendriessche comparing with `UIStackView` would be great! I'm not sure however if checking the constraints is the right level, because `OAStackView` might need different constraints e.g. for backward compatibility. The tests could also compare snapshots of identically set-up `UIStackView`s and `OAStackView`s.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/16/comments,https://github.com/nsomar/OAStackView/issues/16#issuecomment-115394962,https://api.github.com/repos/nsomar/OAStackView/issues/16
nsomar,OAStackView,90771145,https://api.github.com/repos/nsomar/OAStackView/issues/comments/115930272,115930272,MDEyOklzc3VlQ29tbWVudDExNTkzMDI3Mg==,1461052,2015-06-27T01:00:36Z,2015-06-27T01:00:36Z,OWNER,"I think it would be good to have a mix of both of these unit tests. I noticed that having the same frame on different iOS versions might not be possible, as applying the same constraints on iOS7 and iOS8 results in different frames after the layout pass.
I think a decision regarding frame vs constraint must be taken early since it will affect the way the project will be moving forward.

Do you guys have some arguments/opinion on wether OAStackView should favour constraint over frames or the opposite?
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/16/comments,https://github.com/nsomar/OAStackView/issues/16#issuecomment-115930272,https://api.github.com/repos/nsomar/OAStackView/issues/16
nsomar,OAStackView,90771145,https://api.github.com/repos/nsomar/OAStackView/issues/comments/116016439,116016439,MDEyOklzc3VlQ29tbWVudDExNjAxNjQzOQ==,58052,2015-06-27T11:34:29Z,2015-06-27T11:34:29Z,CONTRIBUTOR,"Hmm, do you maybe have an example where the same constraints produce different results on iOS 7 and iOS 8 ?

I mean obviously iOS7 is lacking the baseline and margin oriented NSLayoutAttribute's but for the rest, it seems like constraints should lead to the same outcome.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/16/comments,https://github.com/nsomar/OAStackView/issues/16#issuecomment-116016439,https://api.github.com/repos/nsomar/OAStackView/issues/16
nsomar,OAStackView,90771145,https://api.github.com/repos/nsomar/OAStackView/issues/comments/116089391,116089391,MDEyOklzc3VlQ29tbWVudDExNjA4OTM5MQ==,58052,2015-06-27T15:30:13Z,2015-06-27T15:31:22Z,CONTRIBUTOR,"Another question; does it make sense to mimic `NSLayoutGuide`s using invisible `UIView`s ?

Looking at the constraints generated by `UIStackView`, it is clear that the implementation leans heavily on the use of the new `NSLayoutGuide` class.

As mentioned in the ""Mysteries of Auto Layout, part 2"" WWDC talk (session 219), `NSLayoutGuide` doesn't allow anything new per se, it just does optimises something that used to be done with invisible `UIView`s.

It might be a good idea to build constraints against such invisible `UIView`s where `UIStackView` uses `NSLayoutGuide`s. 

In particular for the `UIStackViewDistributionEqualCentering` and `UIStackViewDistributionEqualSpacing`, I think we'll have to add invisible subviews anyway.

Some of the layout guides used by `UIStackView` are:

## UIViewLayoutMarginsGuide

There is at most a single UIViewLayoutMarginsGuide.

This is basically the new `layoutMarginsGuide` and is used to create constraints of type ""UI-canvas-connection"" _if_ `layoutMarginsRelativeArrangement` is `true`.

## UI-alignment-spanner

There is always a single UI-alignment-spanner.

This layout guide is used to create constraints of type ""UI-spanning-boundary"".

The constraints are between the `arrangedSubviews` edges transverse to the `axis` (so Top and Bottom if axis is Horizontal, Leading and Trailing if Vertical).

These constraints are of priority `UILayoutPriorityRequired`, but they are lenient in that they use `>=` and `<=` relations.

## UI-ordering-spanner

There is always a single UI-ordering-spanner.

Like the UI-alignment-spanner, this guide is used to create constraints of type ""UI-spanning-boundary"".

The constraints are made against the same edges as those from the UI-alignment-spanner.

These constraints are .5 less than `UILayoutPriorityRequired`, but they are strict in the sense that they use `==` relations.

## UI-distributing

There can be 0 or n UI-distributing guides.

These guides are only used when `distribution` is `UIStackViewDistributionEqualSpacing` or `UIStackViewDistributionEqualCentering`.

They basically mimic empty views in between the `arrangedSubviews`.

These guides create constraints labeled ""UI-distributing-edge"". They constrain `arrangedSubviews` edges parallel to the `axis` to make sure the space in between the `arrangedSubviews` is equal.

If there are multiple UI-distributing guides, they will constrain themselves to have equal `width`s or `height`s depending on the `axis` (width for Horizontal, height for Vertical).
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/16/comments,https://github.com/nsomar/OAStackView/issues/16#issuecomment-116089391,https://api.github.com/repos/nsomar/OAStackView/issues/16
nsomar,OAStackView,90771145,https://api.github.com/repos/nsomar/OAStackView/issues/comments/116091450,116091450,MDEyOklzc3VlQ29tbWVudDExNjA5MTQ1MA==,853032,2015-06-27T15:58:43Z,2015-06-27T15:58:43Z,CONTRIBUTOR,"It'd probably be useful, for performance, to make an OALayoutGuide class that's nothing more than a UIView backed with a CATransformLayer
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/16/comments,https://github.com/nsomar/OAStackView/issues/16#issuecomment-116091450,https://api.github.com/repos/nsomar/OAStackView/issues/16
nsomar,OAStackView,90393732,https://api.github.com/repos/nsomar/OAStackView/issues/comments/114514500,114514500,MDEyOklzc3VlQ29tbWVudDExNDUxNDUwMA==,1461052,2015-06-23T13:55:42Z,2015-06-23T13:55:42Z,OWNER,"Grand!
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/15/comments,https://github.com/nsomar/OAStackView/pull/15#issuecomment-114514500,https://api.github.com/repos/nsomar/OAStackView/issues/15
nsomar,OAStackView,90329225,https://api.github.com/repos/nsomar/OAStackView/issues/comments/114995616,114995616,MDEyOklzc3VlQ29tbWVudDExNDk5NTYxNg==,1461052,2015-06-24T20:09:09Z,2015-06-24T20:09:09Z,OWNER,"This looks amazing :+1: 
Do you think you will be able to add some unit test to cover these enums? If not, then I can add them on the weekend.

Thanks for your great addition :+1:  !!
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/14/comments,https://github.com/nsomar/OAStackView/pull/14#issuecomment-114995616,https://api.github.com/repos/nsomar/OAStackView/issues/14
nsomar,OAStackView,90329225,https://api.github.com/repos/nsomar/OAStackView/issues/comments/115996104,115996104,MDEyOklzc3VlQ29tbWVudDExNTk5NjEwNA==,134170,2015-06-27T09:24:11Z,2015-06-27T09:24:11Z,CONTRIBUTOR,"#17 now contains a fix for an issue I discovered yesterday with this PR. Sorry that they're so intertwined. Let me know if I can help.

I won't have time this weekend, so feel free to merge to your likings.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/14/comments,https://github.com/nsomar/OAStackView/pull/14#issuecomment-115996104,https://api.github.com/repos/nsomar/OAStackView/issues/14
nsomar,OAStackView,90329225,https://api.github.com/repos/nsomar/OAStackView/issues/comments/118563774,118563774,MDEyOklzc3VlQ29tbWVudDExODU2Mzc3NA==,1461052,2015-07-04T23:58:07Z,2015-07-04T23:58:07Z,OWNER,"Hey,
Sorry for being super late. I just reviewed this PR and it looks very cool.
However, I ran a couple of tests on UIStackView and it seems that the baseline constraints works as following:

For this example consider the following image

<img width=""382"" alt=""screen shot 2015-07-05 at 00 55 05"" src=""https://cloud.githubusercontent.com/assets/1461052/8509763/557529a2-22b0-11e5-8fb4-ce55e7aa89ad.png"">

In your implementation you are adding baseline constraint for button 2 in relation to 1, and button 3 in relation to 2, etc..
However, In UIStackView, baseline constraint are always added to the first view. So button 2 adds to 1 and button 3 to 1 etc..

The following is a po of the constraints added in UIStackView:

```
<NSLayoutConstraint:0x7f984ac40510 'UI-alignment' UIButton:0x7f984ad0e310'1'.lastBaseline == UIButton:0x7f984ad11670'2'.lastBaseline>,
<NSLayoutConstraint:0x7f984ac42f80 'UI-alignment' UIButton:0x7f984ad0e310'1'.lastBaseline == UIButton:0x7f984ad11e80'3'.lastBaseline>,
<NSLayoutConstraint:0x7f984ac43740 'UI-alignment' UIButton:0x7f984ad0e310'1'.lastBaseline == UILabel:0x7f984ad12460'adasdsdsadasdasda'.lastBaseline>,
```

I think we should consider implementing the same idea as with UIStackView. What is your input on this?
Also, what is your general idea on what unit tests can we use to assert the constraints? should we go with frames or with constraints?
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/14/comments,https://github.com/nsomar/OAStackView/pull/14#issuecomment-118563774,https://api.github.com/repos/nsomar/OAStackView/issues/14
nsomar,OAStackView,90329225,https://api.github.com/repos/nsomar/OAStackView/issues/comments/125775078,125775078,MDEyOklzc3VlQ29tbWVudDEyNTc3NTA3OA==,1461052,2015-07-28T22:51:01Z,2015-07-28T22:51:01Z,OWNER,"@Thomvis Hey, I will be trying to merge both your great PR's this weekend. About my last question. If you dont have free time, I can totally tackle it.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/14/comments,https://github.com/nsomar/OAStackView/pull/14#issuecomment-125775078,https://api.github.com/repos/nsomar/OAStackView/issues/14
nsomar,OAStackView,90329225,https://api.github.com/repos/nsomar/OAStackView/issues/comments/125877751,125877751,MDEyOklzc3VlQ29tbWVudDEyNTg3Nzc1MQ==,134170,2015-07-29T08:18:33Z,2015-07-29T08:18:33Z,CONTRIBUTOR,"@oarrabi sorry, but I won't have time soon to improve this further. I do hope that we can eventually merge more changes from https://github.com/Thomvis/OAStackView back to this repo, but I understand that this will take time from me (if you agree with those changes ;)).
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/14/comments,https://github.com/nsomar/OAStackView/pull/14#issuecomment-125877751,https://api.github.com/repos/nsomar/OAStackView/issues/14
nsomar,OAStackView,90329225,https://api.github.com/repos/nsomar/OAStackView/issues/comments/129040241,129040241,MDEyOklzc3VlQ29tbWVudDEyOTA0MDI0MQ==,1461052,2015-08-08T19:35:18Z,2015-08-08T19:35:18Z,OWNER,"Thanks for the addition, I added some specs and merged this PR, Wonderfull work!
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/14/comments,https://github.com/nsomar/OAStackView/pull/14#issuecomment-129040241,https://api.github.com/repos/nsomar/OAStackView/issues/14
nsomar,OAStackView,90194180,https://api.github.com/repos/nsomar/OAStackView/issues/comments/114318253,114318253,MDEyOklzc3VlQ29tbWVudDExNDMxODI1Mw==,2119834,2015-06-23T01:27:42Z,2015-06-23T01:27:42Z,CONTRIBUTOR,"I believe the motivation is to match the iOS 9 implementation, which uses CATransformLayer. Presumably this is a performance optimization, since the stack view is essentially just a layout container. OAStackView should override `setOpaque:` in addition to `setBackgroundColor:` to be no-ops, as the UIKit version does.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/13/comments,https://github.com/nsomar/OAStackView/issues/13#issuecomment-114318253,https://api.github.com/repos/nsomar/OAStackView/issues/13
nsomar,OAStackView,90194180,https://api.github.com/repos/nsomar/OAStackView/issues/comments/115930353,115930353,MDEyOklzc3VlQ29tbWVudDExNTkzMDM1Mw==,1461052,2015-06-27T01:01:44Z,2015-06-27T01:01:44Z,OWNER,"Hello guys, I will be closing this issue as @garnett updated the lib to silence the log messages (Thanks @garnett  :+1: ).
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/13/comments,https://github.com/nsomar/OAStackView/issues/13#issuecomment-115930353,https://api.github.com/repos/nsomar/OAStackView/issues/13
nsomar,OAStackView,90147038,https://api.github.com/repos/nsomar/OAStackView/issues/comments/114219109,114219109,MDEyOklzc3VlQ29tbWVudDExNDIxOTEwOQ==,134170,2015-06-22T18:51:50Z,2015-06-22T18:51:50Z,CONTRIBUTOR,"Looking a bit further, it seems that the intrinsic content size should cover for this. Not an issue then, probably. Sorry about that, feel free to close this.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/12/comments,https://github.com/nsomar/OAStackView/issues/12#issuecomment-114219109,https://api.github.com/repos/nsomar/OAStackView/issues/12
nsomar,OAStackView,90147038,https://api.github.com/repos/nsomar/OAStackView/issues/comments/114315580,114315580,MDEyOklzc3VlQ29tbWVudDExNDMxNTU4MA==,1461052,2015-06-23T01:04:35Z,2015-06-23T01:04:35Z,OWNER,"Ok, will be closing this.
Thanks for your time :+1: 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/12/comments,https://github.com/nsomar/OAStackView/issues/12#issuecomment-114315580,https://api.github.com/repos/nsomar/OAStackView/issues/12
nsomar,OAStackView,89951414,https://api.github.com/repos/nsomar/OAStackView/issues/comments/119275049,119275049,MDEyOklzc3VlQ29tbWVudDExOTI3NTA0OQ==,864259,2015-07-07T17:24:34Z,2015-07-07T17:32:20Z,NONE,"As a workaround you can add 

myStackView.layoutIfNeeded() in your animation block to animate the change.

(This works on iOS 8)

```
@IBAction func touched(sender: UIButton) {
  UIView.animateWithDuration(0.2, animations: { () -> Void in
    sender.hidden = true
    sender.superview!.layoutIfNeeded()
  })
}
```

This will animate the arrangement giving you nice slide animations, the touched view will still disappear instantly, so its only a partial fix.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/11/comments,https://github.com/nsomar/OAStackView/issues/11#issuecomment-119275049,https://api.github.com/repos/nsomar/OAStackView/issues/11
nsomar,OAStackView,89951414,https://api.github.com/repos/nsomar/OAStackView/issues/comments/119275537,119275537,MDEyOklzc3VlQ29tbWVudDExOTI3NTUzNw==,134170,2015-07-07T17:26:02Z,2015-07-07T17:26:02Z,CONTRIBUTOR,"Normally a view that is set to be hidden will hide immediately. So is UIStackView doing some trickery there? Are they perhaps snapshotting the view or something?
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/11/comments,https://github.com/nsomar/OAStackView/issues/11#issuecomment-119275537,https://api.github.com/repos/nsomar/OAStackView/issues/11
nsomar,OAStackView,89951414,https://api.github.com/repos/nsomar/OAStackView/issues/comments/119275930,119275930,MDEyOklzc3VlQ29tbWVudDExOTI3NTkzMA==,864259,2015-07-07T17:27:45Z,2015-07-07T17:28:08Z,NONE,"in iOS9, it looks like hidden is an animatable property, but I would have to spike it to confirm. My code above simply animates the arrangement, the view still hides instantly. 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/11/comments,https://github.com/nsomar/OAStackView/issues/11#issuecomment-119275930,https://api.github.com/repos/nsomar/OAStackView/issues/11
nsomar,OAStackView,89951414,https://api.github.com/repos/nsomar/OAStackView/issues/comments/119276422,119276422,MDEyOklzc3VlQ29tbWVudDExOTI3NjQyMg==,134170,2015-07-07T17:29:56Z,2015-07-07T17:29:56Z,CONTRIBUTOR,"So that will probably not work pre iOS 9. Snapshotting could be an option, but is a bit hacky. Another way to animate a hiding view is to not set the view hidden, but instead remove it from the arranged views and animate its alpha. (This will need https://github.com/oarrabi/OAStackView/pull/17)
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/11/comments,https://github.com/nsomar/OAStackView/issues/11#issuecomment-119276422,https://api.github.com/repos/nsomar/OAStackView/issues/11
nsomar,OAStackView,89951414,https://api.github.com/repos/nsomar/OAStackView/issues/comments/119278011,119278011,MDEyOklzc3VlQ29tbWVudDExOTI3ODAxMQ==,864259,2015-07-07T17:35:24Z,2015-07-07T17:36:34Z,NONE,"are we wanting the scale animation? or simply a fade? I think setting alpha alongside hidden should achieve that in iOS 8 wouldn't it? 

FYI, the above gif is from TZStackView, not UIStackView, not sure if the scale is in the real thing.

I'd imagine anything we try to do to implement UIStackView in <iOS 9 is going to be hacky :D
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/11/comments,https://github.com/nsomar/OAStackView/issues/11#issuecomment-119278011,https://api.github.com/repos/nsomar/OAStackView/issues/11
nsomar,OAStackView,89951414,https://api.github.com/repos/nsomar/OAStackView/issues/comments/125663917,125663917,MDEyOklzc3VlQ29tbWVudDEyNTY2MzkxNw==,1911951,2015-07-28T16:02:53Z,2015-07-28T16:23:36Z,NONE,"Snapshotting the view may be a sensible approach as if you try and reduce a views width/height constraint to 0 during the animation it may break the views subviews constraints (if it has any) resulting in strange visual defects.

E.g. if a view added to a stack view has multiple subviews which have constraints which stop the view from going less than 20 points wide. If we animate the views width to 0 it will break the constraints. If we used a snapshot for this it wouldn't be a problem. However, shrinking the image made also look ugly? Maybe a shrink and fade of the snapshot would work best?

Alternatively it may be possible to add each arranged view to a container view where the arranged view it pinned to leading and trailing with a less than required priority. The container height/width would be based on the embedded arranged views height/width. That way if we animate the containers height/width to 0 we would get the same animation as the default UIStackView.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/11/comments,https://github.com/nsomar/OAStackView/issues/11#issuecomment-125663917,https://api.github.com/repos/nsomar/OAStackView/issues/11
nsomar,OAStackView,89951414,https://api.github.com/repos/nsomar/OAStackView/issues/comments/125779248,125779248,MDEyOklzc3VlQ29tbWVudDEyNTc3OTI0OA==,1461052,2015-07-28T23:16:32Z,2015-07-28T23:16:32Z,OWNER,"I just reviewed `UIStackView` on iOS 9 Beta 4. I dont think it snapshots the view.

Notice the following gif:
![record](https://cloud.githubusercontent.com/assets/1461052/8946233/bd0c00ca-3586-11e5-9f35-01378b305121.gif)

The constraints added to the grey view are no surprise:
<img width=""260"" alt=""constraints"" src=""https://cloud.githubusercontent.com/assets/1461052/8946250/de064c5e-3586-11e5-9993-0982930c1dff.png"">

Buttons are aligned top,bottom,center. The label has top/bottom/leading.

From the animation, I think that `UIStackView` removes the bottom constraint from the label before animating. However, I still dont know what it does to the buttons as they seem to move.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/11/comments,https://github.com/nsomar/OAStackView/issues/11#issuecomment-125779248,https://api.github.com/repos/nsomar/OAStackView/issues/11
nsomar,OAStackView,89951414,https://api.github.com/repos/nsomar/OAStackView/issues/comments/131870340,131870340,MDEyOklzc3VlQ29tbWVudDEzMTg3MDM0MA==,11229,2015-08-17T15:49:19Z,2015-08-17T15:49:19Z,NONE,"@oarrabi Do you have any updated thoughts on the direction here?
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/11/comments,https://github.com/nsomar/OAStackView/issues/11#issuecomment-131870340,https://api.github.com/repos/nsomar/OAStackView/issues/11
nsomar,OAStackView,89951414,https://api.github.com/repos/nsomar/OAStackView/issues/comments/159435383,159435383,MDEyOklzc3VlQ29tbWVudDE1OTQzNTM4Mw==,1461052,2015-11-24T23:11:25Z,2015-11-24T23:11:25Z,OWNER,"@mthole sorry for the very late answer. No I still don't have a clear solution here. would appreciate any help at the moment.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/11/comments,https://github.com/nsomar/OAStackView/issues/11#issuecomment-159435383,https://api.github.com/repos/nsomar/OAStackView/issues/11
nsomar,OAStackView,89951414,https://api.github.com/repos/nsomar/OAStackView/issues/comments/209045605,209045605,MDEyOklzc3VlQ29tbWVudDIwOTA0NTYwNQ==,482838,2016-04-12T18:36:07Z,2016-04-12T18:36:07Z,NONE,"Another bug I noticed is that if a subview starts out hidden and you try to unhide it with animation, it flies in offscreen.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/11/comments,https://github.com/nsomar/OAStackView/issues/11#issuecomment-209045605,https://api.github.com/repos/nsomar/OAStackView/issues/11
nsomar,OAStackView,89884290,https://api.github.com/repos/nsomar/OAStackView/issues/comments/113902770,113902770,MDEyOklzc3VlQ29tbWVudDExMzkwMjc3MA==,1461052,2015-06-21T13:42:51Z,2015-06-21T13:42:51Z,OWNER,"Hey,

I think the main difference is that its written in swift, which means you will have to include libswiftcore.dylib and other swift libs when compiling your app.

Another main difference is in the way both project deals with changing settings, while in OAStackView performance was key (since you are going to use the stackview in complicated layouts). Changing axes and other settings will not cause the stack view to remove all the constraints.

In TZStackView changes will invalidate the constraints and add them again, this might affect performance badly for bigger, more complicated UIs.

Other differences are in the testing approaches both project uses, I have to say that testing constraint is the best approach, I will be moving the project test suite to constraints in the upcoming days.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/10/comments,https://github.com/nsomar/OAStackView/issues/10#issuecomment-113902770,https://api.github.com/repos/nsomar/OAStackView/issues/10
nsomar,OAStackView,89884290,https://api.github.com/repos/nsomar/OAStackView/issues/comments/113915665,113915665,MDEyOklzc3VlQ29tbWVudDExMzkxNTY2NQ==,188839,2015-06-21T15:39:41Z,2015-06-21T15:39:41Z,NONE,"Thanks for the feedback â˜ºï¸

And sorry, I wasn't aware of this component when I created mine. I was participating WWDC when I heard about this new UIStackView and I was so enthusiastic about it that I decided to create it myself for earlier iOS versions.

But OAStackView has beaten me to the punch! I really did search for something similar when I started, but couldn't find anything!

Btw, another difference: I implemented all distribution options of the UIStackView, OAStackView did just do Fill and FillEqually. Also, just like with UIStackView, you can put subview.hidden = true inside an animation block and it will animate hiding/showing the subview. I implemented that as well with TZStackView
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/10/comments,https://github.com/nsomar/OAStackView/issues/10#issuecomment-113915665,https://api.github.com/repos/nsomar/OAStackView/issues/10
nsomar,OAStackView,89884290,https://api.github.com/repos/nsomar/OAStackView/issues/comments/113915960,113915960,MDEyOklzc3VlQ29tbWVudDExMzkxNTk2MA==,853032,2015-06-21T15:46:45Z,2015-06-21T15:46:45Z,CONTRIBUTOR,"It seems like TZStackView has more features implemented, but OAStackView is faster and has more recognition.

Maybe @tomvanzummeren should contribute back to OAStackView?
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/10/comments,https://github.com/nsomar/OAStackView/issues/10#issuecomment-113915960,https://api.github.com/repos/nsomar/OAStackView/issues/10
nsomar,OAStackView,89884290,https://api.github.com/repos/nsomar/OAStackView/issues/comments/113918258,113918258,MDEyOklzc3VlQ29tbWVudDExMzkxODI1OA==,1461052,2015-06-21T15:58:22Z,2015-06-21T15:58:22Z,OWNER,"@tomvanzummeren and @harlanhaskins The other distribution options have been introduced today by PR https://github.com/oarrabi/OAStackView/pull/8. the readme will be updated to reflect this addition soon.

I am not sure I understand the hiding argument, but OAStackView does indeed handle hiding the views, I didn't try, but setting a subview hidden inside a beginAnimation block should animate it as I only manipulate the layout constraints.

@tomvanzummeren and @harlanhaskins  I am open to discus any contribution option guys :)
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/10/comments,https://github.com/nsomar/OAStackView/issues/10#issuecomment-113918258,https://api.github.com/repos/nsomar/OAStackView/issues/10
nsomar,OAStackView,89884290,https://api.github.com/repos/nsomar/OAStackView/issues/comments/113918562,113918562,MDEyOklzc3VlQ29tbWVudDExMzkxODU2Mg==,188839,2015-06-21T16:04:01Z,2015-06-21T16:04:01Z,NONE,"Try it. Animating the hidden property of a subview doesn't just work out of the box. That's why it's mentioned in a separate part of the UIStackView docs.

But anyway, we both took different approaches. No use to contribute. OAStackView did it with performance in mind and is written in ObjC. Mine is in Swift and I decided performance is not important as you're not adding and removing subviews multiple times per second. I also can't imagine that UIStackView is doing it any differently. With TZStackView I focussed on having the exact same API and behaviour to be able to drop-in replace it with the real UIStackView and see no difference.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/10/comments,https://github.com/nsomar/OAStackView/issues/10#issuecomment-113918562,https://api.github.com/repos/nsomar/OAStackView/issues/10
nsomar,OAStackView,89884290,https://api.github.com/repos/nsomar/OAStackView/issues/comments/113918676,113918676,MDEyOklzc3VlQ29tbWVudDExMzkxODY3Ng==,188839,2015-06-21T16:07:13Z,2015-06-21T16:07:13Z,NONE,"And also:

I created TZStackView just because I wanted it for myself. This time I decided to share it with everyone by putting it on GitHub. I can take it down if it is not appreciated, but I won't contribute. It already took way too much of my spare time to create this one.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/10/comments,https://github.com/nsomar/OAStackView/issues/10#issuecomment-113918676,https://api.github.com/repos/nsomar/OAStackView/issues/10
nsomar,OAStackView,89884290,https://api.github.com/repos/nsomar/OAStackView/issues/comments/113927882,113927882,MDEyOklzc3VlQ29tbWVudDExMzkyNzg4Mg==,1461052,2015-06-21T16:56:36Z,2015-06-21T16:58:43Z,OWNER,"@tomvanzummeren This is what happens when you hide a view inside an animation block
![369f8e6eed](https://cloud.githubusercontent.com/assets/1461052/8272550/c1bec77a-183e-11e5-96f2-137177e6ca3e.gif)

The change was to explicitly call `layoutIfNeeded` in the animation block.
I know that hidden is not animated, but OAStackView has KVO observation on the subviews, that will remove the view and rearrange the constraints accordingly. 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/10/comments,https://github.com/nsomar/OAStackView/issues/10#issuecomment-113927882,https://api.github.com/repos/nsomar/OAStackView/issues/10
nsomar,OAStackView,89884290,https://api.github.com/repos/nsomar/OAStackView/issues/comments/113937918,113937918,MDEyOklzc3VlQ29tbWVudDExMzkzNzkxOA==,188839,2015-06-21T18:21:02Z,2015-06-21T18:21:02Z,NONE,"Well that's no good, is it? The red block disappears immediately. With the real UIStackView, it collapses before it disappears.

On Sun, Jun 21, 2015 at 6:56 PM, Omar Abdelhafith
notifications@github.com wrote:

> @tomvanzummeren This is what happens when you hide a view inside an animation block
> ![369f8e6eed](https://cloud.githubusercontent.com/assets/1461052/8272550/c1bec77a-183e-11e5-96f2-137177e6ca3e.gif)
> 
> ## The change was to explicitly call `layoutIfNeeded` in the animation block.
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/oarrabi/OAStackView/issues/10#issuecomment-113927882
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/10/comments,https://github.com/nsomar/OAStackView/issues/10#issuecomment-113937918,https://api.github.com/repos/nsomar/OAStackView/issues/10
nsomar,OAStackView,89884290,https://api.github.com/repos/nsomar/OAStackView/issues/comments/113938604,113938604,MDEyOklzc3VlQ29tbWVudDExMzkzODYwNA==,188839,2015-06-21T18:37:10Z,2015-06-21T18:40:56Z,NONE,"This is what I mean:

![TZStackView hidden animation](https://github.com/tomvanzummeren/TZStackView/raw/master/assets/TZStackView-hide-animation.gif)

I also implemented it with KVO.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/10/comments,https://github.com/nsomar/OAStackView/issues/10#issuecomment-113938604,https://api.github.com/repos/nsomar/OAStackView/issues/10
nsomar,OAStackView,89884290,https://api.github.com/repos/nsomar/OAStackView/issues/comments/113946189,113946189,MDEyOklzc3VlQ29tbWVudDExMzk0NjE4OQ==,1461052,2015-06-21T19:31:38Z,2015-06-21T19:31:38Z,OWNER,"@tomvanzummeren oh, now I see, thanks, I will create an issue for that
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/10/comments,https://github.com/nsomar/OAStackView/issues/10#issuecomment-113946189,https://api.github.com/repos/nsomar/OAStackView/issues/10
nsomar,OAStackView,89884290,https://api.github.com/repos/nsomar/OAStackView/issues/comments/113946432,113946432,MDEyOklzc3VlQ29tbWVudDExMzk0NjQzMg==,188839,2015-06-21T19:32:24Z,2015-06-21T19:32:24Z,NONE,"Oh wow, so I am contributing after all. Happy to help!
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/10/comments,https://github.com/nsomar/OAStackView/issues/10#issuecomment-113946432,https://api.github.com/repos/nsomar/OAStackView/issues/10
nsomar,OAStackView,89884290,https://api.github.com/repos/nsomar/OAStackView/issues/comments/113947399,113947399,MDEyOklzc3VlQ29tbWVudDExMzk0NzM5OQ==,1461052,2015-06-21T19:35:24Z,2015-06-21T19:35:46Z,OWNER,"@tomvanzummeren thanks for the help :) I will close this issue and open the another one.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/10/comments,https://github.com/nsomar/OAStackView/issues/10#issuecomment-113947399,https://api.github.com/repos/nsomar/OAStackView/issues/10
nsomar,OAStackView,89846195,https://api.github.com/repos/nsomar/OAStackView/issues/comments/113893703,113893703,MDEyOklzc3VlQ29tbWVudDExMzg5MzcwMw==,1461052,2015-06-21T12:21:55Z,2015-06-21T12:21:55Z,OWNER,"Nice catch :+1: 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/9/comments,https://github.com/nsomar/OAStackView/pull/9#issuecomment-113893703,https://api.github.com/repos/nsomar/OAStackView/issues/9
nsomar,OAStackView,89411375,https://api.github.com/repos/nsomar/OAStackView/issues/comments/113320601,113320601,MDEyOklzc3VlQ29tbWVudDExMzMyMDYwMQ==,1461052,2015-06-18T23:49:16Z,2015-06-18T23:49:16Z,OWNER,"Hello, super cool addition :+1: 
Are you planning on adding some test coverage for these distribution? If not, then I might add coverage and merge it over the weekend.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/8/comments,https://github.com/nsomar/OAStackView/pull/8#issuecomment-113320601,https://api.github.com/repos/nsomar/OAStackView/issues/8
nsomar,OAStackView,89411375,https://api.github.com/repos/nsomar/OAStackView/issues/comments/113388695,113388695,MDEyOklzc3VlQ29tbWVudDExMzM4ODY5NQ==,58052,2015-06-19T06:24:50Z,2015-06-19T06:24:50Z,CONTRIBUTOR,"Heya, not sure if I'll have time over the weekend to add tests. 

If I do, I'll try to add some tests. If you've already added them by then, I might try to fill in some other feature, so by all means, don't wait for me :)
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/8/comments,https://github.com/nsomar/OAStackView/pull/8#issuecomment-113388695,https://api.github.com/repos/nsomar/OAStackView/issues/8
nsomar,OAStackView,89411375,https://api.github.com/repos/nsomar/OAStackView/issues/comments/113897275,113897275,MDEyOklzc3VlQ29tbWVudDExMzg5NzI3NQ==,58052,2015-06-21T12:51:45Z,2015-06-21T12:51:45Z,CONTRIBUTOR,"Quick side note, the tests check that the proportional view is within a range of 1 point of the 'calculated reference value', because AutoLayout tries prevent misaligned views (i.e. it will round).
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/8/comments,https://github.com/nsomar/OAStackView/pull/8#issuecomment-113897275,https://api.github.com/repos/nsomar/OAStackView/issues/8
nsomar,OAStackView,89411375,https://api.github.com/repos/nsomar/OAStackView/issues/comments/113903186,113903186,MDEyOklzc3VlQ29tbWVudDExMzkwMzE4Ng==,1461052,2015-06-21T13:48:53Z,2015-06-21T13:48:53Z,OWNER,"Wonderfull addition :+1: merging right away :)
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/8/comments,https://github.com/nsomar/OAStackView/pull/8#issuecomment-113903186,https://api.github.com/repos/nsomar/OAStackView/issues/8
nsomar,OAStackView,89383052,https://api.github.com/repos/nsomar/OAStackView/issues/comments/113338343,113338343,MDEyOklzc3VlQ29tbWVudDExMzMzODM0Mw==,2636382,2015-06-19T01:51:27Z,2015-06-19T01:51:27Z,CONTRIBUTOR,"NSArray *constraints = [self constraintsBetweenView:self andView:previousView inAxis:self.axis];
[self removeConstraints:constraints];

The code above removes the left and right constraints of the previousView.
The left constraint is never replaced only the right constraint is. This code creates a flag which determines if it is the previousView is the first view and replaces the missing constraint.

I don't know if this is the best way to fix this, but its working for me.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/7/comments,https://github.com/nsomar/OAStackView/pull/7#issuecomment-113338343,https://api.github.com/repos/nsomar/OAStackView/issues/7
nsomar,OAStackView,89383052,https://api.github.com/repos/nsomar/OAStackView/issues/comments/114315651,114315651,MDEyOklzc3VlQ29tbWVudDExNDMxNTY1MQ==,1461052,2015-06-23T01:05:17Z,2015-06-23T01:05:17Z,OWNER,"Sorry for being late on this, I will definitely work on this tomorrow.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/7/comments,https://github.com/nsomar/OAStackView/pull/7#issuecomment-114315651,https://api.github.com/repos/nsomar/OAStackView/issues/7
nsomar,OAStackView,89383052,https://api.github.com/repos/nsomar/OAStackView/issues/comments/114995270,114995270,MDEyOklzc3VlQ29tbWVudDExNDk5NTI3MA==,1461052,2015-06-24T20:07:20Z,2015-06-24T20:07:20Z,OWNER,"Hey, I merged this branch from command line since I did a rebase on it. Now it works great.
Before closing this PR, can you confirm that current master solves your issue?

Thanks for your time :)
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/7/comments,https://github.com/nsomar/OAStackView/pull/7#issuecomment-114995270,https://api.github.com/repos/nsomar/OAStackView/issues/7
nsomar,OAStackView,89383052,https://api.github.com/repos/nsomar/OAStackView/issues/comments/115990842,115990842,MDEyOklzc3VlQ29tbWVudDExNTk5MDg0Mg==,134170,2015-06-27T09:13:42Z,2015-06-27T09:13:42Z,CONTRIBUTOR,"FWIW, I encountered layout issues that were solved by merging in the changes from this PR.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/7/comments,https://github.com/nsomar/OAStackView/pull/7#issuecomment-115990842,https://api.github.com/repos/nsomar/OAStackView/issues/7
nsomar,OAStackView,89383052,https://api.github.com/repos/nsomar/OAStackView/issues/comments/116035630,116035630,MDEyOklzc3VlQ29tbWVudDExNjAzNTYzMA==,2636382,2015-06-27T12:33:56Z,2015-06-27T12:33:56Z,CONTRIBUTOR,"Sorry for the late response. Yea it works :+1: 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/7/comments,https://github.com/nsomar/OAStackView/pull/7#issuecomment-116035630,https://api.github.com/repos/nsomar/OAStackView/issues/7
nsomar,OAStackView,89383052,https://api.github.com/repos/nsomar/OAStackView/issues/comments/116111794,116111794,MDEyOklzc3VlQ29tbWVudDExNjExMTc5NA==,1461052,2015-06-27T18:08:57Z,2015-06-27T18:08:57Z,OWNER,"I will be closing this as it seems to have solved @Thomvis  and @izackp, thanks guys for your help :)
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/7/comments,https://github.com/nsomar/OAStackView/pull/7#issuecomment-116111794,https://api.github.com/repos/nsomar/OAStackView/issues/7
nsomar,OAStackView,89383052,https://api.github.com/repos/nsomar/OAStackView/issues/comments/127763795,127763795,MDEyOklzc3VlQ29tbWVudDEyNzc2Mzc5NQ==,2636382,2015-08-04T21:13:36Z,2015-08-05T14:58:05Z,CONTRIBUTOR,"This problem is broken again (edit: nevermind, there was a bug with cocopods where :head was not pulling the latest commit. The simple solution was to update cocopods.)
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/7/comments,https://github.com/nsomar/OAStackView/pull/7#issuecomment-127763795,https://api.github.com/repos/nsomar/OAStackView/issues/7
nsomar,OAStackView,89379079,https://api.github.com/repos/nsomar/OAStackView/issues/comments/113319958,113319958,MDEyOklzc3VlQ29tbWVudDExMzMxOTk1OA==,1461052,2015-06-18T23:43:07Z,2015-06-18T23:43:07Z,OWNER,"I didnt really try it on iOS6, I was planning to confirm iOS 6 correctness over the weekend.
Did you by any chance try it on iOS6?
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/6/comments,https://github.com/nsomar/OAStackView/pull/6#issuecomment-113319958,https://api.github.com/repos/nsomar/OAStackView/issues/6
nsomar,OAStackView,89379079,https://api.github.com/repos/nsomar/OAStackView/issues/comments/113320033,113320033,MDEyOklzc3VlQ29tbWVudDExMzMyMDAzMw==,108218,2015-06-18T23:43:52Z,2015-06-18T23:47:21Z,CONTRIBUTOR,"I don't have an iOS 6 device or the simulator. It does compile with iOS 6 as a deployment target.  My recommendation would be to drop iOS 6 support and just make this the best it can be on iOS 7 & 8
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/6/comments,https://github.com/nsomar/OAStackView/pull/6#issuecomment-113320033,https://api.github.com/repos/nsomar/OAStackView/issues/6
nsomar,OAStackView,89379079,https://api.github.com/repos/nsomar/OAStackView/issues/comments/113893759,113893759,MDEyOklzc3VlQ29tbWVudDExMzg5Mzc1OQ==,1461052,2015-06-21T12:23:38Z,2015-06-21T12:23:38Z,OWNER,"Hey, I think that is sound, I will update merge this and drop iOS 6 :+1: 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/6/comments,https://github.com/nsomar/OAStackView/pull/6#issuecomment-113893759,https://api.github.com/repos/nsomar/OAStackView/issues/6
nsomar,OAStackView,89363630,https://api.github.com/repos/nsomar/OAStackView/issues/comments/113320305,113320305,MDEyOklzc3VlQ29tbWVudDExMzMyMDMwNQ==,1461052,2015-06-18T23:46:27Z,2015-06-18T23:46:27Z,OWNER,"Thanks :+1: 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/5/comments,https://github.com/nsomar/OAStackView/pull/5#issuecomment-113320305,https://api.github.com/repos/nsomar/OAStackView/issues/5
nsomar,OAStackView,88800194,https://api.github.com/repos/nsomar/OAStackView/issues/comments/112532538,112532538,MDEyOklzc3VlQ29tbWVudDExMjUzMjUzOA==,1461052,2015-06-16T19:11:27Z,2015-06-16T19:11:27Z,OWNER,"Great catch, Thanks :+1: 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/4/comments,https://github.com/nsomar/OAStackView/pull/4#issuecomment-112532538,https://api.github.com/repos/nsomar/OAStackView/issues/4
nsomar,OAStackView,88741254,https://api.github.com/repos/nsomar/OAStackView/issues/comments/112448568,112448568,MDEyOklzc3VlQ29tbWVudDExMjQ0ODU2OA==,1461052,2015-06-16T14:21:56Z,2015-06-16T14:21:56Z,OWNER,"Great PR, Thanks :+1: 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/3/comments,https://github.com/nsomar/OAStackView/pull/3#issuecomment-112448568,https://api.github.com/repos/nsomar/OAStackView/issues/3
nsomar,OAStackView,88734535,https://api.github.com/repos/nsomar/OAStackView/issues/comments/112442866,112442866,MDEyOklzc3VlQ29tbWVudDExMjQ0Mjg2Ng==,829783,2015-06-16T13:59:37Z,2015-06-16T13:59:37Z,COLLABORATOR,"Maybe it makes sense to use approach from here to fallback to original iOS 9 implementation: https://github.com/steipete/PSTCollectionView/blob/master/PSTCollectionView/PSTCollectionView.h
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/2/comments,https://github.com/nsomar/OAStackView/issues/2#issuecomment-112442866,https://api.github.com/repos/nsomar/OAStackView/issues/2
nsomar,OAStackView,88734535,https://api.github.com/repos/nsomar/OAStackView/issues/comments/112597021,112597021,MDEyOklzc3VlQ29tbWVudDExMjU5NzAyMQ==,1461052,2015-06-16T22:59:05Z,2015-06-16T22:59:05Z,OWNER,"I am still not sure about how to approach fallback here, as most of the time you would use the stackview from the interface builder. `PSUICollectionView` sounds very promising.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/2/comments,https://github.com/nsomar/OAStackView/issues/2#issuecomment-112597021,https://api.github.com/repos/nsomar/OAStackView/issues/2
nsomar,OAStackView,88734535,https://api.github.com/repos/nsomar/OAStackView/issues/comments/113966864,113966864,MDEyOklzc3VlQ29tbWVudDExMzk2Njg2NA==,58493,2015-06-21T23:06:36Z,2015-06-21T23:06:36Z,NONE,"There are ways to parse things from the interface builder by reading the undocumented keys from `NSCoder` - we did that in PSTCollectionView as well. It's quite a lot trial and error though. I'd first aim for source code level compatibility before starting that task.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/2/comments,https://github.com/nsomar/OAStackView/issues/2#issuecomment-113966864,https://api.github.com/repos/nsomar/OAStackView/issues/2
nsomar,OAStackView,88734535,https://api.github.com/repos/nsomar/OAStackView/issues/comments/114978649,114978649,MDEyOklzc3VlQ29tbWVudDExNDk3ODY0OQ==,1461052,2015-06-24T18:50:04Z,2015-06-24T18:50:04Z,OWNER,"@steipete Thanks for the info, that is very helpful!, Indeed I still have a couple of loose ends to tie in order to be feature complete. I will definitely checkout PSTCollectionView as a reference on how to do seamless integration with backward compatibility.

Thanks :)
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/2/comments,https://github.com/nsomar/OAStackView/issues/2#issuecomment-114978649,https://api.github.com/repos/nsomar/OAStackView/issues/2
nsomar,OAStackView,88734535,https://api.github.com/repos/nsomar/OAStackView/issues/comments/119759205,119759205,MDEyOklzc3VlQ29tbWVudDExOTc1OTIwNQ==,209712,2015-07-08T23:32:40Z,2015-07-08T23:32:40Z,NONE,"@oarrabi I definitely support this issue. Very much desire the PSTCollectionView strategy so we use Apple's implementation on iOS 9. :+1: 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/2/comments,https://github.com/nsomar/OAStackView/issues/2#issuecomment-119759205,https://api.github.com/repos/nsomar/OAStackView/issues/2
nsomar,OAStackView,88734535,https://api.github.com/repos/nsomar/OAStackView/issues/comments/128026561,128026561,MDEyOklzc3VlQ29tbWVudDEyODAyNjU2MQ==,1164565,2015-08-05T14:59:13Z,2015-08-05T14:59:13Z,NONE,"I did the initial part of what @steipete suggested - I created a solution that handles that, but only on code - I called it StackViewProxy. (Any suggestions for a better name?)
I will create a PR soon, but basically, it is a `UIView` that has the stack view (`UIStackView` on iOS 9, `OAStackView` on iOS 8), and passes on to the correct and existing stack view.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/2/comments,https://github.com/nsomar/OAStackView/issues/2#issuecomment-128026561,https://api.github.com/repos/nsomar/OAStackView/issues/2
nsomar,OAStackView,88734535,https://api.github.com/repos/nsomar/OAStackView/issues/comments/128032041,128032041,MDEyOklzc3VlQ29tbWVudDEyODAzMjA0MQ==,853032,2015-08-05T15:10:41Z,2015-08-05T15:10:41Z,CONTRIBUTOR,"It might be more reliable to use an [`NSProxy`](https://developer.apple.com/library/prerelease/ios/documentation/Cocoa/Reference/Foundation/Classes/NSProxy_Class/index.html) instead.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/2/comments,https://github.com/nsomar/OAStackView/issues/2#issuecomment-128032041,https://api.github.com/repos/nsomar/OAStackView/issues/2
nsomar,OAStackView,88734535,https://api.github.com/repos/nsomar/OAStackView/issues/comments/128041283,128041283,MDEyOklzc3VlQ29tbWVudDEyODA0MTI4Mw==,1164565,2015-08-05T15:32:09Z,2015-08-05T15:32:09Z,NONE,"You are right. But I want it to work like this

`myView.addSubview(stackViewProxy)`

Using the UIView approach, it would work. Can the same be said with the NSProxy approach?
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/2/comments,https://github.com/nsomar/OAStackView/issues/2#issuecomment-128041283,https://api.github.com/repos/nsomar/OAStackView/issues/2
nsomar,OAStackView,88734535,https://api.github.com/repos/nsomar/OAStackView/issues/comments/131270609,131270609,MDEyOklzc3VlQ29tbWVudDEzMTI3MDYwOQ==,11229,2015-08-15T00:07:59Z,2015-08-15T00:07:59Z,NONE,"How about just using `@compatibility_alias`? See http://nshipster.com/at-compiler-directives/
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/2/comments,https://github.com/nsomar/OAStackView/issues/2#issuecomment-131270609,https://api.github.com/repos/nsomar/OAStackView/issues/2
nsomar,OAStackView,88734535,https://api.github.com/repos/nsomar/OAStackView/issues/comments/131272819,131272819,MDEyOklzc3VlQ29tbWVudDEzMTI3MjgxOQ==,853032,2015-08-15T00:32:01Z,2015-08-15T00:32:01Z,CONTRIBUTOR,"Another vote here for `@compatibility_alias`
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/2/comments,https://github.com/nsomar/OAStackView/issues/2#issuecomment-131272819,https://api.github.com/repos/nsomar/OAStackView/issues/2
nsomar,OAStackView,88734535,https://api.github.com/repos/nsomar/OAStackView/issues/comments/131532715,131532715,MDEyOklzc3VlQ29tbWVudDEzMTUzMjcxNQ==,1164565,2015-08-16T12:03:34Z,2015-08-16T12:03:34Z,NONE,"Indeed that's great! Thanks for the suggestion, I wasn't aware about it, @mthole and @harlanhaskins
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/2/comments,https://github.com/nsomar/OAStackView/issues/2#issuecomment-131532715,https://api.github.com/repos/nsomar/OAStackView/issues/2
nsomar,OAStackView,88734535,https://api.github.com/repos/nsomar/OAStackView/issues/comments/148323445,148323445,MDEyOklzc3VlQ29tbWVudDE0ODMyMzQ0NQ==,1565368,2015-10-15T09:01:20Z,2015-10-15T09:01:20Z,CONTRIBUTOR,"I made a better way to maintain this, using runtime injection for <iOS9, this allow you to maintain OAStackView as normal UIStackView on storyboard, reference https://github.com/oarrabi/OAStackView/pull/60
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/2/comments,https://github.com/nsomar/OAStackView/issues/2#issuecomment-148323445,https://api.github.com/repos/nsomar/OAStackView/issues/2
nsomar,OAStackView,88734535,https://api.github.com/repos/nsomar/OAStackView/issues/comments/188134206,188134206,MDEyOklzc3VlQ29tbWVudDE4ODEzNDIwNg==,6574383,2016-02-24T08:12:43Z,2016-02-24T08:13:03Z,NONE,"Runtime injection doesn't work for me for some reason.
[[UIStackView alloc] init] returns nil, while objc_getClass(""UIStackView"") returnes a valid class.
iOS 8.3 
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/2/comments,https://github.com/nsomar/OAStackView/issues/2#issuecomment-188134206,https://api.github.com/repos/nsomar/OAStackView/issues/2
nsomar,OAStackView,88734535,https://api.github.com/repos/nsomar/OAStackView/issues/comments/188135277,188135277,MDEyOklzc3VlQ29tbWVudDE4ODEzNTI3Nw==,1565368,2016-02-24T08:18:26Z,2016-02-24T08:18:26Z,CONTRIBUTOR,"For UIStackView alloc] init you would need asm instruction to swich UIStackView symbols like here: https://github.com/forkingdog/FDStackView/blob/master/FDStackView/FDStackView.m#L240 which authors didn't want to include. UIStackView works from storyboard fine.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/2/comments,https://github.com/nsomar/OAStackView/issues/2#issuecomment-188135277,https://api.github.com/repos/nsomar/OAStackView/issues/2
nsomar,OAStackView,88734535,https://api.github.com/repos/nsomar/OAStackView/issues/comments/188136216,188136216,MDEyOklzc3VlQ29tbWVudDE4ODEzNjIxNg==,6574383,2016-02-24T08:22:44Z,2016-02-24T08:22:44Z,NONE,"I see. Thanks!
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/2/comments,https://github.com/nsomar/OAStackView/issues/2#issuecomment-188136216,https://api.github.com/repos/nsomar/OAStackView/issues/2
nsomar,OAStackView,88734535,https://api.github.com/repos/nsomar/OAStackView/issues/comments/188452113,188452113,MDEyOklzc3VlQ29tbWVudDE4ODQ1MjExMw==,209712,2016-02-24T21:03:18Z,2016-02-24T21:03:18Z,NONE,"@mindz-eye If you're not interested in ASM instructions: you could probably pull in the relevant file from this PR for your UIStackView https://github.com/tomvanzummeren/TZStackView/pull/61. It uses a preprocessor trick rather than assembly to make Objective-C work seamlessly. -- Doesn't work for Interface Builder.
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/2/comments,https://github.com/nsomar/OAStackView/issues/2#issuecomment-188452113,https://api.github.com/repos/nsomar/OAStackView/issues/2
nsomar,OAStackView,88594087,https://api.github.com/repos/nsomar/OAStackView/issues/comments/112444729,112444729,MDEyOklzc3VlQ29tbWVudDExMjQ0NDcyOQ==,1461052,2015-06-16T14:08:23Z,2015-06-16T14:08:23Z,OWNER,"Great!! Thanks for the catch
",NA,https://api.github.com/repos/nsomar/OAStackView/issues/1/comments,https://github.com/nsomar/OAStackView/pull/1#issuecomment-112444729,https://api.github.com/repos/nsomar/OAStackView/issues/1
nsomar,Guaka,501502951,https://api.github.com/repos/nsomar/Guaka/issues/comments/537503538,537503538,MDEyOklzc3VlQ29tbWVudDUzNzUwMzUzOA==,8655789,2019-10-02T13:55:11Z,2019-10-02T13:55:11Z,NONE,"# [Codecov](https://codecov.io/gh/nsomar/Guaka/pull/99?src=pr&el=h1) Report
> Merging [#99](https://codecov.io/gh/nsomar/Guaka/pull/99?src=pr&el=desc) into [master](https://codecov.io/gh/nsomar/Guaka/commit/297a1f2ab358079e9f8e4f4a3296499824d10319?src=pr&el=desc) will **increase** coverage by `0.03%`.
> The diff coverage is `97.56%`.

[![Impacted file tree graph](https://codecov.io/gh/nsomar/Guaka/pull/99/graphs/tree.svg?width=650&token=vZ33EVrwnP&height=150&src=pr)](https://codecov.io/gh/nsomar/Guaka/pull/99?src=pr&el=tree)

```diff
@@            Coverage Diff             @@
##           master      #99      +/-   ##
==========================================
+ Coverage   97.05%   97.09%   +0.03%     
==========================================
  Files          40       40              
  Lines        3366     3405      +39     
==========================================
+ Hits         3267     3306      +39     
  Misses         99       99
```


| [Impacted Files](https://codecov.io/gh/nsomar/Guaka/pull/99?src=pr&el=tree) | Coverage Î” | |
|---|---|---|
| [Tests/GuakaTests/TestHelpers.swift](https://codecov.io/gh/nsomar/Guaka/pull/99/diff?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9UZXN0SGVscGVycy5zd2lmdA==) | `92.42% <100%> (+0.11%)` | :arrow_up: |
| [Tests/GuakaTests/CommandExecutionTests.swift](https://codecov.io/gh/nsomar/Guaka/pull/99/diff?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9Db21tYW5kRXhlY3V0aW9uVGVzdHMuc3dpZnQ=) | `100% <100%> (Ã¸)` | :arrow_up: |
| [Tests/GuakaTests/CommandTests.swift](https://codecov.io/gh/nsomar/Guaka/pull/99/diff?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9Db21tYW5kVGVzdHMuc3dpZnQ=) | `95.18% <100%> (+0.44%)` | :arrow_up: |
| [Sources/Guaka/Command/Command.swift](https://codecov.io/gh/nsomar/Guaka/pull/99/diff?src=pr&el=tree#diff-U291cmNlcy9HdWFrYS9Db21tYW5kL0NvbW1hbmQuc3dpZnQ=) | `78.57% <100%> (Ã¸)` | :arrow_up: |
| [Sources/Guaka/Parsing/Command+Parsing.swift](https://codecov.io/gh/nsomar/Guaka/pull/99/diff?src=pr&el=tree#diff-U291cmNlcy9HdWFrYS9QYXJzaW5nL0NvbW1hbmQrUGFyc2luZy5zd2lmdA==) | `97.87% <88.88%> (-2.13%)` | :arrow_down: |
| [Tests/GuakaTests/FlagSetTests.swift](https://codecov.io/gh/nsomar/Guaka/pull/99/diff?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9GbGFnU2V0VGVzdHMuc3dpZnQ=) | `96.42% <0%> (+0.39%)` | :arrow_up: |

------

[Continue to review full report at Codecov](https://codecov.io/gh/nsomar/Guaka/pull/99?src=pr&el=continue).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
> `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/nsomar/Guaka/pull/99?src=pr&el=footer). Last update [297a1f2...35bfea0](https://codecov.io/gh/nsomar/Guaka/pull/99?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).
",NA,https://api.github.com/repos/nsomar/Guaka/issues/99/comments,https://github.com/nsomar/Guaka/pull/99#issuecomment-537503538,https://api.github.com/repos/nsomar/Guaka/issues/99
nsomar,Guaka,476916718,https://api.github.com/repos/nsomar/Guaka/issues/comments/518277605,518277605,MDEyOklzc3VlQ29tbWVudDUxODI3NzYwNQ==,8655789,2019-08-05T15:18:01Z,2019-08-06T13:29:22Z,NONE,"# [Codecov](https://codecov.io/gh/nsomar/Guaka/pull/97?src=pr&el=h1) Report
> Merging [#97](https://codecov.io/gh/nsomar/Guaka/pull/97?src=pr&el=desc) into [master](https://codecov.io/gh/nsomar/Guaka/commit/aa9995d6b05f097ebe40c7106ec753580a41fa6c?src=pr&el=desc) will **decrease** coverage by `0.03%`.
> The diff coverage is `94.73%`.

[![Impacted file tree graph](https://codecov.io/gh/nsomar/Guaka/pull/97/graphs/tree.svg?width=650&token=vZ33EVrwnP&height=150&src=pr)](https://codecov.io/gh/nsomar/Guaka/pull/97?src=pr&el=tree)

```diff
@@            Coverage Diff             @@
##           master      #97      +/-   ##
==========================================
- Coverage   97.09%   97.05%   -0.04%     
==========================================
  Files          40       40              
  Lines        3368     3366       -2     
==========================================
- Hits         3270     3267       -3     
- Misses         98       99       +1
```


| [Impacted Files](https://codecov.io/gh/nsomar/Guaka/pull/97?src=pr&el=tree) | Coverage Î” | |
|---|---|---|
| [Sources/Guaka/Command/Command+Execution.swift](https://codecov.io/gh/nsomar/Guaka/pull/97/diff?src=pr&el=tree#diff-U291cmNlcy9HdWFrYS9Db21tYW5kL0NvbW1hbmQrRXhlY3V0aW9uLnN3aWZ0) | `84.7% <100%> (-0.36%)` | :arrow_down: |
| [Tests/GuakaTests/FlagSetTests.swift](https://codecov.io/gh/nsomar/Guaka/pull/97/diff?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9GbGFnU2V0VGVzdHMuc3dpZnQ=) | `96.03% <100%> (-0.4%)` | :arrow_down: |
| [Sources/Guaka/Execution/CommandExecution.swift](https://codecov.io/gh/nsomar/Guaka/pull/97/diff?src=pr&el=tree#diff-U291cmNlcy9HdWFrYS9FeGVjdXRpb24vQ29tbWFuZEV4ZWN1dGlvbi5zd2lmdA==) | `98.73% <90.9%> (Ã¸)` | :arrow_up: |

------

[Continue to review full report at Codecov](https://codecov.io/gh/nsomar/Guaka/pull/97?src=pr&el=continue).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
> `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/nsomar/Guaka/pull/97?src=pr&el=footer). Last update [aa9995d...41bf9ae](https://codecov.io/gh/nsomar/Guaka/pull/97?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).
",NA,https://api.github.com/repos/nsomar/Guaka/issues/97/comments,https://github.com/nsomar/Guaka/pull/97#issuecomment-518277605,https://api.github.com/repos/nsomar/Guaka/issues/97
nsomar,Guaka,445625600,https://api.github.com/repos/nsomar/Guaka/issues/comments/493691819,493691819,MDEyOklzc3VlQ29tbWVudDQ5MzY5MTgxOQ==,122065,2019-05-18T17:03:51Z,2019-05-18T17:03:51Z,CONTRIBUTOR,Thanks for tagging the repos too!,NA,https://api.github.com/repos/nsomar/Guaka/issues/96/comments,https://github.com/nsomar/Guaka/pull/96#issuecomment-493691819,https://api.github.com/repos/nsomar/Guaka/issues/96
nsomar,Guaka,425491392,https://api.github.com/repos/nsomar/Guaka/issues/comments/476963493,476963493,MDEyOklzc3VlQ29tbWVudDQ3Njk2MzQ5Mw==,8655789,2019-03-27T04:03:24Z,2019-03-27T04:03:24Z,NONE,"# [Codecov](https://codecov.io/gh/nsomar/Guaka/pull/95?src=pr&el=h1) Report
> Merging [#95](https://codecov.io/gh/nsomar/Guaka/pull/95?src=pr&el=desc) into [master](https://codecov.io/gh/nsomar/Guaka/commit/f3da5d6d45373bcd17a5fbb116c33c0b70c09680?src=pr&el=desc) will **increase** coverage by `1.98%`.
> The diff coverage is `100%`.

[![Impacted file tree graph](https://codecov.io/gh/nsomar/Guaka/pull/95/graphs/tree.svg?width=650&token=vZ33EVrwnP&height=150&src=pr)](https://codecov.io/gh/nsomar/Guaka/pull/95?src=pr&el=tree)

```diff
@@            Coverage Diff             @@
##           master      #95      +/-   ##
==========================================
+ Coverage   94.95%   96.94%   +1.98%     
==========================================
  Files          40       40              
  Lines        3072     3072              
==========================================
+ Hits         2917     2978      +61     
+ Misses        155       94      -61
```


| [Impacted Files](https://codecov.io/gh/nsomar/Guaka/pull/95?src=pr&el=tree) | Coverage Î” | |
|---|---|---|
| [Sources/Guaka/Parsing/FlagSet+Parsing.swift](https://codecov.io/gh/nsomar/Guaka/pull/95/diff?src=pr&el=tree#diff-U291cmNlcy9HdWFrYS9QYXJzaW5nL0ZsYWdTZXQrUGFyc2luZy5zd2lmdA==) | `100% <Ã¸> (Ã¸)` | :arrow_up: |
| [Sources/Guaka/Command/Command.swift](https://codecov.io/gh/nsomar/Guaka/pull/95/diff?src=pr&el=tree#diff-U291cmNlcy9HdWFrYS9Db21tYW5kL0NvbW1hbmQuc3dpZnQ=) | `78.57% <100%> (Ã¸)` | :arrow_up: |
| [Sources/Guaka/Flag/Flag.swift](https://codecov.io/gh/nsomar/Guaka/pull/95/diff?src=pr&el=tree#diff-U291cmNlcy9HdWFrYS9GbGFnL0ZsYWcuc3dpZnQ=) | `94.91% <100%> (Ã¸)` | :arrow_up: |
| [Tests/GuakaTests/FlagSetTests.swift](https://codecov.io/gh/nsomar/Guaka/pull/95/diff?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9GbGFnU2V0VGVzdHMuc3dpZnQ=) | `96.42% <0%> (+0.39%)` | :arrow_up: |
| [Tests/GuakaTests/ParsingTests.swift](https://codecov.io/gh/nsomar/Guaka/pull/95/diff?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9QYXJzaW5nVGVzdHMuc3dpZnQ=) | `97.72% <0%> (+3.24%)` | :arrow_up: |
| [Tests/GuakaTests/ValidationTests.swift](https://codecov.io/gh/nsomar/Guaka/pull/95/diff?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9WYWxpZGF0aW9uVGVzdHMuc3dpZnQ=) | `92.06% <0%> (+7.93%)` | :arrow_up: |
| [Tests/GuakaTests/CommandTests.swift](https://codecov.io/gh/nsomar/Guaka/pull/95/diff?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9Db21tYW5kVGVzdHMuc3dpZnQ=) | `94.73% <0%> (+17.1%)` | :arrow_up: |
| [Tests/GuakaTests/FlagTests.swift](https://codecov.io/gh/nsomar/Guaka/pull/95/diff?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9GbGFnVGVzdHMuc3dpZnQ=) | `94.25% <0%> (+18.39%)` | :arrow_up: |

------

[Continue to review full report at Codecov](https://codecov.io/gh/nsomar/Guaka/pull/95?src=pr&el=continue).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
> `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/nsomar/Guaka/pull/95?src=pr&el=footer). Last update [f3da5d6...3dca5c1](https://codecov.io/gh/nsomar/Guaka/pull/95?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).
",NA,https://api.github.com/repos/nsomar/Guaka/issues/95/comments,https://github.com/nsomar/Guaka/pull/95#issuecomment-476963493,https://api.github.com/repos/nsomar/Guaka/issues/95
nsomar,Guaka,406144198,https://api.github.com/repos/nsomar/Guaka/issues/comments/460099169,460099169,MDEyOklzc3VlQ29tbWVudDQ2MDA5OTE2OQ==,8655789,2019-02-03T23:28:33Z,2019-02-03T23:28:33Z,NONE,"# [Codecov](https://codecov.io/gh/nsomar/Guaka/pull/94?src=pr&el=h1) Report
> Merging [#94](https://codecov.io/gh/nsomar/Guaka/pull/94?src=pr&el=desc) into [master](https://codecov.io/gh/nsomar/Guaka/commit/7d6135f0bcb324977698c40a1985f481899b6dce?src=pr&el=desc) will **decrease** coverage by `0.03%`.
> The diff coverage is `n/a`.

[![Impacted file tree graph](https://codecov.io/gh/nsomar/Guaka/pull/94/graphs/tree.svg?width=650&token=vZ33EVrwnP&height=150&src=pr)](https://codecov.io/gh/nsomar/Guaka/pull/94?src=pr&el=tree)

```diff
@@            Coverage Diff             @@
##           master      #94      +/-   ##
==========================================
- Coverage   94.98%   94.95%   -0.04%     
==========================================
  Files          40       40              
  Lines        3072     3072              
==========================================
- Hits         2918     2917       -1     
- Misses        154      155       +1
```


| [Impacted Files](https://codecov.io/gh/nsomar/Guaka/pull/94?src=pr&el=tree) | Coverage Î” | |
|---|---|---|
| [...sts/GuakaTests/HelpGeneratorSubclassingTests.swift](https://codecov.io/gh/nsomar/Guaka/pull/94/diff?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9IZWxwR2VuZXJhdG9yU3ViY2xhc3NpbmdUZXN0cy5zd2lmdA==) | `100% <Ã¸> (Ã¸)` | :arrow_up: |
| [Tests/GuakaTests/ValidationTests.swift](https://codecov.io/gh/nsomar/Guaka/pull/94/diff?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9WYWxpZGF0aW9uVGVzdHMuc3dpZnQ=) | `84.12% <Ã¸> (Ã¸)` | :arrow_up: |
| [Tests/GuakaTests/TestHelpers.swift](https://codecov.io/gh/nsomar/Guaka/pull/94/diff?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9UZXN0SGVscGVycy5zd2lmdA==) | `92.3% <Ã¸> (Ã¸)` | :arrow_up: |
| [Tests/GuakaTests/FlagSetTests.swift](https://codecov.io/gh/nsomar/Guaka/pull/94/diff?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9GbGFnU2V0VGVzdHMuc3dpZnQ=) | `96.03% <0%> (-0.4%)` | :arrow_down: |

------

[Continue to review full report at Codecov](https://codecov.io/gh/nsomar/Guaka/pull/94?src=pr&el=continue).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
> `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/nsomar/Guaka/pull/94?src=pr&el=footer). Last update [7d6135f...1ede287](https://codecov.io/gh/nsomar/Guaka/pull/94?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).
",NA,https://api.github.com/repos/nsomar/Guaka/issues/94/comments,https://github.com/nsomar/Guaka/pull/94#issuecomment-460099169,https://api.github.com/repos/nsomar/Guaka/issues/94
nsomar,Guaka,366747953,https://api.github.com/repos/nsomar/Guaka/issues/comments/426986799,426986799,MDEyOklzc3VlQ29tbWVudDQyNjk4Njc5OQ==,8655789,2018-10-04T11:39:40Z,2018-10-04T11:39:40Z,NONE,"# [Codecov](https://codecov.io/gh/nsomar/Guaka/pull/93?src=pr&el=h1) Report
> Merging [#93](https://codecov.io/gh/nsomar/Guaka/pull/93?src=pr&el=desc) into [master](https://codecov.io/gh/nsomar/Guaka/commit/bca985e49b6069181882b980397ab10372598dfd?src=pr&el=desc) will **not change** coverage.
> The diff coverage is `n/a`.

[![Impacted file tree graph](https://codecov.io/gh/nsomar/Guaka/pull/93/graphs/tree.svg?width=650&token=vZ33EVrwnP&height=150&src=pr)](https://codecov.io/gh/nsomar/Guaka/pull/93?src=pr&el=tree)

```diff
@@           Coverage Diff           @@
##           master      #93   +/-   ##
=======================================
  Coverage   94.98%   94.98%           
=======================================
  Files          40       40           
  Lines        3072     3072           
=======================================
  Hits         2918     2918           
  Misses        154      154
```


| [Impacted Files](https://codecov.io/gh/nsomar/Guaka/pull/93?src=pr&el=tree) | Coverage Î” | |
|---|---|---|
| [Sources/Guaka/Command/Command.swift](https://codecov.io/gh/nsomar/Guaka/pull/93/diff?src=pr&el=tree#diff-U291cmNlcy9HdWFrYS9Db21tYW5kL0NvbW1hbmQuc3dpZnQ=) | `78.57% <Ã¸> (Ã¸)` | :arrow_up: |

------

[Continue to review full report at Codecov](https://codecov.io/gh/nsomar/Guaka/pull/93?src=pr&el=continue).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
> `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/nsomar/Guaka/pull/93?src=pr&el=footer). Last update [bca985e...97bd457](https://codecov.io/gh/nsomar/Guaka/pull/93?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).
",NA,https://api.github.com/repos/nsomar/Guaka/issues/93/comments,https://github.com/nsomar/Guaka/pull/93#issuecomment-426986799,https://api.github.com/repos/nsomar/Guaka/issues/93
nsomar,Guaka,366747953,https://api.github.com/repos/nsomar/Guaka/issues/comments/426987532,426987532,MDEyOklzc3VlQ29tbWVudDQyNjk4NzUzMg==,4506500,2018-10-04T11:42:48Z,2018-10-04T11:42:48Z,COLLABORATOR,This fixes #92.,NA,https://api.github.com/repos/nsomar/Guaka/issues/93/comments,https://github.com/nsomar/Guaka/pull/93#issuecomment-426987532,https://api.github.com/repos/nsomar/Guaka/issues/93
nsomar,Guaka,366297712,https://api.github.com/repos/nsomar/Guaka/issues/comments/426613021,426613021,MDEyOklzc3VlQ29tbWVudDQyNjYxMzAyMQ==,4506500,2018-10-03T12:08:31Z,2018-10-03T12:08:31Z,COLLABORATOR,Hey @Marxon13! Thanks for raising this issue. Would you mind submitting a PR?,NA,https://api.github.com/repos/nsomar/Guaka/issues/92/comments,https://github.com/nsomar/Guaka/issues/92#issuecomment-426613021,https://api.github.com/repos/nsomar/Guaka/issues/92
nsomar,Guaka,362864670,https://api.github.com/repos/nsomar/Guaka/issues/comments/423751019,423751019,MDEyOklzc3VlQ29tbWVudDQyMzc1MTAxOQ==,8655789,2018-09-22T15:15:50Z,2018-09-22T15:15:50Z,NONE,"# [Codecov](https://codecov.io/gh/nsomar/Guaka/pull/91?src=pr&el=h1) Report
> Merging [#91](https://codecov.io/gh/nsomar/Guaka/pull/91?src=pr&el=desc) into [master](https://codecov.io/gh/nsomar/Guaka/commit/c23ece0ec0bde6c94d975a0a02866743f4fdf7e2?src=pr&el=desc) will **decrease** coverage by `0.18%`.
> The diff coverage is `0%`.

[![Impacted file tree graph](https://codecov.io/gh/nsomar/Guaka/pull/91/graphs/tree.svg?width=650&token=vZ33EVrwnP&height=150&src=pr)](https://codecov.io/gh/nsomar/Guaka/pull/91?src=pr&el=tree)

```diff
@@            Coverage Diff             @@
##           master      #91      +/-   ##
==========================================
- Coverage   95.17%   94.98%   -0.19%     
==========================================
  Files          40       40              
  Lines        3065     3072       +7     
==========================================
+ Hits         2917     2918       +1     
- Misses        148      154       +6
```


| [Impacted Files](https://codecov.io/gh/nsomar/Guaka/pull/91?src=pr&el=tree) | Coverage Î” | |
|---|---|---|
| [Sources/Guaka/Flag/Flags.swift](https://codecov.io/gh/nsomar/Guaka/pull/91/diff?src=pr&el=tree#diff-U291cmNlcy9HdWFrYS9GbGFnL0ZsYWdzLnN3aWZ0) | `80% <0%> (-8.89%)` | :arrow_down: |
| [Sources/Guaka/Command/Command.swift](https://codecov.io/gh/nsomar/Guaka/pull/91/diff?src=pr&el=tree#diff-U291cmNlcy9HdWFrYS9Db21tYW5kL0NvbW1hbmQuc3dpZnQ=) | `78.57% <0%> (-6.05%)` | :arrow_down: |
| [Tests/GuakaTests/FlagSetTests.swift](https://codecov.io/gh/nsomar/Guaka/pull/91/diff?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9GbGFnU2V0VGVzdHMuc3dpZnQ=) | `96.42% <0%> (+0.39%)` | :arrow_up: |

------

[Continue to review full report at Codecov](https://codecov.io/gh/nsomar/Guaka/pull/91?src=pr&el=continue).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
> `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/nsomar/Guaka/pull/91?src=pr&el=footer). Last update [c23ece0...40a5223](https://codecov.io/gh/nsomar/Guaka/pull/91?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).
",NA,https://api.github.com/repos/nsomar/Guaka/issues/91/comments,https://github.com/nsomar/Guaka/pull/91#issuecomment-423751019,https://api.github.com/repos/nsomar/Guaka/issues/91
nsomar,Guaka,362864670,https://api.github.com/repos/nsomar/Guaka/issues/comments/423783187,423783187,MDEyOklzc3VlQ29tbWVudDQyMzc4MzE4Nw==,4506500,2018-09-23T00:41:31Z,2018-09-23T00:41:31Z,COLLABORATOR,Codecov is not happy since new code hasn't been covered.,NA,https://api.github.com/repos/nsomar/Guaka/issues/91/comments,https://github.com/nsomar/Guaka/pull/91#issuecomment-423783187,https://api.github.com/repos/nsomar/Guaka/issues/91
nsomar,Guaka,362864670,https://api.github.com/repos/nsomar/Guaka/issues/comments/425464383,425464383,MDEyOklzc3VlQ29tbWVudDQyNTQ2NDM4Mw==,4506500,2018-09-28T15:01:10Z,2018-09-28T15:01:10Z,COLLABORATOR,This was released as part of [0.3.0](https://github.com/nsomar/Guaka/releases/tag/0.3.0).,NA,https://api.github.com/repos/nsomar/Guaka/issues/91/comments,https://github.com/nsomar/Guaka/pull/91#issuecomment-425464383,https://api.github.com/repos/nsomar/Guaka/issues/91
nsomar,Guaka,362287530,https://api.github.com/repos/nsomar/Guaka/issues/comments/423271173,423271173,MDEyOklzc3VlQ29tbWVudDQyMzI3MTE3Mw==,8655789,2018-09-20T17:40:24Z,2018-09-20T17:50:59Z,NONE,"# [Codecov](https://codecov.io/gh/nsomar/Guaka/pull/90?src=pr&el=h1) Report
> Merging [#90](https://codecov.io/gh/nsomar/Guaka/pull/90?src=pr&el=desc) into [master](https://codecov.io/gh/nsomar/Guaka/commit/a1485a7e03ebfc935ad937409078d11d494bd215?src=pr&el=desc) will **increase** coverage by `0.01%`.
> The diff coverage is `100%`.

[![Impacted file tree graph](https://codecov.io/gh/nsomar/Guaka/pull/90/graphs/tree.svg?width=650&token=vZ33EVrwnP&height=150&src=pr)](https://codecov.io/gh/nsomar/Guaka/pull/90?src=pr&el=tree)

```diff
@@            Coverage Diff            @@
##           master     #90      +/-   ##
=========================================
+ Coverage   95.19%   95.2%   +0.01%     
=========================================
  Files          40      40              
  Lines        3370    3065     -305     
=========================================
- Hits         3208    2918     -290     
+ Misses        162     147      -15
```


| [Impacted Files](https://codecov.io/gh/nsomar/Guaka/pull/90?src=pr&el=tree) | Coverage Î” | |
|---|---|---|
| [Tests/GuakaTests/HelpTests.swift](https://codecov.io/gh/nsomar/Guaka/pull/90/diff?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9IZWxwVGVzdHMuc3dpZnQ=) | `100% <Ã¸> (Ã¸)` | :arrow_up: |
| [Tests/GuakaTests/FlagTests.swift](https://codecov.io/gh/nsomar/Guaka/pull/90/diff?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9GbGFnVGVzdHMuc3dpZnQ=) | `75.86% <Ã¸> (Ã¸)` | :arrow_up: |
| [Tests/GuakaTests/ValidationTests.swift](https://codecov.io/gh/nsomar/Guaka/pull/90/diff?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9WYWxpZGF0aW9uVGVzdHMuc3dpZnQ=) | `84.12% <Ã¸> (Ã¸)` | :arrow_up: |
| [Tests/GuakaTests/FlagHelpTests.swift](https://codecov.io/gh/nsomar/Guaka/pull/90/diff?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9GbGFnSGVscFRlc3RzLnN3aWZ0) | `100% <Ã¸> (Ã¸)` | :arrow_up: |
| [Tests/GuakaTests/HelpGeneratorTests.swift](https://codecov.io/gh/nsomar/Guaka/pull/90/diff?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9IZWxwR2VuZXJhdG9yVGVzdHMuc3dpZnQ=) | `100% <Ã¸> (Ã¸)` | :arrow_up: |
| [Tests/GuakaTests/CommandType+Run.swift](https://codecov.io/gh/nsomar/Guaka/pull/90/diff?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9Db21tYW5kVHlwZStSdW4uc3dpZnQ=) | `100% <Ã¸> (Ã¸)` | :arrow_up: |
| [Tests/GuakaTests/CommandHelpTests.swift](https://codecov.io/gh/nsomar/Guaka/pull/90/diff?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9Db21tYW5kSGVscFRlc3RzLnN3aWZ0) | `100% <Ã¸> (Ã¸)` | :arrow_up: |
| [Tests/GuakaTests/CommandParsingTests.swift](https://codecov.io/gh/nsomar/Guaka/pull/90/diff?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9Db21tYW5kUGFyc2luZ1Rlc3RzLnN3aWZ0) | `100% <Ã¸> (Ã¸)` | :arrow_up: |
| [Tests/GuakaTests/CommandExecutionTests.swift](https://codecov.io/gh/nsomar/Guaka/pull/90/diff?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9Db21tYW5kRXhlY3V0aW9uVGVzdHMuc3dpZnQ=) | `100% <Ã¸> (+3.75%)` | :arrow_up: |
| [Tests/GuakaTests/CommandTypeTest.swift](https://codecov.io/gh/nsomar/Guaka/pull/90/diff?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9Db21tYW5kVHlwZVRlc3Quc3dpZnQ=) | `100% <Ã¸> (Ã¸)` | :arrow_up: |
| ... and [21 more](https://codecov.io/gh/nsomar/Guaka/pull/90/diff?src=pr&el=tree-more) | |

------

[Continue to review full report at Codecov](https://codecov.io/gh/nsomar/Guaka/pull/90?src=pr&el=continue).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
> `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/nsomar/Guaka/pull/90?src=pr&el=footer). Last update [a1485a7...f9a6e66](https://codecov.io/gh/nsomar/Guaka/pull/90?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).
",NA,https://api.github.com/repos/nsomar/Guaka/issues/90/comments,https://github.com/nsomar/Guaka/pull/90#issuecomment-423271173,https://api.github.com/repos/nsomar/Guaka/issues/90
nsomar,Guaka,362287530,https://api.github.com/repos/nsomar/Guaka/issues/comments/423607871,423607871,MDEyOklzc3VlQ29tbWVudDQyMzYwNzg3MQ==,1307343,2018-09-21T17:08:27Z,2018-09-21T17:08:27Z,CONTRIBUTOR,Oh that's handy! Does it add the `allTests` to the the test files as well? Or just the generate the LinuxMain.swift?,NA,https://api.github.com/repos/nsomar/Guaka/issues/90/comments,https://github.com/nsomar/Guaka/pull/90#issuecomment-423607871,https://api.github.com/repos/nsomar/Guaka/issues/90
nsomar,Guaka,362287530,https://api.github.com/repos/nsomar/Guaka/issues/comments/423719969,423719969,MDEyOklzc3VlQ29tbWVudDQyMzcxOTk2OQ==,4506500,2018-09-22T06:11:53Z,2018-09-22T06:12:08Z,COLLABORATOR,It generates `LinuxMain.swift` and `XCTestManifests.swift` files that has the `allTests`.,NA,https://api.github.com/repos/nsomar/Guaka/issues/90/comments,https://github.com/nsomar/Guaka/pull/90#issuecomment-423719969,https://api.github.com/repos/nsomar/Guaka/issues/90
nsomar,Guaka,362287530,https://api.github.com/repos/nsomar/Guaka/issues/comments/425464403,425464403,MDEyOklzc3VlQ29tbWVudDQyNTQ2NDQwMw==,4506500,2018-09-28T15:01:14Z,2018-09-28T15:01:14Z,COLLABORATOR,This was released as part of [0.3.0](https://github.com/nsomar/Guaka/releases/tag/0.3.0).,NA,https://api.github.com/repos/nsomar/Guaka/issues/90/comments,https://github.com/nsomar/Guaka/pull/90#issuecomment-425464403,https://api.github.com/repos/nsomar/Guaka/issues/90
nsomar,Guaka,361953281,https://api.github.com/repos/nsomar/Guaka/issues/comments/423177502,423177502,MDEyOklzc3VlQ29tbWVudDQyMzE3NzUwMg==,4506500,2018-09-20T13:11:44Z,2018-09-20T13:11:44Z,COLLABORATOR,@Ponyboy47 You can take a look at a usage example in the Guaka Generator project: https://github.com/getGuaka/guaka-cli/blob/40a0d3b10a44dcd124cdee8c9d4ed67612e8ece9/Sources/guaka-cli/new.swift,NA,https://api.github.com/repos/nsomar/Guaka/issues/89/comments,https://github.com/nsomar/Guaka/issues/89#issuecomment-423177502,https://api.github.com/repos/nsomar/Guaka/issues/89
nsomar,Guaka,361953281,https://api.github.com/repos/nsomar/Guaka/issues/comments/423237709,423237709,MDEyOklzc3VlQ29tbWVudDQyMzIzNzcwOQ==,1307343,2018-09-20T15:58:34Z,2018-09-20T15:58:34Z,CONTRIBUTOR,"Would it be an option to make the fail function into a global Guaka function as opposed to a local function just for commands?

It seems like a design flaw right now because you cannot call fail if your command's `run` is a closure because you don't have access to `self` and you can't reference the command variable since the closure is technically part of the instantiation of the command. It seems that it's a part of `Command` just because it prints the help text automatically.

When I fail a command though I'd prefer to be explicit about the error message, and I'd really just like to be able to call fail the same regardless of whether or not my `run` is a function or a closure.",NA,https://api.github.com/repos/nsomar/Guaka/issues/89/comments,https://github.com/nsomar/Guaka/issues/89#issuecomment-423237709,https://api.github.com/repos/nsomar/Guaka/issues/89
nsomar,Guaka,361953281,https://api.github.com/repos/nsomar/Guaka/issues/comments/423783371,423783371,MDEyOklzc3VlQ29tbWVudDQyMzc4MzM3MQ==,4506500,2018-09-23T00:46:16Z,2018-09-23T00:46:16Z,COLLABORATOR,Looks like you implemented this in #91. Can I close this?,NA,https://api.github.com/repos/nsomar/Guaka/issues/89/comments,https://github.com/nsomar/Guaka/issues/89#issuecomment-423783371,https://api.github.com/repos/nsomar/Guaka/issues/89
nsomar,Guaka,361941380,https://api.github.com/repos/nsomar/Guaka/issues/comments/422975098,422975098,MDEyOklzc3VlQ29tbWVudDQyMjk3NTA5OA==,8655789,2018-09-19T22:08:19Z,2018-09-19T22:08:19Z,NONE,"# [Codecov](https://codecov.io/gh/nsomar/Guaka/pull/88?src=pr&el=h1) Report
> Merging [#88](https://codecov.io/gh/nsomar/Guaka/pull/88?src=pr&el=desc) into [master](https://codecov.io/gh/nsomar/Guaka/commit/ef9dfb62544eb94b70f32d49c5f1c500ee21e9a2?src=pr&el=desc) will **increase** coverage by `<.01%`.
> The diff coverage is `100%`.

[![Impacted file tree graph](https://codecov.io/gh/nsomar/Guaka/pull/88/graphs/tree.svg?width=650&token=vZ33EVrwnP&height=150&src=pr)](https://codecov.io/gh/nsomar/Guaka/pull/88?src=pr&el=tree)

```diff
@@            Coverage Diff             @@
##           master      #88      +/-   ##
==========================================
+ Coverage   95.19%   95.19%   +<.01%     
==========================================
  Files          40       40              
  Lines        3368     3370       +2     
==========================================
+ Hits         3206     3208       +2     
  Misses        162      162
```


| [Impacted Files](https://codecov.io/gh/nsomar/Guaka/pull/88?src=pr&el=tree) | Coverage Î” | |
|---|---|---|
| [Tests/GuakaTests/HelpGeneratorTests.swift](https://codecov.io/gh/nsomar/Guaka/pull/88/diff?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9IZWxwR2VuZXJhdG9yVGVzdHMuc3dpZnQ=) | `100% <100%> (Ã¸)` | :arrow_up: |
| [Tests/GuakaTests/HelpTests.swift](https://codecov.io/gh/nsomar/Guaka/pull/88/diff?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9IZWxwVGVzdHMuc3dpZnQ=) | `100% <100%> (Ã¸)` | :arrow_up: |
| [Sources/Guaka/Help/HelpGeneratorDefaults.swift](https://codecov.io/gh/nsomar/Guaka/pull/88/diff?src=pr&el=tree#diff-U291cmNlcy9HdWFrYS9IZWxwL0hlbHBHZW5lcmF0b3JEZWZhdWx0cy5zd2lmdA==) | `97.53% <100%> (Ã¸)` | :arrow_up: |

------

[Continue to review full report at Codecov](https://codecov.io/gh/nsomar/Guaka/pull/88?src=pr&el=continue).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
> `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/nsomar/Guaka/pull/88?src=pr&el=footer). Last update [ef9dfb6...59c247a](https://codecov.io/gh/nsomar/Guaka/pull/88?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).
",NA,https://api.github.com/repos/nsomar/Guaka/issues/88/comments,https://github.com/nsomar/Guaka/pull/88#issuecomment-422975098,https://api.github.com/repos/nsomar/Guaka/issues/88
nsomar,Guaka,361941380,https://api.github.com/repos/nsomar/Guaka/issues/comments/425464418,425464418,MDEyOklzc3VlQ29tbWVudDQyNTQ2NDQxOA==,4506500,2018-09-28T15:01:17Z,2018-09-28T15:01:17Z,COLLABORATOR,This was released as part of [0.3.0](https://github.com/nsomar/Guaka/releases/tag/0.3.0).,NA,https://api.github.com/repos/nsomar/Guaka/issues/88/comments,https://github.com/nsomar/Guaka/pull/88#issuecomment-425464418,https://api.github.com/repos/nsomar/Guaka/issues/88
nsomar,Guaka,360043685,https://api.github.com/repos/nsomar/Guaka/issues/comments/421226997,421226997,MDEyOklzc3VlQ29tbWVudDQyMTIyNjk5Nw==,4506500,2018-09-14T04:24:34Z,2018-09-14T04:24:34Z,COLLABORATOR,"Yes, both abbreviated and long version should be permitted. You can take a look at the `git commit` command as an example. All below do the same thing.

    git commit -m add -m file
    git commit -m add --message file
    git commit --message add --message file
",NA,https://api.github.com/repos/nsomar/Guaka/issues/87/comments,https://github.com/nsomar/Guaka/issues/87#issuecomment-421226997,https://api.github.com/repos/nsomar/Guaka/issues/87
nsomar,Guaka,360043685,https://api.github.com/repos/nsomar/Guaka/issues/comments/421404246,421404246,MDEyOklzc3VlQ29tbWVudDQyMTQwNDI0Ng==,1307343,2018-09-14T15:59:04Z,2018-09-14T15:59:04Z,CONTRIBUTOR,"Short names are restricted to a single character though. I'm talking about having more than one long names available for a single flag.
If you notice in my example, both used double dashes and the longName 'abr' would basically be an alias for the longer name 'average-bit-rate'.

ie:
```swift
let flag = Flag(longNames: ""abr"", ""average-bit-rate"", type: Int.self, description: ""The bit rate to which to constrain a video encoding"")
```",NA,https://api.github.com/repos/nsomar/Guaka/issues/87/comments,https://github.com/nsomar/Guaka/issues/87#issuecomment-421404246,https://api.github.com/repos/nsomar/Guaka/issues/87
nsomar,Guaka,360043685,https://api.github.com/repos/nsomar/Guaka/issues/comments/421519183,421519183,MDEyOklzc3VlQ29tbWVudDQyMTUxOTE4Mw==,4506500,2018-09-15T01:02:29Z,2018-09-15T01:02:29Z,COLLABORATOR,"I couldn't find any documentation about this in the standards. Do you have an example of a utility that allows this?

By the way, I don't think we should allow having more than long names available for a single flag. It would defeat the purpose of the long name is to be descriptive, and would make it harder for documenting.",NA,https://api.github.com/repos/nsomar/Guaka/issues/87/comments,https://github.com/nsomar/Guaka/issues/87#issuecomment-421519183,https://api.github.com/repos/nsomar/Guaka/issues/87
nsomar,Guaka,360043685,https://api.github.com/repos/nsomar/Guaka/issues/comments/421596727,421596727,MDEyOklzc3VlQ29tbWVudDQyMTU5NjcyNw==,1307343,2018-09-15T16:38:52Z,2018-09-15T16:38:52Z,CONTRIBUTOR,"I can't say that I know of any specific examples and if there's nothing in the standards then I won't push for this. It's not something I specifically _need_ per-say but have found it useful on from time to time.

I do agree that it would make documenting more difficult and after spending a small amount of time seeing if I could implement it, it basically required a substantial rewrite. Definitely not worth the effort for the small amount of benefit in only a small number of situations.",NA,https://api.github.com/repos/nsomar/Guaka/issues/87/comments,https://github.com/nsomar/Guaka/issues/87#issuecomment-421596727,https://api.github.com/repos/nsomar/Guaka/issues/87
nsomar,Guaka,360003575,https://api.github.com/repos/nsomar/Guaka/issues/comments/425464441,425464441,MDEyOklzc3VlQ29tbWVudDQyNTQ2NDQ0MQ==,4506500,2018-09-28T15:01:21Z,2018-09-28T15:01:21Z,COLLABORATOR,This was released as part of [0.3.0](https://github.com/nsomar/Guaka/releases/tag/0.3.0).,NA,https://api.github.com/repos/nsomar/Guaka/issues/86/comments,https://github.com/nsomar/Guaka/pull/86#issuecomment-425464441,https://api.github.com/repos/nsomar/Guaka/issues/86
nsomar,Guaka,359712131,https://api.github.com/repos/nsomar/Guaka/issues/comments/420873693,420873693,MDEyOklzc3VlQ29tbWVudDQyMDg3MzY5Mw==,1307343,2018-09-13T03:43:12Z,2018-09-13T03:43:12Z,CONTRIBUTOR,"I've forked this and begun trying to implement it myself and it is a bit more complex than I'd hoped because of the current design. I'll see if I can't figure it out without changing too much, but we'll see how plausible this really is.

I could easily make Array conform to FlagValue and just split results on a comma or some designated separator, but then you could run into issues where your whole input string and its individual values may both contain the separator character and you've got to figure out how to distinguish between them. So I'd rather have the more familiar usage of just specify an argument multiple times to avoid this kind of complexity.",NA,https://api.github.com/repos/nsomar/Guaka/issues/85/comments,https://github.com/nsomar/Guaka/issues/85#issuecomment-420873693,https://api.github.com/repos/nsomar/Guaka/issues/85
nsomar,Guaka,359712131,https://api.github.com/repos/nsomar/Guaka/issues/comments/420921427,420921427,MDEyOklzc3VlQ29tbWVudDQyMDkyMTQyNw==,4506500,2018-09-13T08:10:07Z,2018-09-13T08:10:07Z,COLLABORATOR,"@Ponyboy47 Thanks for trying this. I agree that options should be able to be used multiple times, as it follows the POSIX guidelines. Awaiting your PR.",NA,https://api.github.com/repos/nsomar/Guaka/issues/85/comments,https://github.com/nsomar/Guaka/issues/85#issuecomment-420921427,https://api.github.com/repos/nsomar/Guaka/issues/85
nsomar,Guaka,273194359,https://api.github.com/repos/nsomar/Guaka/issues/comments/383761947,383761947,MDEyOklzc3VlQ29tbWVudDM4Mzc2MTk0Nw==,15961926,2018-04-24T00:13:04Z,2018-04-24T00:13:04Z,NONE,"This would be really helpful, @nsomar, would you be able to look at this in the near future?",NA,https://api.github.com/repos/nsomar/Guaka/issues/80/comments,https://github.com/nsomar/Guaka/pull/80#issuecomment-383761947,https://api.github.com/repos/nsomar/Guaka/issues/80
nsomar,Guaka,273189406,https://api.github.com/repos/nsomar/Guaka/issues/comments/351090951,351090951,MDEyOklzc3VlQ29tbWVudDM1MTA5MDk1MQ==,320967,2017-12-12T15:45:28Z,2017-12-12T15:45:28Z,NONE,"Is Guaka still maintained? If not, we should join forces in a fork.",NA,https://api.github.com/repos/nsomar/Guaka/issues/79/comments,https://github.com/nsomar/Guaka/pull/79#issuecomment-351090951,https://api.github.com/repos/nsomar/Guaka/issues/79
nsomar,Guaka,273189406,https://api.github.com/repos/nsomar/Guaka/issues/comments/410720718,410720718,MDEyOklzc3VlQ29tbWVudDQxMDcyMDcxOA==,4506500,2018-08-06T14:09:09Z,2018-08-06T14:09:09Z,COLLABORATOR,"@nsomar If you're no longer maintaining this, would you be able to move this project to a GitHub org?",NA,https://api.github.com/repos/nsomar/Guaka/issues/79/comments,https://github.com/nsomar/Guaka/pull/79#issuecomment-410720718,https://api.github.com/repos/nsomar/Guaka/issues/79
nsomar,Guaka,273189406,https://api.github.com/repos/nsomar/Guaka/issues/comments/412488666,412488666,MDEyOklzc3VlQ29tbWVudDQxMjQ4ODY2Ng==,1461052,2018-08-13T11:32:08Z,2018-08-13T11:32:08Z,OWNER,@thii sorry for the late reply. I would be super grateful if someone is interested into becoming the maintainer :),NA,https://api.github.com/repos/nsomar/Guaka/issues/79/comments,https://github.com/nsomar/Guaka/pull/79#issuecomment-412488666,https://api.github.com/repos/nsomar/Guaka/issues/79
nsomar,Guaka,273189406,https://api.github.com/repos/nsomar/Guaka/issues/comments/412567116,412567116,MDEyOklzc3VlQ29tbWVudDQxMjU2NzExNg==,4506500,2018-08-13T15:52:57Z,2018-08-13T15:52:57Z,COLLABORATOR,@nsomar I'm interested :),NA,https://api.github.com/repos/nsomar/Guaka/issues/79/comments,https://github.com/nsomar/Guaka/pull/79#issuecomment-412567116,https://api.github.com/repos/nsomar/Guaka/issues/79
nsomar,Guaka,273189406,https://api.github.com/repos/nsomar/Guaka/issues/comments/413362357,413362357,MDEyOklzc3VlQ29tbWVudDQxMzM2MjM1Nw==,1461052,2018-08-15T22:43:51Z,2018-08-15T22:43:51Z,OWNER,@thii that is fantastic news <3 thank you so much. I just added you to the repo!,NA,https://api.github.com/repos/nsomar/Guaka/issues/79/comments,https://github.com/nsomar/Guaka/pull/79#issuecomment-413362357,https://api.github.com/repos/nsomar/Guaka/issues/79
nsomar,Guaka,261001985,https://api.github.com/repos/nsomar/Guaka/issues/comments/332547688,332547688,MDEyOklzc3VlQ29tbWVudDMzMjU0NzY4OA==,2393781,2017-09-27T14:52:35Z,2017-09-27T14:52:35Z,NONE,"Just noticed that it works with an equal sign `mycommand --args=""--flag value""`",NA,https://api.github.com/repos/nsomar/Guaka/issues/78/comments,https://github.com/nsomar/Guaka/issues/78#issuecomment-332547688,https://api.github.com/repos/nsomar/Guaka/issues/78
nsomar,Guaka,211373881,https://api.github.com/repos/nsomar/Guaka/issues/comments/284251958,284251958,MDEyOklzc3VlQ29tbWVudDI4NDI1MTk1OA==,1461052,2017-03-05T19:05:40Z,2017-03-05T19:05:40Z,OWNER,"Hello @johandk89,

I think the only way is to convert your code to async. You can do that by adding a lock (or a semaphore) and wait until the network call returns.

If you are using GCD then you can use a DispatchWorkItem  and call `wait` on it. https://developer.apple.com/reference/dispatch/dispatchworkitem

```
let myQueue = DispatchQueue(label: ""samplequeue"")
let workItem = DispatchWorkItem {
    do your async work here
}
myQueue.async(execute: workItem)
workItem.wait()
```

You can dispatch both your network request and the work item on the queue (since its serial) and then call wait on the workitem. Since the workitem has been added second it will cause our application to wait until the network request was done.
",NA,https://api.github.com/repos/nsomar/Guaka/issues/74/comments,https://github.com/nsomar/Guaka/issues/74#issuecomment-284251958,https://api.github.com/repos/nsomar/Guaka/issues/74
nsomar,Guaka,205802004,https://api.github.com/repos/nsomar/Guaka/issues/comments/277920892,277920892,MDEyOklzc3VlQ29tbWVudDI3NzkyMDg5Mg==,1461052,2017-02-07T07:23:03Z,2017-02-07T07:23:03Z,OWNER,"Hello @loyaltyarm, 
Can you please try again I revoked the build yesterday since the version was reported incorrectly.",NA,https://api.github.com/repos/nsomar/Guaka/issues/73/comments,https://github.com/nsomar/Guaka/issues/73#issuecomment-277920892,https://api.github.com/repos/nsomar/Guaka/issues/73
nsomar,Guaka,205802004,https://api.github.com/repos/nsomar/Guaka/issues/comments/277921952,277921952,MDEyOklzc3VlQ29tbWVudDI3NzkyMTk1Mg==,2584267,2017-02-07T07:29:46Z,2017-02-07T07:29:46Z,NONE,"Nice! Looks like it is working now, thanks!",NA,https://api.github.com/repos/nsomar/Guaka/issues/73/comments,https://github.com/nsomar/Guaka/issues/73#issuecomment-277921952,https://api.github.com/repos/nsomar/Guaka/issues/73
nsomar,Guaka,205741623,https://api.github.com/repos/nsomar/Guaka/issues/comments/277871496,277871496,MDEyOklzc3VlQ29tbWVudDI3Nzg3MTQ5Ng==,8655789,2017-02-07T01:23:19Z,2017-02-07T02:20:59Z,NONE,"# [Codecov](https://codecov.io/gh/oarrabi/Guaka/pull/72?src=pr&el=h1) Report
> Merging [#72](https://codecov.io/gh/oarrabi/Guaka/pull/72?src=pr&el=desc) into [master](https://codecov.io/gh/oarrabi/Guaka/commit/2b8337f7e915b61dee3345e2bddeabfc52fa4e9b?src=pr&el=desc) will **not impact** coverage.


```diff
@@           Coverage Diff           @@
##           master      #72   +/-   ##
=======================================
  Coverage   93.49%   93.49%           
=======================================
  Files          40       40           
  Lines        3292     3292           
=======================================
  Hits         3078     3078           
  Misses        214      214
```



------

[Continue to review full report at Codecov](https://codecov.io/gh/oarrabi/Guaka/pull/72?src=pr&el=continue).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
> `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/oarrabi/Guaka/pull/72?src=pr&el=footer). Last update [2b8337f...569c165](https://codecov.io/gh/oarrabi/Guaka/compare/2b8337f7e915b61dee3345e2bddeabfc52fa4e9b...569c165bc51224645526e0277abcfe8a5001f798?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",NA,https://api.github.com/repos/nsomar/Guaka/issues/72/comments,https://github.com/nsomar/Guaka/pull/72#issuecomment-277871496,https://api.github.com/repos/nsomar/Guaka/issues/72
nsomar,Guaka,205417238,https://api.github.com/repos/nsomar/Guaka/issues/comments/277522654,277522654,MDEyOklzc3VlQ29tbWVudDI3NzUyMjY1NA==,1461052,2017-02-05T14:18:48Z,2017-02-05T14:18:48Z,OWNER,"Hey @a2 and thanks for the proposal :)

I tried to experiment with bot the approaches. I am a bit more inclined toward the first approach. As ti will be less intrusive to the command run types.

At the moment the run blocks have two types:

```
public typealias Run = (Flags, [String]) -> ()
public typealias ConditionalRun = (Flags, [String]) -> (Bool)
```

If we wanted to go with the second approach, that means we have to make the block throws. 
While this is not a big deal with `Run`, it will look kinda strange with the `ConditionalRun`.

Since with `ConditionalRun` the developer can already choose to return false to stop the running of consecutive calls, it would be weird to also make it throw.

We could however unify both `Run` and `ConditionalRun` so both of them throw and have them throw a `stopExecution` exception when the developer opt to stop the execution, and throw a different error if he wants to exit the command. But it might then feel that we are using exception to control the flow, which is something that I would like to avoid.

It would be helpful to experiment with both of them and see how they look before proceeding with either. 

(cc @goyox86 )",NA,https://api.github.com/repos/nsomar/Guaka/issues/71/comments,https://github.com/nsomar/Guaka/issues/71#issuecomment-277522654,https://api.github.com/repos/nsomar/Guaka/issues/71
nsomar,Guaka,205417238,https://api.github.com/repos/nsomar/Guaka/issues/comments/277524059,277524059,MDEyOklzc3VlQ29tbWVudDI3NzUyNDA1OQ==,241156,2017-02-05T14:42:32Z,2017-02-05T14:42:32Z,CONTRIBUTOR,"In the meantime, I've been working around this issue [like so](https://gist.github.com/a2/71c40264f58a3ffa635f6a5dffb9a4b2).",NA,https://api.github.com/repos/nsomar/Guaka/issues/71/comments,https://github.com/nsomar/Guaka/issues/71#issuecomment-277524059,https://api.github.com/repos/nsomar/Guaka/issues/71
nsomar,Guaka,205417238,https://api.github.com/repos/nsomar/Guaka/issues/comments/431691102,431691102,MDEyOklzc3VlQ29tbWVudDQzMTY5MTEwMg==,1307343,2018-10-21T18:15:13Z,2018-10-21T18:15:13Z,CONTRIBUTOR,Can this be closed with the addition of the global fail function added by #91?,NA,https://api.github.com/repos/nsomar/Guaka/issues/71/comments,https://github.com/nsomar/Guaka/issues/71#issuecomment-431691102,https://api.github.com/repos/nsomar/Guaka/issues/71
nsomar,Guaka,205417238,https://api.github.com/repos/nsomar/Guaka/issues/comments/432205582,432205582,MDEyOklzc3VlQ29tbWVudDQzMjIwNTU4Mg==,241156,2018-10-23T11:17:47Z,2018-10-23T11:17:47Z,CONTRIBUTOR,@Ponyboy47 Looks good to me :),NA,https://api.github.com/repos/nsomar/Guaka/issues/71/comments,https://github.com/nsomar/Guaka/issues/71#issuecomment-432205582,https://api.github.com/repos/nsomar/Guaka/issues/71
nsomar,Guaka,205379383,https://api.github.com/repos/nsomar/Guaka/issues/comments/277518811,277518811,MDEyOklzc3VlQ29tbWVudDI3NzUxODgxMQ==,1461052,2017-02-05T13:10:27Z,2017-02-05T13:10:27Z,OWNER,"Hello @a2,

Thanks for the PR. It appears that there are a handful of tests that are not passing.
The tests are in the `CommandExecutionTests.swift` file.

(PS, I had to run them locally since I misconfigured the travis build configurations. Now its fine, when you next push the test result will appear in the PR)",NA,https://api.github.com/repos/nsomar/Guaka/issues/70/comments,https://github.com/nsomar/Guaka/pull/70#issuecomment-277518811,https://api.github.com/repos/nsomar/Guaka/issues/70
nsomar,Guaka,205379383,https://api.github.com/repos/nsomar/Guaka/issues/comments/277523907,277523907,MDEyOklzc3VlQ29tbWVudDI3NzUyMzkwNw==,8655789,2017-02-05T14:39:46Z,2017-02-05T14:39:46Z,NONE,"# [Codecov](https://codecov.io/gh/oarrabi/Guaka/pull/70?src=pr&el=h1) Report
> Merging [#70](https://codecov.io/gh/oarrabi/Guaka/pull/70?src=pr&el=desc) into [master](https://codecov.io/gh/oarrabi/Guaka/commit/ae3e911aa7bd596495c3aaf6df65e4297e7e8412?src=pr&el=desc) will **increase** coverage by `-0.03%`.


```diff
@@            Coverage Diff             @@
##           master      #70      +/-   ##
==========================================
- Coverage   93.51%   93.49%   -0.03%     
==========================================
  Files          40       40              
  Lines        3287     3292       +5     
==========================================
+ Hits         3074     3078       +4     
- Misses        213      214       +1
```


| [Impacted Files](https://codecov.io/gh/oarrabi/Guaka/pull/70?src=pr&el=tree) | Coverage Î” | |
|---|---|---|
| [Tests/GuakaTests/ErrorsTests.swift](https://codecov.io/gh/oarrabi/Guaka/compare/ae3e911aa7bd596495c3aaf6df65e4297e7e8412...c8f1bcd6e1b7bd87eed3240ff148357ba5aaa99b?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9FcnJvcnNUZXN0cy5zd2lmdA==) | `100% <100%> (Ã¸)` | :white_check_mark: |
| [Tests/GuakaTests/HelpGeneratorTests.swift](https://codecov.io/gh/oarrabi/Guaka/compare/ae3e911aa7bd596495c3aaf6df65e4297e7e8412...c8f1bcd6e1b7bd87eed3240ff148357ba5aaa99b?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9IZWxwR2VuZXJhdG9yVGVzdHMuc3dpZnQ=) | `100% <100%> (Ã¸)` | :white_check_mark: |
| [Tests/GuakaTests/CommandExecutionTests.swift](https://codecov.io/gh/oarrabi/Guaka/compare/ae3e911aa7bd596495c3aaf6df65e4297e7e8412...c8f1bcd6e1b7bd87eed3240ff148357ba5aaa99b?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9Db21tYW5kRXhlY3V0aW9uVGVzdHMuc3dpZnQ=) | `96.24% <100%> (Ã¸)` | :white_check_mark: |
| [...sts/GuakaTests/HelpGeneratorSubclassingTests.swift](https://codecov.io/gh/oarrabi/Guaka/compare/ae3e911aa7bd596495c3aaf6df65e4297e7e8412...c8f1bcd6e1b7bd87eed3240ff148357ba5aaa99b?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9IZWxwR2VuZXJhdG9yU3ViY2xhc3NpbmdUZXN0cy5zd2lmdA==) | `100% <100%> (Ã¸)` | :white_check_mark: |
| [Tests/GuakaTests/HelpTests.swift](https://codecov.io/gh/oarrabi/Guaka/compare/ae3e911aa7bd596495c3aaf6df65e4297e7e8412...c8f1bcd6e1b7bd87eed3240ff148357ba5aaa99b?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9IZWxwVGVzdHMuc3dpZnQ=) | `100% <100%> (Ã¸)` | :white_check_mark: |
| [Sources/Guaka/Help/HelpGeneratorDefaults.swift](https://codecov.io/gh/oarrabi/Guaka/compare/ae3e911aa7bd596495c3aaf6df65e4297e7e8412...c8f1bcd6e1b7bd87eed3240ff148357ba5aaa99b?src=pr&el=tree#diff-U291cmNlcy9HdWFrYS9IZWxwL0hlbHBHZW5lcmF0b3JEZWZhdWx0cy5zd2lmdA==) | `95.67% <83.33%> (-0.5%)` | :x: |

------

[Continue to review full report at Codecov](https://codecov.io/gh/oarrabi/Guaka/pull/70?src=pr&el=continue).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
> `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/oarrabi/Guaka/pull/70?src=pr&el=footer). Last update [ae3e911...c8f1bcd](https://codecov.io/gh/oarrabi/Guaka/compare/ae3e911aa7bd596495c3aaf6df65e4297e7e8412...c8f1bcd6e1b7bd87eed3240ff148357ba5aaa99b?src=pr&el=footer&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",NA,https://api.github.com/repos/nsomar/Guaka/issues/70/comments,https://github.com/nsomar/Guaka/pull/70#issuecomment-277523907,https://api.github.com/repos/nsomar/Guaka/issues/70
nsomar,Guaka,205379383,https://api.github.com/repos/nsomar/Guaka/issues/comments/277539553,277539553,MDEyOklzc3VlQ29tbWVudDI3NzUzOTU1Mw==,241156,2017-02-05T18:45:00Z,2017-02-05T18:45:00Z,CONTRIBUTOR,@oarrabi Updated to fix failing tests.,NA,https://api.github.com/repos/nsomar/Guaka/issues/70/comments,https://github.com/nsomar/Guaka/pull/70#issuecomment-277539553,https://api.github.com/repos/nsomar/Guaka/issues/70
nsomar,Guaka,205379383,https://api.github.com/repos/nsomar/Guaka/issues/comments/277805814,277805814,MDEyOklzc3VlQ29tbWVudDI3NzgwNTgxNA==,1461052,2017-02-06T20:38:05Z,2017-02-06T20:38:05Z,OWNER,"Hello @a2 Great Thanks!! :)
I have added you as a collaborator :) Looking forward to more great PRs like this one ðŸ‘ ",NA,https://api.github.com/repos/nsomar/Guaka/issues/70/comments,https://github.com/nsomar/Guaka/pull/70#issuecomment-277805814,https://api.github.com/repos/nsomar/Guaka/issues/70
nsomar,Guaka,205334113,https://api.github.com/repos/nsomar/Guaka/issues/comments/277454749,277454749,MDEyOklzc3VlQ29tbWVudDI3NzQ1NDc0OQ==,9576,2017-02-04T15:55:45Z,2017-02-04T15:55:45Z,COLLABORATOR,"Yeah but I think our case is a bit different. Because in their case they designed the library and modularized that way to provide a better UX. For us the case was different we just had to do it because such features don't exist today at least in a standard library. is just that we were good citizens and modularized the those things so they could be used on their own. SO I think the approach we should  take it's something along the lines of: ""Oh BTW Guaka is built on top of these libraries, you might want to take a look at them..."" And then we showcase those.

What do you think @oarrabi ?",NA,https://api.github.com/repos/nsomar/Guaka/issues/69/comments,https://github.com/nsomar/Guaka/issues/69#issuecomment-277454749,https://api.github.com/repos/nsomar/Guaka/issues/69
nsomar,Guaka,205334113,https://api.github.com/repos/nsomar/Guaka/issues/comments/277455789,277455789,MDEyOklzc3VlQ29tbWVudDI3NzQ1NTc4OQ==,1461052,2017-02-04T16:11:36Z,2017-02-04T16:11:36Z,OWNER,"If we want to expand we can make coloring part of Guaka, choices, yes/now questions also part of it.
We can then implement progress bar, table etc..

All these can become part of guaka!",NA,https://api.github.com/repos/nsomar/Guaka/issues/69/comments,https://github.com/nsomar/Guaka/issues/69#issuecomment-277455789,https://api.github.com/repos/nsomar/Guaka/issues/69
nsomar,Guaka,205232208,https://api.github.com/repos/nsomar/Guaka/issues/comments/277455206,277455206,MDEyOklzc3VlQ29tbWVudDI3NzQ1NTIwNg==,9576,2017-02-04T16:03:00Z,2017-02-04T16:03:00Z,COLLABORATOR,Thanks for the proposal @ianterrell ! We are looking into it! ,NA,https://api.github.com/repos/nsomar/Guaka/issues/68/comments,https://github.com/nsomar/Guaka/issues/68#issuecomment-277455206,https://api.github.com/repos/nsomar/Guaka/issues/68
nsomar,Guaka,204735025,https://api.github.com/repos/nsomar/Guaka/issues/comments/276825035,276825035,MDEyOklzc3VlQ29tbWVudDI3NjgyNTAzNQ==,8655789,2017-02-02T00:08:31Z,2017-02-02T02:33:43Z,NONE,"# [Codecov](https://codecov.io/gh/oarrabi/Guaka/pull/67?src=pr&el=h1) Report
> Merging [#67](https://codecov.io/gh/oarrabi/Guaka/pull/67?src=pr&el=desc) into [master](https://codecov.io/gh/oarrabi/Guaka/commit/6fdf66fa06276ddc32ca9b77454bc6d0a0118743?src=pr&el=desc) will **not impact** coverage.


```diff
@@           Coverage Diff           @@
##           master      #67   +/-   ##
=======================================
  Coverage   93.44%   93.44%           
=======================================
  Files          40       40           
  Lines        3247     3247           
=======================================
  Hits         3034     3034           
  Misses        213      213
```



------

[Continue to review full report at Codecov](https://codecov.io/gh/oarrabi/Guaka/pull/67?src=pr&el=continue).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
> `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/oarrabi/Guaka/pull/67?src=pr&el=footer). Last update [6fdf66f...71e33b9](https://codecov.io/gh/oarrabi/Guaka/compare/6fdf66fa06276ddc32ca9b77454bc6d0a0118743...71e33b9dbe35370110cf0ac9c8c255de74d5332c?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",NA,https://api.github.com/repos/nsomar/Guaka/issues/67/comments,https://github.com/nsomar/Guaka/pull/67#issuecomment-276825035,https://api.github.com/repos/nsomar/Guaka/issues/67
nsomar,Guaka,204735025,https://api.github.com/repos/nsomar/Guaka/issues/comments/276852585,276852585,MDEyOklzc3VlQ29tbWVudDI3Njg1MjU4NQ==,1461052,2017-02-02T02:44:09Z,2017-02-02T02:44:09Z,OWNER,cc @goyox86 ,NA,https://api.github.com/repos/nsomar/Guaka/issues/67/comments,https://github.com/nsomar/Guaka/pull/67#issuecomment-276852585,https://api.github.com/repos/nsomar/Guaka/issues/67
nsomar,Guaka,204735025,https://api.github.com/repos/nsomar/Guaka/issues/comments/276949294,276949294,MDEyOklzc3VlQ29tbWVudDI3Njk0OTI5NA==,9576,2017-02-02T12:52:00Z,2017-02-02T12:52:00Z,COLLABORATOR,ðŸ‘ ,NA,https://api.github.com/repos/nsomar/Guaka/issues/67/comments,https://github.com/nsomar/Guaka/pull/67#issuecomment-276949294,https://api.github.com/repos/nsomar/Guaka/issues/67
nsomar,Guaka,204683457,https://api.github.com/repos/nsomar/Guaka/issues/comments/276814493,276814493,MDEyOklzc3VlQ29tbWVudDI3NjgxNDQ5Mw==,1461052,2017-02-01T23:15:31Z,2017-02-01T23:15:31Z,OWNER,@TofPlay Thanks for raising this. I have a PR ready #67,NA,https://api.github.com/repos/nsomar/Guaka/issues/66/comments,https://github.com/nsomar/Guaka/issues/66#issuecomment-276814493,https://api.github.com/repos/nsomar/Guaka/issues/66
nsomar,Guaka,204683457,https://api.github.com/repos/nsomar/Guaka/issues/comments/276949384,276949384,MDEyOklzc3VlQ29tbWVudDI3Njk0OTM4NA==,1461052,2017-02-02T12:52:27Z,2017-02-02T12:52:27Z,OWNER,Closing this as PR was merged!,NA,https://api.github.com/repos/nsomar/Guaka/issues/66/comments,https://github.com/nsomar/Guaka/issues/66#issuecomment-276949384,https://api.github.com/repos/nsomar/Guaka/issues/66
nsomar,Guaka,204385733,https://api.github.com/repos/nsomar/Guaka/issues/comments/276451729,276451729,MDEyOklzc3VlQ29tbWVudDI3NjQ1MTcyOQ==,8655789,2017-01-31T18:39:10Z,2017-02-02T03:09:46Z,NONE,"# [Codecov](https://codecov.io/gh/oarrabi/Guaka/pull/64?src=pr&el=h1) Report
> Merging [#64](https://codecov.io/gh/oarrabi/Guaka/pull/64?src=pr&el=desc) into [master](https://codecov.io/gh/oarrabi/Guaka/commit/6fdf66fa06276ddc32ca9b77454bc6d0a0118743?src=pr&el=desc) will **increase** coverage by `0.07%`.


```diff
@@            Coverage Diff             @@
##           master      #64      +/-   ##
==========================================
+ Coverage   93.44%   93.51%   +0.07%     
==========================================
  Files          40       40              
  Lines        3247     3287      +40     
==========================================
+ Hits         3034     3074      +40     
  Misses        213      213
```


| [Impacted Files](https://codecov.io/gh/oarrabi/Guaka/pull/64?src=pr&el=tree) | Coverage Î” | |
|---|---|---|
| [Sources/Guaka/Command/Command.swift](https://codecov.io/gh/oarrabi/Guaka/compare/6fdf66fa06276ddc32ca9b77454bc6d0a0118743...793de12b46b2681d60d1c27112f86e4c31f2dd4a?src=pr&el=tree#diff-U291cmNlcy9HdWFrYS9Db21tYW5kL0NvbW1hbmQuc3dpZnQ=) | `81.53% <Ã¸> (Ã¸)` | :white_check_mark: |
| [Sources/Guaka/Help/CommandHelp.swift](https://codecov.io/gh/oarrabi/Guaka/compare/6fdf66fa06276ddc32ca9b77454bc6d0a0118743...793de12b46b2681d60d1c27112f86e4c31f2dd4a?src=pr&el=tree#diff-U291cmNlcy9HdWFrYS9IZWxwL0NvbW1hbmRIZWxwLnN3aWZ0) | `98.38% <100%> (+0.42%)` | :white_check_mark: |
| [Tests/GuakaTests/HelpGeneratorTests.swift](https://codecov.io/gh/oarrabi/Guaka/compare/6fdf66fa06276ddc32ca9b77454bc6d0a0118743...793de12b46b2681d60d1c27112f86e4c31f2dd4a?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9IZWxwR2VuZXJhdG9yVGVzdHMuc3dpZnQ=) | `100% <100%> (Ã¸)` | :white_check_mark: |
| [Tests/GuakaTests/HelpTests.swift](https://codecov.io/gh/oarrabi/Guaka/compare/6fdf66fa06276ddc32ca9b77454bc6d0a0118743...793de12b46b2681d60d1c27112f86e4c31f2dd4a?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9IZWxwVGVzdHMuc3dpZnQ=) | `100% <100%> (Ã¸)` | :white_check_mark: |
| [Tests/GuakaTests/CommandHelpTests.swift](https://codecov.io/gh/oarrabi/Guaka/compare/6fdf66fa06276ddc32ca9b77454bc6d0a0118743...793de12b46b2681d60d1c27112f86e4c31f2dd4a?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9Db21tYW5kSGVscFRlc3RzLnN3aWZ0) | `100% <100%> (Ã¸)` | :white_check_mark: |
| [Sources/Guaka/Parsing/Command+Parsing.swift](https://codecov.io/gh/oarrabi/Guaka/compare/6fdf66fa06276ddc32ca9b77454bc6d0a0118743...793de12b46b2681d60d1c27112f86e4c31f2dd4a?src=pr&el=tree#diff-U291cmNlcy9HdWFrYS9QYXJzaW5nL0NvbW1hbmQrUGFyc2luZy5zd2lmdA==) | `100% <100%> (Ã¸)` | :white_check_mark: |
| [Tests/GuakaTests/CommandExecutionTests.swift](https://codecov.io/gh/oarrabi/Guaka/compare/6fdf66fa06276ddc32ca9b77454bc6d0a0118743...793de12b46b2681d60d1c27112f86e4c31f2dd4a?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9Db21tYW5kRXhlY3V0aW9uVGVzdHMuc3dpZnQ=) | `96.24% <100%> (+0.14%)` | :white_check_mark: |
| [Sources/Guaka/Help/HelpGeneratorDefaults.swift](https://codecov.io/gh/oarrabi/Guaka/compare/6fdf66fa06276ddc32ca9b77454bc6d0a0118743...793de12b46b2681d60d1c27112f86e4c31f2dd4a?src=pr&el=tree#diff-U291cmNlcy9HdWFrYS9IZWxwL0hlbHBHZW5lcmF0b3JEZWZhdWx0cy5zd2lmdA==) | `96.17% <100%> (Ã¸)` | :white_check_mark: |

------

[Continue to review full report at Codecov](https://codecov.io/gh/oarrabi/Guaka/pull/64?src=pr&el=continue).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
> `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/oarrabi/Guaka/pull/64?src=pr&el=footer). Last update [6fdf66f...793de12](https://codecov.io/gh/oarrabi/Guaka/compare/6fdf66fa06276ddc32ca9b77454bc6d0a0118743...793de12b46b2681d60d1c27112f86e4c31f2dd4a?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",NA,https://api.github.com/repos/nsomar/Guaka/issues/64/comments,https://github.com/nsomar/Guaka/pull/64#issuecomment-276451729,https://api.github.com/repos/nsomar/Guaka/issues/64
nsomar,Guaka,204385733,https://api.github.com/repos/nsomar/Guaka/issues/comments/276488459,276488459,MDEyOklzc3VlQ29tbWVudDI3NjQ4ODQ1OQ==,9576,2017-01-31T20:52:59Z,2017-01-31T20:52:59Z,COLLABORATOR,ðŸ‘ :shipit: on ðŸ’š ,NA,https://api.github.com/repos/nsomar/Guaka/issues/64/comments,https://github.com/nsomar/Guaka/pull/64#issuecomment-276488459,https://api.github.com/repos/nsomar/Guaka/issues/64
nsomar,Guaka,204385733,https://api.github.com/repos/nsomar/Guaka/issues/comments/276489735,276489735,MDEyOklzc3VlQ29tbWVudDI3NjQ4OTczNQ==,13971458,2017-01-31T20:57:45Z,2017-01-31T20:57:45Z,NONE,Nice!,NA,https://api.github.com/repos/nsomar/Guaka/issues/64/comments,https://github.com/nsomar/Guaka/pull/64#issuecomment-276489735,https://api.github.com/repos/nsomar/Guaka/issues/64
nsomar,Guaka,204385733,https://api.github.com/repos/nsomar/Guaka/issues/comments/276846093,276846093,MDEyOklzc3VlQ29tbWVudDI3Njg0NjA5Mw==,1461052,2017-02-02T02:10:09Z,2017-02-02T02:10:09Z,OWNER,"@goyox86 @amg1976 
I also reworked some messages in the generator

https://github.com/oarrabi/Guaka-Generator/pull/5",NA,https://api.github.com/repos/nsomar/Guaka/issues/64/comments,https://github.com/nsomar/Guaka/pull/64#issuecomment-276846093,https://api.github.com/repos/nsomar/Guaka/issues/64
nsomar,Guaka,204385733,https://api.github.com/repos/nsomar/Guaka/issues/comments/276901395,276901395,MDEyOklzc3VlQ29tbWVudDI3NjkwMTM5NQ==,1461052,2017-02-02T08:56:47Z,2017-02-02T08:56:47Z,OWNER,Finally!!,NA,https://api.github.com/repos/nsomar/Guaka/issues/64/comments,https://github.com/nsomar/Guaka/pull/64#issuecomment-276901395,https://api.github.com/repos/nsomar/Guaka/issues/64
nsomar,Guaka,204149217,https://api.github.com/repos/nsomar/Guaka/issues/comments/276212417,276212417,MDEyOklzc3VlQ29tbWVudDI3NjIxMjQxNw==,1461052,2017-01-30T22:28:30Z,2017-01-30T22:28:30Z,OWNER,Fixes #62 ,NA,https://api.github.com/repos/nsomar/Guaka/issues/63/comments,https://github.com/nsomar/Guaka/pull/63#issuecomment-276212417,https://api.github.com/repos/nsomar/Guaka/issues/63
nsomar,Guaka,204149217,https://api.github.com/repos/nsomar/Guaka/issues/comments/276212701,276212701,MDEyOklzc3VlQ29tbWVudDI3NjIxMjcwMQ==,9576,2017-01-30T22:29:32Z,2017-01-30T22:29:32Z,COLLABORATOR,:shipit: on ðŸ ,NA,https://api.github.com/repos/nsomar/Guaka/issues/63/comments,https://github.com/nsomar/Guaka/pull/63#issuecomment-276212701,https://api.github.com/repos/nsomar/Guaka/issues/63
nsomar,Guaka,204149217,https://api.github.com/repos/nsomar/Guaka/issues/comments/276231713,276231713,MDEyOklzc3VlQ29tbWVudDI3NjIzMTcxMw==,8655789,2017-01-31T00:00:54Z,2017-01-31T00:17:06Z,NONE,"# [Codecov](https://codecov.io/gh/oarrabi/Guaka/pull/63?src=pr&el=h1) Report
> Merging [#63](https://codecov.io/gh/oarrabi/Guaka/pull/63?src=pr&el=desc) into [master](https://codecov.io/gh/oarrabi/Guaka/commit/9bfe28d8b3d30b72f1637a8c7b6609ed93ec2e0a?src=pr&el=desc) will **increase** coverage by `0.13%`.


```diff
@@            Coverage Diff             @@
##           master      #63      +/-   ##
==========================================
+ Coverage    93.3%   93.44%   +0.13%     
==========================================
  Files          40       40              
  Lines        3240     3247       +7     
==========================================
+ Hits         3023     3034      +11     
+ Misses        217      213       -4
```


| [Impacted Files](https://codecov.io/gh/oarrabi/Guaka/pull/63?src=pr&el=tree) | Coverage Î” | |
|---|---|---|
| [Sources/Guaka/Execution/CommandExecution.swift](https://codecov.io/gh/oarrabi/Guaka/compare/9bfe28d8b3d30b72f1637a8c7b6609ed93ec2e0a...a9dee97d9896c934e7f52463ec2ada2d64cbd14b?src=pr&el=tree#diff-U291cmNlcy9HdWFrYS9FeGVjdXRpb24vQ29tbWFuZEV4ZWN1dGlvbi5zd2lmdA==) | `97.46% <100%> (+0.03%)` | :white_check_mark: |
| [Tests/GuakaTests/CommandExecutionTests.swift](https://codecov.io/gh/oarrabi/Guaka/compare/9bfe28d8b3d30b72f1637a8c7b6609ed93ec2e0a...a9dee97d9896c934e7f52463ec2ada2d64cbd14b?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9Db21tYW5kRXhlY3V0aW9uVGVzdHMuc3dpZnQ=) | `96.09% <100%> (+0.19%)` | :white_check_mark: |
| [Tests/GuakaTests/TestHelpers.swift](https://codecov.io/gh/oarrabi/Guaka/compare/9bfe28d8b3d30b72f1637a8c7b6609ed93ec2e0a...a9dee97d9896c934e7f52463ec2ada2d64cbd14b?src=pr&el=tree#diff-VGVzdHMvR3Vha2FUZXN0cy9UZXN0SGVscGVycy5zd2lmdA==) | `90.76% <Ã¸> (+6.15%)` | :white_check_mark: |

------

[Continue to review full report at Codecov](https://codecov.io/gh/oarrabi/Guaka/pull/63?src=pr&el=continue).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
> `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/oarrabi/Guaka/pull/63?src=pr&el=footer). Last update [9bfe28d...a9dee97](https://codecov.io/gh/oarrabi/Guaka/compare/9bfe28d8b3d30b72f1637a8c7b6609ed93ec2e0a...a9dee97d9896c934e7f52463ec2ada2d64cbd14b?el=footer&src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",NA,https://api.github.com/repos/nsomar/Guaka/issues/63/comments,https://github.com/nsomar/Guaka/pull/63#issuecomment-276231713,https://api.github.com/repos/nsomar/Guaka/issues/63
nsomar,Guaka,204124899,https://api.github.com/repos/nsomar/Guaka/issues/comments/276211547,276211547,MDEyOklzc3VlQ29tbWVudDI3NjIxMTU0Nw==,1461052,2017-01-30T22:24:52Z,2017-01-30T22:24:52Z,OWNER,"Hello @pb-z,

I found the bug and I am working on a fix now.
Thanks for raising it!",NA,https://api.github.com/repos/nsomar/Guaka/issues/62/comments,https://github.com/nsomar/Guaka/issues/62#issuecomment-276211547,https://api.github.com/repos/nsomar/Guaka/issues/62
nsomar,Guaka,204124899,https://api.github.com/repos/nsomar/Guaka/issues/comments/276305365,276305365,MDEyOklzc3VlQ29tbWVudDI3NjMwNTM2NQ==,1461052,2017-01-31T08:39:37Z,2017-01-31T08:40:00Z,OWNER,"@pb-z This has been fixed. Please delete the `packages` folder and run `swift build` again :)
I will close this issue for now, if it persist, dont hesitate to reopen it!",NA,https://api.github.com/repos/nsomar/Guaka/issues/62/comments,https://github.com/nsomar/Guaka/issues/62#issuecomment-276305365,https://api.github.com/repos/nsomar/Guaka/issues/62
nsomar,Guaka,203977758,https://api.github.com/repos/nsomar/Guaka/issues/comments/276949456,276949456,MDEyOklzc3VlQ29tbWVudDI3Njk0OTQ1Ng==,1461052,2017-02-02T12:52:48Z,2017-02-02T12:52:48Z,OWNER,Closing.. Fixed in #64 ,NA,https://api.github.com/repos/nsomar/Guaka/issues/61/comments,https://github.com/nsomar/Guaka/issues/61#issuecomment-276949456,https://api.github.com/repos/nsomar/Guaka/issues/61
nsomar,Guaka,203975177,https://api.github.com/repos/nsomar/Guaka/issues/comments/276030500,276030500,MDEyOklzc3VlQ29tbWVudDI3NjAzMDUwMA==,1461052,2017-01-30T10:42:36Z,2017-01-30T10:43:03Z,OWNER,"Yes, this is something we are working on. We will keep this for now and go for a different one later.

Thanks for your input :)",NA,https://api.github.com/repos/nsomar/Guaka/issues/60/comments,https://github.com/nsomar/Guaka/issues/60#issuecomment-276030500,https://api.github.com/repos/nsomar/Guaka/issues/60
nsomar,Guaka,203841789,https://api.github.com/repos/nsomar/Guaka/issues/comments/275885198,275885198,MDEyOklzc3VlQ29tbWVudDI3NTg4NTE5OA==,9576,2017-01-29T00:33:33Z,2017-01-29T00:33:33Z,COLLABORATOR,ðŸ‘ :shipit: ,NA,https://api.github.com/repos/nsomar/Guaka/issues/58/comments,https://github.com/nsomar/Guaka/pull/58#issuecomment-275885198,https://api.github.com/repos/nsomar/Guaka/issues/58
nsomar,Guaka,203841789,https://api.github.com/repos/nsomar/Guaka/issues/comments/275885210,275885210,MDEyOklzc3VlQ29tbWVudDI3NTg4NTIxMA==,8655789,2017-01-29T00:33:45Z,2017-01-29T00:33:45Z,NONE,"## [Current coverage](https://codecov.io/gh/oarrabi/Guaka/pull/58?src=pr) is 93.30% (diff: 100%)
> Merging [#58](https://codecov.io/gh/oarrabi/Guaka/pull/58?src=pr) into [master](https://codecov.io/gh/oarrabi/Guaka/branch/master?src=pr) will increase coverage by **0.01%**

```diff
@@             master        #58   diff @@
==========================================
  Files            40         40          
  Lines          3232       3240     +8   
  Methods           0          0          
  Messages          0          0          
  Branches          0          0          
==========================================
+ Hits           3015       3023     +8   
  Misses          217        217          
  Partials          0          0          
```

> Powered by [Codecov](https://codecov.io?src=pr). Last update [9491ecf...0505cca](https://codecov.io/gh/oarrabi/Guaka/compare/9491ecfd3c400a016a08497002e0d65064e90d92...0505cca8eec9456874adfe5be3d104ffc6cb87ee?src=pr)",NA,https://api.github.com/repos/nsomar/Guaka/issues/58/comments,https://github.com/nsomar/Guaka/pull/58#issuecomment-275885210,https://api.github.com/repos/nsomar/Guaka/issues/58
nsomar,Guaka,203840607,https://api.github.com/repos/nsomar/Guaka/issues/comments/275883863,275883863,MDEyOklzc3VlQ29tbWVudDI3NTg4Mzg2Mw==,9576,2017-01-29T00:03:41Z,2017-01-29T00:03:41Z,COLLABORATOR,:shipit: ,NA,https://api.github.com/repos/nsomar/Guaka/issues/57/comments,https://github.com/nsomar/Guaka/pull/57#issuecomment-275883863,https://api.github.com/repos/nsomar/Guaka/issues/57
nsomar,Guaka,203840607,https://api.github.com/repos/nsomar/Guaka/issues/comments/275884005,275884005,MDEyOklzc3VlQ29tbWVudDI3NTg4NDAwNQ==,8655789,2017-01-29T00:06:53Z,2017-01-29T00:06:53Z,NONE,"## [Current coverage](https://codecov.io/gh/oarrabi/Guaka/pull/57?src=pr) is 93.28% (diff: 100%)
> Merging [#57](https://codecov.io/gh/oarrabi/Guaka/pull/57?src=pr) into [master](https://codecov.io/gh/oarrabi/Guaka/branch/master?src=pr) will not change coverage

```diff
@@             master        #57   diff @@
==========================================
  Files            40         40          
  Lines          3232       3232          
  Methods           0          0          
  Messages          0          0          
  Branches          0          0          
==========================================
  Hits           3015       3015          
  Misses          217        217          
  Partials          0          0          
```

> Powered by [Codecov](https://codecov.io?src=pr). Last update [6a5d8ab...1bf3bac](https://codecov.io/gh/oarrabi/Guaka/compare/6a5d8ab7e4aee47b4dfc2dc097dadb8716dc8291...1bf3bac1e873a2870c64193ebd86238986184ce8?src=pr)",NA,https://api.github.com/repos/nsomar/Guaka/issues/57/comments,https://github.com/nsomar/Guaka/pull/57#issuecomment-275884005,https://api.github.com/repos/nsomar/Guaka/issues/57
nsomar,Guaka,203836024,https://api.github.com/repos/nsomar/Guaka/issues/comments/275879748,275879748,MDEyOklzc3VlQ29tbWVudDI3NTg3OTc0OA==,8655789,2017-01-28T22:37:33Z,2017-01-28T22:37:33Z,NONE,"## [Current coverage](https://codecov.io/gh/oarrabi/Guaka/pull/56?src=pr) is 93.28% (diff: 83.52%)
> Merging [#56](https://codecov.io/gh/oarrabi/Guaka/pull/56?src=pr) into [master](https://codecov.io/gh/oarrabi/Guaka/branch/master?src=pr) will decrease coverage by **0.17%**

```diff
@@             master        #56   diff @@
==========================================
  Files            38         40     +2   
  Lines          3151       3232    +81   
  Methods           0          0          
  Messages          0          0          
  Branches          0          0          
==========================================
+ Hits           2945       3015    +70   
- Misses          206        217    +11   
  Partials          0          0          
```

> Powered by [Codecov](https://codecov.io?src=pr). Last update [31ceee9...43efcf8](https://codecov.io/gh/oarrabi/Guaka/compare/31ceee97f25b3b648f39af88bc886caa783df0ed...43efcf8ce92a6168514d1adc07561abdbffb2d56?src=pr)",NA,https://api.github.com/repos/nsomar/Guaka/issues/56/comments,https://github.com/nsomar/Guaka/pull/56#issuecomment-275879748,https://api.github.com/repos/nsomar/Guaka/issues/56
nsomar,Guaka,203836024,https://api.github.com/repos/nsomar/Guaka/issues/comments/275880170,275880170,MDEyOklzc3VlQ29tbWVudDI3NTg4MDE3MA==,1461052,2017-01-28T22:45:58Z,2017-01-28T22:45:58Z,OWNER,Awesome!!,NA,https://api.github.com/repos/nsomar/Guaka/issues/56/comments,https://github.com/nsomar/Guaka/pull/56#issuecomment-275880170,https://api.github.com/repos/nsomar/Guaka/issues/56
nsomar,Guaka,203817010,https://api.github.com/repos/nsomar/Guaka/issues/comments/275859201,275859201,MDEyOklzc3VlQ29tbWVudDI3NTg1OTIwMQ==,8655789,2017-01-28T16:50:22Z,2017-01-28T23:10:34Z,NONE,"## [Current coverage](https://codecov.io/gh/oarrabi/Guaka/pull/55?src=pr) is 93.28% (diff: 100%)
> Merging [#55](https://codecov.io/gh/oarrabi/Guaka/pull/55?src=pr) into [master](https://codecov.io/gh/oarrabi/Guaka/branch/master?src=pr) will not change coverage

```diff
@@             master        #55   diff @@
==========================================
  Files            40         40          
  Lines          3232       3232          
  Methods           0          0          
  Messages          0          0          
  Branches          0          0          
==========================================
  Hits           3015       3015          
  Misses          217        217          
  Partials          0          0          
```

> Powered by [Codecov](https://codecov.io?src=pr). Last update [93b793d...b779829](https://codecov.io/gh/oarrabi/Guaka/compare/93b793d7438f9c3102728b090c383846900ade71...b779829f3f67128b3dbe1f33e44456289ea06c48?src=pr)",NA,https://api.github.com/repos/nsomar/Guaka/issues/55/comments,https://github.com/nsomar/Guaka/pull/55#issuecomment-275859201,https://api.github.com/repos/nsomar/Guaka/issues/55
nsomar,Guaka,200906957,https://api.github.com/repos/nsomar/Guaka/issues/comments/275859357,275859357,MDEyOklzc3VlQ29tbWVudDI3NTg1OTM1Nw==,1461052,2017-01-28T16:52:51Z,2017-01-28T16:52:51Z,OWNER,All done @goyox86 ,NA,https://api.github.com/repos/nsomar/Guaka/issues/54/comments,https://github.com/nsomar/Guaka/issues/54#issuecomment-275859357,https://api.github.com/repos/nsomar/Guaka/issues/54
nsomar,Guaka,200600482,https://api.github.com/repos/nsomar/Guaka/issues/comments/272426288,272426288,MDEyOklzc3VlQ29tbWVudDI3MjQyNjI4OA==,9576,2017-01-13T11:52:04Z,2017-01-13T11:52:04Z,COLLABORATOR,Hey @rbukovansky feel free to submit the PR! After all we are on git and if we mess something up we can go back and try again a different approach ;) ,NA,https://api.github.com/repos/nsomar/Guaka/issues/53/comments,https://github.com/nsomar/Guaka/issues/53#issuecomment-272426288,https://api.github.com/repos/nsomar/Guaka/issues/53
nsomar,Guaka,199961036,https://api.github.com/repos/nsomar/Guaka/issues/comments/275859251,275859251,MDEyOklzc3VlQ29tbWVudDI3NTg1OTI1MQ==,1461052,2017-01-28T16:51:10Z,2017-01-28T16:51:10Z,OWNER,We opted to have `make deploy-local ...`,NA,https://api.github.com/repos/nsomar/Guaka/issues/52/comments,https://github.com/nsomar/Guaka/issues/52#issuecomment-275859251,https://api.github.com/repos/nsomar/Guaka/issues/52
nsomar,Guaka,199961022,https://api.github.com/repos/nsomar/Guaka/issues/comments/275859280,275859280,MDEyOklzc3VlQ29tbWVudDI3NTg1OTI4MA==,1461052,2017-01-28T16:51:33Z,2017-01-28T16:51:33Z,OWNER,No apt for now... we have an installer script for linux,NA,https://api.github.com/repos/nsomar/Guaka/issues/51/comments,https://github.com/nsomar/Guaka/issues/51#issuecomment-275859280,https://api.github.com/repos/nsomar/Guaka/issues/51
nsomar,Guaka,199817796,https://api.github.com/repos/nsomar/Guaka/issues/comments/271720771,271720771,MDEyOklzc3VlQ29tbWVudDI3MTcyMDc3MQ==,1461052,2017-01-10T22:38:20Z,2017-01-10T22:38:20Z,OWNER,"Hello @rbukovansky thank you for using Guaka :) 
We added the MIT license",NA,https://api.github.com/repos/nsomar/Guaka/issues/50/comments,https://github.com/nsomar/Guaka/issues/50#issuecomment-271720771,https://api.github.com/repos/nsomar/Guaka/issues/50
nsomar,Guaka,199817796,https://api.github.com/repos/nsomar/Guaka/issues/comments/271728449,271728449,MDEyOklzc3VlQ29tbWVudDI3MTcyODQ0OQ==,1461052,2017-01-10T23:12:45Z,2017-01-10T23:12:45Z,OWNER,"I am going to close this issue. Please do open other issues if anything seems out of place.
As always PRs are always appreciated :)",NA,https://api.github.com/repos/nsomar/Guaka/issues/50/comments,https://github.com/nsomar/Guaka/issues/50#issuecomment-271728449,https://api.github.com/repos/nsomar/Guaka/issues/50
nsomar,Guaka,199708878,https://api.github.com/repos/nsomar/Guaka/issues/comments/271728930,271728930,MDEyOklzc3VlQ29tbWVudDI3MTcyODkzMA==,1461052,2017-01-10T23:15:23Z,2017-01-10T23:15:23Z,OWNER,WORKSSSS!,NA,https://api.github.com/repos/nsomar/Guaka/issues/49/comments,https://github.com/nsomar/Guaka/issues/49#issuecomment-271728930,https://api.github.com/repos/nsomar/Guaka/issues/49
nsomar,Guaka,199395752,https://api.github.com/repos/nsomar/Guaka/issues/comments/271122060,271122060,MDEyOklzc3VlQ29tbWVudDI3MTEyMjA2MA==,8655789,2017-01-08T01:14:55Z,2017-01-08T22:48:06Z,NONE,"## [Current coverage](https://codecov.io/gh/oarrabi/Guaka/pull/47?src=pr) is 93.46% (diff: 100%)
> Merging [#47](https://codecov.io/gh/oarrabi/Guaka/pull/47?src=pr) into [master](https://codecov.io/gh/oarrabi/Guaka/branch/master?src=pr) will not change coverage

```diff
@@             master        #47   diff @@
==========================================
  Files            38         38          
  Lines          3151       3151          
  Methods           0          0          
  Messages          0          0          
  Branches          0          0          
==========================================
  Hits           2945       2945          
  Misses          206        206          
  Partials          0          0          
```

> Powered by [Codecov](https://codecov.io?src=pr). Last update [066bdfb...dfe4d1b](https://codecov.io/gh/oarrabi/Guaka/compare/066bdfbbc2a9d38a1879f6e1eb39d07f76af7a02...dfe4d1bc79d672928a7963bd26a0ceba2b71ba5a?src=pr)",NA,https://api.github.com/repos/nsomar/Guaka/issues/47/comments,https://github.com/nsomar/Guaka/pull/47#issuecomment-271122060,https://api.github.com/repos/nsomar/Guaka/issues/47
nsomar,Guaka,199395752,https://api.github.com/repos/nsomar/Guaka/issues/comments/271186152,271186152,MDEyOklzc3VlQ29tbWVudDI3MTE4NjE1Mg==,1461052,2017-01-08T22:51:29Z,2017-01-08T22:51:29Z,OWNER,Merging this ,NA,https://api.github.com/repos/nsomar/Guaka/issues/47/comments,https://github.com/nsomar/Guaka/pull/47#issuecomment-271186152,https://api.github.com/repos/nsomar/Guaka/issues/47
nsomar,Guaka,199392014,https://api.github.com/repos/nsomar/Guaka/issues/comments/271118119,271118119,MDEyOklzc3VlQ29tbWVudDI3MTExODExOQ==,8655789,2017-01-07T23:39:44Z,2017-01-07T23:43:04Z,NONE,"## [Current coverage](https://codecov.io/gh/oarrabi/Guaka/pull/46?src=pr) is 93.46% (diff: 88.13%)
> Merging [#46](https://codecov.io/gh/oarrabi/Guaka/pull/46?src=pr) into [master](https://codecov.io/gh/oarrabi/Guaka/branch/master?src=pr) will decrease coverage by **0.79%**

```diff
@@             master        #46   diff @@
==========================================
  Files            37         38     +1   
  Lines          3047       3151   +104   
  Methods           0          0          
  Messages          0          0          
  Branches          0          0          
==========================================
+ Hits           2872       2945    +73   
- Misses          175        206    +31   
  Partials          0          0          
```

> Powered by [Codecov](https://codecov.io?src=pr). Last update [4ffe2ff...0eb45ad](https://codecov.io/gh/oarrabi/Guaka/compare/4ffe2fff82c14e2de01b93714f8e8553f1e01140...0eb45ad841a95ec4d6142665797cb9a051ea80aa?src=pr)",NA,https://api.github.com/repos/nsomar/Guaka/issues/46/comments,https://github.com/nsomar/Guaka/pull/46#issuecomment-271118119,https://api.github.com/repos/nsomar/Guaka/issues/46
nsomar,Guaka,199392014,https://api.github.com/repos/nsomar/Guaka/issues/comments/271151654,271151654,MDEyOklzc3VlQ29tbWVudDI3MTE1MTY1NA==,1461052,2017-01-08T13:37:28Z,2017-01-08T13:37:28Z,OWNER,@goyox86 merging this as its only readme and some improvements,NA,https://api.github.com/repos/nsomar/Guaka/issues/46/comments,https://github.com/nsomar/Guaka/pull/46#issuecomment-271151654,https://api.github.com/repos/nsomar/Guaka/issues/46
nsomar,Guaka,199155165,https://api.github.com/repos/nsomar/Guaka/issues/comments/271186310,271186310,MDEyOklzc3VlQ29tbWVudDI3MTE4NjMxMA==,9576,2017-01-08T22:53:46Z,2017-01-08T22:53:46Z,COLLABORATOR,What versioning scheme are we gonna use? ,NA,https://api.github.com/repos/nsomar/Guaka/issues/45/comments,https://github.com/nsomar/Guaka/issues/45#issuecomment-271186310,https://api.github.com/repos/nsomar/Guaka/issues/45
nsomar,Guaka,199155165,https://api.github.com/repos/nsomar/Guaka/issues/comments/271186370,271186370,MDEyOklzc3VlQ29tbWVudDI3MTE4NjM3MA==,1461052,2017-01-08T22:54:42Z,2017-01-08T22:54:42Z,OWNER,Semantic?,NA,https://api.github.com/repos/nsomar/Guaka/issues/45/comments,https://github.com/nsomar/Guaka/issues/45#issuecomment-271186370,https://api.github.com/repos/nsomar/Guaka/issues/45
nsomar,Guaka,199155165,https://api.github.com/repos/nsomar/Guaka/issues/comments/271186479,271186479,MDEyOklzc3VlQ29tbWVudDI3MTE4NjQ3OQ==,9576,2017-01-08T22:56:16Z,2017-01-08T22:56:16Z,COLLABORATOR,Cool then start with v0.1.0 so we make our way forward to v1.0.0? I'm a bit rusty on semver though,NA,https://api.github.com/repos/nsomar/Guaka/issues/45/comments,https://github.com/nsomar/Guaka/issues/45#issuecomment-271186479,https://api.github.com/repos/nsomar/Guaka/issues/45
nsomar,Guaka,199155165,https://api.github.com/repos/nsomar/Guaka/issues/comments/272047462,272047462,MDEyOklzc3VlQ29tbWVudDI3MjA0NzQ2Mg==,1461052,2017-01-12T01:15:19Z,2017-01-12T01:15:19Z,OWNER,Closing this as we already started with version 0.1.0,NA,https://api.github.com/repos/nsomar/Guaka/issues/45/comments,https://github.com/nsomar/Guaka/issues/45#issuecomment-272047462,https://api.github.com/repos/nsomar/Guaka/issues/45
nsomar,Guaka,199155095,https://api.github.com/repos/nsomar/Guaka/issues/comments/271186233,271186233,MDEyOklzc3VlQ29tbWVudDI3MTE4NjIzMw==,1461052,2017-01-08T22:52:40Z,2017-01-08T22:52:40Z,OWNER,Both are done!!!,NA,https://api.github.com/repos/nsomar/Guaka/issues/44/comments,https://github.com/nsomar/Guaka/issues/44#issuecomment-271186233,https://api.github.com/repos/nsomar/Guaka/issues/44
nsomar,Guaka,199154693,https://api.github.com/repos/nsomar/Guaka/issues/comments/413524209,413524209,MDEyOklzc3VlQ29tbWVudDQxMzUyNDIwOQ==,4506500,2018-08-16T12:10:25Z,2018-08-16T12:10:25Z,COLLABORATOR,"@nsomar Will you be able to do this? You can just move the repos, and I'll do the rest ðŸ˜€",NA,https://api.github.com/repos/nsomar/Guaka/issues/43/comments,https://github.com/nsomar/Guaka/issues/43#issuecomment-413524209,https://api.github.com/repos/nsomar/Guaka/issues/43
nsomar,Guaka,199154693,https://api.github.com/repos/nsomar/Guaka/issues/comments/413729901,413729901,MDEyOklzc3VlQ29tbWVudDQxMzcyOTkwMQ==,1461052,2018-08-17T01:20:59Z,2018-08-17T01:20:59Z,OWNER,Done @thii!,NA,https://api.github.com/repos/nsomar/Guaka/issues/43/comments,https://github.com/nsomar/Guaka/issues/43#issuecomment-413729901,https://api.github.com/repos/nsomar/Guaka/issues/43
nsomar,Guaka,199154693,https://api.github.com/repos/nsomar/Guaka/issues/comments/414356467,414356467,MDEyOklzc3VlQ29tbWVudDQxNDM1NjQ2Nw==,4506500,2018-08-20T15:22:49Z,2018-08-20T15:22:49Z,COLLABORATOR,@nsomar Could you give me access to the above repos?,NA,https://api.github.com/repos/nsomar/Guaka/issues/43/comments,https://github.com/nsomar/Guaka/issues/43#issuecomment-414356467,https://api.github.com/repos/nsomar/Guaka/issues/43
nsomar,Guaka,199154693,https://api.github.com/repos/nsomar/Guaka/issues/comments/414415448,414415448,MDEyOklzc3VlQ29tbWVudDQxNDQxNTQ0OA==,1461052,2018-08-20T18:24:42Z,2018-08-20T18:24:42Z,OWNER,@thii Done! Let me know if it works,NA,https://api.github.com/repos/nsomar/Guaka/issues/43/comments,https://github.com/nsomar/Guaka/issues/43#issuecomment-414415448,https://api.github.com/repos/nsomar/Guaka/issues/43
nsomar,Guaka,199154209,https://api.github.com/repos/nsomar/Guaka/issues/comments/275859220,275859220,MDEyOklzc3VlQ29tbWVudDI3NTg1OTIyMA==,1461052,2017-01-28T16:50:43Z,2017-01-28T16:50:43Z,OWNER,This is done @goyox86 ,NA,https://api.github.com/repos/nsomar/Guaka/issues/41/comments,https://github.com/nsomar/Guaka/issues/41#issuecomment-275859220,https://api.github.com/repos/nsomar/Guaka/issues/41
nsomar,Guaka,191834890,https://api.github.com/repos/nsomar/Guaka/issues/comments/263094926,263094926,MDEyOklzc3VlQ29tbWVudDI2MzA5NDkyNg==,8655789,2016-11-27T00:44:18Z,2016-11-27T15:06:12Z,NONE,"## [Current coverage](https://codecov.io/gh/oarrabi/Guaka/pull/39?src=pr) is 96.03% (diff: 95.56%)
> Merging [#39](https://codecov.io/gh/oarrabi/Guaka/pull/39?src=pr) into [master](https://codecov.io/gh/oarrabi/Guaka/branch/master?src=pr) will increase coverage by **0.23%**

```diff
@@             master        #39   diff @@
==========================================
  Files            38         37     -1   
  Lines          2924       2922     -2   
  Methods           0          0          
  Messages          0          0          
  Branches          0          0          
==========================================
+ Hits           2801       2806     +5   
+ Misses          123        116     -7   
  Partials          0          0          
```

> Powered by [Codecov](https://codecov.io?src=pr). Last update [c5e6e88...c74787d](https://codecov.io/gh/oarrabi/Guaka/compare/c5e6e885ff3cfef31c6fa8ead73b35747cf447b2...c74787d2c8b62b1698841a1743a762469bfe4c91?src=pr)",NA,https://api.github.com/repos/nsomar/Guaka/issues/39/comments,https://github.com/nsomar/Guaka/pull/39#issuecomment-263094926,https://api.github.com/repos/nsomar/Guaka/issues/39
nsomar,Guaka,190532090,https://api.github.com/repos/nsomar/Guaka/issues/comments/261751373,261751373,MDEyOklzc3VlQ29tbWVudDI2MTc1MTM3Mw==,8655789,2016-11-20T01:19:24Z,2016-11-20T01:46:55Z,NONE,"## [Current coverage](https://codecov.io/gh/oarrabi/Guaka/pull/38?src=pr) is 95.79% (diff: 98.30%)

> Merging [#38](https://codecov.io/gh/oarrabi/Guaka/pull/38?src=pr) into [master](https://codecov.io/gh/oarrabi/Guaka/branch/master?src=pr) will increase coverage by **0.75%**

``` diff
@@             master        #38   diff @@
==========================================
  Files            28         38    +10   
  Lines          2299       2924   +625   
  Methods           0          0          
  Messages          0          0          
  Branches          0          0          
==========================================
+ Hits           2185       2801   +616   
- Misses          114        123     +9   
  Partials          0          0          
```

> Powered by [Codecov](https://codecov.io?src=pr). Last update [cd54aac...62ea291](https://codecov.io/gh/oarrabi/Guaka/compare/cd54aacdc7be2bedeff3ff8578e0c3f734da52c6...62ea291c0b3fa81da02386902ace526289bbfe16?src=pr)
",NA,https://api.github.com/repos/nsomar/Guaka/issues/38/comments,https://github.com/nsomar/Guaka/pull/38#issuecomment-261751373,https://api.github.com/repos/nsomar/Guaka/issues/38
nsomar,Guaka,190532090,https://api.github.com/repos/nsomar/Guaka/issues/comments/261752577,261752577,MDEyOklzc3VlQ29tbWVudDI2MTc1MjU3Nw==,1461052,2016-11-20T01:48:10Z,2016-11-20T01:48:10Z,OWNER,"I will merge this, when you are back we will have a walkthrough the code
",NA,https://api.github.com/repos/nsomar/Guaka/issues/38/comments,https://github.com/nsomar/Guaka/pull/38#issuecomment-261752577,https://api.github.com/repos/nsomar/Guaka/issues/38
nsomar,Guaka,190504550,https://api.github.com/repos/nsomar/Guaka/issues/comments/261722903,261722903,MDEyOklzc3VlQ29tbWVudDI2MTcyMjkwMw==,8655789,2016-11-19T16:15:48Z,2016-11-19T16:15:48Z,NONE,"## [Current coverage](https://codecov.io/gh/oarrabi/Guaka/pull/37?src=pr) is 95.04% (diff: 100%)

> Merging [#37](https://codecov.io/gh/oarrabi/Guaka/pull/37?src=pr) into [master](https://codecov.io/gh/oarrabi/Guaka/branch/master?src=pr) will increase coverage by **0.02%**

``` diff
@@             master        #37   diff @@
==========================================
  Files            28         28          
  Lines          2288       2299    +11   
  Methods           0          0          
  Messages          0          0          
  Branches          0          0          
==========================================
+ Hits           2174       2185    +11   
  Misses          114        114          
  Partials          0          0          
```

> Powered by [Codecov](https://codecov.io?src=pr). Last update [0ae5aaf...3eb9d66](https://codecov.io/gh/oarrabi/Guaka/compare/0ae5aaf92de86bc140b274ad97fc2b14e7d70ff2...3eb9d66705ef8768a2059d43394d1d296867bbc0?src=pr)
",NA,https://api.github.com/repos/nsomar/Guaka/issues/37/comments,https://github.com/nsomar/Guaka/pull/37#issuecomment-261722903,https://api.github.com/repos/nsomar/Guaka/issues/37
nsomar,Guaka,190504550,https://api.github.com/repos/nsomar/Guaka/issues/comments/261751277,261751277,MDEyOklzc3VlQ29tbWVudDI2MTc1MTI3Nw==,1461052,2016-11-20T01:17:17Z,2016-11-20T01:17:17Z,OWNER,"I will merge this, when you are back we will have a walkthrough the code
",NA,https://api.github.com/repos/nsomar/Guaka/issues/37/comments,https://github.com/nsomar/Guaka/pull/37#issuecomment-261751277,https://api.github.com/repos/nsomar/Guaka/issues/37
nsomar,Guaka,188943874,https://api.github.com/repos/nsomar/Guaka/issues/comments/260158572,260158572,MDEyOklzc3VlQ29tbWVudDI2MDE1ODU3Mg==,8655789,2016-11-13T00:41:05Z,2016-11-13T00:54:21Z,NONE,"## [Current coverage](https://codecov.io/gh/oarrabi/Guaka/pull/36?src=pr) is 95.01% (diff: 95.85%)

> Merging [#36](https://codecov.io/gh/oarrabi/Guaka/pull/36?src=pr) into [master](https://codecov.io/gh/oarrabi/Guaka/branch/master?src=pr) will increase coverage by **0.04%**

``` diff
@@             master        #36   diff @@
==========================================
  Files            24         28     +4   
  Lines          2149       2288   +139   
  Methods           0          0          
  Messages          0          0          
  Branches          0          0          
==========================================
+ Hits           2041       2174   +133   
- Misses          108        114     +6   
  Partials          0          0          
```

> Powered by [Codecov](https://codecov.io?src=pr). Last update [15ea9e9...5025cd6](https://codecov.io/gh/oarrabi/Guaka/compare/15ea9e983e76e7df218d2891535574085ea5498c...5025cd6e1c55b102c57b8e26c950f21f2bb7b432?src=pr)
",NA,https://api.github.com/repos/nsomar/Guaka/issues/36/comments,https://github.com/nsomar/Guaka/pull/36#issuecomment-260158572,https://api.github.com/repos/nsomar/Guaka/issues/36
nsomar,Guaka,188943874,https://api.github.com/repos/nsomar/Guaka/issues/comments/260738664,260738664,MDEyOklzc3VlQ29tbWVudDI2MDczODY2NA==,1461052,2016-11-15T19:18:51Z,2016-11-15T19:18:51Z,OWNER,"@goyox86 merging this now
",NA,https://api.github.com/repos/nsomar/Guaka/issues/36/comments,https://github.com/nsomar/Guaka/pull/36#issuecomment-260738664,https://api.github.com/repos/nsomar/Guaka/issues/36
nsomar,Guaka,188934284,https://api.github.com/repos/nsomar/Guaka/issues/comments/260147986,260147986,MDEyOklzc3VlQ29tbWVudDI2MDE0Nzk4Ng==,8655789,2016-11-12T20:59:25Z,2016-11-15T21:03:29Z,NONE,"## [Current coverage](https://codecov.io/gh/oarrabi/Guaka/pull/35?src=pr) is 95.01% (diff: 95.85%)

> Merging [#35](https://codecov.io/gh/oarrabi/Guaka/pull/35?src=pr) into [master](https://codecov.io/gh/oarrabi/Guaka/branch/master?src=pr) will increase coverage by **0.04%**

``` diff
@@             master        #35   diff @@
==========================================
  Files            24         28     +4   
  Lines          2149       2288   +139   
  Methods           0          0          
  Messages          0          0          
  Branches          0          0          
==========================================
+ Hits           2041       2174   +133   
- Misses          108        114     +6   
  Partials          0          0          
```

> Powered by [Codecov](https://codecov.io?src=pr). Last update [15ea9e9...0c90dd6](https://codecov.io/gh/oarrabi/Guaka/compare/15ea9e983e76e7df218d2891535574085ea5498c...0c90dd6472e5cf6062dde05826771c7087d40b49?src=pr)
",NA,https://api.github.com/repos/nsomar/Guaka/issues/35/comments,https://github.com/nsomar/Guaka/pull/35#issuecomment-260147986,https://api.github.com/repos/nsomar/Guaka/issues/35
nsomar,Guaka,188934284,https://api.github.com/repos/nsomar/Guaka/issues/comments/260769097,260769097,MDEyOklzc3VlQ29tbWVudDI2MDc2OTA5Nw==,1461052,2016-11-15T21:10:25Z,2016-11-15T21:10:25Z,OWNER,"@goyox86 merging this too
",NA,https://api.github.com/repos/nsomar/Guaka/issues/35/comments,https://github.com/nsomar/Guaka/pull/35#issuecomment-260769097,https://api.github.com/repos/nsomar/Guaka/issues/35
nsomar,Guaka,188915735,https://api.github.com/repos/nsomar/Guaka/issues/comments/260126916,260126916,MDEyOklzc3VlQ29tbWVudDI2MDEyNjkxNg==,8655789,2016-11-12T15:01:49Z,2016-11-12T15:01:49Z,NONE,"## [Current coverage](https://codecov.io/gh/oarrabi/Guaka/pull/34?src=pr) is 94.97% (diff: 100%)

> Merging [#34](https://codecov.io/gh/oarrabi/Guaka/pull/34?src=pr) into [updating-sample-app](https://codecov.io/gh/oarrabi/Guaka/branch/updating-sample-app?src=pr) will not change coverage

``` diff
@@           updating-sample-app        #34   diff @@
=====================================================
  Files                       24         24          
  Lines                     2149       2149          
  Methods                      0          0          
  Messages                     0          0          
  Branches                     0          0          
=====================================================
  Hits                      2041       2041          
  Misses                     108        108          
  Partials                     0          0          
```

> Powered by [Codecov](https://codecov.io?src=pr). Last update [a461a22...e51b084](https://codecov.io/gh/oarrabi/Guaka/compare/a461a2263346a8933a82ed5cd289e3acf458e81c...e51b084dbee8b1bdff7b1d7fdb8490f2c3987e19?src=pr)
",NA,https://api.github.com/repos/nsomar/Guaka/issues/34/comments,https://github.com/nsomar/Guaka/pull/34#issuecomment-260126916,https://api.github.com/repos/nsomar/Guaka/issues/34
nsomar,Guaka,188915735,https://api.github.com/repos/nsomar/Guaka/issues/comments/260128984,260128984,MDEyOklzc3VlQ29tbWVudDI2MDEyODk4NA==,9576,2016-11-12T15:39:02Z,2016-11-12T15:39:02Z,COLLABORATOR,"Looking good! :shipit: ðŸ‘ 
",NA,https://api.github.com/repos/nsomar/Guaka/issues/34/comments,https://github.com/nsomar/Guaka/pull/34#issuecomment-260128984,https://api.github.com/repos/nsomar/Guaka/issues/34
nsomar,Guaka,188915735,https://api.github.com/repos/nsomar/Guaka/issues/comments/260134900,260134900,MDEyOklzc3VlQ29tbWVudDI2MDEzNDkwMA==,1461052,2016-11-12T17:17:00Z,2016-11-12T17:17:00Z,OWNER,"@goyox86 I will close this one and merge #33 
",NA,https://api.github.com/repos/nsomar/Guaka/issues/34/comments,https://github.com/nsomar/Guaka/pull/34#issuecomment-260134900,https://api.github.com/repos/nsomar/Guaka/issues/34
nsomar,Guaka,188908713,https://api.github.com/repos/nsomar/Guaka/issues/comments/260118926,260118926,MDEyOklzc3VlQ29tbWVudDI2MDExODkyNg==,8655789,2016-11-12T12:18:01Z,2016-11-12T17:19:05Z,NONE,"## [Current coverage](https://codecov.io/gh/oarrabi/Guaka/pull/33?src=pr) is 94.97% (diff: 75.00%)

> Merging [#33](https://codecov.io/gh/oarrabi/Guaka/pull/33?src=pr) into [master](https://codecov.io/gh/oarrabi/Guaka/branch/master?src=pr) will decrease coverage by **0.07%**

``` diff
@@             master        #33   diff @@
==========================================
  Files            24         24          
  Lines          2141       2149     +8   
  Methods           0          0          
  Messages          0          0          
  Branches          0          0          
==========================================
+ Hits           2035       2041     +6   
- Misses          106        108     +2   
  Partials          0          0          
```

> Powered by [Codecov](https://codecov.io?src=pr). Last update [059e528...f665c21](https://codecov.io/gh/oarrabi/Guaka/compare/059e528ed509190f66be2c491174c606576863b7...f665c2103b1f7a14e977a5b0de26d847086e75ca?src=pr)
",NA,https://api.github.com/repos/nsomar/Guaka/issues/33/comments,https://github.com/nsomar/Guaka/pull/33#issuecomment-260118926,https://api.github.com/repos/nsomar/Guaka/issues/33
nsomar,Guaka,188908713,https://api.github.com/repos/nsomar/Guaka/issues/comments/260134917,260134917,MDEyOklzc3VlQ29tbWVudDI2MDEzNDkxNw==,1461052,2016-11-12T17:17:19Z,2016-11-12T17:17:19Z,OWNER,"@goyox86 ðŸ‘ please?
",NA,https://api.github.com/repos/nsomar/Guaka/issues/33/comments,https://github.com/nsomar/Guaka/pull/33#issuecomment-260134917,https://api.github.com/repos/nsomar/Guaka/issues/33
nsomar,Guaka,188908713,https://api.github.com/repos/nsomar/Guaka/issues/comments/260144908,260144908,MDEyOklzc3VlQ29tbWVudDI2MDE0NDkwOA==,9576,2016-11-12T20:07:14Z,2016-11-12T20:07:14Z,COLLABORATOR,"ðŸ‘ 
",NA,https://api.github.com/repos/nsomar/Guaka/issues/33/comments,https://github.com/nsomar/Guaka/pull/33#issuecomment-260144908,https://api.github.com/repos/nsomar/Guaka/issues/33
nsomar,Guaka,188643278,https://api.github.com/repos/nsomar/Guaka/issues/comments/259863900,259863900,MDEyOklzc3VlQ29tbWVudDI1OTg2MzkwMA==,8655789,2016-11-11T02:16:28Z,2016-11-12T15:14:55Z,NONE,"## [Current coverage](https://codecov.io/gh/oarrabi/Guaka/pull/32?src=pr) is 95.04% (diff: 97.98%)

> Merging [#32](https://codecov.io/gh/oarrabi/Guaka/pull/32?src=pr) into [master](https://codecov.io/gh/oarrabi/Guaka/branch/master?src=pr) will not change coverage

``` diff
@@             master        #32   diff @@
==========================================
  Files            24         24          
  Lines          2141       2141          
  Methods           0          0          
  Messages          0          0          
  Branches          0          0          
==========================================
  Hits           2035       2035          
  Misses          106        106          
  Partials          0          0          
```

> Powered by [Codecov](https://codecov.io?src=pr). Last update [73c2c00...29dd5c5](https://codecov.io/gh/oarrabi/Guaka/compare/73c2c0017446b357e9ad257601fac240926ad880...29dd5c5204b1aef92e7c6b7fb59ee1e610b60b6a?src=pr)
",NA,https://api.github.com/repos/nsomar/Guaka/issues/32/comments,https://github.com/nsomar/Guaka/pull/32#issuecomment-259863900,https://api.github.com/repos/nsomar/Guaka/issues/32
nsomar,Guaka,188643278,https://api.github.com/repos/nsomar/Guaka/issues/comments/260127519,260127519,MDEyOklzc3VlQ29tbWVudDI2MDEyNzUxOQ==,9576,2016-11-12T15:13:24Z,2016-11-12T15:13:24Z,COLLABORATOR,"@oarrabi Can you please ðŸ‘ this one is very hard to rebase and redo ðŸ˜„ 
",NA,https://api.github.com/repos/nsomar/Guaka/issues/32/comments,https://github.com/nsomar/Guaka/pull/32#issuecomment-260127519,https://api.github.com/repos/nsomar/Guaka/issues/32
nsomar,Guaka,188643278,https://api.github.com/repos/nsomar/Guaka/issues/comments/260133597,260133597,MDEyOklzc3VlQ29tbWVudDI2MDEzMzU5Nw==,1461052,2016-11-12T16:55:18Z,2016-11-12T16:55:18Z,OWNER,"ðŸ‘ 
",NA,https://api.github.com/repos/nsomar/Guaka/issues/32/comments,https://github.com/nsomar/Guaka/pull/32#issuecomment-260133597,https://api.github.com/repos/nsomar/Guaka/issues/32
nsomar,Guaka,188398971,https://api.github.com/repos/nsomar/Guaka/issues/comments/259581349,259581349,MDEyOklzc3VlQ29tbWVudDI1OTU4MTM0OQ==,8655789,2016-11-10T01:56:14Z,2016-11-12T11:28:48Z,NONE,"## [Current coverage](https://codecov.io/gh/oarrabi/Guaka/pull/31?src=pr) is 95.04% (diff: 93.53%)

> Merging [#31](https://codecov.io/gh/oarrabi/Guaka/pull/31?src=pr) into [master](https://codecov.io/gh/oarrabi/Guaka/branch/master?src=pr) will increase coverage by **0.41%**

``` diff
@@             master        #31   diff @@
==========================================
  Files            23         24     +1   
  Lines          1865       2141   +276   
  Methods           0          0          
  Messages          0          0          
  Branches          0          0          
==========================================
+ Hits           1765       2035   +270   
- Misses          100        106     +6   
  Partials          0          0          
```

> Powered by [Codecov](https://codecov.io?src=pr). Last update [def60b3...d2eadd2](https://codecov.io/gh/oarrabi/Guaka/compare/def60b3a387118ebeffb3220e89e002086709098...d2eadd2701fc45939d043e117c34eab6580669f8?src=pr)
",NA,https://api.github.com/repos/nsomar/Guaka/issues/31/comments,https://github.com/nsomar/Guaka/pull/31#issuecomment-259581349,https://api.github.com/repos/nsomar/Guaka/issues/31
nsomar,Guaka,188398971,https://api.github.com/repos/nsomar/Guaka/issues/comments/260116563,260116563,MDEyOklzc3VlQ29tbWVudDI2MDExNjU2Mw==,1461052,2016-11-12T11:26:53Z,2016-11-12T11:26:53Z,OWNER,"I added a fixme, but I need to merge this since I am building on it :)
",NA,https://api.github.com/repos/nsomar/Guaka/issues/31/comments,https://github.com/nsomar/Guaka/pull/31#issuecomment-260116563,https://api.github.com/repos/nsomar/Guaka/issues/31
nsomar,Guaka,188366152,https://api.github.com/repos/nsomar/Guaka/issues/comments/259552129,259552129,MDEyOklzc3VlQ29tbWVudDI1OTU1MjEyOQ==,8655789,2016-11-09T23:02:26Z,2016-11-09T23:02:26Z,NONE,"## [Current coverage](https://codecov.io/gh/oarrabi/Guaka/pull/30?src=pr) is 94.75% (diff: 100%)

> Merging [#30](https://codecov.io/gh/oarrabi/Guaka/pull/30?src=pr) into [master](https://codecov.io/gh/oarrabi/Guaka/branch/master?src=pr) will not change coverage

``` diff
@@             master        #30   diff @@
==========================================
  Files            23         23          
  Lines          1848       1848          
  Methods           0          0          
  Messages          0          0          
  Branches          0          0          
==========================================
  Hits           1751       1751          
  Misses           97         97          
  Partials          0          0          
```

> Powered by [Codecov](https://codecov.io?src=pr). Last update [6ac2b07...bf1c4f5](https://codecov.io/gh/oarrabi/Guaka/compare/6ac2b07d38b1cdf5f06446736e99972f4fb49063...bf1c4f57e7f8df6692bf8741b7e9e65069487338?src=pr)
",NA,https://api.github.com/repos/nsomar/Guaka/issues/30/comments,https://github.com/nsomar/Guaka/pull/30#issuecomment-259552129,https://api.github.com/repos/nsomar/Guaka/issues/30
nsomar,Guaka,188343279,https://api.github.com/repos/nsomar/Guaka/issues/comments/259527861,259527861,MDEyOklzc3VlQ29tbWVudDI1OTUyNzg2MQ==,8655789,2016-11-09T21:12:06Z,2016-11-10T00:05:49Z,NONE,"## [Current coverage](https://codecov.io/gh/oarrabi/Guaka/pull/29?src=pr) is 94.63% (diff: 96.55%)

> Merging [#29](https://codecov.io/gh/oarrabi/Guaka/pull/29?src=pr) into [master](https://codecov.io/gh/oarrabi/Guaka/branch/master?src=pr) will decrease coverage by **0.11%**

``` diff
@@             master        #29   diff @@
==========================================
  Files            23         23          
  Lines          1848       1865    +17   
  Methods           0          0          
  Messages          0          0          
  Branches          0          0          
==========================================
+ Hits           1751       1765    +14   
- Misses           97        100     +3   
  Partials          0          0          
```

> Powered by [Codecov](https://codecov.io?src=pr). Last update [6ac2b07...52ba0d7](https://codecov.io/gh/oarrabi/Guaka/compare/6ac2b07d38b1cdf5f06446736e99972f4fb49063...52ba0d740ebad96875f9a8b5d77d97c24240c659?src=pr)
",NA,https://api.github.com/repos/nsomar/Guaka/issues/29/comments,https://github.com/nsomar/Guaka/pull/29#issuecomment-259527861,https://api.github.com/repos/nsomar/Guaka/issues/29
nsomar,Guaka,188343279,https://api.github.com/repos/nsomar/Guaka/issues/comments/259797117,259797117,MDEyOklzc3VlQ29tbWVudDI1OTc5NzExNw==,9576,2016-11-10T20:22:25Z,2016-11-10T20:22:25Z,COLLABORATOR,":shipit: ðŸ‘ 
",NA,https://api.github.com/repos/nsomar/Guaka/issues/29/comments,https://github.com/nsomar/Guaka/pull/29#issuecomment-259797117,https://api.github.com/repos/nsomar/Guaka/issues/29
nsomar,Guaka,187820238,https://api.github.com/repos/nsomar/Guaka/issues/comments/258989750,258989750,MDEyOklzc3VlQ29tbWVudDI1ODk4OTc1MA==,8655789,2016-11-07T22:53:41Z,2016-11-07T22:53:41Z,NONE,"## [Current coverage](https://codecov.io/gh/oarrabi/Guaka/pull/25?src=pr) is 94.50% (diff: 100%)

> No coverage report found for **master** at 1a01ab7.
> 
> Powered by [Codecov](https://codecov.io?src=pr). Last update [1a01ab7...3a6c353](https://codecov.io/gh/oarrabi/Guaka/compare/1a01ab723be235c31c638935d27446ef5c8fc6b6...3a6c353b56ad95a29ff6d647705370547a7f26c5?src=pr)
",NA,https://api.github.com/repos/nsomar/Guaka/issues/25/comments,https://github.com/nsomar/Guaka/pull/25#issuecomment-258989750,https://api.github.com/repos/nsomar/Guaka/issues/25
nsomar,Guaka,187581109,https://api.github.com/repos/nsomar/Guaka/issues/comments/258702398,258702398,MDEyOklzc3VlQ29tbWVudDI1ODcwMjM5OA==,8655789,2016-11-06T19:04:17Z,2016-11-06T19:04:17Z,NONE,"## [Current coverage](https://codecov.io/gh/oarrabi/Guaka/pull/24?src=pr) is 94.11% (diff: 87.91%)

> Merging [#24](https://codecov.io/gh/oarrabi/Guaka/pull/24?src=pr) into [adding-remove-method](https://codecov.io/gh/oarrabi/Guaka/branch/adding-remove-method?src=pr) will decrease coverage by **0.53%**

``` diff
@@           adding-remove-method        #24   diff @@
======================================================
  Files                        22         23     +1   
  Lines                      1551       1666   +115   
  Methods                       0          0          
  Messages                      0          0          
  Branches                      0          0          
======================================================
+ Hits                       1468       1568   +100   
- Misses                       83         98    +15   
  Partials                      0          0          
```

> Powered by [Codecov](https://codecov.io?src=pr). Last update [285b72b...90f4d63](https://codecov.io/gh/oarrabi/Guaka/compare/285b72b8a2e17da675b4413b36b099d595a7150d...90f4d633bb6768c0949be6eb0c26cdc4f1c910bb?src=pr)
",NA,https://api.github.com/repos/nsomar/Guaka/issues/24/comments,https://github.com/nsomar/Guaka/pull/24#issuecomment-258702398,https://api.github.com/repos/nsomar/Guaka/issues/24
nsomar,Guaka,187574114,https://api.github.com/repos/nsomar/Guaka/issues/comments/258694493,258694493,MDEyOklzc3VlQ29tbWVudDI1ODY5NDQ5Mw==,8655789,2016-11-06T17:05:36Z,2016-11-06T17:05:36Z,NONE,"## [Current coverage](https://codecov.io/gh/oarrabi/Guaka/pull/23?src=pr) is 94.85% (diff: 100%)

> Merging [#23](https://codecov.io/gh/oarrabi/Guaka/pull/23?src=pr) into [adding-remove-method](https://codecov.io/gh/oarrabi/Guaka/branch/adding-remove-method?src=pr) will increase coverage by **0.20%**

``` diff
@@           adding-remove-method        #23   diff @@
======================================================
  Files                        22         22          
  Lines                      1551       1573    +22   
  Methods                       0          0          
  Messages                      0          0          
  Branches                      0          0          
======================================================
+ Hits                       1468       1492    +24   
+ Misses                       83         81     -2   
  Partials                      0          0          
```

> Powered by [Codecov](https://codecov.io?src=pr). Last update [285b72b...0f27b1a](https://codecov.io/gh/oarrabi/Guaka/compare/285b72b8a2e17da675b4413b36b099d595a7150d...0f27b1a1f057485b4f3ed118a02350b6c57eabe4?src=pr)
",NA,https://api.github.com/repos/nsomar/Guaka/issues/23/comments,https://github.com/nsomar/Guaka/pull/23#issuecomment-258694493,https://api.github.com/repos/nsomar/Guaka/issues/23
nsomar,Guaka,187573070,https://api.github.com/repos/nsomar/Guaka/issues/comments/258693265,258693265,MDEyOklzc3VlQ29tbWVudDI1ODY5MzI2NQ==,8655789,2016-11-06T16:47:14Z,2016-11-06T16:47:14Z,NONE,"## [Current coverage](https://codecov.io/gh/oarrabi/Guaka/pull/21?src=pr) is 94.09% (diff: 100%)

> Merging [#21](https://codecov.io/gh/oarrabi/Guaka/pull/21?src=pr) into [master](https://codecov.io/gh/oarrabi/Guaka/branch/master?src=pr) will not change coverage

``` diff
@@             master        #21   diff @@
==========================================
  Files            21         21          
  Lines          1473       1473          
  Methods           0          0          
  Messages          0          0          
  Branches          0          0          
==========================================
  Hits           1386       1386          
  Misses           87         87          
  Partials          0          0          
```

> Powered by [Codecov](https://codecov.io?src=pr). Last update [76ed4ca...f1b4920](https://codecov.io/gh/oarrabi/Guaka/compare/76ed4ca51b44b840e013770e5898c1ecd4724bf4...f1b4920f153bc818902169064e0cb2a6b02a5606?src=pr)
",NA,https://api.github.com/repos/nsomar/Guaka/issues/21/comments,https://github.com/nsomar/Guaka/pull/21#issuecomment-258693265,https://api.github.com/repos/nsomar/Guaka/issues/21
nsomar,Guaka,187572556,https://api.github.com/repos/nsomar/Guaka/issues/comments/259580043,259580043,MDEyOklzc3VlQ29tbWVudDI1OTU4MDA0Mw==,1461052,2016-11-10T01:47:04Z,2016-11-10T01:47:04Z,OWNER,"#31 fixes this
",NA,https://api.github.com/repos/nsomar/Guaka/issues/19/comments,https://github.com/nsomar/Guaka/issues/19#issuecomment-259580043,https://api.github.com/repos/nsomar/Guaka/issues/19
nsomar,Guaka,187572459,https://api.github.com/repos/nsomar/Guaka/issues/comments/259580022,259580022,MDEyOklzc3VlQ29tbWVudDI1OTU4MDAyMg==,1461052,2016-11-10T01:46:55Z,2016-11-10T01:46:55Z,OWNER,"#31 fixes this
",NA,https://api.github.com/repos/nsomar/Guaka/issues/18/comments,https://github.com/nsomar/Guaka/issues/18#issuecomment-259580022,https://api.github.com/repos/nsomar/Guaka/issues/18
nsomar,Guaka,187572355,https://api.github.com/repos/nsomar/Guaka/issues/comments/271416972,271416972,MDEyOklzc3VlQ29tbWVudDI3MTQxNjk3Mg==,1461052,2017-01-09T21:43:45Z,2017-01-09T21:43:45Z,OWNER,Renamed and reset all of them to tag 0.1.0,NA,https://api.github.com/repos/nsomar/Guaka/issues/15/comments,https://github.com/nsomar/Guaka/issues/15#issuecomment-271416972,https://api.github.com/repos/nsomar/Guaka/issues/15
nsomar,Guaka,187571780,https://api.github.com/repos/nsomar/Guaka/issues/comments/275882164,275882164,MDEyOklzc3VlQ29tbWVudDI3NTg4MjE2NA==,9576,2017-01-28T23:26:33Z,2017-01-28T23:26:33Z,COLLABORATOR,Done! Closing this!,NA,https://api.github.com/repos/nsomar/Guaka/issues/6/comments,https://github.com/nsomar/Guaka/issues/6#issuecomment-275882164,https://api.github.com/repos/nsomar/Guaka/issues/6
nsomar,Guaka,187514948,https://api.github.com/repos/nsomar/Guaka/issues/comments/258632525,258632525,MDEyOklzc3VlQ29tbWVudDI1ODYzMjUyNQ==,8655789,2016-11-05T18:43:33Z,2016-11-05T18:43:33Z,NONE,"## [Current coverage](https://codecov.io/gh/oarrabi/Guaka/pull/5?src=pr) is 94.64% (diff: 100%)

> Merging [#5](https://codecov.io/gh/oarrabi/Guaka/pull/5?src=pr) into [adding-remove-method](https://codecov.io/gh/oarrabi/Guaka/branch/adding-remove-method?src=pr) will not change coverage

``` diff
@@           adding-remove-method         #5   diff @@
======================================================
  Files                        22         22          
  Lines                      1551       1551          
  Methods                       0          0          
  Messages                      0          0          
  Branches                      0          0          
======================================================
  Hits                       1468       1468          
  Misses                       83         83          
  Partials                      0          0          
```

> Powered by [Codecov](https://codecov.io?src=pr). Last update [285b72b...ba58e7b](https://codecov.io/gh/oarrabi/Guaka/compare/285b72b8a2e17da675b4413b36b099d595a7150d...ba58e7bed51426f79bd37a9b52ab54b2998c9a47?src=pr)
",NA,https://api.github.com/repos/nsomar/Guaka/issues/5/comments,https://github.com/nsomar/Guaka/pull/5#issuecomment-258632525,https://api.github.com/repos/nsomar/Guaka/issues/5
nsomar,Guaka,187514948,https://api.github.com/repos/nsomar/Guaka/issues/comments/258691652,258691652,MDEyOklzc3VlQ29tbWVudDI1ODY5MTY1Mg==,1461052,2016-11-06T16:22:28Z,2016-11-06T16:22:28Z,OWNER,"@superpeteblaze what do you think about this api?
",NA,https://api.github.com/repos/nsomar/Guaka/issues/5/comments,https://github.com/nsomar/Guaka/pull/5#issuecomment-258691652,https://api.github.com/repos/nsomar/Guaka/issues/5
nsomar,Guaka,187513182,https://api.github.com/repos/nsomar/Guaka/issues/comments/258630623,258630623,MDEyOklzc3VlQ29tbWVudDI1ODYzMDYyMw==,8655789,2016-11-05T18:11:03Z,2016-11-05T18:11:03Z,NONE,"## [Current coverage](https://codecov.io/gh/oarrabi/Guaka/pull/4?src=pr) is 94.80% (diff: 88.23%)

> Merging [#4](https://codecov.io/gh/oarrabi/Guaka/pull/4?src=pr) into [adding-remove-method](https://codecov.io/gh/oarrabi/Guaka/branch/adding-remove-method?src=pr) will increase coverage by **0.15%**

``` diff
@@           adding-remove-method         #4   diff @@
======================================================
  Files                        22         22          
  Lines                      1551       1560     +9   
  Methods                       0          0          
  Messages                      0          0          
  Branches                      0          0          
======================================================
+ Hits                       1468       1479    +11   
+ Misses                       83         81     -2   
  Partials                      0          0          
```

> Powered by [Codecov](https://codecov.io?src=pr). Last update [285b72b...9976752](https://codecov.io/gh/oarrabi/Guaka/compare/285b72b8a2e17da675b4413b36b099d595a7150d...9976752c50d1ce695cb8f13c9f3ef66193e1109f?src=pr)
",NA,https://api.github.com/repos/nsomar/Guaka/issues/4/comments,https://github.com/nsomar/Guaka/pull/4#issuecomment-258630623,https://api.github.com/repos/nsomar/Guaka/issues/4
nsomar,Guaka,187511379,https://api.github.com/repos/nsomar/Guaka/issues/comments/258628774,258628774,MDEyOklzc3VlQ29tbWVudDI1ODYyODc3NA==,8655789,2016-11-05T17:39:28Z,2016-11-05T17:44:57Z,NONE,"## [Current coverage](https://codecov.io/gh/oarrabi/Guaka/pull/3?src=pr) is 94.64% (diff: 100%)

> Merging [#3](https://codecov.io/gh/oarrabi/Guaka/pull/3?src=pr) into [adding-add-command](https://codecov.io/gh/oarrabi/Guaka/branch/adding-add-command?src=pr) will increase coverage by **0.09%**

``` diff
@@           adding-add-command         #3   diff @@
====================================================
  Files                      22         22          
  Lines                    1525       1551    +26   
  Methods                     0          0          
  Messages                    0          0          
  Branches                    0          0          
====================================================
+ Hits                     1442       1468    +26   
  Misses                     83         83          
  Partials                    0          0          
```

> Powered by [Codecov](https://codecov.io?src=pr). Last update [b168198...285b72b](https://codecov.io/gh/oarrabi/Guaka/compare/b168198d808291ff56a5dc685269632a6a1ef168...285b72b8a2e17da675b4413b36b099d595a7150d?src=pr)
",NA,https://api.github.com/repos/nsomar/Guaka/issues/3/comments,https://github.com/nsomar/Guaka/pull/3#issuecomment-258628774,https://api.github.com/repos/nsomar/Guaka/issues/3
nsomar,Guaka,187504466,https://api.github.com/repos/nsomar/Guaka/issues/comments/258620223,258620223,MDEyOklzc3VlQ29tbWVudDI1ODYyMDIyMw==,8655789,2016-11-05T15:51:33Z,2016-11-05T15:51:33Z,NONE,"## [Current coverage](https://codecov.io/gh/oarrabi/Guaka/pull/2?src=pr) is 94.55% (diff: 90.56%)

> Merging [#2](https://codecov.io/gh/oarrabi/Guaka/pull/2?src=pr) into [master](https://codecov.io/gh/oarrabi/Guaka/branch/master?src=pr) will increase coverage by **0.46%**

``` diff
@@             master         #2   diff @@
==========================================
  Files            21         22     +1   
  Lines          1473       1525    +52   
  Methods           0          0          
  Messages          0          0          
  Branches          0          0          
==========================================
+ Hits           1386       1442    +56   
+ Misses           87         83     -4   
  Partials          0          0          
```

> Powered by [Codecov](https://codecov.io?src=pr). Last update [76ed4ca...b168198](https://codecov.io/gh/oarrabi/Guaka/compare/76ed4ca51b44b840e013770e5898c1ecd4724bf4...b168198d808291ff56a5dc685269632a6a1ef168?src=pr)
",NA,https://api.github.com/repos/nsomar/Guaka/issues/2/comments,https://github.com/nsomar/Guaka/pull/2#issuecomment-258620223,https://api.github.com/repos/nsomar/Guaka/issues/2
nsomar,Guaka,187504466,https://api.github.com/repos/nsomar/Guaka/issues/comments/258691621,258691621,MDEyOklzc3VlQ29tbWVudDI1ODY5MTYyMQ==,1461052,2016-11-06T16:22:01Z,2016-11-06T16:22:01Z,OWNER,"Looping in @superpeteblaze
",NA,https://api.github.com/repos/nsomar/Guaka/issues/2/comments,https://github.com/nsomar/Guaka/pull/2#issuecomment-258691621,https://api.github.com/repos/nsomar/Guaka/issues/2
rendro,easy-pie-chart,424590013,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/477839408,477839408,MDEyOklzc3VlQ29tbWVudDQ3NzgzOTQwOA==,45038700,2019-03-29T02:15:15Z,2019-03-29T02:15:15Z,NONE,"[Web Site](https://www.barisdayak.com/google-reklam-uzmani-ads-adwords/) page problem.
thanks. help",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/207/comments,https://github.com/rendro/easy-pie-chart/issues/207#issuecomment-477839408,https://api.github.com/repos/rendro/easy-pie-chart/issues/207
rendro,easy-pie-chart,399942729,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/455771694,455771694,MDEyOklzc3VlQ29tbWVudDQ1NTc3MTY5NA==,46759235,2019-01-19T11:22:48Z,2019-01-19T11:22:48Z,NONE,"I found that in version 1.0.1 colorTrack changes perfectly, but in version 2.1.7 doesn't.  Is it possible to fix?",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/206/comments,https://github.com/rendro/easy-pie-chart/issues/206#issuecomment-455771694,https://api.github.com/repos/rendro/easy-pie-chart/issues/206
rendro,easy-pie-chart,399942729,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/468653199,468653199,MDEyOklzc3VlQ29tbWVudDQ2ODY1MzE5OQ==,40631006,2019-03-01T12:44:41Z,2019-03-01T12:44:41Z,NONE,"Why not:
`$('.some-class').easyPieChart({
barColor:'#ff0000,
trackColor:'#ffffff
});`",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/206/comments,https://github.com/rendro/easy-pie-chart/issues/206#issuecomment-468653199,https://api.github.com/repos/rendro/easy-pie-chart/issues/206
rendro,easy-pie-chart,370313473,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/444551798,444551798,MDEyOklzc3VlQ29tbWVudDQ0NDU1MTc5OA==,40631006,2018-12-05T16:35:30Z,2018-12-05T16:39:37Z,NONE,"You have to initialize them on the accordion event.
I had the same ""issue"" on TAB's so i initialize some charts ""on load"" and some on the Tab callback event.
To start on load i add the class '.onload'.

So my initialization looks like this:
```javascript
$(function () {
    function startAnimateCharts(inside) {
        if (inside===undefined) {
            $('.percentage.onload').each(function () {
                initChart(this);
            });
        }else {
            $(inside).find('.percentage').each(function () {
                initChart(this);
            });
        }
    }
    function initChart(elem) {
        $(elem).easyPieChart({
            animate: 1000,
        });
    }
    startAnimateCharts();
    $('a[data-toggle=""tab""]').on('show.bs.tab', function () {
        startAnimateCharts($(this).attr('href'))
    });
});
```",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/205/comments,https://github.com/rendro/easy-pie-chart/issues/205#issuecomment-444551798,https://api.github.com/repos/rendro/easy-pie-chart/issues/205
rendro,easy-pie-chart,323954545,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/390714395,390714395,MDEyOklzc3VlQ29tbWVudDM5MDcxNDM5NQ==,30520648,2018-05-21T16:53:37Z,2018-05-21T16:53:37Z,NONE,"My solution

![image](https://user-images.githubusercontent.com/30520648/40319213-a04ec4c4-5d27-11e8-9156-425e2465fbf4.png)

```
            <div id=""container_graph"" style=""width: 180px; height: 180px; margin: auto; margin-top: 15px;"">
                @{
                    double val0 = 0;   //total quantity
                    double val1 = 0;   //quantity 1
                    double val2 = 0;   //quantity 2
                    int rot2 = 0;         //rotation graph2

                    double.TryParse(@Model.quantity, out val0);
                    double.TryParse(@Model.quantity1, out val1);
                    double.TryParse(@Model.quantity2, out val2);
                    val1 = (int)((100 * val1) / val0);      //percent 1
                    val2 = (int)((100 * val2) / val0);      //percent 2
                    rot2 = (int)((val1 * 360) / 100 + 9);   //rotation in degree  + 9Â° optional           
                
                    <div id=""chart-bar0"" style=""position: absolute;""></div>
                    if (val1 > 0)
                    {
                    <div id=""chart-bar1"" style=""position: absolute;"" data-track-color=""false"" data-rotate=""0"" data-percent=""@val1"" ></div>
                    }
                    if (val2 > 0)
                    {
                    <div id=""chart-bar2"" style=""position: absolute;"" data-track-color=""false"" data-rotate=""@rot2"" data-percent=""@val2""></div>
                    }                                     
                    <div style=""position: absolute; width: inherit; height: inherit; text-align: center; margin-top: 44px;"">
                        <h3><strong id=""graph_field"">Total<br />
                        </strong>
                            <strong id=""graph_percent"">@(val0) %</strong></h3>
                    </div> 
                }
            </div>
```
```
<script type=""text/javascript"">
        $('#chart-bar0').easyPieChart({
            ""barColor"": null, ""delay"": 300, scaleColor: false, lineWidth: 12, size: 180
        });
        $('#chart-bar1').easyPieChart({
            ""barColor"": #16A77F, ""delay"": 300, scaleColor: false, lineWidth: 12, size: 180
        });
        $('#chart-bar2').easyPieChart({
            ""barColor"": #5FC645, ""delay"": 300, scaleColor: false, lineWidth: 12, size: 180
        });
</script>
```",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/203/comments,https://github.com/rendro/easy-pie-chart/issues/203#issuecomment-390714395,https://api.github.com/repos/rendro/easy-pie-chart/issues/203
rendro,easy-pie-chart,293476667,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/420437346,420437346,MDEyOklzc3VlQ29tbWVudDQyMDQzNzM0Ng==,845031,2018-09-11T21:48:50Z,2018-09-11T21:48:50Z,NONE,"Just place a `<span>` with the percentage text inside it and use CSS to place it inside the circle.. 
This questions is is irrelevant for this script since as a developer you should already know how to do it.",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/202/comments,https://github.com/rendro/easy-pie-chart/issues/202#issuecomment-420437346,https://api.github.com/repos/rendro/easy-pie-chart/issues/202
rendro,easy-pie-chart,293476667,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/494816416,494816416,MDEyOklzc3VlQ29tbWVudDQ5NDgxNjQxNg==,13597641,2019-05-22T14:02:56Z,2019-05-22T14:02:56Z,NONE,It's a valid question. 'Is there an option?' It seems not. Easy to achieve the result in question. The issue ought to be closed.,NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/202/comments,https://github.com/rendro/easy-pie-chart/issues/202#issuecomment-494816416,https://api.github.com/repos/rendro/easy-pie-chart/issues/202
rendro,easy-pie-chart,291846222,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/468654185,468654185,MDEyOklzc3VlQ29tbWVudDQ2ODY1NDE4NQ==,40631006,2019-03-01T12:48:55Z,2019-03-01T12:48:55Z,NONE,"not true:
https://codepen.io/MarioPerini/pen/yweEMZ",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/201/comments,https://github.com/rendro/easy-pie-chart/issues/201#issuecomment-468654185,https://api.github.com/repos/rendro/easy-pie-chart/issues/201
rendro,easy-pie-chart,291846222,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/484777638,484777638,MDEyOklzc3VlQ29tbWVudDQ4NDc3NzYzOA==,33238369,2019-04-19T06:09:37Z,2019-04-19T06:09:37Z,NONE,"@paweljarosz83  did you fix your problem? do you mind sharing with me? I tried on codepen it works but I tried on local js file it doesn't work.
thank you for your help.",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/201/comments,https://github.com/rendro/easy-pie-chart/issues/201#issuecomment-484777638,https://api.github.com/repos/rendro/easy-pie-chart/issues/201
rendro,easy-pie-chart,273128589,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/444563790,444563790,MDEyOklzc3VlQ29tbWVudDQ0NDU2Mzc5MA==,40631006,2018-12-05T17:07:20Z,2018-12-05T17:07:20Z,NONE,"Dublicate of #198

https://github.com/rendro/easy-pie-chart#using-a-gradient


```javascript
$(elem).easyPieChart({
            barColor: function() {
                var ctx = this.renderer.getCtx();
                var canvas = this.renderer.getCanvas();
                var gradient = ctx.createLinearGradient(0,0,canvas.width,0);
                gradient.addColorStop(0, ""#ffe57e"");
                gradient.addColorStop(1, ""#de5900"");
                return gradient;
            }
        });
```",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/200/comments,https://github.com/rendro/easy-pie-chart/issues/200#issuecomment-444563790,https://api.github.com/repos/rendro/easy-pie-chart/issues/200
rendro,easy-pie-chart,273128589,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/462785302,462785302,MDEyOklzc3VlQ29tbWVudDQ2Mjc4NTMwMg==,5278819,2019-02-12T14:42:31Z,2019-02-12T14:42:31Z,NONE,I'd like to see someone provide a working sample of this 'gradient' because I have yet to get any working examples to render,NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/200/comments,https://github.com/rendro/easy-pie-chart/issues/200#issuecomment-462785302,https://api.github.com/repos/rendro/easy-pie-chart/issues/200
rendro,easy-pie-chart,273128589,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/462817024,462817024,MDEyOklzc3VlQ29tbWVudDQ2MjgxNzAyNA==,40631006,2019-02-12T15:58:52Z,2019-02-12T15:58:52Z,NONE,https://codepen.io/MarioPerini/pen/exKeJm,NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/200/comments,https://github.com/rendro/easy-pie-chart/issues/200#issuecomment-462817024,https://api.github.com/repos/rendro/easy-pie-chart/issues/200
rendro,easy-pie-chart,273128589,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/462823625,462823625,MDEyOklzc3VlQ29tbWVudDQ2MjgyMzYyNQ==,5278819,2019-02-12T16:14:40Z,2019-02-12T16:14:40Z,NONE,"Thanks for the quick response. Gradients seem to be relatively tricky to position - especially in a dynamic environment. I have been trying to get some kind of 'ranged' display (not my idea) to be flexible.

https://codepen.io/givan2code/pen/rPKwze",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/200/comments,https://github.com/rendro/easy-pie-chart/issues/200#issuecomment-462823625,https://api.github.com/repos/rendro/easy-pie-chart/issues/200
rendro,easy-pie-chart,273050901,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/468655942,468655942,MDEyOklzc3VlQ29tbWVudDQ2ODY1NTk0Mg==,40631006,2019-03-01T12:55:42Z,2019-03-01T12:55:42Z,NONE,https://codepen.io/MarioPerini/pen/LaGrdR,NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/199/comments,https://github.com/rendro/easy-pie-chart/issues/199#issuecomment-468655942,https://api.github.com/repos/rendro/easy-pie-chart/issues/199
rendro,easy-pie-chart,272097515,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/444562398,444562398,MDEyOklzc3VlQ29tbWVudDQ0NDU2MjM5OA==,40631006,2018-12-05T17:03:22Z,2018-12-05T17:03:22Z,NONE,https://github.com/rendro/easy-pie-chart#using-a-gradient,NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/198/comments,https://github.com/rendro/easy-pie-chart/issues/198#issuecomment-444562398,https://api.github.com/repos/rendro/easy-pie-chart/issues/198
rendro,easy-pie-chart,267351777,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/468656416,468656416,MDEyOklzc3VlQ29tbWVudDQ2ODY1NjQxNg==,40631006,2019-03-01T12:57:22Z,2019-03-01T12:57:22Z,NONE,sure give me your paypal account and i will do that for youðŸ¤£,NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/197/comments,https://github.com/rendro/easy-pie-chart/issues/197#issuecomment-468656416,https://api.github.com/repos/rendro/easy-pie-chart/issues/197
rendro,easy-pie-chart,267351777,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/470665837,470665837,MDEyOklzc3VlQ29tbWVudDQ3MDY2NTgzNw==,3630609,2019-03-07T19:41:48Z,2019-03-07T19:41:48Z,NONE,"> sure give me your paypal account and i will do that for youðŸ¤£

are you kidding or serious or phishing? 
why would you need my paypal account to do this? 
would rather have the option to put in our paypal id/key myself.",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/197/comments,https://github.com/rendro/easy-pie-chart/issues/197#issuecomment-470665837,https://api.github.com/repos/rendro/easy-pie-chart/issues/197
rendro,easy-pie-chart,267351777,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/470685065,470685065,MDEyOklzc3VlQ29tbWVudDQ3MDY4NTA2NQ==,40631006,2019-03-07T20:39:22Z,2019-03-07T20:41:13Z,NONE,"I comment with my real Name and add a ""ðŸ¤£"" to the comment... I dont think this is a good strategy for Phishing ðŸ˜‰
Just kidding... Issue is also 1 year old. Do you need Help?",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/197/comments,https://github.com/rendro/easy-pie-chart/issues/197#issuecomment-470685065,https://api.github.com/repos/rendro/easy-pie-chart/issues/197
rendro,easy-pie-chart,263148864,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/334497301,334497301,MDEyOklzc3VlQ29tbWVudDMzNDQ5NzMwMQ==,1201844,2017-10-05T15:15:49Z,2017-10-05T15:15:49Z,NONE,"Nevermind! I was using a the 'square' lineCap instead of 'butt' :"")",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/196/comments,https://github.com/rendro/easy-pie-chart/issues/196#issuecomment-334497301,https://api.github.com/repos/rendro/easy-pie-chart/issues/196
rendro,easy-pie-chart,224444269,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/297423523,297423523,MDEyOklzc3VlQ29tbWVudDI5NzQyMzUyMw==,6624821,2017-04-26T14:20:10Z,2017-04-26T14:20:10Z,NONE,"i have a similar issue, sometimes it does not render and i have to refresh or adjust the zoom",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/193/comments,https://github.com/rendro/easy-pie-chart/issues/193#issuecomment-297423523,https://api.github.com/repos/rendro/easy-pie-chart/issues/193
rendro,easy-pie-chart,224444269,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/389901568,389901568,MDEyOklzc3VlQ29tbWVudDM4OTkwMTU2OA==,2639930,2018-05-17T15:11:31Z,2018-05-17T15:11:31Z,NONE,same issue,NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/193/comments,https://github.com/rendro/easy-pie-chart/issues/193#issuecomment-389901568,https://api.github.com/repos/rendro/easy-pie-chart/issues/193
rendro,easy-pie-chart,224444269,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/389979692,389979692,MDEyOklzc3VlQ29tbWVudDM4OTk3OTY5Mg==,19664128,2018-05-17T19:21:59Z,2018-05-17T19:21:59Z,NONE,"Check this : https://stackoverflow.com/questions/43629495/css-class-design-is-not-working-to-appended-elements
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/193/comments,https://github.com/rendro/easy-pie-chart/issues/193#issuecomment-389979692,https://api.github.com/repos/rendro/easy-pie-chart/issues/193
rendro,easy-pie-chart,211275761,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/283768233,283768233,MDEyOklzc3VlQ29tbWVudDI4Mzc2ODIzMw==,26120356,2017-03-02T20:19:52Z,2017-03-02T20:19:52Z,NONE,"Meanwhile I've found out how to do it. If anyone is dealing with the same issue and wants to display values up to 10 instead of 100:

1) Multiple the data-percent attribut by 10
2) In script section find line: 
`jQuery(this.el).find('.percent').text(Math.round(percent));`
and divide the percent value by 10, so the edited line will be:
`jQuery(this.el).find('.percent').text(Math.round(percent/10));`",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/191/comments,https://github.com/rendro/easy-pie-chart/issues/191#issuecomment-283768233,https://api.github.com/repos/rendro/easy-pie-chart/issues/191
rendro,easy-pie-chart,202383762,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/274765520,274765520,MDEyOklzc3VlQ29tbWVudDI3NDc2NTUyMA==,5186792,2017-01-24T10:28:16Z,2017-01-24T10:28:16Z,NONE,try set `lineCap` to `butt` ,NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/189/comments,https://github.com/rendro/easy-pie-chart/issues/189#issuecomment-274765520,https://api.github.com/repos/rendro/easy-pie-chart/issues/189
rendro,easy-pie-chart,193317424,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/351305828,351305828,MDEyOklzc3VlQ29tbWVudDM1MTMwNTgyOA==,4281333,2017-12-13T07:24:05Z,2017-12-13T07:24:05Z,NONE,"I was having a similar problem. The probable cause is calling `.easyPieChart()` on an element which is not a jQuery object. Maybe you should wait for `$(document).ready(..)` ? (I don't know VueJS so I can't give you exact answer).
In your case I would check the `$('.easypiechart')` value.

Aside note: It is possible to import jquery easy-pie-chart with:
```js
import 'easy-pie-chart/dist/jquery.easypiechart';
```
Which is for me more elegant way of using it.",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/185/comments,https://github.com/rendro/easy-pie-chart/issues/185#issuecomment-351305828,https://api.github.com/repos/rendro/easy-pie-chart/issues/185
rendro,easy-pie-chart,180010505,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/263375740,263375740,MDEyOklzc3VlQ29tbWVudDI2MzM3NTc0MA==,8641492,2016-11-28T19:53:52Z,2016-11-28T19:54:02Z,NONE,Same question here.,NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/181/comments,https://github.com/rendro/easy-pie-chart/issues/181#issuecomment-263375740,https://api.github.com/repos/rendro/easy-pie-chart/issues/181
rendro,easy-pie-chart,169380449,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/282419378,282419378,MDEyOklzc3VlQ29tbWVudDI4MjQxOTM3OA==,10137,2017-02-24T22:10:32Z,2017-02-24T22:10:32Z,NONE,"Hi Zaid, did you found solution to this problem?
I have the same problem here! Thanks
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/178/comments,https://github.com/rendro/easy-pie-chart/issues/178#issuecomment-282419378,https://api.github.com/repos/rendro/easy-pie-chart/issues/178
rendro,easy-pie-chart,169380449,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/326746415,326746415,MDEyOklzc3VlQ29tbWVudDMyNjc0NjQxNQ==,14292441,2017-09-02T14:12:54Z,2017-09-02T14:13:14Z,NONE,"i got it working after doing the following :
`var active = false;
var element = document.querySelector('.chart');
var chart = new EasyPieChart(element, {
// your options goes here
});
chart.update(0);

function isScrolledIntoView(elem)
{
var docViewTop = jQuery(window).scrollTop();
var docViewBottom = docViewTop + jQuery(window).height();
var elemTop = jQuery(elem).offset().top;
var elemBottom = elemTop + jQuery(elem).height();
return ((elemBottom >= docViewTop) && (elemTop <= docViewBottom) && (elemBottom <= docViewBottom) && (elemTop >= docViewTop));
}

jQuery( document ).ready(function() {
// in my case i only update it when the user is scrolled to it
if(isScrolledIntoView(jQuery('.chart')) && !active) {
charts.forEach(function(chart) {
chart.update(chart.el.dataset.percent);
});
active = true;
}
});`",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/178/comments,https://github.com/rendro/easy-pie-chart/issues/178#issuecomment-326746415,https://api.github.com/repos/rendro/easy-pie-chart/issues/178
rendro,easy-pie-chart,161476515,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/234580600,234580600,MDEyOklzc3VlQ29tbWVudDIzNDU4MDYwMA==,1316850,2016-07-22T15:48:23Z,2016-07-22T15:48:23Z,NONE,"Any more info on this? Seems like 0% would be a common situation when you are using this as a progress chart. 
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/176/comments,https://github.com/rendro/easy-pie-chart/issues/176#issuecomment-234580600,https://api.github.com/repos/rendro/easy-pie-chart/issues/176
rendro,easy-pie-chart,160287735,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/228992209,228992209,MDEyOklzc3VlQ29tbWVudDIyODk5MjIwOQ==,4072649,2016-06-28T08:59:30Z,2016-06-28T08:59:30Z,NONE,"Hi,
I am looking for the same thing. Do you have any progress?
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/174/comments,https://github.com/rendro/easy-pie-chart/issues/174#issuecomment-228992209,https://api.github.com/repos/rendro/easy-pie-chart/issues/174
rendro,easy-pie-chart,160287735,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/229189839,229189839,MDEyOklzc3VlQ29tbWVudDIyOTE4OTgzOQ==,422168,2016-06-28T21:29:44Z,2016-06-28T21:30:04Z,OWNER,"You can delete it in the css. just remove the pseudo element or change its `content` property.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/174/comments,https://github.com/rendro/easy-pie-chart/issues/174#issuecomment-229189839,https://api.github.com/repos/rendro/easy-pie-chart/issues/174
rendro,easy-pie-chart,160287735,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/230121414,230121414,MDEyOklzc3VlQ29tbWVudDIzMDEyMTQxNA==,19274459,2016-07-02T20:49:22Z,2016-07-02T20:49:22Z,NONE,"@rendro thanks
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/174/comments,https://github.com/rendro/easy-pie-chart/issues/174#issuecomment-230121414,https://api.github.com/repos/rendro/easy-pie-chart/issues/174
rendro,easy-pie-chart,160287735,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/230432635,230432635,MDEyOklzc3VlQ29tbWVudDIzMDQzMjYzNQ==,4072649,2016-07-05T09:38:23Z,2016-07-05T09:40:47Z,NONE,"Thanks @rendro. To support  multiple units I added new class for each unit: 
`.celsius {
    display: inline-block;
    line-height: 84px;
    z-index: 2;
    font-size: 16px;
  }
  .celsius:after {
    content: ""\00b0""; 
    margin-left: 0.1em;
    font-size: .8em;
  }`
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/174/comments,https://github.com/rendro/easy-pie-chart/issues/174#issuecomment-230432635,https://api.github.com/repos/rendro/easy-pie-chart/issues/174
rendro,easy-pie-chart,159371455,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/224914930,224914930,MDEyOklzc3VlQ29tbWVudDIyNDkxNDkzMA==,10337838,2016-06-09T14:37:31Z,2016-06-09T14:37:41Z,NONE,"Sorry, I know it is not an issue. But dont know where I ask it.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/173/comments,https://github.com/rendro/easy-pie-chart/issues/173#issuecomment-224914930,https://api.github.com/repos/rendro/easy-pie-chart/issues/173
rendro,easy-pie-chart,159371455,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/225703225,225703225,MDEyOklzc3VlQ29tbWVudDIyNTcwMzIyNQ==,10337838,2016-06-13T20:46:35Z,2016-06-13T20:46:35Z,NONE,"Strange,
There is no answer in anywhere about this.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/173/comments,https://github.com/rendro/easy-pie-chart/issues/173#issuecomment-225703225,https://api.github.com/repos/rendro/easy-pie-chart/issues/173
rendro,easy-pie-chart,143781613,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/212887949,212887949,MDEyOklzc3VlQ29tbWVudDIxMjg4Nzk0OQ==,18594962,2016-04-21T12:17:09Z,2016-04-21T12:17:09Z,NONE,"Hi, If i understand you correctly, you want to click on a color and change the color of the bar.

You have to:
1) update the color
2) update the chart to the same value it already is. This will force a color change to the color you updated in point 1).

Try this:
$("".chart"").data('easyPieChart').options['barColor'] = ""#00B2CA"";
$("".chart"").data('easyPieChart').update(98);

(where 98 is the value the chart currently has, If you set it to another value, the chart will animate the bar to the new value and new color. By setting it to the same value, it won't animate but it will change the color).

I hope this helps.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/172/comments,https://github.com/rendro/easy-pie-chart/issues/172#issuecomment-212887949,https://api.github.com/repos/rendro/easy-pie-chart/issues/172
rendro,easy-pie-chart,143781613,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/238193277,238193277,MDEyOklzc3VlQ29tbWVudDIzODE5MzI3Nw==,18423928,2016-08-08T10:01:21Z,2016-08-08T10:01:21Z,NONE,"Please can you help how I can achieve same thing in Angular JS? Currently I'm trying with below code, but there is no luck.

$scope.options = {
     animate:{
      duration:2000,
    enabled:true
    },
    barColor:'#005A04',
      scaleColor:true,
    lineWidth:20,
      lineCap:'circle'
};
Thanks for your time...
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/172/comments,https://github.com/rendro/easy-pie-chart/issues/172#issuecomment-238193277,https://api.github.com/repos/rendro/easy-pie-chart/issues/172
rendro,easy-pie-chart,143781613,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/334236710,334236710,MDEyOklzc3VlQ29tbWVudDMzNDIzNjcxMA==,12519008,2017-10-04T17:50:10Z,2017-10-04T17:50:10Z,NONE,@josezulu how to do that using angular?,NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/172/comments,https://github.com/rendro/easy-pie-chart/issues/172#issuecomment-334236710,https://api.github.com/repos/rendro/easy-pie-chart/issues/172
rendro,easy-pie-chart,143781613,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/334243242,334243242,MDEyOklzc3VlQ29tbWVudDMzNDI0MzI0Mg==,18594962,2017-10-04T18:12:55Z,2017-10-04T18:12:55Z,NONE,"I'm sorry, I don't know Angular :(",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/172/comments,https://github.com/rendro/easy-pie-chart/issues/172#issuecomment-334243242,https://api.github.com/repos/rendro/easy-pie-chart/issues/172
rendro,easy-pie-chart,143781613,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/360263499,360263499,MDEyOklzc3VlQ29tbWVudDM2MDI2MzQ5OQ==,26156465,2018-01-24T20:30:05Z,2018-01-24T20:30:05Z,NONE,"Hello! Sorry i'm late but to solve this problem in Angular2+ you can follow these steps:

1. Create a ViewChild of the easypiechart in component.html
`<easy-pie-chart #pieChart ></easy-pie-chart>`
2. Add the viewchild to the component.ts
`@ViewChild('pieChart') pieChart:any;`
3. Whenever you want to update the color call this:
`this.pieChart.pieChart.options['barColor'] = ""#00B2CA"";`
I update it on ValueChanges but you can update it whenever. 

You can edit any options you want that way in Angular2+.",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/172/comments,https://github.com/rendro/easy-pie-chart/issues/172#issuecomment-360263499,https://api.github.com/repos/rendro/easy-pie-chart/issues/172
rendro,easy-pie-chart,130680223,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/564689453,564689453,MDEyOklzc3VlQ29tbWVudDU2NDY4OTQ1Mw==,18594962,2019-12-11T19:11:56Z,2019-12-11T19:11:56Z,NONE,"I was able to achieve this by using a function for barColor option when initialising the chart, and adding ""data-barColor"" attribute to the chart element.

`<div class=""chart"" id=""my-chart"" data-percent=""-87"" data-barcolor=""#2BAB00"">`

```
$("".chart"").easyPieChart({
   barColor: function() {
      return this.el.getAttribute(""data-barColor"") ? this.el.getAttribute(""data-barColor"") : ""gray"";
   }
 });
```

Note that I'm not using the `percentage` variable, which seems to prevent the chart from changing from default color to final color as it reaches the target value.

This way when I have many chart elements, it should render each chart with a different color according to the data-barColor value, or gray if the attribute is not set.",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/170/comments,https://github.com/rendro/easy-pie-chart/pull/170#issuecomment-564689453,https://api.github.com/repos/rendro/easy-pie-chart/issues/170
rendro,easy-pie-chart,129598614,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/181656192,181656192,MDEyOklzc3VlQ29tbWVudDE4MTY1NjE5Mg==,1494060,2016-02-09T01:18:55Z,2016-02-09T01:18:55Z,NONE,"I am having trouble with this too. It says in the documentation:

_(the method is scoped to the context of the plugin, so you can access the DOM element via this.el)._

But any time I try to use it, I get errors. For instance:

```
onStep: function(value) {
     this.$el.find('span').text(~~value);
}
```

Yields an error ""TypeError: this.$el is undefined"".
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/169/comments,https://github.com/rendro/easy-pie-chart/issues/169#issuecomment-181656192,https://api.github.com/repos/rendro/easy-pie-chart/issues/169
rendro,easy-pie-chart,129598614,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/182054071,182054071,MDEyOklzc3VlQ29tbWVudDE4MjA1NDA3MQ==,422168,2016-02-09T20:41:54Z,2016-02-09T20:41:54Z,OWNER,"@watermelonkid you can access the element via `this.el` not `this.$el` which are two different JS variables. Try:

``` javascript
onStep: function(value) {
     $(this.el).find('span').text(Math.round(value));
}
```

@Pantela777: Add the `onStep` function as a property to the config object when initializing easy-pie-chart
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/169/comments,https://github.com/rendro/easy-pie-chart/issues/169#issuecomment-182054071,https://api.github.com/repos/rendro/easy-pie-chart/issues/169
rendro,easy-pie-chart,129598614,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/182326274,182326274,MDEyOklzc3VlQ29tbWVudDE4MjMyNjI3NA==,10137,2016-02-10T11:36:25Z,2016-02-10T11:36:25Z,NONE,"I use version 2.1.5
On **onStep** my **value** is empty, I try to write static number, he output, but animate is not work...

```
$('.easy-pie-chart.percentage').each(function(){
    var barColor = $(this).data('color') || '#555555';
    var trackColor = '#E2E2E2';
    var scaleColor = $(this).data('color');
    var size = parseInt($(this).data('size')) || 80;
    $(this).easyPieChart({
        onStep: function(value) {
             $(this.el).find('span').text(Math.round(value));
        },
        barColor: barColor,
        trackColor: trackColor,
        scaleColor: scaleColor,
        lineCap: 'butt',
        scaleLength: 5,
        rotate: 0,
        lineWidth: parseInt(size/10),
        animate: 3000,
        size: size
    }).css('color', barColor);
});
```
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/169/comments,https://github.com/rendro/easy-pie-chart/issues/169#issuecomment-182326274,https://api.github.com/repos/rendro/easy-pie-chart/issues/169
rendro,easy-pie-chart,129598614,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/205779797,205779797,MDEyOklzc3VlQ29tbWVudDIwNTc3OTc5Nw==,492207,2016-04-05T12:31:43Z,2016-04-05T12:31:43Z,NONE,"People should be aware the onStep function is using 3 variables, and if you use the first, that's the ""from"" value.

If you look in the code, you will see this:

`onStep: function(from, to, currentValue)`

So please use ""currentValue"" instead of ""from"" (or ""value"" as in the above examples).
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/169/comments,https://github.com/rendro/easy-pie-chart/issues/169#issuecomment-205779797,https://api.github.com/repos/rendro/easy-pie-chart/issues/169
rendro,easy-pie-chart,129598614,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/245193910,245193910,MDEyOklzc3VlQ29tbWVudDI0NTE5MzkxMA==,5033148,2016-09-07T07:06:42Z,2016-09-07T07:06:42Z,NONE,"@Pantela777, 
I was having this same issue.  What finally made everything work for me was changing the version of easy-pie-chart.js that my page was calling, to this one (version 1.2.3)  http://rendro.github.io/easy-pie-chart/javascripts/jquery.easy-pie-chart.js

Also, I'm not sure if it's mandatory that the number in your html be wrapped in a span element (or if it could, for instance, be wrapped in another element like p or div), but span is what I used.

And here is the code I use to call it:

```
$(function() {
   $('.chart').easyPieChart({
     scaleColor: ""transparent"",
     lineWidth: 18,
     lineCap: 'round',
     barColor: '#229CEF',
     trackColor:    ""#bbb"",
     size: 120,
     rotate: 0,    
     animate: 1000,
     // animate the numbers
    onStep: function(value) {
      this.$el.find('span').text(Math.round(value));
    },
    onStop: function(value, to) {
      this.$el.find('span').text(Math.round(to));
    }
  });
});
```
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/169/comments,https://github.com/rendro/easy-pie-chart/issues/169#issuecomment-245193910,https://api.github.com/repos/rendro/easy-pie-chart/issues/169
rendro,easy-pie-chart,129598614,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/303085620,303085620,MDEyOklzc3VlQ29tbWVudDMwMzA4NTYyMA==,10137,2017-05-22T12:27:13Z,2017-05-22T12:40:34Z,NONE,"I understand this, but I want to start animate from **0%** to **100%**, and then change with exp. **52 000$** (if possible change with **fadeIn/fadeOut** effect).

One of solution I think add block with **display: none;** and call show it with **onStop**, also need hide percent...

I try to set **52 000$** in **data-percent=""""**, but animate is not work, I understand because 100% is very quickly ups...",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/169/comments,https://github.com/rendro/easy-pie-chart/issues/169#issuecomment-303085620,https://api.github.com/repos/rendro/easy-pie-chart/issues/169
rendro,easy-pie-chart,129598614,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/354610199,354610199,MDEyOklzc3VlQ29tbWVudDM1NDYxMDE5OQ==,11913073,2017-12-31T15:38:17Z,2017-12-31T15:39:34Z,NONE,"check the demo on git hub ( i should have done that from the start huhu) gotta like bad documentation.
what you need is : 
html
`	<span class=""chart"" data-percent=""86"">
				<span class=""percent""></span>
	     	</span>`
css 
`.chart {
  position: relative;
  display: inline-block;
  width: 110px;
  height: 110px;
  margin-top: 50px;
  margin-bottom: 50px;
  text-align: center;
}
.chart canvas {
  position: absolute;
  top: 0;
  left: 0;
}
.percent {
  display: inline-block;
  line-height: 110px;
  z-index: 2;
}
.percent:after {
  content: '%';
  margin-left: 0.1em;
  font-size: .8em;
}`

jquery not javaquery lol

> $(function() {
	  $('.chart').easyPieChart({
			easing: 'easeOut',
			onStep: function(from, to, percent) {
				$(this.el).find('.percent').text(Math.round(percent));
			}	
		});
}); 

Et voila ! ",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/169/comments,https://github.com/rendro/easy-pie-chart/issues/169#issuecomment-354610199,https://api.github.com/repos/rendro/easy-pie-chart/issues/169
rendro,easy-pie-chart,127977915,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/230112748,230112748,MDEyOklzc3VlQ29tbWVudDIzMDExMjc0OA==,3372879,2016-07-02T17:23:01Z,2016-07-02T17:23:01Z,NONE,"@doczoidberg, did you got any luck with this?
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/166/comments,https://github.com/rendro/easy-pie-chart/issues/166#issuecomment-230112748,https://api.github.com/repos/rendro/easy-pie-chart/issues/166
rendro,easy-pie-chart,127977915,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/230112773,230112773,MDEyOklzc3VlQ29tbWVudDIzMDExMjc3Mw==,1972209,2016-07-02T17:23:30Z,2016-07-02T17:23:30Z,NONE,"no
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/166/comments,https://github.com/rendro/easy-pie-chart/issues/166#issuecomment-230112773,https://api.github.com/repos/rendro/easy-pie-chart/issues/166
rendro,easy-pie-chart,127977915,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/230112819,230112819,MDEyOklzc3VlQ29tbWVudDIzMDExMjgxOQ==,3372879,2016-07-02T17:24:28Z,2016-07-02T17:24:28Z,NONE,"it's a pity
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/166/comments,https://github.com/rendro/easy-pie-chart/issues/166#issuecomment-230112819,https://api.github.com/repos/rendro/easy-pie-chart/issues/166
rendro,easy-pie-chart,127977915,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/233621292,233621292,MDEyOklzc3VlQ29tbWVudDIzMzYyMTI5Mg==,20076512,2016-07-19T12:47:47Z,2016-07-19T12:47:47Z,NONE,"+1, would love ng2 support too
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/166/comments,https://github.com/rendro/easy-pie-chart/issues/166#issuecomment-233621292,https://api.github.com/repos/rendro/easy-pie-chart/issues/166
rendro,easy-pie-chart,127977915,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/307030159,307030159,MDEyOklzc3VlQ29tbWVudDMwNzAzMDE1OQ==,6689596,2017-06-08T08:04:06Z,2017-06-08T08:04:06Z,NONE,Did anyone get luck with this ?,NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/166/comments,https://github.com/rendro/easy-pie-chart/issues/166#issuecomment-307030159,https://api.github.com/repos/rendro/easy-pie-chart/issues/166
rendro,easy-pie-chart,127977915,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/314993333,314993333,MDEyOklzc3VlQ29tbWVudDMxNDk5MzMzMw==,3693094,2017-07-13T07:16:44Z,2017-07-13T07:16:44Z,NONE,https://www.npmjs.com/package/ng2modules-easypiechart,NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/166/comments,https://github.com/rendro/easy-pie-chart/issues/166#issuecomment-314993333,https://api.github.com/repos/rendro/easy-pie-chart/issues/166
rendro,easy-pie-chart,127977915,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/331778723,331778723,MDEyOklzc3VlQ29tbWVudDMzMTc3ODcyMw==,15693697,2017-09-25T05:11:35Z,2017-09-25T05:11:35Z,NONE,"To implement easy-pie-chart in ng2, 
Please traverse this repo for reference - https://github.com/akveo/ngx-admin/blob/ng2-admin/src/app/pages/dashboard/pieChart/pieChart.component.ts",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/166/comments,https://github.com/rendro/easy-pie-chart/issues/166#issuecomment-331778723,https://api.github.com/repos/rendro/easy-pie-chart/issues/166
rendro,easy-pie-chart,127977915,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/395038087,395038087,MDEyOklzc3VlQ29tbWVudDM5NTAzODA4Nw==,38454,2018-06-06T11:37:01Z,2018-06-06T11:37:01Z,NONE,"[shameless plug]
FYI here - https://www.npmjs.com/package/ngx-easypiechart . ",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/166/comments,https://github.com/rendro/easy-pie-chart/issues/166#issuecomment-395038087,https://api.github.com/repos/rendro/easy-pie-chart/issues/166
rendro,easy-pie-chart,122567759,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/165209109,165209109,MDEyOklzc3VlQ29tbWVudDE2NTIwOTEwOQ==,5200235,2015-12-16T18:49:31Z,2015-12-16T18:49:31Z,NONE,"```
.chart canvas {
    transform: rotate(-180deg);
}
```

Did the trick
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/165/comments,https://github.com/rendro/easy-pie-chart/issues/165#issuecomment-165209109,https://api.github.com/repos/rendro/easy-pie-chart/issues/165
rendro,easy-pie-chart,122567759,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/165214035,165214035,MDEyOklzc3VlQ29tbWVudDE2NTIxNDAzNQ==,422168,2015-12-16T19:06:51Z,2015-12-16T19:07:36Z,OWNER,"Or you use the `rotate` option. Then the whole canvas will be rotated. This also works in browsers that do not support css transforms.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/165/comments,https://github.com/rendro/easy-pie-chart/issues/165#issuecomment-165214035,https://api.github.com/repos/rendro/easy-pie-chart/issues/165
rendro,easy-pie-chart,122567759,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/165444874,165444874,MDEyOklzc3VlQ29tbWVudDE2NTQ0NDg3NA==,5200235,2015-12-17T12:50:07Z,2015-12-17T12:50:07Z,NONE,":+1:  Thanks @rendro 
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/165/comments,https://github.com/rendro/easy-pie-chart/issues/165#issuecomment-165444874,https://api.github.com/repos/rendro/easy-pie-chart/issues/165
rendro,easy-pie-chart,117316850,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/224884058,224884058,MDEyOklzc3VlQ29tbWVudDIyNDg4NDA1OA==,10337838,2016-06-09T12:47:43Z,2016-06-09T12:48:36Z,NONE,"add these between **head** clause

```
<style type=""text/css"">
        .percent {
            display: inline-block;
            line-height: 110px;
            z-index: 2;
        }

        .percent:after {
            content: '%';
            margin-left: 0.1em;
            font-size: .8em;
        }
</style>
```
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/164/comments,https://github.com/rendro/easy-pie-chart/issues/164#issuecomment-224884058,https://api.github.com/repos/rendro/easy-pie-chart/issues/164
rendro,easy-pie-chart,115093392,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/168631799,168631799,MDEyOklzc3VlQ29tbWVudDE2ODYzMTc5OQ==,154553,2016-01-04T10:14:27Z,2016-01-04T10:14:27Z,NONE,":+1: using bower latest version is `2.1.6`
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/163/comments,https://github.com/rendro/easy-pie-chart/issues/163#issuecomment-168631799,https://api.github.com/repos/rendro/easy-pie-chart/issues/163
rendro,easy-pie-chart,115093392,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/169032593,169032593,MDEyOklzc3VlQ29tbWVudDE2OTAzMjU5Mw==,10260008,2016-01-05T15:19:12Z,2016-01-05T15:19:12Z,NONE,"thank you!
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/163/comments,https://github.com/rendro/easy-pie-chart/issues/163#issuecomment-169032593,https://api.github.com/repos/rendro/easy-pie-chart/issues/163
rendro,easy-pie-chart,113761996,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/177336517,177336517,MDEyOklzc3VlQ29tbWVudDE3NzMzNjUxNw==,4361296,2016-01-30T23:28:58Z,2016-01-30T23:28:58Z,NONE,"+1
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/162/comments,https://github.com/rendro/easy-pie-chart/issues/162#issuecomment-177336517,https://api.github.com/repos/rendro/easy-pie-chart/issues/162
rendro,easy-pie-chart,110496443,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/146677250,146677250,MDEyOklzc3VlQ29tbWVudDE0NjY3NzI1MA==,5648875,2015-10-08T20:30:45Z,2015-10-08T20:30:45Z,NONE,"I'm also trying to figure this out, as I wanted to use the code in a product I'm working on, but my understanding is that the GPL may not allow it to be used if there is profit involved.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/161/comments,https://github.com/rendro/easy-pie-chart/issues/161#issuecomment-146677250,https://api.github.com/repos/rendro/easy-pie-chart/issues/161
rendro,easy-pie-chart,110496443,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/146819983,146819983,MDEyOklzc3VlQ29tbWVudDE0NjgxOTk4Mw==,3691490,2015-10-09T10:02:12Z,2015-10-09T10:02:12Z,NONE,"What about MIT or LGPL?
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/161/comments,https://github.com/rendro/easy-pie-chart/issues/161#issuecomment-146819983,https://api.github.com/repos/rendro/easy-pie-chart/issues/161
rendro,easy-pie-chart,110496443,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/146950825,146950825,MDEyOklzc3VlQ29tbWVudDE0Njk1MDgyNQ==,5648875,2015-10-09T18:10:04Z,2015-10-09T18:21:07Z,NONE,"From what I've seen, MIT is always allowed to be used for any purpose so long as the license is included. I am not very familiar with GPL though.

Edit: From what I see [here](http://programmers.stackexchange.com/questions/37231/using-a-gpl-licensed-library-in-a-commercial-app), it looks like you can use code, under a GPL license, commercially as long as you also open source the code. In some cases as mentioned below, you may not have to do that though. I'll throw in a quoted answer from that link:

> GPL Required you to release source code for your distribution (In case of Full GPL but not lesser-GPL which is much more common in libraries)
> 
> But do not mix up with the following concepts.
> 
> You can still distribute any work for a fee thus making profit. (As long as you ship the binary with the source code)
> You don't have to provide source code, if you don't ship/distribute any binaries, in case of SaaS (Software as a Service)""

In the case with easy-pie-chart, I don't know what is intended by the author or what would be considered appropriate use, so that's why I'm also curious which version of the GPL applies.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/161/comments,https://github.com/rendro/easy-pie-chart/issues/161#issuecomment-146950825,https://api.github.com/repos/rendro/easy-pie-chart/issues/161
rendro,easy-pie-chart,110496443,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/618313100,618313100,MDEyOklzc3VlQ29tbWVudDYxODMxMzEwMA==,10137,2020-04-23T10:11:51Z,2020-04-23T10:11:51Z,NONE,"This is a good question.
I see the Repo with MIT license but scrolled to the bottom, I see MIT & GPL dual licenses... which one can I use?",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/161/comments,https://github.com/rendro/easy-pie-chart/issues/161#issuecomment-618313100,https://api.github.com/repos/rendro/easy-pie-chart/issues/161
rendro,easy-pie-chart,106326254,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/216823212,216823212,MDEyOklzc3VlQ29tbWVudDIxNjgyMzIxMg==,1949729,2016-05-04T10:28:23Z,2016-05-04T10:28:23Z,NONE,"+1
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/156/comments,https://github.com/rendro/easy-pie-chart/issues/156#issuecomment-216823212,https://api.github.com/repos/rendro/easy-pie-chart/issues/156
rendro,easy-pie-chart,105582605,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/282619853,282619853,MDEyOklzc3VlQ29tbWVudDI4MjYxOTg1Mw==,14306232,2017-02-27T03:29:34Z,2017-02-27T03:31:18Z,NONE,"Hey @thallysondias I had the same issue and figured out that because you're firing the update function on every scroll event, the plugin can't animate it. I got around this by setting a boolean to check whether the chart had already been updated:
```

var isActive = false;

if($(window).scrollTop() > $(window).height() + 600) {
    if(isActive === false){ 
        $('.chart').data('easyPieChart').update(100);
        isActive = true;
    }
} 
```

Hope this helps :)",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/155/comments,https://github.com/rendro/easy-pie-chart/issues/155#issuecomment-282619853,https://api.github.com/repos/rendro/easy-pie-chart/issues/155
rendro,easy-pie-chart,105582605,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/444539852,444539852,MDEyOklzc3VlQ29tbWVudDQ0NDUzOTg1Mg==,40631006,2018-12-05T16:04:52Z,2018-12-05T16:04:52Z,NONE,on scroll events should be debounced in general,NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/155/comments,https://github.com/rendro/easy-pie-chart/issues/155#issuecomment-444539852,https://api.github.com/repos/rendro/easy-pie-chart/issues/155
rendro,easy-pie-chart,105582605,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/463446978,463446978,MDEyOklzc3VlQ29tbWVudDQ2MzQ0Njk3OA==,1627445,2019-02-14T01:25:39Z,2019-02-14T01:26:29Z,NONE,"The way to do this is to create an initial chart which is not animated without the `barColor` and then reset it + recreate again with the `barColor` and animation once scrolling.

Something like:
```
// prepare chart
$t.easyPieChart({
  animate    : { enabled : false },
  barColor   : false,
});
```

```
$(window).scroll(function(){
  // your logic
  $t.easyPieChart({
    animate    : { enabled : true },
    barColor   : '#fff',
  });
});
```",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/155/comments,https://github.com/rendro/easy-pie-chart/issues/155#issuecomment-463446978,https://api.github.com/repos/rendro/easy-pie-chart/issues/155
rendro,easy-pie-chart,101759548,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/132375353,132375353,MDEyOklzc3VlQ29tbWVudDEzMjM3NTM1Mw==,441774,2015-08-18T22:28:54Z,2015-08-18T22:28:54Z,NONE,"The call will be like this:

``` js
$('.c-chart-pie').easyPieChart({
  'class': 'c-chart-pie__canvas'
});
```
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/152/comments,https://github.com/rendro/easy-pie-chart/issues/152#issuecomment-132375353,https://api.github.com/repos/rendro/easy-pie-chart/issues/152
rendro,easy-pie-chart,100772496,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/564690388,564690388,MDEyOklzc3VlQ29tbWVudDU2NDY5MDM4OA==,18594962,2019-12-11T19:14:22Z,2019-12-11T19:14:22Z,NONE,Maybe this helps: https://github.com/rendro/easy-pie-chart/pull/170#issuecomment-564689453,NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/151/comments,https://github.com/rendro/easy-pie-chart/issues/151#issuecomment-564690388,https://api.github.com/repos/rendro/easy-pie-chart/issues/151
rendro,easy-pie-chart,98492193,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/135071415,135071415,MDEyOklzc3VlQ29tbWVudDEzNTA3MTQxNQ==,1726340,2015-08-26T15:41:43Z,2015-08-26T15:41:43Z,NONE,"+1
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/150/comments,https://github.com/rendro/easy-pie-chart/issues/150#issuecomment-135071415,https://api.github.com/repos/rendro/easy-pie-chart/issues/150
rendro,easy-pie-chart,98492193,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/136896183,136896183,MDEyOklzc3VlQ29tbWVudDEzNjg5NjE4Mw==,5926324,2015-09-01T23:59:53Z,2015-09-01T23:59:53Z,NONE,"+1
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/150/comments,https://github.com/rendro/easy-pie-chart/issues/150#issuecomment-136896183,https://api.github.com/repos/rendro/easy-pie-chart/issues/150
rendro,easy-pie-chart,98492193,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/138496314,138496314,MDEyOklzc3VlQ29tbWVudDEzODQ5NjMxNA==,6792600,2015-09-08T09:37:29Z,2015-09-08T09:37:29Z,NONE,"got this issue also using yeoman. i remove the require.js and the easypie work with no issue but i'm not sure the effect of removing the require.js
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/150/comments,https://github.com/rendro/easy-pie-chart/issues/150#issuecomment-138496314,https://api.github.com/repos/rendro/easy-pie-chart/issues/150
rendro,easy-pie-chart,98492193,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/143503710,143503710,MDEyOklzc3VlQ29tbWVudDE0MzUwMzcxMA==,6327243,2015-09-26T23:12:49Z,2015-09-26T23:12:49Z,NONE,"@azhaziqgoh did it work with grunt build? I have it working in dev but once I go to build I get the error again.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/150/comments,https://github.com/rendro/easy-pie-chart/issues/150#issuecomment-143503710,https://api.github.com/repos/rendro/easy-pie-chart/issues/150
rendro,easy-pie-chart,98492193,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/144693705,144693705,MDEyOklzc3VlQ29tbWVudDE0NDY5MzcwNQ==,6792600,2015-10-01T10:51:52Z,2015-10-01T10:51:52Z,NONE,"@tetsuoreynolds i'm removing the require.js totally from bower.json reside in easy-pie so that when I do grunt build it does not download the require.js
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/150/comments,https://github.com/rendro/easy-pie-chart/issues/150#issuecomment-144693705,https://api.github.com/repos/rendro/easy-pie-chart/issues/150
rendro,easy-pie-chart,98492193,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/164980879,164980879,MDEyOklzc3VlQ29tbWVudDE2NDk4MDg3OQ==,2457635,2015-12-16T03:45:11Z,2015-12-16T03:45:11Z,NONE,"+1
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/150/comments,https://github.com/rendro/easy-pie-chart/issues/150#issuecomment-164980879,https://api.github.com/repos/rendro/easy-pie-chart/issues/150
rendro,easy-pie-chart,98492193,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/170442363,170442363,MDEyOklzc3VlQ29tbWVudDE3MDQ0MjM2Mw==,4860561,2016-01-11T06:20:16Z,2016-01-11T06:33:54Z,NONE,"+1

How are you able to resolve this ? Manually removing require.js from index.html helped get rid of the error. But when I run grunt build, it gets included automatically and I cannot control this.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/150/comments,https://github.com/rendro/easy-pie-chart/issues/150#issuecomment-170442363,https://api.github.com/repos/rendro/easy-pie-chart/issues/150
rendro,easy-pie-chart,98492193,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/171052129,171052129,MDEyOklzc3VlQ29tbWVudDE3MTA1MjEyOQ==,2935721,2016-01-12T20:56:53Z,2016-01-12T20:56:53Z,NONE,"+1 I am getting it with angular...
Uncaught Error: Mismatched anonymous define() module: function (angular) {
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/150/comments,https://github.com/rendro/easy-pie-chart/issues/150#issuecomment-171052129,https://api.github.com/repos/rendro/easy-pie-chart/issues/150
rendro,easy-pie-chart,98492193,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/186512840,186512840,MDEyOklzc3VlQ29tbWVudDE4NjUxMjg0MA==,2935721,2016-02-20T05:14:17Z,2016-02-20T05:14:17Z,NONE,"I solved my issue by update my bower to override the jquery.easy-pie-chart dependencies

`""overrides"": {
    ""jquery.easy-pie-chart"": {
      ""dependencies"": {
        ""jquery"": "">=1.9.0""
      },
      ""main"": [
        ""dist/angular.easypiechart.min.js""
      ]
    }
`
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/150/comments,https://github.com/rendro/easy-pie-chart/issues/150#issuecomment-186512840,https://api.github.com/repos/rendro/easy-pie-chart/issues/150
rendro,easy-pie-chart,98492193,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/251637001,251637001,MDEyOklzc3VlQ29tbWVudDI1MTYzNzAwMQ==,16737524,2016-10-05T10:19:59Z,2016-10-05T10:30:31Z,NONE,"It seems like this is still happening as the latest release is 2.1.6 so that's what bower installs when anyone uses it to include this script. 

The issue appears to have been fixed for 2.1.7 as [the latest bower.json](https://github.com/rendro/easy-pie-chart/blob/master/bower.json) shows no dependency for require.js.

@rendro Would it be possible to get 2.1.7 released, or does it still require further development?
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/150/comments,https://github.com/rendro/easy-pie-chart/issues/150#issuecomment-251637001,https://api.github.com/repos/rendro/easy-pie-chart/issues/150
rendro,easy-pie-chart,97988555,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/126032713,126032713,MDEyOklzc3VlQ29tbWVudDEyNjAzMjcxMw==,507025,2015-07-29T17:46:15Z,2015-07-29T18:46:14Z,NONE,"It looks like calculated CSS styles are missing too.... such as lineheight, etc, which also contribute to recreating the styles in your demo (specifically the percentage in the middle of the chart)

```
        // merge user options into default options
        for (var i in defaultOptions) {
            if (defaultOptions.hasOwnProperty(i)) {
                options[i] = opts && typeof(opts[i]) !== 'undefined' ? opts[i] : defaultOptions[i];
                if (typeof(options[i]) === 'function') {
                    options[i] = options[i].bind(this);
                }
            }
        }

        $(el).addClass('easyPieChart').css({
            width: options.size,
            height: options.size,
            lineHeight: """" + options.size + ""px""
        }); 
```
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/149/comments,https://github.com/rendro/easy-pie-chart/issues/149#issuecomment-126032713,https://api.github.com/repos/rendro/easy-pie-chart/issues/149
rendro,easy-pie-chart,97988555,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/129122415,129122415,MDEyOklzc3VlQ29tbWVudDEyOTEyMjQxNQ==,1090113,2015-08-09T05:33:47Z,2015-08-09T05:33:47Z,NONE,"Yep. I can't make this to work. Only works on codepen [here](http://codepen.io/rendro/pen/vrezp). If I move the exactly generated CSS and HTML into my code with the latest version will not work.

Not sure why, I'm still working with it. As a start `easyChartPie` class is not applying. Anyway, adding the class manually in the inspector won't work either...
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/149/comments,https://github.com/rendro/easy-pie-chart/issues/149#issuecomment-129122415,https://api.github.com/repos/rendro/easy-pie-chart/issues/149
rendro,easy-pie-chart,97988555,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/129126898,129126898,MDEyOklzc3VlQ29tbWVudDEyOTEyNjg5OA==,1090113,2015-08-09T05:58:19Z,2015-08-09T05:58:19Z,NONE,"Ok, this is what I did. I've downloaded this link:

http://rendro.github.io/easy-pie-chart/javascripts/jquery.easy-pie-chart.js

As @helgatheviking said, will work OK. I think this is related to this [bug](https://github.com/rendro/easy-pie-chart/issues/93)
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/149/comments,https://github.com/rendro/easy-pie-chart/issues/149#issuecomment-129126898,https://api.github.com/repos/rendro/easy-pie-chart/issues/149
rendro,easy-pie-chart,95144894,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/121725688,121725688,MDEyOklzc3VlQ29tbWVudDEyMTcyNTY4OA==,422168,2015-07-15T19:49:25Z,2015-07-15T19:49:25Z,OWNER,"Thanks!
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/147/comments,https://github.com/rendro/easy-pie-chart/pull/147#issuecomment-121725688,https://api.github.com/repos/rendro/easy-pie-chart/issues/147
rendro,easy-pie-chart,94941703,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/126023814,126023814,MDEyOklzc3VlQ29tbWVudDEyNjAyMzgxNA==,507025,2015-07-29T17:16:03Z,2015-07-29T17:16:03Z,NONE,"At least with the version that is in the demo page it does/did work
http://jsfiddle.net/qym5yqs3/4/
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/146/comments,https://github.com/rendro/easy-pie-chart/issues/146#issuecomment-126023814,https://api.github.com/repos/rendro/easy-pie-chart/issues/146
rendro,easy-pie-chart,91040676,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/115362290,115362290,MDEyOklzc3VlQ29tbWVudDExNTM2MjI5MA==,13053727,2015-06-25T18:58:23Z,2015-06-25T18:58:23Z,NONE,"My solution:

Result = MyValue \* 100 / 5.
Thanks!
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/145/comments,https://github.com/rendro/easy-pie-chart/issues/145#issuecomment-115362290,https://api.github.com/repos/rendro/easy-pie-chart/issues/145
rendro,easy-pie-chart,82786853,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/108207455,108207455,MDEyOklzc3VlQ29tbWVudDEwODIwNzQ1NQ==,12671380,2015-06-03T06:17:25Z,2015-06-03T06:17:25Z,NONE,"sry guys, i had a old version of uikit... therefore it didnt work.
inview.uk.scrollspy (new version) - uk.scrollspy.inview (old version)
here the code to start the animation on view with uikit (new version):
$(function() {
    $('#div-id-here').on('inview.uk.scrollspy', function(){
        $('.chart').easyPieChart({
            //your options goes here
        });
    });
});
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/143/comments,https://github.com/rendro/easy-pie-chart/issues/143#issuecomment-108207455,https://api.github.com/repos/rendro/easy-pie-chart/issues/143
rendro,easy-pie-chart,82116795,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/106608844,106608844,MDEyOklzc3VlQ29tbWVudDEwNjYwODg0NA==,12533449,2015-05-28T21:44:38Z,2015-05-28T21:44:38Z,NONE,"@rendro Any ideas why this Travis CI build fails? Details are a bit sketchy... 

PhantomJS is complaining saying this: 
 _angular easypiechart directive should have percentage default value 0_

I didn't mess with any of the default values, simply added another parameter to the update function and included 7ever's setOptions function.

Everything seems to function properly on my end. 
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/142/comments,https://github.com/rendro/easy-pie-chart/pull/142#issuecomment-106608844,https://api.github.com/repos/rendro/easy-pie-chart/issues/142
rendro,easy-pie-chart,82116795,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/106614141,106614141,MDEyOklzc3VlQ29tbWVudDEwNjYxNDE0MQ==,422168,2015-05-28T22:08:49Z,2015-05-28T22:08:49Z,OWNER,"@enkimatt Don't worry about the Travis CI  results. The rest cases are broken at the moment. I'll have a look at your PR soon. Thanks for your contribution.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/142/comments,https://github.com/rendro/easy-pie-chart/pull/142#issuecomment-106614141,https://api.github.com/repos/rendro/easy-pie-chart/issues/142
rendro,easy-pie-chart,82116795,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/106615562,106615562,MDEyOklzc3VlQ29tbWVudDEwNjYxNTU2Mg==,12533449,2015-05-28T22:16:01Z,2015-05-28T22:16:01Z,NONE,"My contribution is minimal, I should be thanking you - so, Thank you.. LOL. This chart is awesome. 
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/142/comments,https://github.com/rendro/easy-pie-chart/pull/142#issuecomment-106615562,https://api.github.com/repos/rendro/easy-pie-chart/issues/142
rendro,easy-pie-chart,78994611,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/209779293,209779293,MDEyOklzc3VlQ29tbWVudDIwOTc3OTI5Mw==,15407254,2016-04-14T06:25:18Z,2016-04-14T06:25:18Z,NONE,"put excanvas and polyfill in front of css.   
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/140/comments,https://github.com/rendro/easy-pie-chart/issues/140#issuecomment-209779293,https://api.github.com/repos/rendro/easy-pie-chart/issues/140
rendro,easy-pie-chart,78554442,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/243045702,243045702,MDEyOklzc3VlQ29tbWVudDI0MzA0NTcwMg==,1563651,2016-08-29T06:53:20Z,2016-08-29T06:53:20Z,NONE,"Subscribe
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/139/comments,https://github.com/rendro/easy-pie-chart/issues/139#issuecomment-243045702,https://api.github.com/repos/rendro/easy-pie-chart/issues/139
rendro,easy-pie-chart,78554442,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/243201994,243201994,MDEyOklzc3VlQ29tbWVudDI0MzIwMTk5NA==,422168,2016-08-29T17:58:03Z,2016-08-29T17:58:03Z,OWNER,"yes but you have to make sure the selectors are mutually exclusive.

In your case the first call

``` javascript
$('.chart').easyPieChart({
    size: 80
});
```

also creates charts for `.chart.big`.

This should work:

``` javascript
$('.chart.small').easyPieChart({
    size: 80
});

$('.chart.big').easyPieChart({
    size: 100
});
```
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/139/comments,https://github.com/rendro/easy-pie-chart/issues/139#issuecomment-243201994,https://api.github.com/repos/rendro/easy-pie-chart/issues/139
rendro,easy-pie-chart,78554442,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/243400966,243400966,MDEyOklzc3VlQ29tbWVudDI0MzQwMDk2Ng==,2394719,2016-08-30T10:40:20Z,2016-08-30T10:40:20Z,NONE,"Oh right that makes sense, thanks
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/139/comments,https://github.com/rendro/easy-pie-chart/issues/139#issuecomment-243400966,https://api.github.com/repos/rendro/easy-pie-chart/issues/139
rendro,easy-pie-chart,70731188,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/96588709,96588709,MDEyOklzc3VlQ29tbWVudDk2NTg4NzA5,422168,2015-04-27T09:57:18Z,2015-04-27T09:57:18Z,OWNER,"Can you please amend the pull request and make the comparison type save again. Thanks for the contribution!
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/138/comments,https://github.com/rendro/easy-pie-chart/pull/138#issuecomment-96588709,https://api.github.com/repos/rendro/easy-pie-chart/issues/138
rendro,easy-pie-chart,70731188,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/97008041,97008041,MDEyOklzc3VlQ29tbWVudDk3MDA4MDQx,834942,2015-04-28T10:30:52Z,2015-04-28T10:30:52Z,CONTRIBUTOR,"There you go. Thank you for making the library. :)
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/138/comments,https://github.com/rendro/easy-pie-chart/pull/138#issuecomment-97008041,https://api.github.com/repos/rendro/easy-pie-chart/issues/138
rendro,easy-pie-chart,70731188,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/100244648,100244648,MDEyOklzc3VlQ29tbWVudDEwMDI0NDY0OA==,422168,2015-05-08T14:15:55Z,2015-05-08T14:15:55Z,OWNER,"You are welcome, and thanks again for the PR!
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/138/comments,https://github.com/rendro/easy-pie-chart/pull/138#issuecomment-100244648,https://api.github.com/repos/rendro/easy-pie-chart/issues/138
rendro,easy-pie-chart,70169270,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/95359716,95359716,MDEyOklzc3VlQ29tbWVudDk1MzU5NzE2,422168,2015-04-22T22:52:25Z,2015-04-22T22:52:25Z,OWNER,"Happy to accept the PR. I'll send you the username per email.
Thanks!
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/137/comments,https://github.com/rendro/easy-pie-chart/pull/137#issuecomment-95359716,https://api.github.com/repos/rendro/easy-pie-chart/issues/137
rendro,easy-pie-chart,70169270,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/145576055,145576055,MDEyOklzc3VlQ29tbWVudDE0NTU3NjA1NQ==,8638987,2015-10-05T15:45:40Z,2015-10-05T15:45:40Z,NONE,"Hi,

How do I use the Angular version of easypiechart using the Meteor package?

Many thanks!
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/137/comments,https://github.com/rendro/easy-pie-chart/pull/137#issuecomment-145576055,https://api.github.com/repos/rendro/easy-pie-chart/issues/137
rendro,easy-pie-chart,68984575,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/100244814,100244814,MDEyOklzc3VlQ29tbWVudDEwMDI0NDgxNA==,422168,2015-05-08T14:16:48Z,2015-05-08T14:16:48Z,OWNER,"Thanks!
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/136/comments,https://github.com/rendro/easy-pie-chart/pull/136#issuecomment-100244814,https://api.github.com/repos/rendro/easy-pie-chart/issues/136
rendro,easy-pie-chart,68168843,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/92461982,92461982,MDEyOklzc3VlQ29tbWVudDkyNDYxOTgy,901289,2015-04-13T19:02:18Z,2015-04-13T19:02:18Z,NONE,"ðŸ• or ðŸ°
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/135/comments,https://github.com/rendro/easy-pie-chart/issues/135#issuecomment-92461982,https://api.github.com/repos/rendro/easy-pie-chart/issues/135
rendro,easy-pie-chart,68168843,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/95323703,95323703,MDEyOklzc3VlQ29tbWVudDk1MzIzNzAz,419625,2015-04-22T20:17:07Z,2015-04-22T20:17:07Z,NONE,":+1: if you're going to call it a pie chart, have an option to make it look like a pie chart...
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/135/comments,https://github.com/rendro/easy-pie-chart/issues/135#issuecomment-95323703,https://api.github.com/repos/rendro/easy-pie-chart/issues/135
rendro,easy-pie-chart,66950715,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/90686523,90686523,MDEyOklzc3VlQ29tbWVudDkwNjg2NTIz,5385573,2015-04-07T18:15:26Z,2015-04-07T18:15:26Z,NONE,"I should add, I am using v2.1.6 of jquery.easypiechart.js found in the zip file here: http://rendro.github.io/easy-pie-chart/
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/133/comments,https://github.com/rendro/easy-pie-chart/issues/133#issuecomment-90686523,https://api.github.com/repos/rendro/easy-pie-chart/issues/133
rendro,easy-pie-chart,66950715,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/115016930,115016930,MDEyOklzc3VlQ29tbWVudDExNTAxNjkzMA==,123440,2015-06-24T21:25:24Z,2015-06-24T21:25:24Z,NONE,"Either ensure you are using a supported version of jQuery or use [the non-jquery](https://github.com/rendro/easy-pie-chart/blob/master/dist/easypiechart.js) version of the plugin. 
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/133/comments,https://github.com/rendro/easy-pie-chart/issues/133#issuecomment-115016930,https://api.github.com/repos/rendro/easy-pie-chart/issues/133
rendro,easy-pie-chart,65945702,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/89349843,89349843,MDEyOklzc3VlQ29tbWVudDg5MzQ5ODQz,4167817,2015-04-03T16:47:41Z,2015-04-03T16:47:41Z,NONE,"I am also see this problem.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/131/comments,https://github.com/rendro/easy-pie-chart/issues/131#issuecomment-89349843,https://api.github.com/repos/rendro/easy-pie-chart/issues/131
rendro,easy-pie-chart,65945702,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/89372965,89372965,MDEyOklzc3VlQ29tbWVudDg5MzcyOTY1,4167817,2015-04-03T17:45:58Z,2015-04-03T17:45:58Z,NONE,"It is working for me now. It appears AddThis has pushed a fix on their end?
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/131/comments,https://github.com/rendro/easy-pie-chart/issues/131#issuecomment-89372965,https://api.github.com/repos/rendro/easy-pie-chart/issues/131
rendro,easy-pie-chart,65945702,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/89380768,89380768,MDEyOklzc3VlQ29tbWVudDg5MzgwNzY4,422168,2015-04-03T18:23:37Z,2015-04-03T18:23:37Z,OWNER,"Errors with third parties scripts won't be fixed. Probably an error of those scripts. Thanks!
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/131/comments,https://github.com/rendro/easy-pie-chart/issues/131#issuecomment-89380768,https://api.github.com/repos/rendro/easy-pie-chart/issues/131
rendro,easy-pie-chart,65945702,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/89391077,89391077,MDEyOklzc3VlQ29tbWVudDg5MzkxMDc3,154323,2015-04-03T18:53:06Z,2015-04-03T18:53:06Z,NONE,"@logicboard : I still experience the problem. I call the following script : http://s7.addthis.com/js/250/addthis_widget.js 

What about you ?
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/131/comments,https://github.com/rendro/easy-pie-chart/issues/131#issuecomment-89391077,https://api.github.com/repos/rendro/easy-pie-chart/issues/131
rendro,easy-pie-chart,65945702,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/90833099,90833099,MDEyOklzc3VlQ29tbWVudDkwODMzMDk5,154323,2015-04-08T07:44:25Z,2015-04-08T07:44:25Z,NONE,"For information, I reported to addThis team. The probably try to fix it !
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/131/comments,https://github.com/rendro/easy-pie-chart/issues/131#issuecomment-90833099,https://api.github.com/repos/rendro/easy-pie-chart/issues/131
rendro,easy-pie-chart,65945702,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/93324884,93324884,MDEyOklzc3VlQ29tbWVudDkzMzI0ODg0,154323,2015-04-15T10:59:05Z,2015-04-15T10:59:05Z,NONE,"It seems they fixed it. Thanks to them!
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/131/comments,https://github.com/rendro/easy-pie-chart/issues/131#issuecomment-93324884,https://api.github.com/repos/rendro/easy-pie-chart/issues/131
rendro,easy-pie-chart,64030259,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/85570356,85570356,MDEyOklzc3VlQ29tbWVudDg1NTcwMzU2,767394,2015-03-24T15:51:17Z,2015-03-24T15:51:17Z,NONE,"I am experiencing the same issue.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/130/comments,https://github.com/rendro/easy-pie-chart/issues/130#issuecomment-85570356,https://api.github.com/repos/rendro/easy-pie-chart/issues/130
rendro,easy-pie-chart,64030259,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/85596624,85596624,MDEyOklzc3VlQ29tbWVudDg1NTk2NjI0,4449609,2015-03-24T16:56:57Z,2015-03-24T16:57:43Z,NONE,"Found an answer in a previously closed issue here : [Value percentage into the circle](https://github.com/rendro/easy-pie-chart/issues/93)

Still it might be interesting to specify it in the instructions, no ? 
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/130/comments,https://github.com/rendro/easy-pie-chart/issues/130#issuecomment-85596624,https://api.github.com/repos/rendro/easy-pie-chart/issues/130
rendro,easy-pie-chart,64030259,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/148074381,148074381,MDEyOklzc3VlQ29tbWVudDE0ODA3NDM4MQ==,1926014,2015-10-14T14:52:26Z,2015-10-14T14:52:26Z,NONE,"Hi, 

Mine those not work how did you do it? 
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/130/comments,https://github.com/rendro/easy-pie-chart/issues/130#issuecomment-148074381,https://api.github.com/repos/rendro/easy-pie-chart/issues/130
rendro,easy-pie-chart,64030259,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/283024023,283024023,MDEyOklzc3VlQ29tbWVudDI4MzAyNDAyMw==,10184207,2017-02-28T12:15:26Z,2017-02-28T12:28:46Z,NONE,"Just make your directive like this,
`<div class=""percent chart"" easypiechart options=""options"" percent=""percent""><span style=""line-height:110px;"">{{ percent }}</span></div>`",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/130/comments,https://github.com/rendro/easy-pie-chart/issues/130#issuecomment-283024023,https://api.github.com/repos/rendro/easy-pie-chart/issues/130
rendro,easy-pie-chart,58418923,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/143230247,143230247,MDEyOklzc3VlQ29tbWVudDE0MzIzMDI0Nw==,2866604,2015-09-25T13:54:10Z,2015-09-25T13:54:10Z,NONE,"+1
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/127/comments,https://github.com/rendro/easy-pie-chart/issues/127#issuecomment-143230247,https://api.github.com/repos/rendro/easy-pie-chart/issues/127
rendro,easy-pie-chart,58418923,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/325888330,325888330,MDEyOklzc3VlQ29tbWVudDMyNTg4ODMzMA==,74789,2017-08-30T05:47:42Z,2017-08-30T05:47:42Z,NONE,+1,NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/127/comments,https://github.com/rendro/easy-pie-chart/issues/127#issuecomment-325888330,https://api.github.com/repos/rendro/easy-pie-chart/issues/127
rendro,easy-pie-chart,58391490,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/75387854,75387854,MDEyOklzc3VlQ29tbWVudDc1Mzg3ODU0,7863186,2015-02-21T19:22:51Z,2015-02-21T19:22:51Z,NONE,"You should add data-percent to the element that you call it!

Fixed code : http://jsfiddle.net/g8wkvy0o/1/
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/126/comments,https://github.com/rendro/easy-pie-chart/issues/126#issuecomment-75387854,https://api.github.com/repos/rendro/easy-pie-chart/issues/126
rendro,easy-pie-chart,57544459,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/75196645,75196645,MDEyOklzc3VlQ29tbWVudDc1MTk2NjQ1,746429,2015-02-20T06:43:33Z,2015-02-20T06:43:33Z,COLLABORATOR,"+1, @rendro what do you think ?
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/124/comments,https://github.com/rendro/easy-pie-chart/issues/124#issuecomment-75196645,https://api.github.com/repos/rendro/easy-pie-chart/issues/124
rendro,easy-pie-chart,57544459,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/75485573,75485573,MDEyOklzc3VlQ29tbWVudDc1NDg1NTcz,422168,2015-02-23T03:37:37Z,2015-02-23T03:37:37Z,OWNER,"I'll do that this week. Probably Tuesday or Wednesday. But having this lib on npm totally makes sense.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/124/comments,https://github.com/rendro/easy-pie-chart/issues/124#issuecomment-75485573,https://api.github.com/repos/rendro/easy-pie-chart/issues/124
rendro,easy-pie-chart,57544459,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/99905450,99905450,MDEyOklzc3VlQ29tbWVudDk5OTA1NDUw,443865,2015-05-07T15:14:52Z,2015-05-07T15:14:52Z,NONE,"+1
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/124/comments,https://github.com/rendro/easy-pie-chart/issues/124#issuecomment-99905450,https://api.github.com/repos/rendro/easy-pie-chart/issues/124
rendro,easy-pie-chart,57544459,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/99960498,99960498,MDEyOklzc3VlQ29tbWVudDk5OTYwNDk4,422168,2015-05-07T17:55:39Z,2015-05-07T17:55:39Z,OWNER,"Published to npm: https://www.npmjs.com/package/easy-pie-chart
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/124/comments,https://github.com/rendro/easy-pie-chart/issues/124#issuecomment-99960498,https://api.github.com/repos/rendro/easy-pie-chart/issues/124
rendro,easy-pie-chart,57355981,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/88525377,88525377,MDEyOklzc3VlQ29tbWVudDg4NTI1Mzc3,8512969,2015-04-01T15:36:52Z,2015-04-01T15:36:52Z,NONE,"+1 thanks for this
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/123/comments,https://github.com/rendro/easy-pie-chart/issues/123#issuecomment-88525377,https://api.github.com/repos/rendro/easy-pie-chart/issues/123
rendro,easy-pie-chart,54109940,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/69672688,69672688,MDEyOklzc3VlQ29tbWVudDY5NjcyNjg4,746429,2015-01-13T00:10:01Z,2015-01-13T00:10:01Z,COLLABORATOR,"@sgtpepper43, thanks :+1: 
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/121/comments,https://github.com/rendro/easy-pie-chart/pull/121#issuecomment-69672688,https://api.github.com/repos/rendro/easy-pie-chart/issues/121
rendro,easy-pie-chart,53004925,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/68221442,68221442,MDEyOklzc3VlQ29tbWVudDY4MjIxNDQy,422168,2014-12-28T22:16:15Z,2014-12-28T22:16:51Z,OWNER,"Thank you for your contribution!
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/119/comments,https://github.com/rendro/easy-pie-chart/pull/119#issuecomment-68221442,https://api.github.com/repos/rendro/easy-pie-chart/issues/119
rendro,easy-pie-chart,52980140,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/303880518,303880518,MDEyOklzc3VlQ29tbWVudDMwMzg4MDUxOA==,1935243,2017-05-24T23:25:47Z,2017-05-24T23:25:47Z,NONE,Is it?,NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/118/comments,https://github.com/rendro/easy-pie-chart/issues/118#issuecomment-303880518,https://api.github.com/repos/rendro/easy-pie-chart/issues/118
rendro,easy-pie-chart,52980140,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/303887269,303887269,MDEyOklzc3VlQ29tbWVudDMwMzg4NzI2OQ==,422168,2017-05-25T00:12:24Z,2017-05-25T00:13:00Z,OWNER,no. it is not. Keep an eye on branch v3.0,NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/118/comments,https://github.com/rendro/easy-pie-chart/issues/118#issuecomment-303887269,https://api.github.com/repos/rendro/easy-pie-chart/issues/118
rendro,easy-pie-chart,52980140,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/390715332,390715332,MDEyOklzc3VlQ29tbWVudDM5MDcxNTMzMg==,30520648,2018-05-21T16:56:40Z,2018-05-21T16:56:40Z,NONE,read here https://github.com/rendro/easy-pie-chart/issues/203,NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/118/comments,https://github.com/rendro/easy-pie-chart/issues/118#issuecomment-390715332,https://api.github.com/repos/rendro/easy-pie-chart/issues/118
rendro,easy-pie-chart,52977004,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/71774961,71774961,MDEyOklzc3VlQ29tbWVudDcxNzc0OTYx,685492,2015-01-28T03:24:57Z,2015-01-28T03:24:57Z,NONE,"+1000 :)
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/117/comments,https://github.com/rendro/easy-pie-chart/issues/117#issuecomment-71774961,https://api.github.com/repos/rendro/easy-pie-chart/issues/117
rendro,easy-pie-chart,52977004,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/76541963,76541963,MDEyOklzc3VlQ29tbWVudDc2NTQxOTYz,680180,2015-02-28T19:37:41Z,2015-02-28T19:37:41Z,NONE,"Also would love this feature ! 
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/117/comments,https://github.com/rendro/easy-pie-chart/issues/117#issuecomment-76541963,https://api.github.com/repos/rendro/easy-pie-chart/issues/117
rendro,easy-pie-chart,52977004,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/92385753,92385753,MDEyOklzc3VlQ29tbWVudDkyMzg1NzUz,901289,2015-04-13T14:43:05Z,2015-04-13T14:44:31Z,NONE,"+1
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/117/comments,https://github.com/rendro/easy-pie-chart/issues/117#issuecomment-92385753,https://api.github.com/repos/rendro/easy-pie-chart/issues/117
rendro,easy-pie-chart,52977004,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/92388378,92388378,MDEyOklzc3VlQ29tbWVudDkyMzg4Mzc4,988504,2015-04-13T14:53:49Z,2015-04-13T14:53:49Z,NONE,"Absolute positioning and transparent backgrounds :)

> On 13 Apr 2015, at 17:43, Richard Cook notifications@github.com wrote:
> 
> +1
> 
> @seleckis https://github.com/seleckis How did you do this?
> 
> â€”
> Reply to this email directly or view it on GitHub https://github.com/rendro/easy-pie-chart/issues/117#issuecomment-92385753.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/117/comments,https://github.com/rendro/easy-pie-chart/issues/117#issuecomment-92388378,https://api.github.com/repos/rendro/easy-pie-chart/issues/117
rendro,easy-pie-chart,52977004,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/92462162,92462162,MDEyOklzc3VlQ29tbWVudDkyNDYyMTYy,901289,2015-04-13T19:02:52Z,2015-04-13T19:02:52Z,NONE,"@seleckis :)
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/117/comments,https://github.com/rendro/easy-pie-chart/issues/117#issuecomment-92462162,https://api.github.com/repos/rendro/easy-pie-chart/issues/117
rendro,easy-pie-chart,52977004,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/248348578,248348578,MDEyOklzc3VlQ29tbWVudDI0ODM0ODU3OA==,16162536,2016-09-20T16:05:11Z,2016-09-20T16:05:11Z,NONE,"![img_15092016_214843](https://cloud.githubusercontent.com/assets/16162536/18678578/132e6a04-7f7a-11e6-848c-244464729a0e.png)
how i create this
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/117/comments,https://github.com/rendro/easy-pie-chart/issues/117#issuecomment-248348578,https://api.github.com/repos/rendro/easy-pie-chart/issues/117
rendro,easy-pie-chart,52977004,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/390715596,390715596,MDEyOklzc3VlQ29tbWVudDM5MDcxNTU5Ng==,30520648,2018-05-21T16:57:31Z,2018-05-21T16:57:31Z,NONE,read here https://github.com/rendro/easy-pie-chart/issues/203,NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/117/comments,https://github.com/rendro/easy-pie-chart/issues/117#issuecomment-390715596,https://api.github.com/repos/rendro/easy-pie-chart/issues/117
rendro,easy-pie-chart,52963353,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/68206474,68206474,MDEyOklzc3VlQ29tbWVudDY4MjA2NDc0,422168,2014-12-28T12:59:37Z,2014-12-28T12:59:37Z,OWNER,"Currently this is not possible, but you can implement the math yourself before calling the `update` method
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/116/comments,https://github.com/rendro/easy-pie-chart/issues/116#issuecomment-68206474,https://api.github.com/repos/rendro/easy-pie-chart/issues/116
rendro,easy-pie-chart,52963353,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/333240820,333240820,MDEyOklzc3VlQ29tbWVudDMzMzI0MDgyMA==,10137,2017-09-29T21:14:30Z,2017-09-29T21:14:30Z,NONE,"If you want to update the upper bound of the chart but still maintain control of the detailed value shown see the following.

In my case, I wanted the control to have a range from 0-10 (e.g a value of 5 would result in the control being 50% full). The advice above will update the scale, but also the value being displayed (e.g. 5/10 -> 50/100. If you want to maintain the value (5) but adjust the scale (100->10), see the following. 

Note, I've also included a sample of how to update the value color separately from the control color. If you'd like to add units or additional content to the value see the 'content' option for the percent:after css class below.

```css
.simple-pie {
    position: relative;
    display: inline-block;
    width: 110px;
    height: 110px;
    margin-top: 5px;
    margin-bottom: 5px;
    text-align: center
}
.simple-pie canvas {
    position: absolute;
    top: 0;
    left: 0;
}
.percent {
    display: inline-block;
    line-height: 110px;
    z-index: 2;
}
.percent:after {
    content: """";
    margin-left: .1em;
    font-size: .8em;
}
```

```html
<script src=""jquery.easypiechart.min.js""></script>

<span id=""gauge"" class=""simple-pie"" data-percent=""100"">
    <span id=""gauge-value"" class=""percent""></span>
</span>
```

```js
      var cur = foo();
      var scaledValue = (100 * cur / 10);
      var elem = $(""#gauge"");
      if (elem) {
        elem.easyPieChart({
            animate: 2000,
            barColor: ""#26B99A"",
            trackColor: ""#D3D3D3"",
            scaleColor: false,
            lineWidth: 20,
            trackWidth: 16,
            lineCap: ""butt"",
            percent: scaledValue,
            onStep: function (from, to, percent) {
                $(this.el).find('.percent').text(Number((percent / 10).toFixed(1)));
            }
        });

        setTimeout(function() {
            elem.data(""easyPieChart"").update(scaledValue);
        }, 1000);

        $(""#gauge-value"").css(""color"", getRiskScoreColor(cur));
      }
```",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/116/comments,https://github.com/rendro/easy-pie-chart/issues/116#issuecomment-333240820,https://api.github.com/repos/rendro/easy-pie-chart/issues/116
rendro,easy-pie-chart,51814280,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/66786870,66786870,MDEyOklzc3VlQ29tbWVudDY2Nzg2ODcw,422168,2014-12-12T15:27:41Z,2014-12-12T15:27:41Z,OWNER,"Could you please provide a description or this issue will be closed.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/114/comments,https://github.com/rendro/easy-pie-chart/issues/114#issuecomment-66786870,https://api.github.com/repos/rendro/easy-pie-chart/issues/114
rendro,easy-pie-chart,51814280,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/66792021,66792021,MDEyOklzc3VlQ29tbWVudDY2NzkyMDIx,6518373,2014-12-12T16:00:45Z,2014-12-12T16:00:45Z,NONE,"Provided a description
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/114/comments,https://github.com/rendro/easy-pie-chart/issues/114#issuecomment-66792021,https://api.github.com/repos/rendro/easy-pie-chart/issues/114
rendro,easy-pie-chart,51814280,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/75329639,75329639,MDEyOklzc3VlQ29tbWVudDc1MzI5NjM5,1276945,2015-02-20T22:16:52Z,2015-02-20T22:16:52Z,NONE,"date-percent=""-25""
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/114/comments,https://github.com/rendro/easy-pie-chart/issues/114#issuecomment-75329639,https://api.github.com/repos/rendro/easy-pie-chart/issues/114
rendro,easy-pie-chart,51585168,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/67015122,67015122,MDEyOklzc3VlQ29tbWVudDY3MDE1MTIy,422168,2014-12-15T15:55:53Z,2014-12-15T15:55:53Z,OWNER,"Thanks a lot for your contribution!
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/113/comments,https://github.com/rendro/easy-pie-chart/pull/113#issuecomment-67015122,https://api.github.com/repos/rendro/easy-pie-chart/issues/113
rendro,easy-pie-chart,48208344,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/62324866,62324866,MDEyOklzc3VlQ29tbWVudDYyMzI0ODY2,422168,2014-11-09T23:17:26Z,2014-11-09T23:17:26Z,OWNER,"All possible options are documented in the readme file in the section ""Options"". Currently the easy pie chart does not support a method/option to do this as already discussed in the issue #110 
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/111/comments,https://github.com/rendro/easy-pie-chart/issues/111#issuecomment-62324866,https://api.github.com/repos/rendro/easy-pie-chart/issues/111
rendro,easy-pie-chart,48208344,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/62327124,62327124,MDEyOklzc3VlQ29tbWVudDYyMzI3MTI0,1345518,2014-11-10T00:18:14Z,2014-11-10T00:18:14Z,NONE,"Ok. You seemed to outline a good solution in that post. Thanks for pointing that out!
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/111/comments,https://github.com/rendro/easy-pie-chart/issues/111#issuecomment-62327124,https://api.github.com/repos/rendro/easy-pie-chart/issues/111
rendro,easy-pie-chart,47326577,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/61237866,61237866,MDEyOklzc3VlQ29tbWVudDYxMjM3ODY2,422168,2014-10-31T09:42:01Z,2014-10-31T09:45:43Z,OWNER,"This plugin does not cover this option yet. If there is a high demand we can add this feature.

The math should be straight forward: A normal circle has 360deg (2Ï€ rad). An open circle (lets say 25% open) can be drawn with the arc function of the canvas:

``` javascript
cx.beginPath();
cx.arc(100, 75, 50, 0, (2 * Math.PI) * 3 / 4);
cx.stroke();
```

Find more information here:
https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API/Tutorial/Drawing_shapes#Arcs
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/110/comments,https://github.com/rendro/easy-pie-chart/issues/110#issuecomment-61237866,https://api.github.com/repos/rendro/easy-pie-chart/issues/110
rendro,easy-pie-chart,47326577,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/62004660,62004660,MDEyOklzc3VlQ29tbWVudDYyMDA0NjYw,105211,2014-11-06T16:20:30Z,2014-11-06T16:20:30Z,NONE,"I came across this repo/issue looking for a partial circle solution.
+1
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/110/comments,https://github.com/rendro/easy-pie-chart/issues/110#issuecomment-62004660,https://api.github.com/repos/rendro/easy-pie-chart/issues/110
rendro,easy-pie-chart,47326577,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/68478140,68478140,MDEyOklzc3VlQ29tbWVudDY4NDc4MTQw,10137,2015-01-01T01:43:55Z,2015-01-01T01:43:55Z,NONE,"I came across this repo searching for a semi circle type chart, and found this issue thread.
+1
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/110/comments,https://github.com/rendro/easy-pie-chart/issues/110#issuecomment-68478140,https://api.github.com/repos/rendro/easy-pie-chart/issues/110
rendro,easy-pie-chart,47326577,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/74011293,74011293,MDEyOklzc3VlQ29tbWVudDc0MDExMjkz,1782893,2015-02-12T03:22:08Z,2015-02-12T03:22:08Z,NONE,"@rendro if you get the chance, I think a bunch of us would like a plug and play solution to make this happen :)
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/110/comments,https://github.com/rendro/easy-pie-chart/issues/110#issuecomment-74011293,https://api.github.com/repos/rendro/easy-pie-chart/issues/110
rendro,easy-pie-chart,47326577,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/168963274,168963274,MDEyOklzc3VlQ29tbWVudDE2ODk2MzI3NA==,297321,2016-01-05T10:13:27Z,2016-01-05T10:13:27Z,NONE,"+1
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/110/comments,https://github.com/rendro/easy-pie-chart/issues/110#issuecomment-168963274,https://api.github.com/repos/rendro/easy-pie-chart/issues/110
rendro,easy-pie-chart,47147425,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/200739300,200739300,MDEyOklzc3VlQ29tbWVudDIwMDczOTMwMA==,18025650,2016-03-24T08:51:12Z,2016-03-24T08:51:12Z,NONE,"As my understanding, this plugin supports only IE9+, the chart can not display in IE8. Nothing about the line width. 
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/109/comments,https://github.com/rendro/easy-pie-chart/issues/109#issuecomment-200739300,https://api.github.com/repos/rendro/easy-pie-chart/issues/109
rendro,easy-pie-chart,46954480,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/60816714,60816714,MDEyOklzc3VlQ29tbWVudDYwODE2NzE0,2161844,2014-10-28T19:30:14Z,2014-10-28T19:30:39Z,NONE,"OK... so I have found a work around which works... I thought I'd share it in case it isn't my fault.

I changed

""dist/jquery.easypiechart.js"" to ""dist/angular.easypiechart.js""

on line 5 of ""/bower_components/angular.easy-pie-chart/bower.json""...

Seems to work...
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/108/comments,https://github.com/rendro/easy-pie-chart/issues/108#issuecomment-60816714,https://api.github.com/repos/rendro/easy-pie-chart/issues/108
rendro,easy-pie-chart,46954480,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/60893268,60893268,MDEyOklzc3VlQ29tbWVudDYwODkzMjY4,422168,2014-10-29T09:18:51Z,2014-10-29T09:18:51Z,OWNER,"The problem is that you can not defined different distributions for different environments. jQuery is still more popular than angular that is why we have it in there per default. Thanks for sharing your work around.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/108/comments,https://github.com/rendro/easy-pie-chart/issues/108#issuecomment-60893268,https://api.github.com/repos/rendro/easy-pie-chart/issues/108
rendro,easy-pie-chart,46954480,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/60908722,60908722,MDEyOklzc3VlQ29tbWVudDYwOTA4NzIy,2161844,2014-10-29T11:37:02Z,2014-10-29T11:37:02Z,NONE,"I see... So my approach is valid then? Thanks for your response and great work!
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/108/comments,https://github.com/rendro/easy-pie-chart/issues/108#issuecomment-60908722,https://api.github.com/repos/rendro/easy-pie-chart/issues/108
rendro,easy-pie-chart,46954480,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/67775490,67775490,MDEyOklzc3VlQ29tbWVudDY3Nzc1NDkw,67263,2014-12-21T16:24:42Z,2014-12-21T16:24:42Z,NONE,"+1 

I had to drop back to 2.1.3 to avoid any issues with gruntjs/bower auto injection into my index.html, as I'm not using requirejs either. 

I think the long term solution is to break off the angularjs module to another github project and make it a different distribution that is dependent on easypiechart.js. Thought, that may also mean breaking off easypiechart.js to make it a root dependency for the jquery users, but that's not necessarily a bad thing either, just extra work (which may be out of scope of this project).
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/108/comments,https://github.com/rendro/easy-pie-chart/issues/108#issuecomment-67775490,https://api.github.com/repos/rendro/easy-pie-chart/issues/108
rendro,easy-pie-chart,46954480,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/68122013,68122013,MDEyOklzc3VlQ29tbWVudDY4MTIyMDEz,746429,2014-12-26T04:10:30Z,2014-12-26T04:10:30Z,COLLABORATOR,"Agree with @jordanburke, the best approach would be to break it up to its own repo.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/108/comments,https://github.com/rendro/easy-pie-chart/issues/108#issuecomment-68122013,https://api.github.com/repos/rendro/easy-pie-chart/issues/108
rendro,easy-pie-chart,46954480,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/171061014,171061014,MDEyOklzc3VlQ29tbWVudDE3MTA2MTAxNA==,2935721,2016-01-12T21:25:48Z,2016-01-12T21:25:48Z,NONE,"You should be using Gulp overrides. https://www.npmjs.com/package/gulp-bower-overrides

```
  ""overrides"": {
    ""jquery.easy-pie-chart"": {
      ""main"": [
        ""dist/angular.easypiechart.js""
      ]
    }
}
```
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/108/comments,https://github.com/rendro/easy-pie-chart/issues/108#issuecomment-171061014,https://api.github.com/repos/rendro/easy-pie-chart/issues/108
rendro,easy-pie-chart,44377503,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/57469128,57469128,MDEyOklzc3VlQ29tbWVudDU3NDY5MTI4,422168,2014-10-01T14:07:42Z,2014-10-01T14:07:42Z,OWNER,"The plugin does not provide any functionality that would allow that, I'm sorry.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/106/comments,https://github.com/rendro/easy-pie-chart/issues/106#issuecomment-57469128,https://api.github.com/repos/rendro/easy-pie-chart/issues/106
rendro,easy-pie-chart,44377503,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/57548984,57548984,MDEyOklzc3VlQ29tbWVudDU3NTQ4OTg0,5360713,2014-10-01T22:11:05Z,2014-10-01T22:11:05Z,NONE,"ok thanks :+1: 
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/106/comments,https://github.com/rendro/easy-pie-chart/issues/106#issuecomment-57548984,https://api.github.com/repos/rendro/easy-pie-chart/issues/106
rendro,easy-pie-chart,43311102,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/56271645,56271645,MDEyOklzc3VlQ29tbWVudDU2MjcxNjQ1,422168,2014-09-20T15:52:09Z,2014-09-20T15:52:09Z,OWNER,"Because it is very hard to overwrite inline style declarations with a stylesheet. So you have to add the width, height and lineheight properties to your CSS.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/105/comments,https://github.com/rendro/easy-pie-chart/issues/105#issuecomment-56271645,https://api.github.com/repos/rendro/easy-pie-chart/issues/105
rendro,easy-pie-chart,43311102,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/56315768,56315768,MDEyOklzc3VlQ29tbWVudDU2MzE1NzY4,14259,2014-09-21T22:50:13Z,2014-09-21T22:50:13Z,NONE,"""very hard"" ? can you upgrade the web site example with the latest version?

Thanks.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/105/comments,https://github.com/rendro/easy-pie-chart/issues/105#issuecomment-56315768,https://api.github.com/repos/rendro/easy-pie-chart/issues/105
rendro,easy-pie-chart,43311102,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/56409256,56409256,MDEyOklzc3VlQ29tbWVudDU2NDA5MjU2,422168,2014-09-22T17:33:31Z,2014-09-23T05:13:59Z,OWNER,"Very hard in terms of that you need `!important` in your CSS which is a bad style and should be avoided. The plugin itself should be as explicit as possible instead of doing lots of things implicit.

Will update the gh-page soon.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/105/comments,https://github.com/rendro/easy-pie-chart/issues/105#issuecomment-56409256,https://api.github.com/repos/rendro/easy-pie-chart/issues/105
rendro,easy-pie-chart,42404591,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/60227401,60227401,MDEyOklzc3VlQ29tbWVudDYwMjI3NDAx,3606355,2014-10-23T11:48:30Z,2014-10-23T11:48:30Z,NONE,"Same issue on Chrome and Internet Explorer. 
After some refresh get the correct value. 

Please anyone fix it?
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/102/comments,https://github.com/rendro/easy-pie-chart/issues/102#issuecomment-60227401,https://api.github.com/repos/rendro/easy-pie-chart/issues/102
rendro,easy-pie-chart,42404591,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/60229803,60229803,MDEyOklzc3VlQ29tbWVudDYwMjI5ODAz,422168,2014-10-23T12:14:08Z,2014-10-23T12:14:08Z,OWNER,"This is a Math.round / Math.floor problem. The fix will be available soon.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/102/comments,https://github.com/rendro/easy-pie-chart/issues/102#issuecomment-60229803,https://api.github.com/repos/rendro/easy-pie-chart/issues/102
rendro,easy-pie-chart,42404591,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/63210257,63210257,MDEyOklzc3VlQ29tbWVudDYzMjEwMjU3,9781519,2014-11-16T08:35:24Z,2014-11-16T08:35:24Z,NONE,"Any updates on this bug?
It's a nice plugin but completly useless to me if it doesn't display accurate values.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/102/comments,https://github.com/rendro/easy-pie-chart/issues/102#issuecomment-63210257,https://api.github.com/repos/rendro/easy-pie-chart/issues/102
rendro,easy-pie-chart,42309044,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/56295698,56295698,MDEyOklzc3VlQ29tbWVudDU2Mjk1Njk4,251466,2014-09-21T11:09:40Z,2014-09-21T11:09:40Z,NONE,"EnvoyÃ© de mon iPad

> Le 9 sept. 2014 Ã  15:59, dajy notifications@github.com a Ã©crit :
> 
> Hello rendo,
> 
> I am trying to make this plugin responsive.
> Is there a way that i can update the size ?
> 
> $('.pieanimation').on('click', function(e) {
> $('.chart').size('easyPieChart').update($xsize);
> });
> 
> â€”
> Reply to this email directly or view it on GitHub.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/101/comments,https://github.com/rendro/easy-pie-chart/issues/101#issuecomment-56295698,https://api.github.com/repos/rendro/easy-pie-chart/issues/101
rendro,easy-pie-chart,42309044,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/57992940,57992940,MDEyOklzc3VlQ29tbWVudDU3OTkyOTQw,4720634,2014-10-06T09:24:59Z,2014-10-06T09:24:59Z,NONE,"second this, is there a way to update the bar colour as well? 
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/101/comments,https://github.com/rendro/easy-pie-chart/issues/101#issuecomment-57992940,https://api.github.com/repos/rendro/easy-pie-chart/issues/101
rendro,easy-pie-chart,42309044,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/58004882,58004882,MDEyOklzc3VlQ29tbWVudDU4MDA0ODgy,4720634,2014-10-06T11:33:52Z,2014-10-06T11:33:52Z,NONE,"Rewind<<<< dug through the issues and found this - 

`var pie_options = {
    ....,
    barColor : ""#bbb""
};
$(selector).easyPieChart(pie_options);
var api = $(selector).data('easyPieChart');
$(selector).on(""mouseover"", function() {
    var value =  $(selector).data('percent');
    api.options.barColor = '#123456';
    api.update(value);
});`

That worked for me!
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/101/comments,https://github.com/rendro/easy-pie-chart/issues/101#issuecomment-58004882,https://api.github.com/repos/rendro/easy-pie-chart/issues/101
rendro,easy-pie-chart,42309044,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/58017861,58017861,MDEyOklzc3VlQ29tbWVudDU4MDE3ODYx,422168,2014-10-06T13:40:41Z,2014-10-06T13:40:41Z,OWNER,"@ruudbwai you can pass a function as `barColor` that takes the percentage as a parameter and returns a CSS color string (see documentation).

``` javascript
$('.chart').easyPieChart({
    barColor: function(percent) {
        if (percent > 50) {
            return 'red';
        } else {
            return 'green';
        }
    }
})
```
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/101/comments,https://github.com/rendro/easy-pie-chart/issues/101#issuecomment-58017861,https://api.github.com/repos/rendro/easy-pie-chart/issues/101
rendro,easy-pie-chart,42309044,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/58017989,58017989,MDEyOklzc3VlQ29tbWVudDU4MDE3OTg5,422168,2014-10-06T13:41:33Z,2014-10-06T13:41:33Z,OWNER,"@dajy currently there is no chance to update the size, but this feature is on our list
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/101/comments,https://github.com/rendro/easy-pie-chart/issues/101#issuecomment-58017989,https://api.github.com/repos/rendro/easy-pie-chart/issues/101
rendro,easy-pie-chart,42309044,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/299911149,299911149,MDEyOklzc3VlQ29tbWVudDI5OTkxMTE0OQ==,17005780,2017-05-08T16:03:53Z,2017-05-08T16:03:53Z,NONE,now we have 2017... is this feature now implemented?,NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/101/comments,https://github.com/rendro/easy-pie-chart/issues/101#issuecomment-299911149,https://api.github.com/repos/rendro/easy-pie-chart/issues/101
rendro,easy-pie-chart,42309044,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/303482008,303482008,MDEyOklzc3VlQ29tbWVudDMwMzQ4MjAwOA==,422168,2017-05-23T17:57:45Z,2017-05-24T00:45:47Z,OWNER,@dilotec-2015 its 2017 and you still have not sent me a pull request. Open source means participation not demanding features. Comments like yours are the reason why people like me who dedicated some of their time to build free and open software struggle to support it.,NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/101/comments,https://github.com/rendro/easy-pie-chart/issues/101#issuecomment-303482008,https://api.github.com/repos/rendro/easy-pie-chart/issues/101
rendro,easy-pie-chart,42309044,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/338393880,338393880,MDEyOklzc3VlQ29tbWVudDMzODM5Mzg4MA==,32397825,2017-10-21T13:18:03Z,2017-10-21T13:18:03Z,NONE,"hey everyone - you just need to target the Canvas element via CSS in a media-query:

@media screen and (max-width: 767px) {
	.cs-wrap canvas[width=""150""] {
		width: 120px;
	}â€‹
	.cs-wrap canvas[height=""150""] {
		height: 120px;
	}â€‹
}

you will need to restyle the font-sizes etc. - but its actually pretty easy - just a little extra typing (thanks for providing :-))",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/101/comments,https://github.com/rendro/easy-pie-chart/issues/101#issuecomment-338393880,https://api.github.com/repos/rendro/easy-pie-chart/issues/101
rendro,easy-pie-chart,42309044,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/391172405,391172405,MDEyOklzc3VlQ29tbWVudDM5MTE3MjQwNQ==,12821814,2018-05-22T23:25:49Z,2018-05-22T23:26:38Z,NONE,"The fix that I used was that I added the `easy-pie-canvas` class to the `canvas` element, see [THIS pull request I just submitted.](https://github.com/rendro/easy-pie-chart/pull/204)

If you have this class name, you can simply do the following in your stylesheet:

```
.easy-pie-canvas {
  height: 50px;
  width: 50px;
}
```

But make sure you first remove the default widths/heights from the canvases:
```
$("".easy-pie-canvas"").css(""width"", """");
$("".easy-pie-canvas"").css(""height"", """");
```

Thanks for the package @rendro!",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/101/comments,https://github.com/rendro/easy-pie-chart/issues/101#issuecomment-391172405,https://api.github.com/repos/rendro/easy-pie-chart/issues/101
rendro,easy-pie-chart,41802623,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/55924065,55924065,MDEyOklzc3VlQ29tbWVudDU1OTI0MDY1,422168,2014-09-17T16:51:16Z,2014-09-17T16:51:16Z,OWNER,"> easy pie chart is a lightweight plugin to draw simple, animated pie charts for single values

This plugin is designed for single values. Sorry for the misleading name.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/100/comments,https://github.com/rendro/easy-pie-chart/issues/100#issuecomment-55924065,https://api.github.com/repos/rendro/easy-pie-chart/issues/100
rendro,easy-pie-chart,41745435,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/54220746,54220746,MDEyOklzc3VlQ29tbWVudDU0MjIwNzQ2,746429,2014-09-02T21:30:44Z,2014-09-02T21:30:44Z,COLLABORATOR,"Can you provide a [plnkr](plnkr.co/edit/) example for it ?
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/99/comments,https://github.com/rendro/easy-pie-chart/issues/99#issuecomment-54220746,https://api.github.com/repos/rendro/easy-pie-chart/issues/99
rendro,easy-pie-chart,41745435,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/54234583,54234583,MDEyOklzc3VlQ29tbWVudDU0MjM0NTgz,1108563,2014-09-02T23:50:11Z,2014-09-02T23:50:11Z,NONE,"The screenshot is from that site of the plugin. This one: http://rendro.github.io/easy-pie-chart/
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/99/comments,https://github.com/rendro/easy-pie-chart/issues/99#issuecomment-54234583,https://api.github.com/repos/rendro/easy-pie-chart/issues/99
rendro,easy-pie-chart,41745435,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/54741317,54741317,MDEyOklzc3VlQ29tbWVudDU0NzQxMzE3,1108563,2014-09-07T08:58:26Z,2014-09-07T08:58:26Z,NONE,"Hmm, this is no longer the case. Maybe it was a bug in the Chrome dev version.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/99/comments,https://github.com/rendro/easy-pie-chart/issues/99#issuecomment-54741317,https://api.github.com/repos/rendro/easy-pie-chart/issues/99
rendro,easy-pie-chart,41745435,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/54741332,54741332,MDEyOklzc3VlQ29tbWVudDU0NzQxMzMy,746429,2014-09-07T08:59:12Z,2014-09-07T08:59:12Z,COLLABORATOR,"ok, thanks anyway for reporting.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/99/comments,https://github.com/rendro/easy-pie-chart/issues/99#issuecomment-54741332,https://api.github.com/repos/rendro/easy-pie-chart/issues/99
rendro,easy-pie-chart,36572282,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/51444469,51444469,MDEyOklzc3VlQ29tbWVudDUxNDQ0NDY5,422168,2014-08-07T08:29:23Z,2014-08-07T08:29:23Z,OWNER,"The bower version is set to 2.1.5. Should work properly.

See: https://github.com/rendro/easy-pie-chart/blob/master/bower.json#L3
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/97/comments,https://github.com/rendro/easy-pie-chart/issues/97#issuecomment-51444469,https://api.github.com/repos/rendro/easy-pie-chart/issues/97
rendro,easy-pie-chart,36362945,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/47330193,47330193,MDEyOklzc3VlQ29tbWVudDQ3MzMwMTkz,746429,2014-06-27T10:42:23Z,2014-06-27T10:42:23Z,COLLABORATOR,"Do you mean this ? https://github.com/rendro/easy-pie-chart/blob/master/demo/style.css
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/95/comments,https://github.com/rendro/easy-pie-chart/issues/95#issuecomment-47330193,https://api.github.com/repos/rendro/easy-pie-chart/issues/95
rendro,easy-pie-chart,36362945,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/57034417,57034417,MDEyOklzc3VlQ29tbWVudDU3MDM0NDE3,1356163,2014-09-26T23:55:12Z,2014-09-26T23:55:12Z,NONE,"Yeah the reason this is relevant is that the github readme doesn't mention it, but the website says you should add it. It's also a little bit confusing, is the css required(as it's only in the demo folder, but the website suggests it is), or is it truly just a demo file?

I'm not looking for the answers, I'm just explaining the problem.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/95/comments,https://github.com/rendro/easy-pie-chart/issues/95#issuecomment-57034417,https://api.github.com/repos/rendro/easy-pie-chart/issues/95
rendro,easy-pie-chart,36362945,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/71799710,71799710,MDEyOklzc3VlQ29tbWVudDcxNzk5NzEw,84020,2015-01-28T08:49:35Z,2015-01-28T08:49:35Z,NONE,"I'm confused too - is the CSS required or is it just used in the demo?
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/95/comments,https://github.com/rendro/easy-pie-chart/issues/95#issuecomment-71799710,https://api.github.com/repos/rendro/easy-pie-chart/issues/95
rendro,easy-pie-chart,36362945,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/103905447,103905447,MDEyOklzc3VlQ29tbWVudDEwMzkwNTQ0Nw==,2394719,2015-05-20T14:28:48Z,2015-05-20T14:36:18Z,NONE,"The style.css contains styles for both the demo page and the chart itself, you just have to get rid of the page styles and you can use it.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/95/comments,https://github.com/rendro/easy-pie-chart/issues/95#issuecomment-103905447,https://api.github.com/repos/rendro/easy-pie-chart/issues/95
rendro,easy-pie-chart,36362945,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/103907048,103907048,MDEyOklzc3VlQ29tbWVudDEwMzkwNzA0OA==,181588,2015-05-20T14:33:57Z,2015-05-20T14:33:57Z,NONE,"that's really developer friendly...

On Wed, May 20, 2015 at 4:28 PM, RichardMisencik notifications@github.com
wrote:

> The style.css contains style for both the demo page and the chart itself,
> you just have to get rid of the page styles and you can use it.
> 
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/rendro/easy-pie-chart/issues/95#issuecomment-103905447
> .
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/95/comments,https://github.com/rendro/easy-pie-chart/issues/95#issuecomment-103907048,https://api.github.com/repos/rendro/easy-pie-chart/issues/95
rendro,easy-pie-chart,36362945,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/103908715,103908715,MDEyOklzc3VlQ29tbWVudDEwMzkwODcxNQ==,2394719,2015-05-20T14:37:12Z,2015-05-20T14:37:12Z,NONE,"I agree, took me a while to find it
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/95/comments,https://github.com/rendro/easy-pie-chart/issues/95#issuecomment-103908715,https://api.github.com/repos/rendro/easy-pie-chart/issues/95
rendro,easy-pie-chart,36362945,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/158915765,158915765,MDEyOklzc3VlQ29tbWVudDE1ODkxNTc2NQ==,1743919,2015-11-23T12:05:23Z,2015-11-23T12:05:36Z,NONE,"Me too, it took time to figure it out! Please make it part of Docs, Readme and package
@rendro 
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/95/comments,https://github.com/rendro/easy-pie-chart/issues/95#issuecomment-158915765,https://api.github.com/repos/rendro/easy-pie-chart/issues/95
rendro,easy-pie-chart,34949054,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/45199562,45199562,MDEyOklzc3VlQ29tbWVudDQ1MTk5NTYy,746429,2014-06-05T09:37:08Z,2014-06-05T09:37:34Z,COLLABORATOR,"Can you create a [jsfiddle](www.jsfiddle.net) for it ?
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/94/comments,https://github.com/rendro/easy-pie-chart/issues/94#issuecomment-45199562,https://api.github.com/repos/rendro/easy-pie-chart/issues/94
rendro,easy-pie-chart,34949054,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/45235225,45235225,MDEyOklzc3VlQ29tbWVudDQ1MjM1MjI1,7010632,2014-06-05T15:36:34Z,2014-06-05T15:36:34Z,NONE,"Found the problem...
Using this on onStep:     

```
this.$el.find('span').text(~~value);
```

was causing rounding problems...  now i use Math.round(value).

You can can close the ticket. 
Thanks for this amazing plugin.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/94/comments,https://github.com/rendro/easy-pie-chart/issues/94#issuecomment-45235225,https://api.github.com/repos/rendro/easy-pie-chart/issues/94
rendro,easy-pie-chart,33741145,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/43470741,43470741,MDEyOklzc3VlQ29tbWVudDQzNDcwNzQx,5965056,2014-05-19T06:50:31Z,2014-05-19T06:54:16Z,NONE,"I had the same problem. I had to manually fix it by setting a line height in css equal to the size of the chart, in your case 300 and then padding-left to what works in my case 13px. Not sure if this is a bug or just the way its done. Basically your chart span and the percentage span appear to occupying the same dom box but they are aligned at the baseline of that box. i.e. bottom left so the line height of your percentage value is suppressing the height of the percentage on screen. You'll likely need some script to correct padding when the value ticks from 99 to 100 a obviously this entails an extra character.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/93/comments,https://github.com/rendro/easy-pie-chart/issues/93#issuecomment-43470741,https://api.github.com/repos/rendro/easy-pie-chart/issues/93
rendro,easy-pie-chart,33741145,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/43597439,43597439,MDEyOklzc3VlQ29tbWVudDQzNTk3NDM5,422168,2014-05-20T08:10:38Z,2014-05-20T08:10:38Z,OWNER,"You can apply the styling from the demo folder:

Make sure to change with and height according to your canvas size.

``` css
.chart {
  position: relative;
  display: inline-block;
  width: 110px;
  height: 110px;
  margin-top: 50px;
  margin-bottom: 50px;
  text-align: center;
}
.chart canvas {
  position: absolute;
  top: 0;
  left: 0;
}
.percent {
  display: inline-block;
  line-height: 110px;
  z-index: 2;
}
.percent:after {
  content: '%';
  margin-left: 0.1em;
  font-size: .8em;
}
```
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/93/comments,https://github.com/rendro/easy-pie-chart/issues/93#issuecomment-43597439,https://api.github.com/repos/rendro/easy-pie-chart/issues/93
rendro,easy-pie-chart,33741145,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/129109760,129109760,MDEyOklzc3VlQ29tbWVudDEyOTEwOTc2MA==,1090113,2015-08-09T04:40:47Z,2015-08-09T04:40:47Z,NONE,"This is closed as not being a bug or closed as can't make it dynamic? For every pie chart we need a different width/height?
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/93/comments,https://github.com/rendro/easy-pie-chart/issues/93#issuecomment-129109760,https://api.github.com/repos/rendro/easy-pie-chart/issues/93
rendro,easy-pie-chart,33741145,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/148074859,148074859,MDEyOklzc3VlQ29tbWVudDE0ODA3NDg1OQ==,1926014,2015-10-14T14:54:05Z,2015-10-14T14:54:05Z,NONE,"HI, 

I am having this problem with the angularJS directive please help 
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/93/comments,https://github.com/rendro/easy-pie-chart/issues/93#issuecomment-148074859,https://api.github.com/repos/rendro/easy-pie-chart/issues/93
rendro,easy-pie-chart,33741145,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/151687424,151687424,MDEyOklzc3VlQ29tbWVudDE1MTY4NzQyNA==,1918526,2015-10-28T00:58:07Z,2015-10-28T00:58:07Z,NONE,"Looking at the Angular version (angular.easypiechart.js), I don't see that the Angular version writes the actual percentage amount (i.e. ""73%"") on the canvas - is this correct?  I see no parameters in the options object to enable this either.  Is this capability only for the jQuery version?
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/93/comments,https://github.com/rendro/easy-pie-chart/issues/93#issuecomment-151687424,https://api.github.com/repos/rendro/easy-pie-chart/issues/93
rendro,easy-pie-chart,33741145,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/227798001,227798001,MDEyOklzc3VlQ29tbWVudDIyNzc5ODAwMQ==,3236791,2016-06-22T16:22:22Z,2016-06-22T16:22:22Z,NONE,"Hi @rendro it seems the angular version doesn't create the percentage label inside the canvas. Could you help us?
thanks
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/93/comments,https://github.com/rendro/easy-pie-chart/issues/93#issuecomment-227798001,https://api.github.com/repos/rendro/easy-pie-chart/issues/93
rendro,easy-pie-chart,33741145,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/233082608,233082608,MDEyOklzc3VlQ29tbWVudDIzMzA4MjYwOA==,3233618,2016-07-15T22:16:29Z,2016-07-15T23:23:35Z,NONE,"@rendro Hi there, i have the same issue with the angular version. Are you planning on adding this in the near future?

Please advise, kind regards,
Julius

Never mind :

<div class=""wrappereasypie"">
        <div easypiechart class=""openorderwidget"" options=""optionsopen"" percent=""percentopen"">
        <div class=""easypielabel"">
            {{openorderspercentage}}%<br/>
            <span>Open:</span><br/>
            {{openorders.length}}
        </div>

```
    </div>

    <div easypiechart class=""openorderwidget"" options=""optionshandled"" percent=""percenthandled"">
    <div class=""easypielabel"">
        {{handledorderspercentage}}%<br/>
        <span>Afgehandeld</span><br/>
        {{handledorders.length}}

    </div>
    </div>
</div>
```

.openorderwidget {
    width : 120px;
    position : relative;
    float : left;
}
.easypielabel {
    width : 110px;
    text-align: center;
    position: relative;
    z-index: auto;
    top : 85px;
    float : left;
    display : block;
}
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/93/comments,https://github.com/rendro/easy-pie-chart/issues/93#issuecomment-233082608,https://api.github.com/repos/rendro/easy-pie-chart/issues/93
rendro,easy-pie-chart,33741145,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/398165742,398165742,MDEyOklzc3VlQ29tbWVudDM5ODE2NTc0Mg==,7242712,2018-06-18T19:20:11Z,2018-06-18T19:20:11Z,NONE,Does anyone know if the angular version was ever updated to add the percentage to the middle?,NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/93/comments,https://github.com/rendro/easy-pie-chart/issues/93#issuecomment-398165742,https://api.github.com/repos/rendro/easy-pie-chart/issues/93
rendro,easy-pie-chart,32897045,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/47330505,47330505,MDEyOklzc3VlQ29tbWVudDQ3MzMwNTA1,746429,2014-06-27T10:47:09Z,2014-06-27T10:47:09Z,COLLABORATOR,"can you provide the exact changes you made ?
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/92/comments,https://github.com/rendro/easy-pie-chart/pull/92#issuecomment-47330505,https://api.github.com/repos/rendro/easy-pie-chart/issues/92
rendro,easy-pie-chart,32897045,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/47437325,47437325,MDEyOklzc3VlQ29tbWVudDQ3NDM3MzI1,6107887,2014-06-28T20:14:11Z,2014-06-28T20:14:11Z,NONE,"To trace my modifications, associate my comments in the file below:

https://github.com/RempelOliveira/easy-pie-chart/blob/Issue%2390/dist/jquery.easypiechart.issue-90.js

// change var percent and isNegative
//var drawBackground = function() {
// add param percent on drawCircle method
//remove drawBackground method
//remove drawBackground method
//add this condition

Can you compare this file with original too. But, I believe this to be sufficient.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/92/comments,https://github.com/rendro/easy-pie-chart/pull/92#issuecomment-47437325,https://api.github.com/repos/rendro/easy-pie-chart/issues/92
rendro,easy-pie-chart,31486640,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/40443902,40443902,MDEyOklzc3VlQ29tbWVudDQwNDQzOTAy,345964,2014-04-15T04:10:39Z,2014-05-06T09:32:33Z,NONE,"Here's how I've solved a similar problem.  You can simplify for your needs.

FYI, This is an angular function, but it would work the same in vanilla js.

``` javascript
$scope.barColor = function () {

    var score = parseInt($scope.tile.Rank, 10);
    var target = parseInt($scope.tile.GoalValue, 10);

    if (target === null) {
        target = 50;
    }

    var color = """";

    if (score < (target - (target * 0.2)))
        color = ""#ee1010"";
    else if (score < target)
        color = ""#e6c908"";
    else {
        color = ""#74b818"";
    }
    if (score === 0) {
        color = ""#666666"";
    }

    return color;

};
```
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/91/comments,https://github.com/rendro/easy-pie-chart/issues/91#issuecomment-40443902,https://api.github.com/repos/rendro/easy-pie-chart/issues/91
rendro,easy-pie-chart,31486640,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/143316329,143316329,MDEyOklzc3VlQ29tbWVudDE0MzMxNjMyOQ==,14841661,2015-09-25T18:32:59Z,2015-09-25T18:32:59Z,NONE,"I removed:
var color;
    if (typeof(options.barColor) === 'function') {
            color = options.barColor(percent);
    } else {
            color = options.barColor;
    } 
in the JQuery file and replaced it with:

if(percent >= 90){
    color = '#5ebe01'; //green
}else if(percent <= 89 && percent >= 51){
    color = '#fe7903';//orange
}else if(percent <= 50){ 
    color = '#ed2e08'; //red
}

which worked for me.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/91/comments,https://github.com/rendro/easy-pie-chart/issues/91#issuecomment-143316329,https://api.github.com/repos/rendro/easy-pie-chart/issues/91
rendro,easy-pie-chart,31395188,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/42282845,42282845,MDEyOklzc3VlQ29tbWVudDQyMjgyODQ1,746429,2014-05-06T09:34:54Z,2014-05-06T09:35:51Z,COLLABORATOR,"Can you post the code example here or even better can you create a pull request for it ? 

It seems that this is the same problem described in #81.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/90/comments,https://github.com/rendro/easy-pie-chart/issues/90#issuecomment-42282845,https://api.github.com/repos/rendro/easy-pie-chart/issues/90
rendro,easy-pie-chart,31395188,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/42300093,42300093,MDEyOklzc3VlQ29tbWVudDQyMzAwMDkz,6107887,2014-05-06T13:15:07Z,2014-05-06T13:15:51Z,NONE,"@mrzmyr @rendro The author should review the code before that. I did not make this change for other plugins, such as angular e vanilla js. I made this change in just one jquery.easypiechart.js file. Of course it is enough to guide them to resolve this issue according to your need.

If I make an pullRequest there will be no problems as to what I described above? 
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/90/comments,https://github.com/rendro/easy-pie-chart/issues/90#issuecomment-42300093,https://api.github.com/repos/rendro/easy-pie-chart/issues/90
rendro,easy-pie-chart,31395188,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/42300255,42300255,MDEyOklzc3VlQ29tbWVudDQyMzAwMjU1,746429,2014-05-06T13:16:48Z,2014-05-06T13:16:48Z,COLLABORATOR,"If you make a PR i can dive in to it and bringt into the other files so Robert doesn't have the effort to review it. 
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/90/comments,https://github.com/rendro/easy-pie-chart/issues/90#issuecomment-42300255,https://api.github.com/repos/rendro/easy-pie-chart/issues/90
rendro,easy-pie-chart,31395188,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/42303577,42303577,MDEyOklzc3VlQ29tbWVudDQyMzAzNTc3,6107887,2014-05-06T13:46:42Z,2014-05-06T13:46:42Z,NONE,"push to master?
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/90/comments,https://github.com/rendro/easy-pie-chart/issues/90#issuecomment-42303577,https://api.github.com/repos/rendro/easy-pie-chart/issues/90
rendro,easy-pie-chart,31395188,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/42304495,42304495,MDEyOklzc3VlQ29tbWVudDQyMzA0NDk1,746429,2014-05-06T13:54:47Z,2014-05-06T13:54:47Z,COLLABORATOR,"1. Fork the project 
2. Clone your fork
3. create a feature branch
4. push branch to your feature branch
5. create pull request
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/90/comments,https://github.com/rendro/easy-pie-chart/issues/90#issuecomment-42304495,https://api.github.com/repos/rendro/easy-pie-chart/issues/90
rendro,easy-pie-chart,31395188,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/42308465,42308465,MDEyOklzc3VlQ29tbWVudDQyMzA4NDY1,6107887,2014-05-06T14:25:20Z,2014-05-06T14:30:37Z,NONE,"Create #92.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/90/comments,https://github.com/rendro/easy-pie-chart/issues/90#issuecomment-42308465,https://api.github.com/repos/rendro/easy-pie-chart/issues/90
rendro,easy-pie-chart,31395188,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/42310858,42310858,MDEyOklzc3VlQ29tbWVudDQyMzEwODU4,746429,2014-05-06T14:41:44Z,2014-05-06T14:41:44Z,COLLABORATOR,"@rendro, http://www.diffchecker.com/gzxdy2i3
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/90/comments,https://github.com/rendro/easy-pie-chart/issues/90#issuecomment-42310858,https://api.github.com/repos/rendro/easy-pie-chart/issues/90
rendro,easy-pie-chart,31395188,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/42350001,42350001,MDEyOklzc3VlQ29tbWVudDQyMzUwMDAx,422168,2014-05-06T19:49:12Z,2014-05-06T19:49:12Z,OWNER,"I will look into it.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/90/comments,https://github.com/rendro/easy-pie-chart/issues/90#issuecomment-42350001,https://api.github.com/repos/rendro/easy-pie-chart/issues/90
rendro,easy-pie-chart,30700436,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/42283012,42283012,MDEyOklzc3VlQ29tbWVudDQyMjgzMDEy,746429,2014-05-06T09:36:51Z,2014-05-06T09:36:51Z,COLLABORATOR,"There is no plan to provide a webjar.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/89/comments,https://github.com/rendro/easy-pie-chart/issues/89#issuecomment-42283012,https://api.github.com/repos/rendro/easy-pie-chart/issues/89
rendro,easy-pie-chart,30409585,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/42174486,42174486,MDEyOklzc3VlQ29tbWVudDQyMTc0NDg2,1757492,2014-05-05T10:11:34Z,2014-05-05T10:11:34Z,NONE,"View the demo code: https://github.com/rendro/easy-pie-chart/blob/master/demo/angular.html
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/88/comments,https://github.com/rendro/easy-pie-chart/issues/88#issuecomment-42174486,https://api.github.com/repos/rendro/easy-pie-chart/issues/88
rendro,easy-pie-chart,30409585,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/42282383,42282383,MDEyOklzc3VlQ29tbWVudDQyMjgyMzgz,746429,2014-05-06T09:28:04Z,2014-05-06T09:28:04Z,COLLABORATOR,"This is done by CSS and custom updates. This is done in angular by two-data-binding and the jQuery way is to use the event and then update the node's content. (counting number)
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/88/comments,https://github.com/rendro/easy-pie-chart/issues/88#issuecomment-42282383,https://api.github.com/repos/rendro/easy-pie-chart/issues/88
rendro,easy-pie-chart,30314979,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/42311333,42311333,MDEyOklzc3VlQ29tbWVudDQyMzExMzMz,746429,2014-05-06T14:44:43Z,2014-05-06T14:46:00Z,COLLABORATOR,"@7ever is this the same as your #87 PR ?
@rendro, what do you think about it?
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/86/comments,https://github.com/rendro/easy-pie-chart/pull/86#issuecomment-42311333,https://api.github.com/repos/rendro/easy-pie-chart/issues/86
rendro,easy-pie-chart,30314979,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/42327140,42327140,MDEyOklzc3VlQ29tbWVudDQyMzI3MTQw,422168,2014-05-06T16:43:51Z,2014-05-06T16:43:51Z,OWNER,"Once stopped there is no way to start again.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/86/comments,https://github.com/rendro/easy-pie-chart/pull/86#issuecomment-42327140,https://api.github.com/repos/rendro/easy-pie-chart/issues/86
rendro,easy-pie-chart,29721360,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/38038147,38038147,MDEyOklzc3VlQ29tbWVudDM4MDM4MTQ3,3908658,2014-03-19T10:56:29Z,2014-03-19T12:41:19Z,NONE,"Oky i use this for getting started at click. But a restart would be nice. :+1: 

```
$("".startbutton"").click(function() {
        $('.chart').easyPieChart({
    //Here comes the Options
        });
    });
```
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/85/comments,https://github.com/rendro/easy-pie-chart/issues/85#issuecomment-38038147,https://api.github.com/repos/rendro/easy-pie-chart/issues/85
rendro,easy-pie-chart,29721360,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/38112253,38112253,MDEyOklzc3VlQ29tbWVudDM4MTEyMjUz,422168,2014-03-19T21:56:56Z,2014-03-19T21:56:56Z,OWNER,"You can set the initial percent value to 0 or leave it blank. Then you can trigger the animation with the update method. Pass the value to it to which you want the chart to be updated.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/85/comments,https://github.com/rendro/easy-pie-chart/issues/85#issuecomment-38112253,https://api.github.com/repos/rendro/easy-pie-chart/issues/85
rendro,easy-pie-chart,28710976,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/36633216,36633216,MDEyOklzc3VlQ29tbWVudDM2NjMzMjE2,422168,2014-03-04T15:06:09Z,2014-03-04T15:06:09Z,OWNER,"Currently not. I labeled this issue ""feature request"".
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/84/comments,https://github.com/rendro/easy-pie-chart/issues/84#issuecomment-36633216,https://api.github.com/repos/rendro/easy-pie-chart/issues/84
rendro,easy-pie-chart,28710976,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/66487889,66487889,MDEyOklzc3VlQ29tbWVudDY2NDg3ODg5,70500,2014-12-10T17:19:46Z,2014-12-10T17:19:46Z,CONTRIBUTOR,"PR #113 adds the feature.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/84/comments,https://github.com/rendro/easy-pie-chart/issues/84#issuecomment-66487889,https://api.github.com/repos/rendro/easy-pie-chart/issues/84
rendro,easy-pie-chart,28678160,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/36612193,36612193,MDEyOklzc3VlQ29tbWVudDM2NjEyMTkz,422168,2014-03-04T10:44:58Z,2014-03-04T10:46:22Z,OWNER,"IE8 is still not supported.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/83/comments,https://github.com/rendro/easy-pie-chart/issues/83#issuecomment-36612193,https://api.github.com/repos/rendro/easy-pie-chart/issues/83
rendro,easy-pie-chart,28598315,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/36496784,36496784,MDEyOklzc3VlQ29tbWVudDM2NDk2Nzg0,422168,2014-03-03T10:17:12Z,2014-03-03T10:17:12Z,OWNER,"IE8 is not supported, because IE8 is too old.

[See the browser support section in the readme for details](https://github.com/rendro/easy-pie-chart#browser-support).

---

If you want to support IE8 you need excanvas, a canvas polyfill. See an example in the demo folder. The file is called [old-ie.html](https://github.com/rendro/easy-pie-chart/blob/master/demo/old-ie.html).
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/82/comments,https://github.com/rendro/easy-pie-chart/issues/82#issuecomment-36496784,https://api.github.com/repos/rendro/easy-pie-chart/issues/82
rendro,easy-pie-chart,28443168,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/36360006,36360006,MDEyOklzc3VlQ29tbWVudDM2MzYwMDA2,422168,2014-02-28T15:20:01Z,2014-02-28T15:20:01Z,OWNER,"Did you try this only in the simulator or is it the same on a real device. Might be the autoscaling for displays with a high pixel density. 
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/81/comments,https://github.com/rendro/easy-pie-chart/issues/81#issuecomment-36360006,https://api.github.com/repos/rendro/easy-pie-chart/issues/81
rendro,easy-pie-chart,28443168,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/36360342,36360342,MDEyOklzc3VlQ29tbWVudDM2MzYwMzQy,139261,2014-02-28T15:23:21Z,2014-02-28T15:23:21Z,NONE,"Nope. happens on the device as well. Please let me know if there's any info I can provide to you.

![img_0535](https://f.cloud.github.com/assets/139261/2295131/3920b99e-a08c-11e3-9115-c46b105da1b5.JPG)
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/81/comments,https://github.com/rendro/easy-pie-chart/issues/81#issuecomment-36360342,https://api.github.com/repos/rendro/easy-pie-chart/issues/81
rendro,easy-pie-chart,28443168,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/36361905,36361905,MDEyOklzc3VlQ29tbWVudDM2MzYxOTA1,422168,2014-02-28T15:33:18Z,2014-02-28T15:33:18Z,OWNER,"I will do so. Have to dig deeper and try to reproduce the rendering issue. Thanks for reporting. I'll keep you informed
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/81/comments,https://github.com/rendro/easy-pie-chart/issues/81#issuecomment-36361905,https://api.github.com/repos/rendro/easy-pie-chart/issues/81
rendro,easy-pie-chart,28443168,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/36362006,36362006,MDEyOklzc3VlQ29tbWVudDM2MzYyMDA2,139261,2014-02-28T15:34:15Z,2014-02-28T15:34:15Z,NONE,":thumbsup: 
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/81/comments,https://github.com/rendro/easy-pie-chart/issues/81#issuecomment-36362006,https://api.github.com/repos/rendro/easy-pie-chart/issues/81
rendro,easy-pie-chart,28443168,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/38151808,38151808,MDEyOklzc3VlQ29tbWVudDM4MTUxODA4,6513342,2014-03-20T10:20:14Z,2014-03-20T10:31:36Z,NONE,"Confirmed this issue also occurred on my end as well, by using latest 2.1.4 version.
However it works fine if using older version, 1.6.2.
(Test machine: Samsung Galaxy Tab 7.7 with Android 4.1.2 and Samsung Galaxy Grand with Android 4.2.2; both using stock browser)
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/81/comments,https://github.com/rendro/easy-pie-chart/issues/81#issuecomment-38151808,https://api.github.com/repos/rendro/easy-pie-chart/issues/81
rendro,easy-pie-chart,28443168,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/41435931,41435931,MDEyOklzc3VlQ29tbWVudDQxNDM1OTMx,4186986,2014-04-25T20:28:01Z,2014-04-25T20:28:01Z,NONE,"Reporting the same problem on Samsung S2, Android ver. 4.1.2 on the built-in browser.
So something has changed between version 1.6.2 and 2.1.4 that causes this.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/81/comments,https://github.com/rendro/easy-pie-chart/issues/81#issuecomment-41435931,https://api.github.com/repos/rendro/easy-pie-chart/issues/81
rendro,easy-pie-chart,28436525,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/36361485,36361485,MDEyOklzc3VlQ29tbWVudDM2MzYxNDg1,422168,2014-02-28T15:30:37Z,2014-02-28T15:30:37Z,OWNER,"Thanks, I fixed it and released version 2.1.5

see commit 1cfe5ca0e6759b45a06582f2eff1046686ce36a8
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/80/comments,https://github.com/rendro/easy-pie-chart/issues/80#issuecomment-36361485,https://api.github.com/repos/rendro/easy-pie-chart/issues/80
rendro,easy-pie-chart,28434985,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/36361666,36361666,MDEyOklzc3VlQ29tbWVudDM2MzYxNjY2,422168,2014-02-28T15:31:41Z,2014-02-28T15:31:41Z,OWNER,"Currently not, I am sorry. But this is a feature that is going on the list: Nice to haves! Expect it to be included in the next release
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/79/comments,https://github.com/rendro/easy-pie-chart/issues/79#issuecomment-36361666,https://api.github.com/repos/rendro/easy-pie-chart/issues/79
rendro,easy-pie-chart,27075456,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/34361581,34361581,MDEyOklzc3VlQ29tbWVudDM0MzYxNTgx,385649,2014-02-06T19:36:45Z,2014-02-06T19:37:01Z,NONE,"Ok, fixed it myself, sorry.

The problem was I was using the core `easypiechart.js` script from the `master` branch. Need to use it from the svg branch [here](https://raw2.github.com/rendro/easy-pie-chart/svgrenderer/src/easypiechart.js)

Thanks a lot for this plugin!
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/78/comments,https://github.com/rendro/easy-pie-chart/issues/78#issuecomment-34361581,https://api.github.com/repos/rendro/easy-pie-chart/issues/78
rendro,easy-pie-chart,27075456,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/34365163,34365163,MDEyOklzc3VlQ29tbWVudDM0MzY1MTYz,422168,2014-02-06T20:10:44Z,2014-02-06T20:10:44Z,OWNER,"Great! Keep in mind, that the SVG renderer is not stable yet. If you experience any issues I would appreciate your feedback to improve it further.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/78/comments,https://github.com/rendro/easy-pie-chart/issues/78#issuecomment-34365163,https://api.github.com/repos/rendro/easy-pie-chart/issues/78
rendro,easy-pie-chart,27075456,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/34365936,34365936,MDEyOklzc3VlQ29tbWVudDM0MzY1OTM2,385649,2014-02-06T20:18:50Z,2014-02-06T20:18:50Z,NONE,"The only issue I've had so far is it won't render the bar color in IE9. I haven't tried other IE browsers. It's worked well in Chrome and FF.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/78/comments,https://github.com/rendro/easy-pie-chart/issues/78#issuecomment-34365936,https://api.github.com/repos/rendro/easy-pie-chart/issues/78
rendro,easy-pie-chart,27075456,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/175295759,175295759,MDEyOklzc3VlQ29tbWVudDE3NTI5NTc1OQ==,16908421,2016-01-26T23:46:15Z,2016-01-26T23:46:15Z,NONE,"HI Rendro or MichaelCastelbuono,

Can you please give the working link for the SVG core code?

I found this link is not working 'https://raw2.github.com/rendro/easy-pie-chart/svgrenderer/src/easypiechart.js'.

I got it from the last thread:
'The problem was I was using the core easypiechart.js script from the master branch. Need to use it from the svg branch here'

Thank you so much for your help

Regards,

Ed
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/78/comments,https://github.com/rendro/easy-pie-chart/issues/78#issuecomment-175295759,https://api.github.com/repos/rendro/easy-pie-chart/issues/78
rendro,easy-pie-chart,27075456,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/175653694,175653694,MDEyOklzc3VlQ29tbWVudDE3NTY1MzY5NA==,385649,2016-01-27T14:21:44Z,2016-01-27T14:21:44Z,NONE,"Not sure why the link changed, but it should be this file https://github.com/rendro/easy-pie-chart/blob/svgrenderer/src/easypiechart.js -- note, I haven't tested or used this in awhile, but this is the file from the `svgrenderer` branch so it should be the same one I used.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/78/comments,https://github.com/rendro/easy-pie-chart/issues/78#issuecomment-175653694,https://api.github.com/repos/rendro/easy-pie-chart/issues/78
rendro,easy-pie-chart,26724516,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/33873770,33873770,MDEyOklzc3VlQ29tbWVudDMzODczNzcw,422168,2014-02-01T15:03:23Z,2014-02-01T15:03:23Z,OWNER,"Thanks!
The version numbers are unified again across all distributors to version 2.1.4
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/77/comments,https://github.com/rendro/easy-pie-chart/issues/77#issuecomment-33873770,https://api.github.com/repos/rendro/easy-pie-chart/issues/77
rendro,easy-pie-chart,26621044,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/35834036,35834036,MDEyOklzc3VlQ29tbWVudDM1ODM0MDM2,422168,2014-02-23T15:30:49Z,2014-02-23T15:30:49Z,OWNER,"In this case, you should implement your own update method that transforms your values into new percentage values and use that to call the update method of the plugin. And then change the label in the center to your real percentage.

This is a highly customized behavior which requires changing the implementation.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/76/comments,https://github.com/rendro/easy-pie-chart/issues/76#issuecomment-35834036,https://api.github.com/repos/rendro/easy-pie-chart/issues/76
rendro,easy-pie-chart,26621044,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/327529764,327529764,MDEyOklzc3VlQ29tbWVudDMyNzUyOTc2NA==,515999,2017-09-06T15:56:16Z,2017-09-06T15:58:01Z,NONE,"@gforster, did you ever get around to this? I am looking for a similar implementation.
In my case, the circle should reflect 0-40 as 0 to 100% respectively.

edit: I see @rendro is working on a V3. :-)",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/76/comments,https://github.com/rendro/easy-pie-chart/issues/76#issuecomment-327529764,https://api.github.com/repos/rendro/easy-pie-chart/issues/76
rendro,easy-pie-chart,26621044,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/327665539,327665539,MDEyOklzc3VlQ29tbWVudDMyNzY2NTUzOQ==,422168,2017-09-07T02:29:28Z,2017-09-07T02:29:28Z,OWNER,"Didn't mean to close it ðŸ˜ƒ 

As of now you can use the update method to set you value and apply your own transform:

```javascript
function transformMyValue(inputValue) {
    return inputValue / 40 * 100;
}

$('.chart').data('easyPieChart').update(transformMyValue(35));
```",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/76/comments,https://github.com/rendro/easy-pie-chart/issues/76#issuecomment-327665539,https://api.github.com/repos/rendro/easy-pie-chart/issues/76
rendro,easy-pie-chart,26621044,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/329267633,329267633,MDEyOklzc3VlQ29tbWVudDMyOTI2NzYzMw==,515999,2017-09-13T19:11:25Z,2017-09-13T19:11:25Z,NONE,Updating its value will update the canvas as well. For now I'm just updating the innerHTML 40ms after the rendering.,NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/76/comments,https://github.com/rendro/easy-pie-chart/issues/76#issuecomment-329267633,https://api.github.com/repos/rendro/easy-pie-chart/issues/76
rendro,easy-pie-chart,26283730,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/33282547,33282547,MDEyOklzc3VlQ29tbWVudDMzMjgyNTQ3,134505,2014-01-25T06:38:12Z,2014-01-25T06:39:39Z,CONTRIBUTOR,"The codepan sample is not working :)

```
Refused to execute script from
'https://raw2.github.com/quikly/easy-pie-chart/gradient/dist/easypiechart.js'
because its MIME type ('text/plain') is not executable,
and strict MIME type checking is enabled
```
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/73/comments,https://github.com/rendro/easy-pie-chart/pull/73#issuecomment-33282547,https://api.github.com/repos/rendro/easy-pie-chart/issues/73
rendro,easy-pie-chart,26283730,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/33294826,33294826,MDEyOklzc3VlQ29tbWVudDMzMjk0ODI2,172409,2014-01-25T17:44:30Z,2014-01-25T17:44:30Z,CONTRIBUTOR,"Oops! I looks like github was serving it to me only because I was logged in! Sorry about that. Let's try this instead:

http://www.codepen.io/anon/pen/Hxcyn
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/73/comments,https://github.com/rendro/easy-pie-chart/pull/73#issuecomment-33294826,https://api.github.com/repos/rendro/easy-pie-chart/issues/73
rendro,easy-pie-chart,26283730,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/33316080,33316080,MDEyOklzc3VlQ29tbWVudDMzMzE2MDgw,422168,2014-01-26T12:53:05Z,2014-01-26T12:53:05Z,OWNER,":+1: This is a good idea. Though I would prefer naming the getter methods `getCtx` and `getCanvas` to show that these are getter for private properties
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/73/comments,https://github.com/rendro/easy-pie-chart/pull/73#issuecomment-33316080,https://api.github.com/repos/rendro/easy-pie-chart/issues/73
rendro,easy-pie-chart,26283730,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/33318280,33318280,MDEyOklzc3VlQ29tbWVudDMzMzE4Mjgw,746429,2014-01-26T14:42:20Z,2014-01-26T14:42:20Z,COLLABORATOR,"agree with @rendro. Can you add this @stereoscott ? So then we can merge it :)
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/73/comments,https://github.com/rendro/easy-pie-chart/pull/73#issuecomment-33318280,https://api.github.com/repos/rendro/easy-pie-chart/issues/73
rendro,easy-pie-chart,26283730,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/33325877,33325877,MDEyOklzc3VlQ29tbWVudDMzMzI1ODc3,172409,2014-01-26T18:38:06Z,2014-01-26T18:38:06Z,CONTRIBUTOR,"Thanks for the feedback all. I've updated the pull request with `getCtx` and `getCanvas` and updated the example at http://www.codepen.io/stereoscott/pen/bhjeF
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/73/comments,https://github.com/rendro/easy-pie-chart/pull/73#issuecomment-33325877,https://api.github.com/repos/rendro/easy-pie-chart/issues/73
rendro,easy-pie-chart,26283730,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/33583781,33583781,MDEyOklzc3VlQ29tbWVudDMzNTgzNzgx,746429,2014-01-29T13:24:54Z,2014-01-29T13:25:07Z,COLLABORATOR,"I'll merge it when i get a bit time to add examples for this into the readme example section.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/73/comments,https://github.com/rendro/easy-pie-chart/pull/73#issuecomment-33583781,https://api.github.com/repos/rendro/easy-pie-chart/issues/73
rendro,easy-pie-chart,26157676,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/33589871,33589871,MDEyOklzc3VlQ29tbWVudDMzNTg5ODcx,746429,2014-01-29T14:42:58Z,2014-01-29T14:43:29Z,COLLABORATOR,"hey @Sceneshift, thanks for your report. Checkout the recently added demo page: [`requirejs.html`](https://github.com/rendro/easy-pie-chart/blob/master/demo/requirejs.html).
With this commit 2f6dc70163e0cc08a0958aa75fa4bb67334a40db i renamed the amd modules to you now have prefixed for each version (angular, jquery).
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/72/comments,https://github.com/rendro/easy-pie-chart/issues/72#issuecomment-33589871,https://api.github.com/repos/rendro/easy-pie-chart/issues/72
rendro,easy-pie-chart,26046674,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/32966547,32966547,MDEyOklzc3VlQ29tbWVudDMyOTY2NTQ3,746429,2014-01-21T21:39:33Z,2014-01-21T21:39:33Z,COLLABORATOR,"I guess you are talking about the demo. This is a custom element which is updated in line https://github.com/rendro/easy-pie-chart/blob/docs-automatic-generation/demo/jquery.html#L31.

``` html
<span class=""chart"" data-percent=""86"">
       <span class=""percent""></span>
</span>
```
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/71/comments,https://github.com/rendro/easy-pie-chart/issues/71#issuecomment-32966547,https://api.github.com/repos/rendro/easy-pie-chart/issues/71
rendro,easy-pie-chart,26046674,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/33014343,33014343,MDEyOklzc3VlQ29tbWVudDMzMDE0MzQz,6464906,2014-01-22T11:41:00Z,2014-01-22T11:41:00Z,NONE,"i saw yesterday i was not uploading the css :) thanks!!

On Tue, Jan 21, 2014 at 9:39 PM, mrzmyr notifications@github.com wrote:

> I guess you are talking about the demo. This is a custom element which is
> updated in line
> https://github.com/rendro/easy-pie-chart/blob/docs-automatic-generation/demo/jquery.html#L31
> .
> 
> <span class=""chart"" data-percent=""86"">
>        <span class=""percent""></span></span>
> 
> â€”
> Reply to this email directly or view it on GitHubhttps://github.com/rendro/easy-pie-chart/issues/71#issuecomment-32966547
> .

## 

_Loja de bijuteria online_

http://silencesecret-biju.blogspot.com/

Escolha as suas prendinhas..delicie-se... :)
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/71/comments,https://github.com/rendro/easy-pie-chart/issues/71#issuecomment-33014343,https://api.github.com/repos/rendro/easy-pie-chart/issues/71
rendro,easy-pie-chart,26044458,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/33356842,33356842,MDEyOklzc3VlQ29tbWVudDMzMzU2ODQy,422168,2014-01-27T10:38:28Z,2014-01-27T10:38:28Z,OWNER,"Do you know if it is possible to register two bower components for one repository?
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/70/comments,https://github.com/rendro/easy-pie-chart/issues/70#issuecomment-33356842,https://api.github.com/repos/rendro/easy-pie-chart/issues/70
rendro,easy-pie-chart,26044458,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/92121980,92121980,MDEyOklzc3VlQ29tbWVudDkyMTIxOTgw,328404,2015-04-12T20:41:47Z,2015-04-12T20:41:47Z,CONTRIBUTOR,"I don't think it is.  There can only be one main section in the `bower.json`.  How do you current publish to npm?  Cause it could be built into the build process to switch the `main` section and publish to npm for both angular and regular.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/70/comments,https://github.com/rendro/easy-pie-chart/issues/70#issuecomment-92121980,https://api.github.com/repos/rendro/easy-pie-chart/issues/70
rendro,easy-pie-chart,26044458,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/153823738,153823738,MDEyOklzc3VlQ29tbWVudDE1MzgyMzczOA==,731936,2015-11-04T18:39:01Z,2015-11-04T18:39:01Z,NONE,"+1 please
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/70/comments,https://github.com/rendro/easy-pie-chart/issues/70#issuecomment-153823738,https://api.github.com/repos/rendro/easy-pie-chart/issues/70
rendro,easy-pie-chart,25844674,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/32686313,32686313,MDEyOklzc3VlQ29tbWVudDMyNjg2MzEz,746429,2014-01-18T16:47:45Z,2014-01-18T16:47:45Z,COLLABORATOR,"thanks ! :shaved_ice: 
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/69/comments,https://github.com/rendro/easy-pie-chart/pull/69#issuecomment-32686313,https://api.github.com/repos/rendro/easy-pie-chart/issues/69
rendro,easy-pie-chart,25648385,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/32369297,32369297,MDEyOklzc3VlQ29tbWVudDMyMzY5Mjk3,422168,2014-01-15T15:08:38Z,2014-01-15T15:08:38Z,OWNER,"Currently you cant do this, but you can transform the number before passing it into the update method:

``` javascript
chart.update(100 * yourValue / 20);
```
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/68/comments,https://github.com/rendro/easy-pie-chart/issues/68#issuecomment-32369297,https://api.github.com/repos/rendro/easy-pie-chart/issues/68
rendro,easy-pie-chart,25648385,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/333240104,333240104,MDEyOklzc3VlQ29tbWVudDMzMzI0MDEwNA==,10137,2017-09-29T21:10:44Z,2017-09-29T21:10:44Z,NONE,"If you want to update the upper bound of the chart but still maintain control of the detailed value shown see the following.

In my case, I wanted the control to have a range from 0-10 (e.g a value of 5 would result in the control being 50% full). The advice above will update the scale, but also the value being displayed (e.g. 5/10 -> 50/100. If you want to maintain the value (5) but adjust the scale (100->10), see the following. 

Note, I've also included a sample of how to update the value color separately from the control color. If you'd like to add units or additional content to the value see the 'content' option for the percent:after css class below.

```css
.simple-pie {
    position: relative;
    display: inline-block;
    width: 110px;
    height: 110px;
    margin-top: 5px;
    margin-bottom: 5px;
    text-align: center
}
.simple-pie canvas {
    position: absolute;
    top: 0;
    left: 0;
}
.percent {
    display: inline-block;
    line-height: 110px;
    z-index: 2;
}
.percent:after {
    content: """";
    margin-left: .1em;
    font-size: .8em;
}
```

```html
<script src=""jquery.easypiechart.min.js""></script>

<span id=""gauge"" class=""simple-pie"" data-percent=""100"">
    <span id=""gauge-value"" class=""percent""></span>
</span>
```

```js
      var cur = foo();
      var scaledValue = (100 * cur / 10);
      var elem = $(""#gauge"");
      if (elem) {
        elem.easyPieChart({
            animate: 2000,
            barColor: ""#26B99A"",
            trackColor: ""#D3D3D3"",
            scaleColor: false,
            lineWidth: 20,
            trackWidth: 16,
            lineCap: ""butt"",
            percent: scaledValue,
            onStep: function (from, to, percent) {
                $(this.el).find('.percent').text(Number((percent / 10).toFixed(1)));
            }
        });

        setTimeout(function() {
            elem.data(""easyPieChart"").update(scaledValue);
        }, 1000);

        $(""#gauge-value"").css(""color"", getRiskScoreColor(cur));
      }
```",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/68/comments,https://github.com/rendro/easy-pie-chart/issues/68#issuecomment-333240104,https://api.github.com/repos/rendro/easy-pie-chart/issues/68
rendro,easy-pie-chart,25570924,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/32462753,32462753,MDEyOklzc3VlQ29tbWVudDMyNDYyNzUz,6399182,2014-01-16T11:55:08Z,2014-01-16T11:55:08Z,NONE,"It's a miracle... it's working again... Best, ixtract
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/67/comments,https://github.com/rendro/easy-pie-chart/issues/67#issuecomment-32462753,https://api.github.com/repos/rendro/easy-pie-chart/issues/67
rendro,easy-pie-chart,25462482,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/32123464,32123464,MDEyOklzc3VlQ29tbWVudDMyMTIzNDY0,2479372,2014-01-12T14:19:08Z,2014-01-12T14:19:08Z,NONE,"Real problem is here:
options[i] = options[i].bind(this);

I temporary solved it with:
if (typeof(options[i]) === 'function' &&  i != 'renderer') {

Until you find a better solution.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/66/comments,https://github.com/rendro/easy-pie-chart/issues/66#issuecomment-32123464,https://api.github.com/repos/rendro/easy-pie-chart/issues/66
rendro,easy-pie-chart,25462482,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/33590423,33590423,MDEyOklzc3VlQ29tbWVudDMzNTkwNDIz,746429,2014-01-29T14:48:35Z,2014-01-29T14:48:35Z,COLLABORATOR,"We would appreciate if you create a PR to fix it.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/66/comments,https://github.com/rendro/easy-pie-chart/issues/66#issuecomment-33590423,https://api.github.com/repos/rendro/easy-pie-chart/issues/66
rendro,easy-pie-chart,25462482,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/33687923,33687923,MDEyOklzc3VlQ29tbWVudDMzNjg3OTIz,2479372,2014-01-30T13:37:50Z,2014-01-30T13:37:50Z,NONE,"It is not a real fix. I believe you need to find a better solution based on what happening there. You probably need to see how mootools break bind() method and then rewrite bind() method in next line of your code in the way that it can be executed when we have mootools and jQuery side by side.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/66/comments,https://github.com/rendro/easy-pie-chart/issues/66#issuecomment-33687923,https://api.github.com/repos/rendro/easy-pie-chart/issues/66
rendro,easy-pie-chart,25462482,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/33688965,33688965,MDEyOklzc3VlQ29tbWVudDMzNjg4OTY1,422168,2014-01-30T13:51:41Z,2014-01-30T13:51:57Z,OWNER,"You should probably open an issue for mootools, that they should not break native JavaScript methods.

Having a fix for every possible other third party library should not be in the source of this plugin.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/66/comments,https://github.com/rendro/easy-pie-chart/issues/66#issuecomment-33688965,https://api.github.com/repos/rendro/easy-pie-chart/issues/66
rendro,easy-pie-chart,25462482,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/165679938,165679938,MDEyOklzc3VlQ29tbWVudDE2NTY3OTkzOA==,16343843,2015-12-18T05:52:42Z,2015-12-18T05:52:42Z,NONE,"Thank you so much!
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/66/comments,https://github.com/rendro/easy-pie-chart/issues/66#issuecomment-165679938,https://api.github.com/repos/rendro/easy-pie-chart/issues/66
rendro,easy-pie-chart,25330425,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/31967876,31967876,MDEyOklzc3VlQ29tbWVudDMxOTY3ODc2,746429,2014-01-09T19:38:27Z,2014-01-09T19:38:27Z,COLLABORATOR,"We're planning to bring dynamic options into easypiechart. For now you can use dynamic percentage values as you see in this example: http://plnkr.co/edit/HEyMt7AfI05yVxR8K8mu?p=preview
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/65/comments,https://github.com/rendro/easy-pie-chart/issues/65#issuecomment-31967876,https://api.github.com/repos/rendro/easy-pie-chart/issues/65
rendro,easy-pie-chart,25330425,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/32015357,32015357,MDEyOklzc3VlQ29tbWVudDMyMDE1MzU3,435704,2014-01-10T10:02:41Z,2014-01-10T10:02:41Z,NONE,"Thanks, i'll wait for that feature. 
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/65/comments,https://github.com/rendro/easy-pie-chart/issues/65#issuecomment-32015357,https://api.github.com/repos/rendro/easy-pie-chart/issues/65
rendro,easy-pie-chart,25330425,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/44001838,44001838,MDEyOklzc3VlQ29tbWVudDQ0MDAxODM4,1113178,2014-05-23T12:12:50Z,2014-05-23T12:12:50Z,NONE,"+1 for this feature
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/65/comments,https://github.com/rendro/easy-pie-chart/issues/65#issuecomment-44001838,https://api.github.com/repos/rendro/easy-pie-chart/issues/65
rendro,easy-pie-chart,25330425,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/67417514,67417514,MDEyOklzc3VlQ29tbWVudDY3NDE3NTE0,466584,2014-12-17T23:37:11Z,2014-12-17T23:37:11Z,NONE,"+1
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/65/comments,https://github.com/rendro/easy-pie-chart/issues/65#issuecomment-67417514,https://api.github.com/repos/rendro/easy-pie-chart/issues/65
rendro,easy-pie-chart,25330425,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/77841372,77841372,MDEyOklzc3VlQ29tbWVudDc3ODQxMzcy,6861239,2015-03-09T12:02:14Z,2015-03-09T12:02:14Z,NONE,"+1
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/65/comments,https://github.com/rendro/easy-pie-chart/issues/65#issuecomment-77841372,https://api.github.com/repos/rendro/easy-pie-chart/issues/65
rendro,easy-pie-chart,25330425,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/77842470,77842470,MDEyOklzc3VlQ29tbWVudDc3ODQyNDcw,6861239,2015-03-09T12:11:55Z,2015-03-09T12:11:55Z,NONE,"Actually +100
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/65/comments,https://github.com/rendro/easy-pie-chart/issues/65#issuecomment-77842470,https://api.github.com/repos/rendro/easy-pie-chart/issues/65
rendro,easy-pie-chart,25330425,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/112310687,112310687,MDEyOklzc3VlQ29tbWVudDExMjMxMDY4Nw==,2383863,2015-06-16T06:41:31Z,2015-06-16T06:41:31Z,NONE,"+1
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/65/comments,https://github.com/rendro/easy-pie-chart/issues/65#issuecomment-112310687,https://api.github.com/repos/rendro/easy-pie-chart/issues/65
rendro,easy-pie-chart,25330425,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/128443816,128443816,MDEyOklzc3VlQ29tbWVudDEyODQ0MzgxNg==,3223947,2015-08-06T17:00:46Z,2015-08-06T17:01:15Z,NONE,"+1
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/65/comments,https://github.com/rendro/easy-pie-chart/issues/65#issuecomment-128443816,https://api.github.com/repos/rendro/easy-pie-chart/issues/65
rendro,easy-pie-chart,25330425,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/128446258,128446258,MDEyOklzc3VlQ29tbWVudDEyODQ0NjI1OA==,746429,2015-08-06T17:09:16Z,2015-08-06T17:09:16Z,COLLABORATOR,"PR's are welcome. :octocat::v:
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/65/comments,https://github.com/rendro/easy-pie-chart/issues/65#issuecomment-128446258,https://api.github.com/repos/rendro/easy-pie-chart/issues/65
rendro,easy-pie-chart,25330425,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/132466719,132466719,MDEyOklzc3VlQ29tbWVudDEzMjQ2NjcxOQ==,3110622,2015-08-19T07:06:04Z,2015-08-19T07:06:04Z,NONE,"Setting options dynamically would be great, especially colors
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/65/comments,https://github.com/rendro/easy-pie-chart/issues/65#issuecomment-132466719,https://api.github.com/repos/rendro/easy-pie-chart/issues/65
rendro,easy-pie-chart,25330425,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/133420502,133420502,MDEyOklzc3VlQ29tbWVudDEzMzQyMDUwMg==,591648,2015-08-21T13:02:52Z,2015-08-21T13:02:52Z,NONE,"+1
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/65/comments,https://github.com/rendro/easy-pie-chart/issues/65#issuecomment-133420502,https://api.github.com/repos/rendro/easy-pie-chart/issues/65
rendro,easy-pie-chart,25330425,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/234732181,234732181,MDEyOklzc3VlQ29tbWVudDIzNDczMjE4MQ==,2390687,2016-07-23T18:12:42Z,2016-07-23T18:12:42Z,NONE,"+1
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/65/comments,https://github.com/rendro/easy-pie-chart/issues/65#issuecomment-234732181,https://api.github.com/repos/rendro/easy-pie-chart/issues/65
rendro,easy-pie-chart,25330425,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/256357372,256357372,MDEyOklzc3VlQ29tbWVudDI1NjM1NzM3Mg==,8337455,2016-10-26T14:04:23Z,2016-10-26T14:04:23Z,NONE,"+1
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/65/comments,https://github.com/rendro/easy-pie-chart/issues/65#issuecomment-256357372,https://api.github.com/repos/rendro/easy-pie-chart/issues/65
rendro,easy-pie-chart,25330425,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/313126979,313126979,MDEyOklzc3VlQ29tbWVudDMxMzEyNjk3OQ==,20644595,2017-07-05T14:51:49Z,2017-07-05T14:51:49Z,NONE,+1,NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/65/comments,https://github.com/rendro/easy-pie-chart/issues/65#issuecomment-313126979,https://api.github.com/repos/rendro/easy-pie-chart/issues/65
rendro,easy-pie-chart,25330425,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/314302130,314302130,MDEyOklzc3VlQ29tbWVudDMxNDMwMjEzMA==,4306219,2017-07-11T02:08:09Z,2017-07-11T02:08:09Z,NONE,+1,NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/65/comments,https://github.com/rendro/easy-pie-chart/issues/65#issuecomment-314302130,https://api.github.com/repos/rendro/easy-pie-chart/issues/65
rendro,easy-pie-chart,24781681,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/31220084,31220084,MDEyOklzc3VlQ29tbWVudDMxMjIwMDg0,746429,2013-12-26T12:43:59Z,2013-12-26T12:43:59Z,COLLABORATOR,"Hey mediastuttgart, thank you for your contribution.
I think the `enable` key in the `animation` is redundant. What about just using `false` is its disabled else using a the object.

**Example** (enabled)

```
animation: {
    duration: 1000
}
```

**Example** (disabled)

```
animation: false
```
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/64/comments,https://github.com/rendro/easy-pie-chart/pull/64#issuecomment-31220084,https://api.github.com/repos/rendro/easy-pie-chart/issues/64
rendro,easy-pie-chart,24781681,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/31249001,31249001,MDEyOklzc3VlQ29tbWVudDMxMjQ5MDAx,134505,2013-12-27T06:30:07Z,2013-12-27T06:36:03Z,CONTRIBUTOR,"@mrzmyr yeah basically you're right. if i think about it, then i would also put the easing function inside the animate object so it can easily be extended in the future with more animation options, assuming that animations are enabled by default.

Animations disabled:

```
animation: false
```

Animations enabled:

```
animation: {
    duration: 1000,
    easing: function () {}
}
```

The problem comes with the api functions to enable or disable animations. Disabling animations with api would also kill the easing function. So with this approach it would keep an enabled key to easily enable or disable animations with the api.

Animations enabled:

```
animation: {
    enabled: true,
    duration: 1000,
    easing: function () {}
}
```
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/64/comments,https://github.com/rendro/easy-pie-chart/pull/64#issuecomment-31249001,https://api.github.com/repos/rendro/easy-pie-chart/issues/64
rendro,easy-pie-chart,24781681,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/31968073,31968073,MDEyOklzc3VlQ29tbWVudDMxOTY4MDcz,746429,2014-01-09T19:40:48Z,2014-01-09T19:40:48Z,COLLABORATOR,"Thanks for your contribution.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/64/comments,https://github.com/rendro/easy-pie-chart/pull/64#issuecomment-31968073,https://api.github.com/repos/rendro/easy-pie-chart/issues/64
rendro,easy-pie-chart,24385809,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/32236534,32236534,MDEyOklzc3VlQ29tbWVudDMyMjM2NTM0,1156106,2014-01-14T03:44:49Z,2014-01-14T03:44:49Z,NONE,"+1. I'm having the same issue with IE8. This fix solves it.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/63/comments,https://github.com/rendro/easy-pie-chart/pull/63#issuecomment-32236534,https://api.github.com/repos/rendro/easy-pie-chart/issues/63
rendro,easy-pie-chart,22952006,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/29570676,29570676,MDEyOklzc3VlQ29tbWVudDI5NTcwNjc2,422168,2013-12-01T09:56:36Z,2013-12-01T09:56:36Z,OWNER,"Thank you for your contribution. I don't think that it needs to translate to angular or vanilla.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/62/comments,https://github.com/rendro/easy-pie-chart/pull/62#issuecomment-29570676,https://api.github.com/repos/rendro/easy-pie-chart/issues/62
rendro,easy-pie-chart,22597910,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/28775936,28775936,MDEyOklzc3VlQ29tbWVudDI4Nzc1OTM2,422168,2013-11-19T09:25:44Z,2013-11-19T09:25:44Z,OWNER,"The problem was that the EasyPieChart plugin tried to load the 'jQuery' module but module names are case sensitive. I changed it to 'jquery', tested it and it worked fine with your config above.

See version 2.1.1 (20d9260)
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/61/comments,https://github.com/rendro/easy-pie-chart/issues/61#issuecomment-28775936,https://api.github.com/repos/rendro/easy-pie-chart/issues/61
rendro,easy-pie-chart,22597910,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/28775989,28775989,MDEyOklzc3VlQ29tbWVudDI4Nzc1OTg5,4660728,2013-11-19T09:26:56Z,2013-11-19T09:26:56Z,NONE,"Thanks a lot !
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/61/comments,https://github.com/rendro/easy-pie-chart/issues/61#issuecomment-28775989,https://api.github.com/repos/rendro/easy-pie-chart/issues/61
rendro,easy-pie-chart,22344373,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/28126431,28126431,MDEyOklzc3VlQ29tbWVudDI4MTI2NDMx,422168,2013-11-09T12:59:03Z,2013-11-09T12:59:03Z,OWNER,"With CSS. Just look in the demo folder, there you'll find the example from the screenshot
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/60/comments,https://github.com/rendro/easy-pie-chart/issues/60#issuecomment-28126431,https://api.github.com/repos/rendro/easy-pie-chart/issues/60
rendro,easy-pie-chart,22344373,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/41513728,41513728,MDEyOklzc3VlQ29tbWVudDQxNTEzNzI4,1371300,2014-04-27T23:55:40Z,2014-04-27T23:58:01Z,NONE,"Can you please add **THIS** in the docs?
Why you dont make that? A lot of people asking the **same question** and im new on `easy-pie-chart` and had the same question - Why? On **all basic setups** is only describe how to ""install"" the chart, and **the CSS Part** is for me also for the basic setup!

```
 <div class=""chart"" data-percent=""73"" data-scale-color=""#ffb400"">73%</div>

 <script src=""https://ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js""></script>
 <script src=""/path/to/jquery.easy-pie-chart.js""></script>
 <script>
     $(function() {
         $('.chart').easyPieChart({
             //your options goes here
         });
     });
 </script>
```
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/60/comments,https://github.com/rendro/easy-pie-chart/issues/60#issuecomment-41513728,https://api.github.com/repos/rendro/easy-pie-chart/issues/60
rendro,easy-pie-chart,22344373,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/41536395,41536395,MDEyOklzc3VlQ29tbWVudDQxNTM2Mzk1,422168,2014-04-28T08:58:30Z,2014-04-28T08:58:30Z,OWNER,"@hovida Sure, it is a good idea to add a basic css example to the basic setup as well. I did not feel the need as this is supposed to be a JavaScript plugin that is meant to be customized. I'll see when I have time to add **THIS** to the documentation. If you want it to be available soon, feel free to **fork the repository and open a pull request**. Open source projects like this are driven by contributions rather than demanding issues on github.

Cheers, Rob
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/60/comments,https://github.com/rendro/easy-pie-chart/issues/60#issuecomment-41536395,https://api.github.com/repos/rendro/easy-pie-chart/issues/60
rendro,easy-pie-chart,22329374,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/31989343,31989343,MDEyOklzc3VlQ29tbWVudDMxOTg5MzQz,422168,2014-01-09T23:39:49Z,2014-01-09T23:39:49Z,OWNER,"Remove the $ in the first line and everything should work.

``` javascript
jQuery(document).ready(function() {
    $('.chart').easyPieChart({
        size: '200',
        lineWidth: '6',
        lineCap: 'square',
        scaleColor: '#d5d5d5',
        trackColor: '#eeeeee',
        barColor: '#0099cc',
        easing: 'easeOutBounce',
        onStep: function(from, to, percent) {
            $(this.el).find('.percent').text(Math.round(percent));
        }
    });

    var chart = window.chart = $('.chart').data('easyPieChart');

    $('.js_update').on('click', function() {
        chart.update(Math.random()*100);
    });
});
```
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/59/comments,https://github.com/rendro/easy-pie-chart/issues/59#issuecomment-31989343,https://api.github.com/repos/rendro/easy-pie-chart/issues/59
rendro,easy-pie-chart,22254724,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/29571615,29571615,MDEyOklzc3VlQ29tbWVudDI5NTcxNjE1,422168,2013-12-01T11:08:38Z,2013-12-01T11:08:38Z,OWNER,"Thank you! Somehow git does not show that it was merged, but it is included in version 2.1.3
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/58/comments,https://github.com/rendro/easy-pie-chart/pull/58#issuecomment-29571615,https://api.github.com/repos/rendro/easy-pie-chart/issues/58
rendro,easy-pie-chart,22254724,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/29574146,29574146,MDEyOklzc3VlQ29tbWVudDI5NTc0MTQ2,1736020,2013-12-01T13:59:58Z,2013-12-01T13:59:58Z,CONTRIBUTOR,":smile: 
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/58/comments,https://github.com/rendro/easy-pie-chart/pull/58#issuecomment-29574146,https://api.github.com/repos/rendro/easy-pie-chart/issues/58
rendro,easy-pie-chart,22161268,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/37728866,37728866,MDEyOklzc3VlQ29tbWVudDM3NzI4ODY2,422168,2014-03-15T15:38:30Z,2014-03-15T15:38:30Z,OWNER,"Might be an issue of excanvas. Unless you found a simple solution, I'll close this issue because IE8 is not supported by default.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/57/comments,https://github.com/rendro/easy-pie-chart/issues/57#issuecomment-37728866,https://api.github.com/repos/rendro/easy-pie-chart/issues/57
rendro,easy-pie-chart,21800125,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/37860687,37860687,MDEyOklzc3VlQ29tbWVudDM3ODYwNjg3,3017112,2014-03-17T19:40:23Z,2014-03-17T19:40:23Z,NONE,"+1 for responsive
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/56/comments,https://github.com/rendro/easy-pie-chart/issues/56#issuecomment-37860687,https://api.github.com/repos/rendro/easy-pie-chart/issues/56
rendro,easy-pie-chart,21800125,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/42163063,42163063,MDEyOklzc3VlQ29tbWVudDQyMTYzMDYz,1510170,2014-05-05T06:55:15Z,2014-05-05T06:55:15Z,NONE,"+1 responsive
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/56/comments,https://github.com/rendro/easy-pie-chart/issues/56#issuecomment-42163063,https://api.github.com/repos/rendro/easy-pie-chart/issues/56
rendro,easy-pie-chart,21800125,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/46972453,46972453,MDEyOklzc3VlQ29tbWVudDQ2OTcyNDUz,1113178,2014-06-24T13:47:11Z,2014-06-24T13:47:11Z,NONE,"+1 for responsiveness, or a way to re-draw the chart in AngularJS
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/56/comments,https://github.com/rendro/easy-pie-chart/issues/56#issuecomment-46972453,https://api.github.com/repos/rendro/easy-pie-chart/issues/56
rendro,easy-pie-chart,21800125,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/52946421,52946421,MDEyOklzc3VlQ29tbWVudDUyOTQ2NDIx,323273,2014-08-21T16:36:26Z,2014-08-21T16:36:26Z,NONE,"+1 responsive
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/56/comments,https://github.com/rendro/easy-pie-chart/issues/56#issuecomment-52946421,https://api.github.com/repos/rendro/easy-pie-chart/issues/56
rendro,easy-pie-chart,21800125,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/85021332,85021332,MDEyOklzc3VlQ29tbWVudDg1MDIxMzMy,6680911,2015-03-23T14:24:29Z,2015-03-23T14:24:29Z,NONE,"+1 for responsive? im lost
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/56/comments,https://github.com/rendro/easy-pie-chart/issues/56#issuecomment-85021332,https://api.github.com/repos/rendro/easy-pie-chart/issues/56
rendro,easy-pie-chart,21800125,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/151966694,151966694,MDEyOklzc3VlQ29tbWVudDE1MTk2NjY5NA==,8269529,2015-10-28T19:34:37Z,2015-10-28T19:34:37Z,NONE,"+1 for responsive
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/56/comments,https://github.com/rendro/easy-pie-chart/issues/56#issuecomment-151966694,https://api.github.com/repos/rendro/easy-pie-chart/issues/56
rendro,easy-pie-chart,21800125,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/187243787,187243787,MDEyOklzc3VlQ29tbWVudDE4NzI0Mzc4Nw==,2221150,2016-02-22T15:59:17Z,2016-02-22T15:59:17Z,NONE,"+1 for responsive ... or percentage size.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/56/comments,https://github.com/rendro/easy-pie-chart/issues/56#issuecomment-187243787,https://api.github.com/repos/rendro/easy-pie-chart/issues/56
rendro,easy-pie-chart,21800125,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/215626680,215626680,MDEyOklzc3VlQ29tbWVudDIxNTYyNjY4MA==,8813958,2016-04-29T04:53:24Z,2016-04-29T04:53:24Z,NONE,":+1: 
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/56/comments,https://github.com/rendro/easy-pie-chart/issues/56#issuecomment-215626680,https://api.github.com/repos/rendro/easy-pie-chart/issues/56
rendro,easy-pie-chart,21800125,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/228032226,228032226,MDEyOklzc3VlQ29tbWVudDIyODAzMjIyNg==,10337838,2016-06-23T12:17:23Z,2016-06-23T12:17:23Z,NONE,"+1
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/56/comments,https://github.com/rendro/easy-pie-chart/issues/56#issuecomment-228032226,https://api.github.com/repos/rendro/easy-pie-chart/issues/56
rendro,easy-pie-chart,21800125,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/241352833,241352833,MDEyOklzc3VlQ29tbWVudDI0MTM1MjgzMw==,527057,2016-08-22T08:58:40Z,2016-08-22T08:58:54Z,NONE,"I'm using this as workaround. Not an elegant solution maybe, but it works for me:

``` javascript
var $chart = $('.amount-chart'),
    size = parseFloat( $chart.outerWidth() ),
    clearSet;

$chart.easyPieChart({
    size: size,
    animate: 2000
});

$(window).on('resize', function(){

   size = parseFloat( $chart.outerWidth() );

   $chart.css({
      height: size
   });

   clearTimeout(clearSet);
   clearSet = setTimeout(function(){
      $chart.removeData('easyPieChart').find('canvas').remove();
      $chart.easyPieChart({
         size: size,
         animate: 1
      });
   }, 100);
 });    
```

Here a jsfiddle example with setTimeout lines commented: https://jsfiddle.net/70vy5smy/2/
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/56/comments,https://github.com/rendro/easy-pie-chart/issues/56#issuecomment-241352833,https://api.github.com/repos/rendro/easy-pie-chart/issues/56
rendro,easy-pie-chart,21662238,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/33590559,33590559,MDEyOklzc3VlQ29tbWVudDMzNTkwNTU5,746429,2014-01-29T14:50:04Z,2014-01-29T14:50:04Z,COLLABORATOR,"You can style two of them and overlap them. We're not planning to add this in the near future.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/55/comments,https://github.com/rendro/easy-pie-chart/issues/55#issuecomment-33590559,https://api.github.com/repos/rendro/easy-pie-chart/issues/55
rendro,easy-pie-chart,21648201,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/27180926,27180926,MDEyOklzc3VlQ29tbWVudDI3MTgwOTI2,422168,2013-10-27T22:14:59Z,2013-10-27T22:15:13Z,OWNER,"I'll change parseInt to parseFloat internally. Then all you have to do is to update your onStep-method in the plugin options to format the output as desired.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/54/comments,https://github.com/rendro/easy-pie-chart/issues/54#issuecomment-27180926,https://api.github.com/repos/rendro/easy-pie-chart/issues/54
rendro,easy-pie-chart,21648201,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/27181140,27181140,MDEyOklzc3VlQ29tbWVudDI3MTgxMTQw,422168,2013-10-27T22:23:41Z,2013-10-27T22:23:41Z,OWNER,"Fixed in 8b541543ce35749e4b024bb6b8a5acfcbf1704be

Will be released with version 2.1.0
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/54/comments,https://github.com/rendro/easy-pie-chart/issues/54#issuecomment-27181140,https://api.github.com/repos/rendro/easy-pie-chart/issues/54
rendro,easy-pie-chart,21648201,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/253514156,253514156,MDEyOklzc3VlQ29tbWVudDI1MzUxNDE1Ng==,1781,2016-10-13T13:33:58Z,2016-10-13T13:33:58Z,NONE,"I still can't find an option to use decimal percentages in the center of the gauge.  How can I use the onStep method to do this?
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/54/comments,https://github.com/rendro/easy-pie-chart/issues/54#issuecomment-253514156,https://api.github.com/repos/rendro/easy-pie-chart/issues/54
rendro,easy-pie-chart,21646163,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/27167734,27167734,MDEyOklzc3VlQ29tbWVudDI3MTY3NzM0,746429,2013-10-27T12:03:49Z,2013-10-27T12:03:49Z,COLLABORATOR,"Its not possible to use the parameters with dynamic values. This is related to the #47 issue.
You can checkout the development branch to use it. This will be available in the next version: https://github.com/rendro/easy-pie-chart/issues?milestone=1&page=1&state=closed
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/53/comments,https://github.com/rendro/easy-pie-chart/issues/53#issuecomment-27167734,https://api.github.com/repos/rendro/easy-pie-chart/issues/53
rendro,easy-pie-chart,21646163,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/27199444,27199444,MDEyOklzc3VlQ29tbWVudDI3MTk5NDQ0,422168,2013-10-28T09:57:03Z,2013-10-28T09:57:03Z,OWNER,"Version 2.1.0 released. See #47 or the readme documentation for information regarding the integration.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/53/comments,https://github.com/rendro/easy-pie-chart/issues/53#issuecomment-27199444,https://api.github.com/repos/rendro/easy-pie-chart/issues/53
rendro,easy-pie-chart,21607661,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/27146430,27146430,MDEyOklzc3VlQ29tbWVudDI3MTQ2NDMw,422168,2013-10-26T13:39:33Z,2013-10-26T13:39:33Z,OWNER,"Don't use `animate:true` but a number in ms for the options that defines how long the animation lasts:

`animate:1000` for example
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/52/comments,https://github.com/rendro/easy-pie-chart/issues/52#issuecomment-27146430,https://api.github.com/repos/rendro/easy-pie-chart/issues/52
rendro,easy-pie-chart,21164738,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/26528233,26528233,MDEyOklzc3VlQ29tbWVudDI2NTI4MjMz,422168,2013-10-17T17:09:56Z,2013-10-17T17:09:56Z,OWNER,"To what?
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/50/comments,https://github.com/rendro/easy-pie-chart/issues/50#issuecomment-26528233,https://api.github.com/repos/rendro/easy-pie-chart/issues/50
rendro,easy-pie-chart,21164738,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/27199507,27199507,MDEyOklzc3VlQ29tbWVudDI3MTk5NTA3,422168,2013-10-28T09:58:27Z,2013-10-28T09:58:27Z,OWNER,"Insufficient issue description
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/50/comments,https://github.com/rendro/easy-pie-chart/issues/50#issuecomment-27199507,https://api.github.com/repos/rendro/easy-pie-chart/issues/50
rendro,easy-pie-chart,21066965,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/26608095,26608095,MDEyOklzc3VlQ29tbWVudDI2NjA4MDk1,746429,2013-10-18T16:07:26Z,2013-10-18T16:07:26Z,COLLABORATOR,"Added in caf64b8, just need to turn on Travis-CI repo: https://travis-ci.org/rendro/easy-pie-chart
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/49/comments,https://github.com/rendro/easy-pie-chart/issues/49#issuecomment-26608095,https://api.github.com/repos/rendro/easy-pie-chart/issues/49
rendro,easy-pie-chart,20911534,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/26368200,26368200,MDEyOklzc3VlQ29tbWVudDI2MzY4MjAw,422168,2013-10-15T20:14:46Z,2013-10-15T20:14:46Z,OWNER,"Sorry, I can't reproduce this issue.

If I try your fiddle and return to the tab after some time, it is at 90% and not at 100% as you reported.

Or do I get it wrong?
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/48/comments,https://github.com/rendro/easy-pie-chart/issues/48#issuecomment-26368200,https://api.github.com/repos/rendro/easy-pie-chart/issues/48
rendro,easy-pie-chart,20911534,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/26368664,26368664,MDEyOklzc3VlQ29tbWVudDI2MzY4NjY0,422168,2013-10-15T20:20:25Z,2013-10-15T20:20:40Z,OWNER,"I found the problem: You are using **version 1.2.3** for your fiddle.

[Please use version 2.0.5 that you can download here](https://github.com/rendro/easy-pie-chart/releases/tag/2.0.5)
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/48/comments,https://github.com/rendro/easy-pie-chart/issues/48#issuecomment-26368664,https://api.github.com/repos/rendro/easy-pie-chart/issues/48
rendro,easy-pie-chart,20908728,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/26368033,26368033,MDEyOklzc3VlQ29tbWVudDI2MzY4MDMz,422168,2013-10-15T20:12:47Z,2013-10-15T20:12:47Z,OWNER,"I appreciate this approach as we already discussed. Defining the options in a controller seems more convenient to me than the css-style version that has to be parsed when the module is initialized. 
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/47/comments,https://github.com/rendro/easy-pie-chart/issues/47#issuecomment-26368033,https://api.github.com/repos/rendro/easy-pie-chart/issues/47
rendro,easy-pie-chart,20908728,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/26395651,26395651,MDEyOklzc3VlQ29tbWVudDI2Mzk1NjUx,1343813,2013-10-16T06:32:14Z,2013-10-16T06:32:14Z,NONE,"hi! this proposed structure is exactly what iam searching for! +1  options would be more flexible and reusable ...
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/47/comments,https://github.com/rendro/easy-pie-chart/issues/47#issuecomment-26395651,https://api.github.com/repos/rendro/easy-pie-chart/issues/47
rendro,easy-pie-chart,20908728,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/26556598,26556598,MDEyOklzc3VlQ29tbWVudDI2NTU2NTk4,345964,2013-10-17T21:56:59Z,2013-10-17T21:57:21Z,NONE,"I made this change locally, works great for my purposes. But if you release a version with this feature, I'll update to that release.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/47/comments,https://github.com/rendro/easy-pie-chart/issues/47#issuecomment-26556598,https://api.github.com/repos/rendro/easy-pie-chart/issues/47
rendro,easy-pie-chart,20908728,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/26558631,26558631,MDEyOklzc3VlQ29tbWVudDI2NTU4NjMx,422168,2013-10-17T22:29:45Z,2013-10-17T22:29:45Z,OWNER,"@thelarz the community and the repository would benefit by you opening a pull request with your changes
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/47/comments,https://github.com/rendro/easy-pie-chart/issues/47#issuecomment-26558631,https://api.github.com/repos/rendro/easy-pie-chart/issues/47
rendro,easy-pie-chart,20908728,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/26566246,26566246,MDEyOklzc3VlQ29tbWVudDI2NTY2MjQ2,345964,2013-10-18T01:22:59Z,2013-10-18T01:22:59Z,NONE,"I'd be glad to. Check my work, I'm no JS guru.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/47/comments,https://github.com/rendro/easy-pie-chart/issues/47#issuecomment-26566246,https://api.github.com/repos/rendro/easy-pie-chart/issues/47
rendro,easy-pie-chart,20908728,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/26596162,26596162,MDEyOklzc3VlQ29tbWVudDI2NTk2MTYy,422168,2013-10-18T13:41:07Z,2013-10-18T13:41:07Z,OWNER,"Can you please open a pull request or tell me where to find your code. Never mind, no one needs to be a guru to make a meaningful contribution ;)
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/47/comments,https://github.com/rendro/easy-pie-chart/issues/47#issuecomment-26596162,https://api.github.com/repos/rendro/easy-pie-chart/issues/47
rendro,easy-pie-chart,20908728,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/26603856,26603856,MDEyOklzc3VlQ29tbWVudDI2NjAzODU2,746429,2013-10-18T15:15:15Z,2013-10-18T15:15:35Z,COLLABORATOR,"added to development branch: https://github.com/rendro/easy-pie-chart/pull/51 and will be released in 2.1.0
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/47/comments,https://github.com/rendro/easy-pie-chart/issues/47#issuecomment-26603856,https://api.github.com/repos/rendro/easy-pie-chart/issues/47
rendro,easy-pie-chart,20908728,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/26609402,26609402,MDEyOklzc3VlQ29tbWVudDI2NjA5NDAy,345964,2013-10-18T16:23:36Z,2013-10-18T16:23:36Z,NONE,"Sorry, out of pocket today but I'm anxious to get this going. I had to
clone the repo and copy my changes in, figure out grunt, etc. Was working
on a bug I created  that is causing the interior value to not display.

Sent from my iPhone

On Oct 18, 2013, at 8:41 AM, Robert Fleischmann notifications@github.com
wrote:

Can you please open a pull request or tell me where to find your code.
Never mind, no one needs to be a guru to make a meaningful contribution ;)

â€”
Reply to this email directly or view it on
GitHubhttps://github.com/rendro/easy-pie-chart/issues/47#issuecomment-26596162
.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/47/comments,https://github.com/rendro/easy-pie-chart/issues/47#issuecomment-26609402,https://api.github.com/repos/rendro/easy-pie-chart/issues/47
rendro,easy-pie-chart,20848405,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/26401291,26401291,MDEyOklzc3VlQ29tbWVudDI2NDAxMjkx,422168,2013-10-16T08:40:57Z,2013-10-16T08:40:57Z,OWNER,"I'll look into it. Should be not to hard to add AMD support
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/45/comments,https://github.com/rendro/easy-pie-chart/issues/45#issuecomment-26401291,https://api.github.com/repos/rendro/easy-pie-chart/issues/45
rendro,easy-pie-chart,20848405,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/27181951,27181951,MDEyOklzc3VlQ29tbWVudDI3MTgxOTUx,422168,2013-10-27T23:00:34Z,2013-10-27T23:00:34Z,OWNER,"Added in commit 70c389b3aebc0b810bab326e05f0483acb7e495c.

Will be released with version 2.1.0
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/45/comments,https://github.com/rendro/easy-pie-chart/issues/45#issuecomment-27181951,https://api.github.com/repos/rendro/easy-pie-chart/issues/45
rendro,easy-pie-chart,20800088,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/26367746,26367746,MDEyOklzc3VlQ29tbWVudDI2MzY3NzQ2,422168,2013-10-15T20:09:21Z,2013-10-15T20:09:21Z,OWNER,"See #47 
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/44/comments,https://github.com/rendro/easy-pie-chart/issues/44#issuecomment-26367746,https://api.github.com/repos/rendro/easy-pie-chart/issues/44
rendro,easy-pie-chart,20795303,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/26042168,26042168,MDEyOklzc3VlQ29tbWVudDI2MDQyMTY4,422168,2013-10-10T10:13:57Z,2013-10-10T10:13:57Z,OWNER,"Done in commit 0489a0d90eb9af495449128af17f411b22561f63
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/43/comments,https://github.com/rendro/easy-pie-chart/issues/43#issuecomment-26042168,https://api.github.com/repos/rendro/easy-pie-chart/issues/43
rendro,easy-pie-chart,20774804,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/26036353,26036353,MDEyOklzc3VlQ29tbWVudDI2MDM2MzUz,422168,2013-10-10T08:22:45Z,2013-10-10T08:22:45Z,OWNER,"Thanks a lot for your contribution!
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/42/comments,https://github.com/rendro/easy-pie-chart/pull/42#issuecomment-26036353,https://api.github.com/repos/rendro/easy-pie-chart/issues/42
rendro,easy-pie-chart,19942864,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/24982496,24982496,MDEyOklzc3VlQ29tbWVudDI0OTgyNDk2,422168,2013-09-24T08:10:32Z,2013-09-24T08:10:32Z,OWNER,"Check [this plugin](https://github.com/dlueth/qoopido.emerge) and combine it with easy pie chart. I won't add this certain behavior to this plugin to keep it as small as possible.

Untested code:

``` javascript
$(function(){
    var $chart = $('.chart');
    $chart.one('emerged.emerge', function(event) {
        // initialize the plugin the first time it enters the viewport 
        $chart.easyPieChart({ /* your options */ });
    }).emerge({
        interval:   20,     // default
        threshold:  'auto', // default
        recur:      true,   // default
        auto:       0.5,    // default (meaning 0.5 * screen width/height threshold)
        visibility: true    // default
    });
});
```
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/41/comments,https://github.com/rendro/easy-pie-chart/issues/41#issuecomment-24982496,https://api.github.com/repos/rendro/easy-pie-chart/issues/41
rendro,easy-pie-chart,19536299,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/24915894,24915894,MDEyOklzc3VlQ29tbWVudDI0OTE1ODk0,422168,2013-09-23T12:22:47Z,2013-09-23T12:22:47Z,OWNER,"This is not easy, since it requires to manually render the gradient. Canvas has two built in methods for gradients (linear and radial). The one needed here is a radar like gradient which is a 360 deg bent linear gradient. I'll see if it is possible to implement something like that within the boundaries of performance and lines of new code.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/40/comments,https://github.com/rendro/easy-pie-chart/issues/40#issuecomment-24915894,https://api.github.com/repos/rendro/easy-pie-chart/issues/40
rendro,easy-pie-chart,17658166,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/22143138,22143138,MDEyOklzc3VlQ29tbWVudDIyMTQzMTM4,422168,2013-08-05T21:39:55Z,2013-08-05T21:39:55Z,OWNER,"Thanks @frankvanrest for your contribution!
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/39/comments,https://github.com/rendro/easy-pie-chart/pull/39#issuecomment-22143138,https://api.github.com/repos/rendro/easy-pie-chart/issues/39
rendro,easy-pie-chart,17411103,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/24889000,24889000,MDEyOklzc3VlQ29tbWVudDI0ODg5MDAw,422168,2013-09-22T19:44:12Z,2013-09-22T19:44:12Z,OWNER,"You can initialize the plugin:

``` javascript
var your_desired_value = 65;
$('.chart').easyPieChart({ /* your options */ });
$('.chart').data('easyPieChart').update(your_desired_value);
```
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/38/comments,https://github.com/rendro/easy-pie-chart/issues/38#issuecomment-24889000,https://api.github.com/repos/rendro/easy-pie-chart/issues/38
rendro,easy-pie-chart,17222281,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/24888782,24888782,MDEyOklzc3VlQ29tbWVudDI0ODg4Nzgy,422168,2013-09-22T19:33:32Z,2013-09-22T19:33:32Z,OWNER,"Should be fixed in version 2.0.0. Please revalidate your issue.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/37/comments,https://github.com/rendro/easy-pie-chart/issues/37#issuecomment-24888782,https://api.github.com/repos/rendro/easy-pie-chart/issues/37
rendro,easy-pie-chart,17107444,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/21424459,21424459,MDEyOklzc3VlQ29tbWVudDIxNDI0NDU5,422168,2013-07-23T15:55:54Z,2013-07-23T15:55:54Z,OWNER,"The `trackColor` and `scaleColor` accepts more than only hex-color values e.g. rgb, rgba, hsl, or color values such as ""red"", ""green"", ...

I think the plugin does not need to check against false implementation, and since these values are normally set by the developer I see no need in validating them. Just pass valid colors (like you need in css) and everything should work cross-browser.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/36/comments,https://github.com/rendro/easy-pie-chart/issues/36#issuecomment-21424459,https://api.github.com/repos/rendro/easy-pie-chart/issues/36
rendro,easy-pie-chart,16966500,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/21355301,21355301,MDEyOklzc3VlQ29tbWVudDIxMzU1MzAx,2683512,2013-07-22T16:10:29Z,2013-07-22T16:10:29Z,NONE,"I've noticed that it only happens when using the animate property. In that case, every data-percent you put it returns that number -1. Maybe rendro can help ;)
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/35/comments,https://github.com/rendro/easy-pie-chart/issues/35#issuecomment-21355301,https://api.github.com/repos/rendro/easy-pie-chart/issues/35
rendro,easy-pie-chart,16966500,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/21357914,21357914,MDEyOklzc3VlQ29tbWVudDIxMzU3OTE0,422168,2013-07-22T16:44:51Z,2013-07-22T16:44:51Z,OWNER,"Do you update the value with the onStep method and with that construct `~~currentValue`? Because that may be the reason. Use `round(currentValue)` for more precise rounding, because `~~87.99` will result in `87` (it just strips away all stuff behind the decimal point). Another approach (or you use both) would be to set the value with the onStop method as well. It takes the two parameters currentValue and to. The to value is what you want to be displayed

I hope I could help ;)
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/35/comments,https://github.com/rendro/easy-pie-chart/issues/35#issuecomment-21357914,https://api.github.com/repos/rendro/easy-pie-chart/issues/35
rendro,easy-pie-chart,16966500,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/21360382,21360382,MDEyOklzc3VlQ29tbWVudDIxMzYwMzgy,2683512,2013-07-22T17:21:56Z,2013-07-22T17:21:56Z,NONE,"I am just using the files inside your ""examples"" folder without changing anything ;) It looks like there's something wrong there too
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/35/comments,https://github.com/rendro/easy-pie-chart/issues/35#issuecomment-21360382,https://api.github.com/repos/rendro/easy-pie-chart/issues/35
rendro,easy-pie-chart,16966500,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/21364434,21364434,MDEyOklzc3VlQ29tbWVudDIxMzY0NDM0,422168,2013-07-22T18:21:09Z,2013-07-22T18:21:09Z,OWNER,"Ok, thanks, I'll look into it
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/35/comments,https://github.com/rendro/easy-pie-chart/issues/35#issuecomment-21364434,https://api.github.com/repos/rendro/easy-pie-chart/issues/35
rendro,easy-pie-chart,16966500,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/21394509,21394509,MDEyOklzc3VlQ29tbWVudDIxMzk0NTA5,3403564,2013-07-23T05:50:48Z,2013-07-23T05:50:48Z,NONE,"yes actually if i comment out from the onstep function the 
this.$el.find('span').text(~~value);
it appears to work ok as far as it concerns the percentatge.  great tip to use round .
thanks a million. again such a great job!
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/35/comments,https://github.com/rendro/easy-pie-chart/issues/35#issuecomment-21394509,https://api.github.com/repos/rendro/easy-pie-chart/issues/35
rendro,easy-pie-chart,16862897,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/24888757,24888757,MDEyOklzc3VlQ29tbWVudDI0ODg4NzU3,422168,2013-09-22T19:32:20Z,2013-09-22T19:32:20Z,OWNER,"I implemented the ability to use the jQuery easing plugin with the version 2.0.0.

Thanks @revaxarts for your pull request!
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/34/comments,https://github.com/rendro/easy-pie-chart/pull/34#issuecomment-24888757,https://api.github.com/repos/rendro/easy-pie-chart/issues/34
rendro,easy-pie-chart,16829208,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/21100128,21100128,MDEyOklzc3VlQ29tbWVudDIxMTAwMTI4,422168,2013-07-17T08:55:47Z,2013-07-17T08:55:47Z,OWNER,"Thank you!
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/33/comments,https://github.com/rendro/easy-pie-chart/pull/33#issuecomment-21100128,https://api.github.com/repos/rendro/easy-pie-chart/issues/33
rendro,easy-pie-chart,16782953,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/21100251,21100251,MDEyOklzc3VlQ29tbWVudDIxMTAwMjUx,422168,2013-07-17T08:58:56Z,2013-07-17T08:58:56Z,OWNER,"The Date.now error should be fixed with pull request #33 in version [1.2.3](https://github.com/rendro/easy-pie-chart/releases/tag/1.2.3).
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/32/comments,https://github.com/rendro/easy-pie-chart/issues/32#issuecomment-21100251,https://api.github.com/repos/rendro/easy-pie-chart/issues/32
rendro,easy-pie-chart,16617519,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/20958702,20958702,MDEyOklzc3VlQ29tbWVudDIwOTU4NzAy,422168,2013-07-15T09:12:41Z,2013-07-15T09:12:41Z,OWNER,"@revaxarts thanks for your contribution
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/31/comments,https://github.com/rendro/easy-pie-chart/pull/31#issuecomment-20958702,https://api.github.com/repos/rendro/easy-pie-chart/issues/31
rendro,easy-pie-chart,16616314,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/21100291,21100291,MDEyOklzc3VlQ29tbWVudDIxMTAwMjkx,422168,2013-07-17T08:59:59Z,2013-07-17T08:59:59Z,OWNER,"Duplicate #29 
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/30/comments,https://github.com/rendro/easy-pie-chart/issues/30#issuecomment-21100291,https://api.github.com/repos/rendro/easy-pie-chart/issues/30
rendro,easy-pie-chart,16616266,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/21236110,21236110,MDEyOklzc3VlQ29tbWVudDIxMjM2MTEw,491230,2013-07-19T07:46:33Z,2013-07-19T07:46:33Z,NONE,"I'm also seeing random numbers, when animating, but on OS X 10.8.4 with Chrome 30
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/29/comments,https://github.com/rendro/easy-pie-chart/issues/29#issuecomment-21236110,https://api.github.com/repos/rendro/easy-pie-chart/issues/29
rendro,easy-pie-chart,16616266,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/22139241,22139241,MDEyOklzc3VlQ29tbWVudDIyMTM5MjQx,58522,2013-08-05T20:38:34Z,2013-08-05T20:43:55Z,CONTRIBUTOR,"Bug had to do with animations not being finished when the window.requestanimationframe was not called (e.g. when the page was in another browser-tab during the animation).
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/29/comments,https://github.com/rendro/easy-pie-chart/issues/29#issuecomment-22139241,https://api.github.com/repos/rendro/easy-pie-chart/issues/29
rendro,easy-pie-chart,16616266,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/22143185,22143185,MDEyOklzc3VlQ29tbWVudDIyMTQzMTg1,422168,2013-08-05T21:40:41Z,2013-08-05T21:40:41Z,OWNER,"Thanks for digging into it!
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/29/comments,https://github.com/rendro/easy-pie-chart/issues/29#issuecomment-22143185,https://api.github.com/repos/rendro/easy-pie-chart/issues/29
rendro,easy-pie-chart,16408282,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/24888888,24888888,MDEyOklzc3VlQ29tbWVudDI0ODg4ODg4,422168,2013-09-22T19:38:36Z,2013-09-22T19:38:36Z,OWNER,"This should be implemented in the `onStep` or `onStop` callbacks, because this behavior is not always expected andy easy to implement.

onStep implementation

``` javascript
onStep: function(from, to, currentValue) {
    $(this.el).data('precent', currentValue); // jQuery
    this.el.dataset.percent = currentValue; // vanilla approach
},
```

onStop implementation:

``` javascript
onStop: function(from, to) {
    $(this.el).data('precent', to); // jQuery
    this.el.dataset.percent = to; // vanilla approach
}
```
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/28/comments,https://github.com/rendro/easy-pie-chart/issues/28#issuecomment-24888888,https://api.github.com/repos/rendro/easy-pie-chart/issues/28
rendro,easy-pie-chart,16001012,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/42311236,42311236,MDEyOklzc3VlQ29tbWVudDQyMzExMjM2,746429,2014-05-06T14:44:10Z,2014-05-06T14:44:10Z,COLLABORATOR,"@rendro, what do you think about it?
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/27/comments,https://github.com/rendro/easy-pie-chart/pull/27#issuecomment-42311236,https://api.github.com/repos/rendro/easy-pie-chart/issues/27
rendro,easy-pie-chart,16001012,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/42326995,42326995,MDEyOklzc3VlQ29tbWVudDQyMzI2OTk1,422168,2014-05-06T16:42:30Z,2014-05-06T16:42:30Z,OWNER,"Since there are only a few changes needed I am fine with it. Maybe we find a more descriptive key for the option.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/27/comments,https://github.com/rendro/easy-pie-chart/pull/27#issuecomment-42326995,https://api.github.com/repos/rendro/easy-pie-chart/issues/27
rendro,easy-pie-chart,15785992,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/24888921,24888921,MDEyOklzc3VlQ29tbWVudDI0ODg4OTIx,422168,2013-09-22T19:40:13Z,2013-09-22T19:40:13Z,OWNER,"This would require to change the render algorithm, and won't be implemented.

You'd have to manipulate the draw method of the canvas renderer to draw a triangle at the end of the line.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/26/comments,https://github.com/rendro/easy-pie-chart/issues/26#issuecomment-24888921,https://api.github.com/repos/rendro/easy-pie-chart/issues/26
rendro,easy-pie-chart,15753696,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/19712369,19712369,MDEyOklzc3VlQ29tbWVudDE5NzEyMzY5,422168,2013-06-19T20:26:18Z,2013-06-19T20:26:18Z,OWNER,"Great idea. I took your changes and added them to te coffeescript file as it is the real source file and raised the version number to 1.2.1

Thanks for your contribution :octocat: 
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/25/comments,https://github.com/rendro/easy-pie-chart/pull/25#issuecomment-19712369,https://api.github.com/repos/rendro/easy-pie-chart/issues/25
rendro,easy-pie-chart,15753696,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/19712475,19712475,MDEyOklzc3VlQ29tbWVudDE5NzEyNDc1,450220,2013-06-19T20:28:02Z,2013-06-19T20:28:02Z,CONTRIBUTOR,"My pleasure, thanks for the great plugin.

I'm not familiar with coffeescript, sorry about that!
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/25/comments,https://github.com/rendro/easy-pie-chart/pull/25#issuecomment-19712475,https://api.github.com/repos/rendro/easy-pie-chart/issues/25
rendro,easy-pie-chart,15753696,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/19713372,19713372,MDEyOklzc3VlQ29tbWVudDE5NzEzMzcy,422168,2013-06-19T20:41:36Z,2013-06-19T20:41:36Z,OWNER,"Since several people seem to be not familiar with coffeescript I think about rewriting it in plain & clean JS because it is sad that contributors are not listed as contributors the way it is now.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/25/comments,https://github.com/rendro/easy-pie-chart/pull/25#issuecomment-19713372,https://api.github.com/repos/rendro/easy-pie-chart/issues/25
rendro,easy-pie-chart,15753696,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/19714787,19714787,MDEyOklzc3VlQ29tbWVudDE5NzE0Nzg3,450220,2013-06-19T21:03:37Z,2013-06-19T21:03:37Z,CONTRIBUTOR,"Well, I definitely won't down the value of coffeescript. It seems to be rapidly growing in popularity and it's definitely on my todo list.
Anyhow, it doesn't exactly look like there will be a one-to-rule-them-all front-end standard (other than vanilla JS, perhaps) on the web any time soon, so losing whatever benefits you may gain by a technology (no matter how ""esoteric"") in the interest of common editability seems like it might be a losing battle.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/25/comments,https://github.com/rendro/easy-pie-chart/pull/25#issuecomment-19714787,https://api.github.com/repos/rendro/easy-pie-chart/issues/25
rendro,easy-pie-chart,15736391,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/19710981,19710981,MDEyOklzc3VlQ29tbWVudDE5NzEwOTgx,422168,2013-06-19T20:06:17Z,2013-06-19T20:06:17Z,OWNER,"Very nice idea @m1n0 but concerning the implementation I agree with @revaxarts.
I updated the chart to version 1.2.0 supporting the rotation of the chart.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/24/comments,https://github.com/rendro/easy-pie-chart/pull/24#issuecomment-19710981,https://api.github.com/repos/rendro/easy-pie-chart/issues/24
rendro,easy-pie-chart,15563831,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/19519293,19519293,MDEyOklzc3VlQ29tbWVudDE5NTE5Mjkz,422168,2013-06-16T21:17:20Z,2013-06-16T21:17:20Z,OWNER,"Currently not. The only way is to initialise the plugin with 0% and update the value with the update method as soon as the chart is visible in the viewport. I'll take that as a feature for future releases.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/23/comments,https://github.com/rendro/easy-pie-chart/issues/23#issuecomment-19519293,https://api.github.com/repos/rendro/easy-pie-chart/issues/23
rendro,easy-pie-chart,15561634,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/19519214,19519214,MDEyOklzc3VlQ29tbWVudDE5NTE5MjE0,422168,2013-06-16T21:13:10Z,2013-06-16T21:15:45Z,OWNER,"You can configure the size in the options object during the initialisation of the plugin. You can read this in the readme file.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/22/comments,https://github.com/rendro/easy-pie-chart/issues/22#issuecomment-19519214,https://api.github.com/repos/rendro/easy-pie-chart/issues/22
rendro,easy-pie-chart,15512040,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/19519357,19519357,MDEyOklzc3VlQ29tbWVudDE5NTE5MzU3,422168,2013-06-16T21:20:33Z,2013-06-16T21:20:33Z,OWNER,"You can use different easing animations by adding them to the plugin. Currently only a easeInOutQuad function is implemented. If you find a way to use the jQuery easing functions please let me know for later releases.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/21/comments,https://github.com/rendro/easy-pie-chart/issues/21#issuecomment-19519357,https://api.github.com/repos/rendro/easy-pie-chart/issues/21
rendro,easy-pie-chart,15512040,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/24888954,24888954,MDEyOklzc3VlQ29tbWVudDI0ODg4OTU0,422168,2013-09-22T19:41:48Z,2013-09-22T19:41:48Z,OWNER,"You can use the [jQuery easing plugin](http://gsgd.co.uk/sandbox/jquery/easing/) with version 2.0.0.

Load the easing plugin in your code and set the easing option to the name of the function as a string (see demo/jquery.html of the repository)
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/21/comments,https://github.com/rendro/easy-pie-chart/issues/21#issuecomment-24888954,https://api.github.com/repos/rendro/easy-pie-chart/issues/21
rendro,easy-pie-chart,15512040,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/24889510,24889510,MDEyOklzc3VlQ29tbWVudDI0ODg5NTEw,610869,2013-09-22T20:10:34Z,2013-09-22T20:10:34Z,NONE,"Awesome! 
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/21/comments,https://github.com/rendro/easy-pie-chart/issues/21#issuecomment-24889510,https://api.github.com/repos/rendro/easy-pie-chart/issues/21
rendro,easy-pie-chart,15355032,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/19519323,19519323,MDEyOklzc3VlQ29tbWVudDE5NTE5MzIz,422168,2013-06-16T21:18:45Z,2013-06-16T21:18:45Z,OWNER,"No you can't with the current version of the plugin, without modifying the code and this feature is not planned for later releases
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/20/comments,https://github.com/rendro/easy-pie-chart/issues/20#issuecomment-19519323,https://api.github.com/repos/rendro/easy-pie-chart/issues/20
rendro,easy-pie-chart,15355032,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/35705565,35705565,MDEyOklzc3VlQ29tbWVudDM1NzA1NTY1,1425854,2014-02-21T07:40:56Z,2014-02-21T07:40:56Z,NONE,"Really need this feature. Adding shadow is really cool for the track (inner shadow).

It seems not possible for custom css?

Thanks for your awesome plugin.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/20/comments,https://github.com/rendro/easy-pie-chart/issues/20#issuecomment-35705565,https://api.github.com/repos/rendro/easy-pie-chart/issues/20
rendro,easy-pie-chart,15355032,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/35807682,35807682,MDEyOklzc3VlQ29tbWVudDM1ODA3Njgy,746429,2014-02-22T17:00:31Z,2014-02-22T17:00:54Z,COLLABORATOR,"@Uranbold, you can try using `-webkit-filter: drop-shadow(0px 0px 2px #000);` ([documentation](https://developer.mozilla.org/en-US/docs/Web/CSS/filter))
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/20/comments,https://github.com/rendro/easy-pie-chart/issues/20#issuecomment-35807682,https://api.github.com/repos/rendro/easy-pie-chart/issues/20
rendro,easy-pie-chart,15273622,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/19185777,19185777,MDEyOklzc3VlQ29tbWVudDE5MTg1Nzc3,422168,2013-06-10T08:17:35Z,2013-06-10T08:17:35Z,OWNER,"Thank you! I added your changes to the coffee-script file as this is the source and committed all the changes.
I really forgot to (re-)implement the onStop method.

I also changed the parseInt to the parseFloat function to make it possible to define more detailed steps for big charts.

Thanks again!
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/19/comments,https://github.com/rendro/easy-pie-chart/pull/19#issuecomment-19185777,https://api.github.com/repos/rendro/easy-pie-chart/issues/19
rendro,easy-pie-chart,14747352,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/18445753,18445753,MDEyOklzc3VlQ29tbWVudDE4NDQ1NzUz,422168,2013-05-25T11:34:23Z,2013-05-25T11:34:23Z,OWNER,"Thanks, great idea to make it available via bower!
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/18/comments,https://github.com/rendro/easy-pie-chart/pull/18#issuecomment-18445753,https://api.github.com/repos/rendro/easy-pie-chart/issues/18
rendro,easy-pie-chart,13728681,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/17135510,17135510,MDEyOklzc3VlQ29tbWVudDE3MTM1NTEw,422168,2013-04-28T15:09:44Z,2013-04-28T15:09:44Z,OWNER,"Retina support was added in the commit https://github.com/rendro/easy-pie-chart/commit/b6c418adbe7308e44147801a3c026ad42ad36c98

What exactly do you mean with ""the canvas shrinks""?
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/17/comments,https://github.com/rendro/easy-pie-chart/issues/17#issuecomment-17135510,https://api.github.com/repos/rendro/easy-pie-chart/issues/17
rendro,easy-pie-chart,13473585,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/16798671,16798671,MDEyOklzc3VlQ29tbWVudDE2Nzk4Njcx,422168,2013-04-22T16:23:34Z,2013-04-22T16:23:34Z,OWNER,"If you call the easyPieChart method twice on one element you will create a new pie chart instance rather than updating the old one. Currently there is no other way to smoothly change the color from one to another than passing a function as the option for plugin.

You could get a result by

a) forking the plugin and adding a color-update method

OR

b) update the color with the api and manually trigger the update method:

```
var pie_options = {
    ....,
    barColor : ""#bbb""
};
$(selector).easyPieChart(pie_options);
var api = $(selector).data('easyPieChart');
$(selector).on(""mouseover"", function() {
    var value =  $(selector).data('percent');
    api.options.barColor = '#123456';
    api.update(value);
});
```

This is not elegant but it works.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/16/comments,https://github.com/rendro/easy-pie-chart/issues/16#issuecomment-16798671,https://api.github.com/repos/rendro/easy-pie-chart/issues/16
rendro,easy-pie-chart,13473585,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/16842075,16842075,MDEyOklzc3VlQ29tbWVudDE2ODQyMDc1,514682,2013-04-23T06:47:07Z,2013-04-23T06:47:07Z,NONE,"thanks fr the solution, it was what I needed!
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/16/comments,https://github.com/rendro/easy-pie-chart/issues/16#issuecomment-16842075,https://api.github.com/repos/rendro/easy-pie-chart/issues/16
rendro,easy-pie-chart,13473585,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/46033888,46033888,MDEyOklzc3VlQ29tbWVudDQ2MDMzODg4,3227428,2014-06-13T16:49:17Z,2014-06-13T16:49:17Z,NONE,"Hello, how do you implement the solution into the coding? I am new to this.

Thanks
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/16/comments,https://github.com/rendro/easy-pie-chart/issues/16#issuecomment-46033888,https://api.github.com/repos/rendro/easy-pie-chart/issues/16
rendro,easy-pie-chart,12049611,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/15022510,15022510,MDEyOklzc3VlQ29tbWVudDE1MDIyNTEw,3037634,2013-03-17T13:16:50Z,2013-03-17T13:16:50Z,NONE,"Solved. If someone want to do this, You need to change this ""JQuery('.updateEasyPieChart').on('mouseover', function(e) {...}"" to this ""jQuery ('.updateEasyPieChart').hover(function(e) {...}"" but with a variable that the value is zero
And then, you need to update the class for each element with the same ""data-percent"". That's all
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/15/comments,https://github.com/rendro/easy-pie-chart/issues/15#issuecomment-15022510,https://api.github.com/repos/rendro/easy-pie-chart/issues/15
rendro,easy-pie-chart,12021352,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/15048718,15048718,MDEyOklzc3VlQ29tbWVudDE1MDQ4NzE4,422168,2013-03-18T10:53:57Z,2013-03-18T10:53:57Z,OWNER,"**Not an issue!**
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/14/comments,https://github.com/rendro/easy-pie-chart/issues/14#issuecomment-15048718,https://api.github.com/repos/rendro/easy-pie-chart/issues/14
rendro,easy-pie-chart,10920379,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/13481725,13481725,MDEyOklzc3VlQ29tbWVudDEzNDgxNzI1,422168,2013-02-13T09:54:44Z,2013-02-13T09:54:44Z,OWNER,"The plugin was designed to provide a lightweight solution for a single pie chart case. I think if someone likes to support older browsers they should detect the feature with modernizr and implement a good fallback strategy. But in my opinion this should never be part of the basic library. What do you think? Should a lightweight plugin provide fallback for every possible case?
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/13/comments,https://github.com/rendro/easy-pie-chart/issues/13#issuecomment-13481725,https://api.github.com/repos/rendro/easy-pie-chart/issues/13
rendro,easy-pie-chart,10920379,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/13490074,13490074,MDEyOklzc3VlQ29tbWVudDEzNDkwMDc0,1027554,2013-02-13T13:12:30Z,2013-02-13T13:12:30Z,NONE,"I do agree that it is the implementors responsibility to do a majority of the testing.  However testing for canvas support is a simple test that can make the plugin more resilient.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/13/comments,https://github.com/rendro/easy-pie-chart/issues/13#issuecomment-13490074,https://api.github.com/repos/rendro/easy-pie-chart/issues/13
rendro,easy-pie-chart,10920379,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/13560418,13560418,MDEyOklzc3VlQ29tbWVudDEzNTYwNDE4,422168,2013-02-14T16:13:55Z,2013-02-14T16:13:55Z,OWNER,"I agree on the fact that browser features have to be detected when implementing plugins that heavily rely on new technologies. But I see this as a responsibility of the one who is implementing the stuff, because when every plugin carries tests for their features you end up with bloated JavaScript assets and KB's of code overhead every user has to download on their first request. This results in a decrease of user experience because of longer load times and some tests are really heavy and they are executed for every plugin again and again.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/13/comments,https://github.com/rendro/easy-pie-chart/issues/13#issuecomment-13560418,https://api.github.com/repos/rendro/easy-pie-chart/issues/13
rendro,easy-pie-chart,10865112,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/13391338,13391338,MDEyOklzc3VlQ29tbWVudDEzMzkxMzM4,422168,2013-02-11T17:24:45Z,2013-02-11T17:24:45Z,OWNER,"No. The canvas element needs its size to be set in px. You can scale the image with css or redraw the graph on resize (but the performance will be very bad). Maybe you should check out SVGs for scalable graphics.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/12/comments,https://github.com/rendro/easy-pie-chart/issues/12#issuecomment-13391338,https://api.github.com/repos/rendro/easy-pie-chart/issues/12
rendro,easy-pie-chart,10773106,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/26119170,26119170,MDEyOklzc3VlQ29tbWVudDI2MTE5MTcw,2685357,2013-10-11T07:29:23Z,2013-10-11T07:29:34Z,NONE,"I was wondering the same, and would appreciate an answer.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/11/comments,https://github.com/rendro/easy-pie-chart/issues/11#issuecomment-26119170,https://api.github.com/repos/rendro/easy-pie-chart/issues/11
rendro,easy-pie-chart,10773106,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/26134184,26134184,MDEyOklzc3VlQ29tbWVudDI2MTM0MTg0,422168,2013-10-11T12:45:58Z,2013-10-11T12:45:58Z,OWNER,"The canvas itself is behaves an image element in HTML. Thus, if you want it the canvas to behave ""responsive"" the plugin would have to fetch the size of its parent element and redraw the canvas on every resize event which would result in a really bad performance.

As a solution I'd rather implement a method to update the options of the plugin (which includes the size). So everyone can implement the resize behavior on their behalf. Currently this is not that easy because once the canvas renderer is initialized, sizes can't be changed.

In the future, when the svg renderer is implemented and tested, you might use it instead of the canvas renderer because an SVG is easier to scale.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/11/comments,https://github.com/rendro/easy-pie-chart/issues/11#issuecomment-26134184,https://api.github.com/repos/rendro/easy-pie-chart/issues/11
rendro,easy-pie-chart,10542764,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/12999718,12999718,MDEyOklzc3VlQ29tbWVudDEyOTk5NzE4,422168,2013-02-01T15:50:01Z,2013-02-01T15:50:01Z,OWNER,"Do you want to display the ""test"" text right below the 55% like you see in the image?
![shot](https://f.cloud.github.com/assets/422168/118744/c8794a9c-6c86-11e2-9b5e-8e45570f58a7.png)

Then you should change the line height and add a padding to the top to center your text again. The plugin does not provide that, that's why it is called **easy** and not **complex** pie chart.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/10/comments,https://github.com/rendro/easy-pie-chart/issues/10#issuecomment-12999718,https://api.github.com/repos/rendro/easy-pie-chart/issues/10
rendro,easy-pie-chart,10542764,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/13005649,13005649,MDEyOklzc3VlQ29tbWVudDEzMDA1NjQ5,542854,2013-02-01T17:56:28Z,2013-02-01T17:56:28Z,NONE,"Hi Robert,

Thank you, that's exactly what I was looking for.  Only I can't get it to work by changing the line height, perhaps you could add a comment here with the code you used to create the image you showed?

Thanks again,
- Adam
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/10/comments,https://github.com/rendro/easy-pie-chart/issues/10#issuecomment-13005649,https://api.github.com/repos/rendro/easy-pie-chart/issues/10
rendro,easy-pie-chart,10542764,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/13066987,13066987,MDEyOklzc3VlQ29tbWVudDEzMDY2OTg3,422168,2013-02-04T08:50:16Z,2013-02-04T08:50:16Z,OWNER,"https://github.com/rendro/easy-pie-chart/blob/master/jquery.easy-pie-chart.coffee#L39-43

Change it to:

```
@$el.css {
    width: @options.size
    height: @options.size
    lineHeight: ""#{@options.size/2}px""
}
```

Or play around with the lineHeight parameter. It's basic css written in JS so you can edit everything in the developer console in Chrome or Firebug or Whatever browser you like
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/10/comments,https://github.com/rendro/easy-pie-chart/issues/10#issuecomment-13066987,https://api.github.com/repos/rendro/easy-pie-chart/issues/10
rendro,easy-pie-chart,10542764,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/13097702,13097702,MDEyOklzc3VlQ29tbWVudDEzMDk3NzAy,542854,2013-02-04T20:33:16Z,2013-02-04T20:33:16Z,NONE,"Ah!   Got it.  Thanks very much for your help!
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/10/comments,https://github.com/rendro/easy-pie-chart/issues/10#issuecomment-13097702,https://api.github.com/repos/rendro/easy-pie-chart/issues/10
rendro,easy-pie-chart,10498595,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/13227116,13227116,MDEyOklzc3VlQ29tbWVudDEzMjI3MTE2,422168,2013-02-07T09:01:13Z,2013-02-07T09:01:13Z,OWNER,"Thanks!
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/9/comments,https://github.com/rendro/easy-pie-chart/pull/9#issuecomment-13227116,https://api.github.com/repos/rendro/easy-pie-chart/issues/9
rendro,easy-pie-chart,10379310,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/12796285,12796285,MDEyOklzc3VlQ29tbWVudDEyNzk2Mjg1,1576709,2013-01-28T18:31:09Z,2013-01-28T18:31:09Z,NONE,"I think it might actually be an issue with the polyfill/shim thing excanvas. I've tried a couple of others but this one seems to be the best and works well except for this one problem.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/8/comments,https://github.com/rendro/easy-pie-chart/issues/8#issuecomment-12796285,https://api.github.com/repos/rendro/easy-pie-chart/issues/8
rendro,easy-pie-chart,10379310,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/12811203,12811203,MDEyOklzc3VlQ29tbWVudDEyODExMjAz,422168,2013-01-28T23:15:04Z,2013-01-28T23:15:04Z,OWNER,"Maybe you can try to set it to 0.00001 instead of 0 ;)
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/8/comments,https://github.com/rendro/easy-pie-chart/issues/8#issuecomment-12811203,https://api.github.com/repos/rendro/easy-pie-chart/issues/8
rendro,easy-pie-chart,10379310,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/12827973,12827973,MDEyOklzc3VlQ29tbWVudDEyODI3OTcz,1576709,2013-01-29T10:08:47Z,2013-01-29T10:08:47Z,NONE,"That doesn't seem to work. It still gives the same bug as before. I also tried 0.1% as well with no luck.
I'm cheated for now by placing an image over the top just for IE8 and below that looks just like the empty pie chart ;)

Any ideas what's causing this, or if there's another way around it?
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/8/comments,https://github.com/rendro/easy-pie-chart/issues/8#issuecomment-12827973,https://api.github.com/repos/rendro/easy-pie-chart/issues/8
rendro,easy-pie-chart,10379310,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/21792491,21792491,MDEyOklzc3VlQ29tbWVudDIxNzkyNDkx,2910831,2013-07-30T13:59:03Z,2013-07-30T13:59:03Z,NONE,"Hi there,

Did this bug ever get fixed?
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/8/comments,https://github.com/rendro/easy-pie-chart/issues/8#issuecomment-21792491,https://api.github.com/repos/rendro/easy-pie-chart/issues/8
rendro,easy-pie-chart,10379310,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/21792570,21792570,MDEyOklzc3VlQ29tbWVudDIxNzkyNTcw,1576709,2013-07-30T14:00:01Z,2013-07-30T14:00:01Z,NONE,"I didn't get time on the project to look deeper into it, and ended up just forcing a partial score by default.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/8/comments,https://github.com/rendro/easy-pie-chart/issues/8#issuecomment-21792570,https://api.github.com/repos/rendro/easy-pie-chart/issues/8
rendro,easy-pie-chart,10379310,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/21799120,21799120,MDEyOklzc3VlQ29tbWVudDIxNzk5MTIw,422168,2013-07-30T15:28:45Z,2013-07-30T15:28:45Z,OWNER,"Well, I think its not the highest priority to fix a bug that occurs using a polyfill library. IE8 is not supported by the plugin since the canvas element itself is not supported. So maybe it's a bug that belongs to excanvas and not to the easy pie chart. If someone has a solution to this that does not need more than ~5 lines of code, open a pull request.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/8/comments,https://github.com/rendro/easy-pie-chart/issues/8#issuecomment-21799120,https://api.github.com/repos/rendro/easy-pie-chart/issues/8
rendro,easy-pie-chart,10379310,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/24908389,24908389,MDEyOklzc3VlQ29tbWVudDI0OTA4Mzg5,422168,2013-09-23T09:40:19Z,2013-09-23T09:40:19Z,OWNER,"Fixed in version 2.0.0
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/8/comments,https://github.com/rendro/easy-pie-chart/issues/8#issuecomment-24908389,https://api.github.com/repos/rendro/easy-pie-chart/issues/8
rendro,easy-pie-chart,10379310,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/67673350,67673350,MDEyOklzc3VlQ29tbWVudDY3NjczMzUw,8260065,2014-12-19T18:01:14Z,2014-12-19T18:01:14Z,NONE,"I had the same problem as his friend.
What I did was do the following:

// Draw bar
drawCircle (color, options.lineWidth, percent / 100);
if (percent == 0) {
drawCircle ('# CAE3FF' options.lineWidth, percent / 100);}
else {
drawCircle (color, options.lineWidth, percent / 100);
}
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/8/comments,https://github.com/rendro/easy-pie-chart/issues/8#issuecomment-67673350,https://api.github.com/repos/rendro/easy-pie-chart/issues/8
rendro,easy-pie-chart,9887926,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/12229739,12229739,MDEyOklzc3VlQ29tbWVudDEyMjI5NzM5,422168,2013-01-14T17:37:07Z,2013-01-14T17:37:07Z,OWNER,"You can configure the chart with a callback function as barColor option to handle the different colors:

```
$('.chart').easyPieChart({
    barColor: function(percent) {
        return (percent > 67) ? 'green' : 'red';
    },
    animate: 1000
});
```

The line you drew it is not implemented and I won't add it to the repository because this is a very special case and I want the plugin to be as universal as possible. But feel free to add it by your self, this is one of the advantages of open source code ;)
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/7/comments,https://github.com/rendro/easy-pie-chart/issues/7#issuecomment-12229739,https://api.github.com/repos/rendro/easy-pie-chart/issues/7
rendro,easy-pie-chart,9700052,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/11917117,11917117,MDEyOklzc3VlQ29tbWVudDExOTE3MTE3,467511,2013-01-05T17:55:32Z,2013-01-05T17:55:32Z,NONE,"I've actually adjusted for @2x (or any other density) by setting size to double the physical size needed and then overwriting css to same physical dimensions.

Example:

//js
size: '210'

/_css_/
.easyPieChart {width:105px !important; height:105px !important; line-height:105px !important;
     canvas {width:105px; height:105px;}
}
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/6/comments,https://github.com/rendro/easy-pie-chart/issues/6#issuecomment-11917117,https://api.github.com/repos/rendro/easy-pie-chart/issues/6
rendro,easy-pie-chart,9700052,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/12900466,12900466,MDEyOklzc3VlQ29tbWVudDEyOTAwNDY2,58522,2013-01-30T17:10:19Z,2013-01-30T17:10:19Z,CONTRIBUTOR,"I've added the following lines at line 42 to add retina support:

if(window.devicePixelRatio == 2) {
          _this.canvas.setAttribute('style','width:'+_this.options.size+'px;height:'+_this.options.size+'px;');
          _this.canvas.setAttribute('width',_this.options.size \* 2);
          _this.canvas.setAttribute('height',_this.options.size \* 2);
         _this.ctx.scale(2, 2);
        }
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/6/comments,https://github.com/rendro/easy-pie-chart/issues/6#issuecomment-12900466,https://api.github.com/repos/rendro/easy-pie-chart/issues/6
rendro,easy-pie-chart,9700052,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/13231176,13231176,MDEyOklzc3VlQ29tbWVudDEzMjMxMTc2,422168,2013-02-07T11:06:54Z,2013-02-07T11:06:54Z,OWNER,"I added support for high resolution displays in this commit: https://github.com/rendro/easy-pie-chart/commit/b6c418adbe7308e44147801a3c026ad42ad36c98

The graph scales to fit the screen perfectly since some android devices have a devicePixelRatio which is not exactly 2 but 1.5
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/6/comments,https://github.com/rendro/easy-pie-chart/issues/6#issuecomment-13231176,https://api.github.com/repos/rendro/easy-pie-chart/issues/6
rendro,easy-pie-chart,9190085,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/13200241,13200241,MDEyOklzc3VlQ29tbWVudDEzMjAwMjQx,2790594,2013-02-06T19:28:45Z,2013-02-06T19:29:43Z,NONE,"To fix it insert the following line in the JS File in the drawLine function.

_this.ctx.lineWidth = _this.options.lineWidth;
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/5/comments,https://github.com/rendro/easy-pie-chart/issues/5#issuecomment-13200241,https://api.github.com/repos/rendro/easy-pie-chart/issues/5
rendro,easy-pie-chart,9190085,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/13226960,13226960,MDEyOklzc3VlQ29tbWVudDEzMjI2OTYw,422168,2013-02-07T08:55:31Z,2013-02-07T08:55:31Z,OWNER,"Thanks @coachfedinho. I included this in the following commit: https://github.com/rendro/easy-pie-chart/commit/7884c3c174a40be495e0112eb45c00a91205e290
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/5/comments,https://github.com/rendro/easy-pie-chart/issues/5#issuecomment-13226960,https://api.github.com/repos/rendro/easy-pie-chart/issues/5
rendro,easy-pie-chart,7377885,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/9197124,9197124,MDEyOklzc3VlQ29tbWVudDkxOTcxMjQ=,422168,2012-10-06T10:40:42Z,2012-10-06T10:40:42Z,OWNER,"Try using the method `html` of jQuery instead of `text`to parse it as HTML:

```
$('.chart span').html('<img src=""images/ajax-loader.gif"">');
```

Not quite an pie chart issue. [Look here for more information](http://api.jquery.com/category/Manipulation/).
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/4/comments,https://github.com/rendro/easy-pie-chart/issues/4#issuecomment-9197124,https://api.github.com/repos/rendro/easy-pie-chart/issues/4
rendro,easy-pie-chart,5649805,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/7050665,7050665,MDEyOklzc3VlQ29tbWVudDcwNTA2NjU=,422168,2012-07-17T21:48:11Z,2012-07-17T21:48:11Z,OWNER,"Hi,

I will look into it, and I will fix the bug with the infinite loop. But I'm not sure about the best behavior. Should I just prevent an update while any animation, or trigger the new one. If I trigger the new one, the bar jumps to the end value of the first animation and then starts to animate to the updated value. Chaining would be another optionâ€¦ What do you think is the best option here @Brian0720 ?
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/3/comments,https://github.com/rendro/easy-pie-chart/issues/3#issuecomment-7050665,https://api.github.com/repos/rendro/easy-pie-chart/issues/3
rendro,easy-pie-chart,5649805,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/7057138,7057138,MDEyOklzc3VlQ29tbWVudDcwNTcxMzg=,1959412,2012-07-18T05:28:22Z,2012-07-18T05:28:22Z,NONE,"Hi,
I am using your work for my project here and for that I greatly appreciate your work. Two thumbs up for ya!! In my project, there might be an update taking place any minute, so I think it will greatly help me with my project if whenever I need to update the bar, it updates it to the right spot from where it left off. So if it needs to jumps to the end value of the first animation and starts to animate to the updated value, that's totally fine. 
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/3/comments,https://github.com/rendro/easy-pie-chart/issues/3#issuecomment-7057138,https://api.github.com/repos/rendro/easy-pie-chart/issues/3
rendro,easy-pie-chart,5649805,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/7314860,7314860,MDEyOklzc3VlQ29tbWVudDczMTQ4NjA=,1959412,2012-07-27T20:05:08Z,2012-07-27T20:05:08Z,NONE,"Hows it coming along? 
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/3/comments,https://github.com/rendro/easy-pie-chart/issues/3#issuecomment-7314860,https://api.github.com/repos/rendro/easy-pie-chart/issues/3
rendro,easy-pie-chart,5649805,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/7336822,7336822,MDEyOklzc3VlQ29tbWVudDczMzY4MjI=,422168,2012-07-28T19:53:14Z,2012-07-28T19:53:14Z,OWNER,"I just pushed to the master branch with the fix to avoid multiple animations. The current one is stopped and the new one starts if you call the update method during an animation. The new animations starts from the end point of the previous animation even if this endpoint is not reached yet.
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/3/comments,https://github.com/rendro/easy-pie-chart/issues/3#issuecomment-7336822,https://api.github.com/repos/rendro/easy-pie-chart/issues/3
rendro,easy-pie-chart,5582979,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/6964481,6964481,MDEyOklzc3VlQ29tbWVudDY5NjQ0ODE=,422168,2012-07-13T14:26:47Z,2012-07-13T14:26:47Z,OWNER,"I thought about this behavior while developing the plugin and I was not sure how to implement it. But I changed the code with [this commit](https://github.com/rendro/easy-pie-chart/commit/7f1b823e23a590775f5b637e3529766ffd9fc7fa).

If anyone wants the old behavior he has so set the scaleColor to transparent instead of false like ""rgba(0,0,0,0)"".
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/2/comments,https://github.com/rendro/easy-pie-chart/issues/2#issuecomment-6964481,https://api.github.com/repos/rendro/easy-pie-chart/issues/2
rendro,easy-pie-chart,5560433,https://api.github.com/repos/rendro/easy-pie-chart/issues/comments/6933802,6933802,MDEyOklzc3VlQ29tbWVudDY5MzM4MDI=,422168,2012-07-12T12:34:41Z,2012-07-12T12:34:41Z,OWNER,"I fixed it, there was a unnecessary line of code that caused the error. I removed it in [this commit](https://github.com/rendro/easy-pie-chart/commit/06ffa20c7f3d953a8babd5e8f22aee0ea6bccc49)

I tested it on FF 13.0.1 on Mac and it worked.

Tkanks for reporting this bug!
",NA,https://api.github.com/repos/rendro/easy-pie-chart/issues/1/comments,https://github.com/rendro/easy-pie-chart/issues/1#issuecomment-6933802,https://api.github.com/repos/rendro/easy-pie-chart/issues/1
rendro,vintageJS,704350347,https://api.github.com/repos/rendro/vintageJS/issues/comments/700764275,700764275,MDEyOklzc3VlQ29tbWVudDcwMDc2NDI3NQ==,4609507,2020-09-29T14:58:51Z,2020-09-29T14:58:51Z,NONE,"Closing this issue, there is source code in assets. Not sure about permission to use it, though.",NA,https://api.github.com/repos/rendro/vintageJS/issues/40/comments,https://github.com/rendro/vintageJS/issues/40#issuecomment-700764275,https://api.github.com/repos/rendro/vintageJS/issues/40
rendro,vintageJS,420446512,https://api.github.com/repos/rendro/vintageJS/issues/comments/678288312,678288312,MDEyOklzc3VlQ29tbWVudDY3ODI4ODMxMg==,4609507,2020-08-21T13:23:11Z,2020-08-21T13:23:28Z,NONE,"I know it's an old question, but if I'm not mistaken, viewFinder image should be black&white. Darker bits on the viewFinder image will be more visible than lighter ones on the final image.",NA,https://api.github.com/repos/rendro/vintageJS/issues/39/comments,https://github.com/rendro/vintageJS/issues/39#issuecomment-678288312,https://api.github.com/repos/rendro/vintageJS/issues/39
rendro,vintageJS,266529909,https://api.github.com/repos/rendro/vintageJS/issues/comments/338079584,338079584,MDEyOklzc3VlQ29tbWVudDMzODA3OTU4NA==,422168,2017-10-20T01:15:45Z,2017-10-20T01:15:45Z,OWNER,Thanks @bjornbos for putting up the PR and adding the true grayscale filter! Much appreciated!,NA,https://api.github.com/repos/rendro/vintageJS/issues/38/comments,https://github.com/rendro/vintageJS/pull/38#issuecomment-338079584,https://api.github.com/repos/rendro/vintageJS/issues/38
rendro,vintageJS,251175530,https://api.github.com/repos/rendro/vintageJS/issues/comments/323402501,323402501,MDEyOklzc3VlQ29tbWVudDMyMzQwMjUwMQ==,422168,2017-08-18T16:40:24Z,2017-08-18T16:40:24Z,OWNER,Thanks @PlatinumSeed for reporting. I added the 2.0.1 tag to the current head. Bower should pull the new version automatically now. Let me know if the issue remains.,NA,https://api.github.com/repos/rendro/vintageJS/issues/37/comments,https://github.com/rendro/vintageJS/issues/37#issuecomment-323402501,https://api.github.com/repos/rendro/vintageJS/issues/37
rendro,vintageJS,216663953,https://api.github.com/repos/rendro/vintageJS/issues/comments/289030876,289030876,MDEyOklzc3VlQ29tbWVudDI4OTAzMDg3Ng==,422168,2017-03-24T14:03:12Z,2017-03-24T15:46:45Z,OWNER,"If you are using the latest version (2.0.0 or higher) the exported variable should be `vintageJS` not `VintageJS` because it's only a function, not a constructor.

Check out the  [example](https://github.com/rendro/vintageJS/blob/master/examples/example.js#L48-L50) if you're looking for a reference implementation.",NA,https://api.github.com/repos/rendro/vintageJS/issues/36/comments,https://github.com/rendro/vintageJS/issues/36#issuecomment-289030876,https://api.github.com/repos/rendro/vintageJS/issues/36
rendro,vintageJS,216663953,https://api.github.com/repos/rendro/vintageJS/issues/comments/289188366,289188366,MDEyOklzc3VlQ29tbWVudDI4OTE4ODM2Ng==,10137,2017-03-25T04:35:47Z,2017-03-25T04:35:47Z,NONE,"Actually i am using it with ionic (AngularJS 1).
How can i save that image after applying filter ?",NA,https://api.github.com/repos/rendro/vintageJS/issues/36/comments,https://github.com/rendro/vintageJS/issues/36#issuecomment-289188366,https://api.github.com/repos/rendro/vintageJS/issues/36
rendro,vintageJS,216663953,https://api.github.com/repos/rendro/vintageJS/issues/comments/289473475,289473475,MDEyOklzc3VlQ29tbWVudDI4OTQ3MzQ3NQ==,422168,2017-03-27T14:37:48Z,2017-03-27T14:37:48Z,OWNER,Which version of vintageJS are you using?,NA,https://api.github.com/repos/rendro/vintageJS/issues/36/comments,https://github.com/rendro/vintageJS/issues/36#issuecomment-289473475,https://api.github.com/repos/rendro/vintageJS/issues/36
rendro,vintageJS,183731168,https://api.github.com/repos/rendro/vintageJS/issues/comments/284246398,284246398,MDEyOklzc3VlQ29tbWVudDI4NDI0NjM5OA==,422168,2017-03-05T17:45:24Z,2017-03-05T17:45:24Z,OWNER,"Won't merge after update to v2.
I'll put it on the list for the angular directive for version 2",NA,https://api.github.com/repos/rendro/vintageJS/issues/33/comments,https://github.com/rendro/vintageJS/pull/33#issuecomment-284246398,https://api.github.com/repos/rendro/vintageJS/issues/33
rendro,vintageJS,169810597,https://api.github.com/repos/rendro/vintageJS/issues/comments/284186960,284186960,MDEyOklzc3VlQ29tbWVudDI4NDE4Njk2MA==,422168,2017-03-04T21:57:13Z,2017-03-04T21:57:13Z,OWNER,Version v2.0 will support using existing canvases as a source for the effect,NA,https://api.github.com/repos/rendro/vintageJS/issues/32/comments,https://github.com/rendro/vintageJS/issues/32#issuecomment-284186960,https://api.github.com/repos/rendro/vintageJS/issues/32
rendro,vintageJS,169810597,https://api.github.com/repos/rendro/vintageJS/issues/comments/284246202,284246202,MDEyOklzc3VlQ29tbWVudDI4NDI0NjIwMg==,422168,2017-03-05T17:42:20Z,2017-03-05T17:42:20Z,OWNER,"Use version 2.0.0 and call vintagejs with a canvas element.

```javascript
const canvas = document.querySelector('canvas');
const context = canvas.getContext('2d');
vintagejs(canvas, { brightness: -0.13, constrast: 0.15 })
  .then(res => {
    context.drawImage(res.getCanvas(), 0, 0, canvas.width, canvas.height);
  });
```",NA,https://api.github.com/repos/rendro/vintageJS/issues/32/comments,https://github.com/rendro/vintageJS/issues/32#issuecomment-284246202,https://api.github.com/repos/rendro/vintageJS/issues/32
rendro,vintageJS,155086562,https://api.github.com/repos/rendro/vintageJS/issues/comments/284186851,284186851,MDEyOklzc3VlQ29tbWVudDI4NDE4Njg1MQ==,422168,2017-03-04T21:56:44Z,2017-03-04T21:56:44Z,OWNER,I'll add support for base64 urls in v2.1,NA,https://api.github.com/repos/rendro/vintageJS/issues/31/comments,https://github.com/rendro/vintageJS/issues/31#issuecomment-284186851,https://api.github.com/repos/rendro/vintageJS/issues/31
rendro,vintageJS,155086562,https://api.github.com/repos/rendro/vintageJS/issues/comments/284264320,284264320,MDEyOklzc3VlQ29tbWVudDI4NDI2NDMyMA==,422168,2017-03-05T21:29:18Z,2017-03-05T21:29:18Z,OWNER,Just added support for base64 data uri and regular URIs for vintagejs. Next up is creating an angular directive. Probably in new repo. I'll keep you updated,NA,https://api.github.com/repos/rendro/vintageJS/issues/31/comments,https://github.com/rendro/vintageJS/issues/31#issuecomment-284264320,https://api.github.com/repos/rendro/vintageJS/issues/31
rendro,vintageJS,104140996,https://api.github.com/repos/rendro/vintageJS/issues/comments/284186671,284186671,MDEyOklzc3VlQ29tbWVudDI4NDE4NjY3MQ==,422168,2017-03-04T21:55:47Z,2017-03-04T21:55:47Z,OWNER,There is no way to extract this information after the effect was applied. You have to manage this state outside of the plugin.,NA,https://api.github.com/repos/rendro/vintageJS/issues/30/comments,https://github.com/rendro/vintageJS/issues/30#issuecomment-284186671,https://api.github.com/repos/rendro/vintageJS/issues/30
rendro,vintageJS,70409723,https://api.github.com/repos/rendro/vintageJS/issues/comments/95865761,95865761,MDEyOklzc3VlQ29tbWVudDk1ODY1NzYx,422168,2015-04-24T09:15:24Z,2015-04-24T09:16:51Z,OWNER,"Per default vintageJS keeps the original image as a reference, so whenever you call `vintage(effect)` the effect will be calculated based on the original source. If you want to add an effect on top of another effect, you must call `apply()` in between. This will set the current effect as a source for the next effect.

So without calling apply, this is what will happen:

``` javascript
vjsApi.vintage(effectA);
// image will only have effect A
vjsApi.vintage(effectB);
// image will only have effect B
```

If you call apply in between:

``` javascript
vjsApi.vintage(effectA);
// image will only have effect A
vjsApi.apply();
vjsApi.vintage(effectB);
// image will only have effect A + effect B
```

---

I just realized, that `apply` is a really bad name because if clashes with the native `apply` to invoke functions in javascript
",NA,https://api.github.com/repos/rendro/vintageJS/issues/29/comments,https://github.com/rendro/vintageJS/issues/29#issuecomment-95865761,https://api.github.com/repos/rendro/vintageJS/issues/29
rendro,vintageJS,70409723,https://api.github.com/repos/rendro/vintageJS/issues/comments/96452507,96452507,MDEyOklzc3VlQ29tbWVudDk2NDUyNTA3,12084544,2015-04-27T00:42:32Z,2015-04-27T00:42:32Z,NONE,"Thanks
",NA,https://api.github.com/repos/rendro/vintageJS/issues/29/comments,https://github.com/rendro/vintageJS/issues/29#issuecomment-96452507,https://api.github.com/repos/rendro/vintageJS/issues/29
rendro,vintageJS,67942437,https://api.github.com/repos/rendro/vintageJS/issues/comments/92332396,92332396,MDEyOklzc3VlQ29tbWVudDkyMzMyMzk2,7311802,2015-04-13T12:18:18Z,2015-04-13T12:18:18Z,NONE,"any response ?
",NA,https://api.github.com/repos/rendro/vintageJS/issues/28/comments,https://github.com/rendro/vintageJS/issues/28#issuecomment-92332396,https://api.github.com/repos/rendro/vintageJS/issues/28
rendro,vintageJS,67942437,https://api.github.com/repos/rendro/vintageJS/issues/comments/95866022,95866022,MDEyOklzc3VlQ29tbWVudDk1ODY2MDIy,422168,2015-04-24T09:17:24Z,2015-04-24T09:17:24Z,OWNER,"I'm afraid there is no solution other than reducing the size of the image you apply the effect to.
",NA,https://api.github.com/repos/rendro/vintageJS/issues/28/comments,https://github.com/rendro/vintageJS/issues/28#issuecomment-95866022,https://api.github.com/repos/rendro/vintageJS/issues/28
rendro,vintageJS,67940441,https://api.github.com/repos/rendro/vintageJS/issues/comments/95867009,95867009,MDEyOklzc3VlQ29tbWVudDk1ODY3MDA5,422168,2015-04-24T09:22:04Z,2015-04-24T09:22:16Z,OWNER,"Regarding your first question:

You need to call `apply` in between to make the second effect to be applied on top of the first one. You can check vintagejs.com to see that it works to change the applied effect. vintagejs.com uses the latest version of this plugin.

Regarding your second question:
Can you give me some more information on how you try this. If you visit http://rendro.github.io/vintageJS/ you see that it is possible to apply effects to different images. Looks like it is an issue with your implementation rather than the plugin itself.
",NA,https://api.github.com/repos/rendro/vintageJS/issues/27/comments,https://github.com/rendro/vintageJS/issues/27#issuecomment-95867009,https://api.github.com/repos/rendro/vintageJS/issues/27
rendro,vintageJS,67940441,https://api.github.com/repos/rendro/vintageJS/issues/comments/95904766,95904766,MDEyOklzc3VlQ29tbWVudDk1OTA0NzY2,7311802,2015-04-24T11:31:54Z,2015-04-24T11:31:54Z,NONE,"Thanks for your redsponse, if fixed the problem
On Apr 24, 2015 12:22 PM, ""Robert Fleischmann"" notifications@github.com
wrote:

> Regarding your first question:
> 
> You need to call apply in between to make the second effect to be applied
> on top of the first one. You can check vintagejs.com to see that it works
> to change the applied effect. vintagejs.com uses the latest version of
> this plugin.
> 
> Regarding your second question:
> Can you give me some more information on how you try this. If you visit
> http://rendro.github.io/vintageJS/ you see that it is possible to apply
> effects to different images. Looks like it is an issue with your
> implementation rather than the plugin isself.
> 
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/rendro/vintageJS/issues/27#issuecomment-95867009.
",NA,https://api.github.com/repos/rendro/vintageJS/issues/27/comments,https://github.com/rendro/vintageJS/issues/27#issuecomment-95904766,https://api.github.com/repos/rendro/vintageJS/issues/27
rendro,vintageJS,64611375,https://api.github.com/repos/rendro/vintageJS/issues/comments/219613275,219613275,MDEyOklzc3VlQ29tbWVudDIxOTYxMzI3NQ==,422168,2016-05-17T03:55:17Z,2016-05-17T03:55:17Z,OWNER,"Thanks for the PR!
",NA,https://api.github.com/repos/rendro/vintageJS/issues/26/comments,https://github.com/rendro/vintageJS/pull/26#issuecomment-219613275,https://api.github.com/repos/rendro/vintageJS/issues/26
rendro,vintageJS,64600500,https://api.github.com/repos/rendro/vintageJS/issues/comments/284246348,284246348,MDEyOklzc3VlQ29tbWVudDI4NDI0NjM0OA==,422168,2017-03-05T17:44:40Z,2017-03-05T17:44:40Z,OWNER,"Seems to be your $('#infoAlert1').offcanvas('show'); method which does things in an async fashion. the callback is called before the effect is calculated.

A way to resolve this would be to calculate the effect in a webworker. This is currently not supported, but I hope to add an example for web worker support in v2+",NA,https://api.github.com/repos/rendro/vintageJS/issues/25/comments,https://github.com/rendro/vintageJS/issues/25#issuecomment-284246348,https://api.github.com/repos/rendro/vintageJS/issues/25
rendro,vintageJS,62858868,https://api.github.com/repos/rendro/vintageJS/issues/comments/136509580,136509580,MDEyOklzc3VlQ29tbWVudDEzNjUwOTU4MA==,10790175,2015-08-31T21:42:27Z,2015-08-31T21:42:27Z,NONE,"Yes it works.
",NA,https://api.github.com/repos/rendro/vintageJS/issues/24/comments,https://github.com/rendro/vintageJS/issues/24#issuecomment-136509580,https://api.github.com/repos/rendro/vintageJS/issues/24
rendro,vintageJS,58124959,https://api.github.com/repos/rendro/vintageJS/issues/comments/75485868,75485868,MDEyOklzc3VlQ29tbWVudDc1NDg1ODY4,422168,2015-02-23T03:42:41Z,2015-02-23T03:42:41Z,OWNER,"The script shouldn't change values of the alpha channel. Maybe you'll need to change the mime property of the config object to ""image/png"". If not the image will be rendered as a JPEG. This is probably why you get black areas. 

Let me know if that solves the problem. If not I'll have a more detailed look into your issue. 

Thanks!
",NA,https://api.github.com/repos/rendro/vintageJS/issues/23/comments,https://github.com/rendro/vintageJS/issues/23#issuecomment-75485868,https://api.github.com/repos/rendro/vintageJS/issues/23
rendro,vintageJS,48118384,https://api.github.com/repos/rendro/vintageJS/issues/comments/62360547,62360547,MDEyOklzc3VlQ29tbWVudDYyMzYwNTQ3,422168,2014-11-10T09:35:52Z,2014-11-10T09:35:52Z,OWNER,"Hey @X-Ryl669 

Thank you very much for your detailed feedback and the ideas for further improving the performance.

> In other words, profile your code, spot where you spend time and only optimize here. There is no point in gaining a Âµs for a loop test that happens only a few time.

I think if you can save a Âµs in a loop that is called 1 million times you save 1s in the end. That is why I chose to save time wherever possible. Creating a LUT for the current set of options will definitely reduce the the script execution time a lot. I'll update the plugin soon with your feedback in mind!

Cheers,
Robert
",NA,https://api.github.com/repos/rendro/vintageJS/issues/22/comments,https://github.com/rendro/vintageJS/issues/22#issuecomment-62360547,https://api.github.com/repos/rendro/vintageJS/issues/22
rendro,vintageJS,47555729,https://api.github.com/repos/rendro/vintageJS/issues/comments/61484901,61484901,MDEyOklzc3VlQ29tbWVudDYxNDg0OTAx,422168,2014-11-03T14:28:50Z,2014-11-03T14:33:21Z,OWNER,"Hey @tbranyen,
If you could give me owner privs I'll be happy to manage future versions from now on.
Thank you!
",NA,https://api.github.com/repos/rendro/vintageJS/issues/21/comments,https://github.com/rendro/vintageJS/issues/21#issuecomment-61484901,https://api.github.com/repos/rendro/vintageJS/issues/21
rendro,vintageJS,47555729,https://api.github.com/repos/rendro/vintageJS/issues/comments/98009001,98009001,MDEyOklzc3VlQ29tbWVudDk4MDA5MDAx,181635,2015-05-01T00:37:06Z,2015-05-01T00:37:06Z,NONE,"Whoops, rendro... totally miss this message
",NA,https://api.github.com/repos/rendro/vintageJS/issues/21/comments,https://github.com/rendro/vintageJS/issues/21#issuecomment-98009001,https://api.github.com/repos/rendro/vintageJS/issues/21
rendro,vintageJS,47555729,https://api.github.com/repos/rendro/vintageJS/issues/comments/98009039,98009039,MDEyOklzc3VlQ29tbWVudDk4MDA5MDM5,181635,2015-05-01T00:37:27Z,2015-05-01T00:37:27Z,NONE,"Added you to the owners for the package on NPM.
",NA,https://api.github.com/repos/rendro/vintageJS/issues/21/comments,https://github.com/rendro/vintageJS/issues/21#issuecomment-98009039,https://api.github.com/repos/rendro/vintageJS/issues/21
rendro,vintageJS,47555729,https://api.github.com/repos/rendro/vintageJS/issues/comments/98132000,98132000,MDEyOklzc3VlQ29tbWVudDk4MTMyMDAw,422168,2015-05-01T13:19:42Z,2015-05-01T13:19:42Z,OWNER,"Thank you!
",NA,https://api.github.com/repos/rendro/vintageJS/issues/21/comments,https://github.com/rendro/vintageJS/issues/21#issuecomment-98132000,https://api.github.com/repos/rendro/vintageJS/issues/21
rendro,vintageJS,46179425,https://api.github.com/repos/rendro/vintageJS/issues/comments/59627361,59627361,MDEyOklzc3VlQ29tbWVudDU5NjI3MzYx,422168,2014-10-18T19:52:49Z,2014-10-18T19:52:49Z,OWNER,"Try to decrease the image size. What size is the image you are using the effect on? You'll get good results at about 800x800 px max size.
",NA,https://api.github.com/repos/rendro/vintageJS/issues/20/comments,https://github.com/rendro/vintageJS/issues/20#issuecomment-59627361,https://api.github.com/repos/rendro/vintageJS/issues/20
rendro,vintageJS,46179425,https://api.github.com/repos/rendro/vintageJS/issues/comments/59627577,59627577,MDEyOklzc3VlQ29tbWVudDU5NjI3NTc3,5412072,2014-10-18T19:59:36Z,2014-10-18T19:59:36Z,NONE,"hi, thanks for your quick answer :) it uses 600x600 px :(

â€”
Maximilian Czerninwww.czern.in

max@czern.in

On Sat, Oct 18, 2014 at 9:52 PM, Robert Fleischmann
notifications@github.com wrote:

> ## Try to decrease the image size. What size is the image you are using the effect on? You'll get good results at about 800x800 px max size.
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/rendro/vintageJS/issues/20#issuecomment-59627361
",NA,https://api.github.com/repos/rendro/vintageJS/issues/20/comments,https://github.com/rendro/vintageJS/issues/20#issuecomment-59627577,https://api.github.com/repos/rendro/vintageJS/issues/20
rendro,vintageJS,46179425,https://api.github.com/repos/rendro/vintageJS/issues/comments/59645324,59645324,MDEyOklzc3VlQ29tbWVudDU5NjQ1MzI0,422168,2014-10-19T10:15:29Z,2014-10-19T10:15:29Z,OWNER,"Must be an issue with phone gap I guess. I tried the [demo site](http://rendro.github.io/vintageJS/) on an iPhone 5 and it applies the effect in under a second. Have you tried debugging into the code to see what takes up most of the time? 600x600 should run fast on an iPhone 6, but it will take a while on an iPhone 4, though not up to 10 sec. 
",NA,https://api.github.com/repos/rendro/vintageJS/issues/20/comments,https://github.com/rendro/vintageJS/issues/20#issuecomment-59645324,https://api.github.com/repos/rendro/vintageJS/issues/20
rendro,vintageJS,46179425,https://api.github.com/repos/rendro/vintageJS/issues/comments/60479265,60479265,MDEyOklzc3VlQ29tbWVudDYwNDc5MjY1,5412072,2014-10-25T11:10:09Z,2014-10-25T11:10:09Z,NONE,"Yeah.. maybe... on android it works like a charm (even on old devices)
",NA,https://api.github.com/repos/rendro/vintageJS/issues/20/comments,https://github.com/rendro/vintageJS/issues/20#issuecomment-60479265,https://api.github.com/repos/rendro/vintageJS/issues/20
rendro,vintageJS,42166922,https://api.github.com/repos/rendro/vintageJS/issues/comments/95867602,95867602,MDEyOklzc3VlQ29tbWVudDk1ODY3NjAy,422168,2015-04-24T09:24:00Z,2015-04-24T09:24:00Z,OWNER,"Please ask for help on stackoverflow. Only issues regarding the code of this repo should go here. Thanks
",NA,https://api.github.com/repos/rendro/vintageJS/issues/19/comments,https://github.com/rendro/vintageJS/issues/19#issuecomment-95867602,https://api.github.com/repos/rendro/vintageJS/issues/19
rendro,vintageJS,41133940,https://api.github.com/repos/rendro/vintageJS/issues/comments/92333533,92333533,MDEyOklzc3VlQ29tbWVudDkyMzMzNTMz,7311802,2015-04-13T12:25:20Z,2015-04-13T12:25:20Z,NONE,"Hi, 
see example:

 var options = {
        onError: function() {
            alert('ERROR');
        }
    };
    var effect = {
        vignette: 0.6,
        sepia: true
    };
    var vintageImg = $('img#yourImage').vintage(options, effect);

vintageImg.reset();

should works for you :)
",NA,https://api.github.com/repos/rendro/vintageJS/issues/18/comments,https://github.com/rendro/vintageJS/issues/18#issuecomment-92333533,https://api.github.com/repos/rendro/vintageJS/issues/18
rendro,vintageJS,40014003,https://api.github.com/repos/rendro/vintageJS/issues/comments/51876566,51876566,MDEyOklzc3VlQ29tbWVudDUxODc2NTY2,422168,2014-08-12T06:00:55Z,2014-08-12T06:00:55Z,OWNER,"Could you be more specific?
",NA,https://api.github.com/repos/rendro/vintageJS/issues/17/comments,https://github.com/rendro/vintageJS/issues/17#issuecomment-51876566,https://api.github.com/repos/rendro/vintageJS/issues/17
rendro,vintageJS,40014003,https://api.github.com/repos/rendro/vintageJS/issues/comments/51942290,51942290,MDEyOklzc3VlQ29tbWVudDUxOTQyMjkw,775157,2014-08-12T16:50:21Z,2014-08-12T16:50:21Z,NONE,"Hi,
I will try. So here (http://rendro.github.io/vintageJS/) you have explained your plugin perfectly. But when i come to ""Vitage-Api"" in that page itself, i have some methods like ""apply()"" and ""reset()"" .
I searched on the internet for some examples on how to use them, but didn't find a clue. Can you explain those ones by one example ? Thanks.
",NA,https://api.github.com/repos/rendro/vintageJS/issues/17/comments,https://github.com/rendro/vintageJS/issues/17#issuecomment-51942290,https://api.github.com/repos/rendro/vintageJS/issues/17
rendro,vintageJS,26465792,https://api.github.com/repos/rendro/vintageJS/issues/comments/33593313,33593313,MDEyOklzc3VlQ29tbWVudDMzNTkzMzEz,422168,2014-01-29T15:18:29Z,2014-01-29T15:18:29Z,OWNER,"Yes, you are right. I'll remove this soon and replace the path with a comment.
",NA,https://api.github.com/repos/rendro/vintageJS/issues/16/comments,https://github.com/rendro/vintageJS/issues/16#issuecomment-33593313,https://api.github.com/repos/rendro/vintageJS/issues/16
rendro,vintageJS,25888604,https://api.github.com/repos/rendro/vintageJS/issues/comments/33640062,33640062,MDEyOklzc3VlQ29tbWVudDMzNjQwMDYy,422168,2014-01-29T22:40:21Z,2014-01-29T22:40:21Z,OWNER,"This would be a quite interesting project to create vintage.m library, but I consider my skills in objective C rather basic.

But I would like to see a port of vintageJS. Would you be able to bootstrap such a project?
",NA,https://api.github.com/repos/rendro/vintageJS/issues/15/comments,https://github.com/rendro/vintageJS/issues/15#issuecomment-33640062,https://api.github.com/repos/rendro/vintageJS/issues/15
rendro,vintageJS,16029085,https://api.github.com/repos/rendro/vintageJS/issues/comments/20248056,20248056,MDEyOklzc3VlQ29tbWVudDIwMjQ4MDU2,422168,2013-06-30T14:39:58Z,2013-06-30T14:39:58Z,OWNER,"Thank you, this is really great! :+1: 

I'm thinking of putting the angular stuff into an extra branch, because the default way of implementing vintageJS is vanilla or jQuery. But it is nice to support even more libs! Will dig into it in a few days since I'm quite busy these days.
",NA,https://api.github.com/repos/rendro/vintageJS/issues/13/comments,https://github.com/rendro/vintageJS/pull/13#issuecomment-20248056,https://api.github.com/repos/rendro/vintageJS/issues/13
rendro,vintageJS,16029085,https://api.github.com/repos/rendro/vintageJS/issues/comments/20461035,20461035,MDEyOklzc3VlQ29tbWVudDIwNDYxMDM1,383994,2013-07-04T06:29:45Z,2013-07-04T06:29:45Z,NONE,"This would be great! We could use this for sure
",NA,https://api.github.com/repos/rendro/vintageJS/issues/13/comments,https://github.com/rendro/vintageJS/pull/13#issuecomment-20461035,https://api.github.com/repos/rendro/vintageJS/issues/13
rendro,vintageJS,16029085,https://api.github.com/repos/rendro/vintageJS/issues/comments/20461126,20461126,MDEyOklzc3VlQ29tbWVudDIwNDYxMTI2,383994,2013-07-04T06:33:36Z,2013-07-04T06:33:36Z,NONE,"Btw, because of the small size of implementing this, I suggest that instead of putting it on another branch you simply make it a separate build/file, or an extension to the core lib. but clearly that's your call
",NA,https://api.github.com/repos/rendro/vintageJS/issues/13/comments,https://github.com/rendro/vintageJS/pull/13#issuecomment-20461126,https://api.github.com/repos/rendro/vintageJS/issues/13
rendro,vintageJS,16029085,https://api.github.com/repos/rendro/vintageJS/issues/comments/20463683,20463683,MDEyOklzc3VlQ29tbWVudDIwNDYzNjgz,1982461,2013-07-04T07:53:29Z,2013-07-04T07:53:55Z,CONTRIBUTOR,"Since there is no change to the vanilla core, it could be built easily with a gruntfile combining the directive and the core so all developement made on the core is reflected
",NA,https://api.github.com/repos/rendro/vintageJS/issues/13/comments,https://github.com/rendro/vintageJS/pull/13#issuecomment-20463683,https://api.github.com/repos/rendro/vintageJS/issues/13
rendro,vintageJS,16029085,https://api.github.com/repos/rendro/vintageJS/issues/comments/20823548,20823548,MDEyOklzc3VlQ29tbWVudDIwODIzNTQ4,422168,2013-07-11T16:16:58Z,2013-07-11T16:16:58Z,OWNER,"@jonschlinkert and @dpiccone I thin a buildscript is a great idea to keep the core clean while providing a good support for angular out of the box as well. I'll look into this as soon as possible.
",NA,https://api.github.com/repos/rendro/vintageJS/issues/13/comments,https://github.com/rendro/vintageJS/pull/13#issuecomment-20823548,https://api.github.com/repos/rendro/vintageJS/issues/13
rendro,vintageJS,16029085,https://api.github.com/repos/rendro/vintageJS/issues/comments/21048504,21048504,MDEyOklzc3VlQ29tbWVudDIxMDQ4NTA0,1982461,2013-07-16T15:09:05Z,2013-07-16T15:09:05Z,CONTRIBUTOR,"I added a (very basic) grunt task to build the angular version in the /dist folder, the same structure can be used to keep the core clean and build the jQuery version too
",NA,https://api.github.com/repos/rendro/vintageJS/issues/13/comments,https://github.com/rendro/vintageJS/pull/13#issuecomment-21048504,https://api.github.com/repos/rendro/vintageJS/issues/13
rendro,vintageJS,16029085,https://api.github.com/repos/rendro/vintageJS/issues/comments/21514682,21514682,MDEyOklzc3VlQ29tbWVudDIxNTE0Njgy,422168,2013-07-24T20:43:54Z,2013-07-24T20:43:54Z,OWNER,"Finally found the time to implement the build script and the beautiful angular support. Thanks again man! This is really great!
",NA,https://api.github.com/repos/rendro/vintageJS/issues/13/comments,https://github.com/rendro/vintageJS/pull/13#issuecomment-21514682,https://api.github.com/repos/rendro/vintageJS/issues/13
rendro,vintageJS,15339246,https://api.github.com/repos/rendro/vintageJS/issues/comments/19901951,19901951,MDEyOklzc3VlQ29tbWVudDE5OTAxOTUx,422168,2013-06-24T11:41:39Z,2013-06-24T11:41:39Z,OWNER,"It is possible if you change [these lines](https://github.com/rendro/vintageJS/blob/master/vintage.js#L233-235) to add the image data encoded in base64 as a css background data-url attribute.

In addition to that you'd need to extract the background image before applying the effect as the plugin expects an image element.

So in general it is possible, but it requires some tweaks.
",NA,https://api.github.com/repos/rendro/vintageJS/issues/12/comments,https://github.com/rendro/vintageJS/issues/12#issuecomment-19901951,https://api.github.com/repos/rendro/vintageJS/issues/12
rendro,vintageJS,14842184,https://api.github.com/repos/rendro/vintageJS/issues/comments/18560423,18560423,MDEyOklzc3VlQ29tbWVudDE4NTYwNDIz,422168,2013-05-28T15:59:35Z,2013-05-28T15:59:35Z,OWNER,"vintageJS is not designed to work on HiRes images. The algorithm runs on your JavaScript Engine and and has to manipulate/recalculate (imageHeight_imageWidth_3(rgb)) values. If you do the math you see that it gets messy for big images.

If you have an idea to implement high performance calculation for big images that'd be great.
",NA,https://api.github.com/repos/rendro/vintageJS/issues/11/comments,https://github.com/rendro/vintageJS/issues/11#issuecomment-18560423,https://api.github.com/repos/rendro/vintageJS/issues/11
rendro,vintageJS,14842184,https://api.github.com/repos/rendro/vintageJS/issues/comments/20823399,20823399,MDEyOklzc3VlQ29tbWVudDIwODIzMzk5,422168,2013-07-11T16:14:48Z,2013-07-11T16:14:48Z,OWNER,"One way would be to outsource the calculation of the effect into a webworker ([support seems to be okay](http://caniuse.com/#feat=webworkers)). That does not speed up anything but the UI is not blocked during the calculation of the effect.

**Downside:**
- New js-file -> new http-reuqest
- Webworker has no window or document objects which means it only suites for doing the pure math for any imageData Object
",NA,https://api.github.com/repos/rendro/vintageJS/issues/11/comments,https://github.com/rendro/vintageJS/issues/11#issuecomment-20823399,https://api.github.com/repos/rendro/vintageJS/issues/11
rendro,vintageJS,12431612,https://api.github.com/repos/rendro/vintageJS/issues/comments/15446942,15446942,MDEyOklzc3VlQ29tbWVudDE1NDQ2OTQy,422168,2013-03-26T08:45:41Z,2013-03-26T08:45:41Z,OWNER,"I already thought about removing the jQuery dependency because it is only used for extending the options-objects and basic but I had no time to refractor the code. I think providing two versions would be nice, a jQuery dependent version as a plugin and a vanilla version if one wants to use it the good old way.
",NA,https://api.github.com/repos/rendro/vintageJS/issues/10/comments,https://github.com/rendro/vintageJS/issues/10#issuecomment-15446942,https://api.github.com/repos/rendro/vintageJS/issues/10
rendro,vintageJS,12431612,https://api.github.com/repos/rendro/vintageJS/issues/comments/15465282,15465282,MDEyOklzc3VlQ29tbWVudDE1NDY1Mjgy,1041792,2013-03-26T15:30:09Z,2013-03-26T15:30:09Z,NONE,"Well if a vanilla version was made, minimal code would be needed to make it jQyery compatible. If I have time later today, I'll have a go at it.
",NA,https://api.github.com/repos/rendro/vintageJS/issues/10/comments,https://github.com/rendro/vintageJS/issues/10#issuecomment-15465282,https://api.github.com/repos/rendro/vintageJS/issues/10
rendro,vintageJS,12431612,https://api.github.com/repos/rendro/vintageJS/issues/comments/18120009,18120009,MDEyOklzc3VlQ29tbWVudDE4MTIwMDA5,422168,2013-05-19T16:11:10Z,2013-05-19T16:11:10Z,OWNER,"See v1.1.0
",NA,https://api.github.com/repos/rendro/vintageJS/issues/10/comments,https://github.com/rendro/vintageJS/issues/10#issuecomment-18120009,https://api.github.com/repos/rendro/vintageJS/issues/10
rendro,vintageJS,12431612,https://api.github.com/repos/rendro/vintageJS/issues/comments/18120102,18120102,MDEyOklzc3VlQ29tbWVudDE4MTIwMTAy,1041792,2013-05-19T16:18:02Z,2013-05-19T16:18:02Z,NONE,"Badass, dude. Awesome work. What are your thoughts about creating presets that can be called via strings? Like recreating Instagram filters, I.e.

``` javascript
new Vintage(this, 'amaro');
```
",NA,https://api.github.com/repos/rendro/vintageJS/issues/10/comments,https://github.com/rendro/vintageJS/issues/10#issuecomment-18120102,https://api.github.com/repos/rendro/vintageJS/issues/10
rendro,vintageJS,12431612,https://api.github.com/repos/rendro/vintageJS/issues/comments/18120111,18120111,MDEyOklzc3VlQ29tbWVudDE4MTIwMTEx,1041792,2013-05-19T16:18:54Z,2013-05-19T16:18:54Z,NONE,"My only reasoning for something like that is this is a dope ass repo that deserves more attention than it has received, and I feel something like this could help. Great work, again!
",NA,https://api.github.com/repos/rendro/vintageJS/issues/10/comments,https://github.com/rendro/vintageJS/issues/10#issuecomment-18120111,https://api.github.com/repos/rendro/vintageJS/issues/10
rendro,vintageJS,12431612,https://api.github.com/repos/rendro/vintageJS/issues/comments/18120319,18120319,MDEyOklzc3VlQ29tbWVudDE4MTIwMzE5,422168,2013-05-19T16:30:27Z,2013-05-19T16:30:27Z,OWNER,"Thanks.

Adding presets that can be called via strings is a nifty idea for the jquery version of the script. I'd rather prefer the vanilla version to stay as clean as possible.

But I would need help creating fancy filters :)
",NA,https://api.github.com/repos/rendro/vintageJS/issues/10/comments,https://github.com/rendro/vintageJS/issues/10#issuecomment-18120319,https://api.github.com/repos/rendro/vintageJS/issues/10
rendro,vintageJS,12431612,https://api.github.com/repos/rendro/vintageJS/issues/comments/18123445,18123445,MDEyOklzc3VlQ29tbWVudDE4MTIzNDQ1,1041792,2013-05-19T19:39:28Z,2013-05-19T19:39:28Z,NONE,"Think about it though. Cast away jQuery completely, that's a random luxury.
The code that I write aims to be as simple as possible, with or without
jQuery. I see no point in ever using jQuery unless it is truly needed, and
with that feature request in mind, I don't think it'd be needed at all.

A solution would be to add something like vintage.presets.js to your page
BEFORE adding vintage.js so that way it becomes this extensible (but
completely optional/separate) function of this rad code you've got!
Thoughts?

On Sunday, May 19, 2013, Robert Fleischmann wrote:

> Thanks.
> 
> Adding presets that can be called via strings is a nifty idea for the
> jquery version of the script. I'd rather prefer the vanilla version to stay
> as clean as possible.
> 
> But I would need help creating fancy filters :)
> 
> â€”
> Reply to this email directly or view it on GitHubhttps://github.com/rendro/vintageJS/issues/10#issuecomment-18120319
> .
",NA,https://api.github.com/repos/rendro/vintageJS/issues/10/comments,https://github.com/rendro/vintageJS/issues/10#issuecomment-18123445,https://api.github.com/repos/rendro/vintageJS/issues/10
rendro,vintageJS,12431612,https://api.github.com/repos/rendro/vintageJS/issues/comments/18145753,18145753,MDEyOklzc3VlQ29tbWVudDE4MTQ1NzUz,422168,2013-05-20T12:55:20Z,2013-05-20T12:55:20Z,OWNER,"I really like the idea of adding the presets before including the vintage.js script and create a small modular and extensible solution.

---

But I'd like to provide a jQuery version because it's included in many projects and the plugin overhead is quite small compared to the increased user base. There are still a lot of people out there that try to play around with javascript and start using jQuery. For every experienced developer the vanilla version will always be preferred (at least I hope so).
",NA,https://api.github.com/repos/rendro/vintageJS/issues/10/comments,https://github.com/rendro/vintageJS/issues/10#issuecomment-18145753,https://api.github.com/repos/rendro/vintageJS/issues/10
rendro,vintageJS,11435830,https://api.github.com/repos/rendro/vintageJS/issues/comments/14149132,14149132,MDEyOklzc3VlQ29tbWVudDE0MTQ5MTMy,241048,2013-02-27T00:31:54Z,2013-02-27T00:31:54Z,NONE,"This wouldn't be possible because of browser security restrictions. Once the user provides an image url, you could fetch the image to your server and then serve it from there.
",NA,https://api.github.com/repos/rendro/vintageJS/issues/9/comments,https://github.com/rendro/vintageJS/issues/9#issuecomment-14149132,https://api.github.com/repos/rendro/vintageJS/issues/9
rendro,vintageJS,11435830,https://api.github.com/repos/rendro/vintageJS/issues/comments/14149354,14149354,MDEyOklzc3VlQ29tbWVudDE0MTQ5MzU0,645900,2013-02-27T00:38:08Z,2013-02-27T00:38:08Z,NONE,"you say to download to the server then load the image from the server then apply filter? sounds interesting thanks a lot
",NA,https://api.github.com/repos/rendro/vintageJS/issues/9/comments,https://github.com/rendro/vintageJS/issues/9#issuecomment-14149354,https://api.github.com/repos/rendro/vintageJS/issues/9
rendro,vintageJS,11435830,https://api.github.com/repos/rendro/vintageJS/issues/comments/14184256,14184256,MDEyOklzc3VlQ29tbWVudDE0MTg0MjU2,422168,2013-02-27T16:44:22Z,2013-02-27T16:44:22Z,OWNER,"Or you write a proxy that serves the image. So you don't have to download it to your server but you solve the same-origin issue.
",NA,https://api.github.com/repos/rendro/vintageJS/issues/9/comments,https://github.com/rendro/vintageJS/issues/9#issuecomment-14184256,https://api.github.com/repos/rendro/vintageJS/issues/9
rendro,vintageJS,9388510,https://api.github.com/repos/rendro/vintageJS/issues/comments/11522892,11522892,MDEyOklzc3VlQ29tbWVudDExNTIyODky,422168,2012-12-19T09:15:49Z,2012-12-19T09:15:49Z,OWNER,"You can prevent multiple effects by setting `allowMultiEffect` to false in your options.

To reset the image, just set the src attribute to the original image path again.

Cheers!
",NA,https://api.github.com/repos/rendro/vintageJS/issues/8/comments,https://github.com/rendro/vintageJS/issues/8#issuecomment-11522892,https://api.github.com/repos/rendro/vintageJS/issues/8
rendro,vintageJS,9388510,https://api.github.com/repos/rendro/vintageJS/issues/comments/11547578,11547578,MDEyOklzc3VlQ29tbWVudDExNTQ3NTc4,3076839,2012-12-19T20:45:00Z,2012-12-19T20:45:00Z,NONE,"For me, allowMultiEffect disables adding any new effects if an effect is already applied. But what I was looking for was, it to remove the applied effect and apply the new effect.
I had achieved this by changing the src to the original src and then applying the effect. I was wondering if there was a better way.
Thanks!
",NA,https://api.github.com/repos/rendro/vintageJS/issues/8/comments,https://github.com/rendro/vintageJS/issues/8#issuecomment-11547578,https://api.github.com/repos/rendro/vintageJS/issues/8
rendro,vintageJS,9388510,https://api.github.com/repos/rendro/vintageJS/issues/comments/11565710,11565710,MDEyOklzc3VlQ29tbWVudDExNTY1NzEw,422168,2012-12-20T09:31:42Z,2012-12-20T09:31:42Z,OWNER,"In this version of vintagejs there is no better solution. Why don't you just send me a pull request with your solution :)
",NA,https://api.github.com/repos/rendro/vintageJS/issues/8/comments,https://github.com/rendro/vintageJS/issues/8#issuecomment-11565710,https://api.github.com/repos/rendro/vintageJS/issues/8
rendro,vintageJS,8789983,https://api.github.com/repos/rendro/vintageJS/issues/comments/10828625,10828625,MDEyOklzc3VlQ29tbWVudDEwODI4NjI1,27511,2012-11-28T23:44:53Z,2012-11-28T23:44:53Z,NONE,"Hmm  is it because I am also using amazon s3?
",NA,https://api.github.com/repos/rendro/vintageJS/issues/7/comments,https://github.com/rendro/vintageJS/issues/7#issuecomment-10828625,https://api.github.com/repos/rendro/vintageJS/issues/7
rendro,vintageJS,8789983,https://api.github.com/repos/rendro/vintageJS/issues/comments/10841504,10841504,MDEyOklzc3VlQ29tbWVudDEwODQxNTA0,422168,2012-11-29T10:09:55Z,2012-11-29T10:09:55Z,OWNER,"It is a general restriction by the browser vendors, that you can not modify the image data of a canvas element if you drew an image from a different domain on it. To work around this, you can implement a proxy script on your server that serves the s3 images from your domain.
",NA,https://api.github.com/repos/rendro/vintageJS/issues/7/comments,https://github.com/rendro/vintageJS/issues/7#issuecomment-10841504,https://api.github.com/repos/rendro/vintageJS/issues/7
rendro,vintageJS,8789983,https://api.github.com/repos/rendro/vintageJS/issues/comments/10856925,10856925,MDEyOklzc3VlQ29tbWVudDEwODU2OTI1,27511,2012-11-29T17:11:33Z,2012-11-29T17:11:33Z,NONE,"thanks for the info!
",NA,https://api.github.com/repos/rendro/vintageJS/issues/7/comments,https://github.com/rendro/vintageJS/issues/7#issuecomment-10856925,https://api.github.com/repos/rendro/vintageJS/issues/7
rendro,vintageJS,6702321,https://api.github.com/repos/rendro/vintageJS/issues/comments/9911088,9911088,MDEyOklzc3VlQ29tbWVudDk5MTEwODg=,550487,2012-10-30T15:57:56Z,2012-10-30T15:57:56Z,NONE,"It would be nice if I could apply it to the canvas directly
",NA,https://api.github.com/repos/rendro/vintageJS/issues/6/comments,https://github.com/rendro/vintageJS/issues/6#issuecomment-9911088,https://api.github.com/repos/rendro/vintageJS/issues/6
rendro,vintageJS,6702321,https://api.github.com/repos/rendro/vintageJS/issues/comments/9975776,9975776,MDEyOklzc3VlQ29tbWVudDk5NzU3NzY=,422168,2012-11-01T10:18:43Z,2012-11-01T10:18:43Z,OWNER,"You can create a vintageJS instance only on images as it is an image manipulation library. But if you display a bigger image in a lightbox, you can create an instance of vintageJS for this image after the lightbox is loaded.

You should check the callbacks of the lightbox script you use. If you already have the image loaded in a canvas element, you can create an image element via javascript that is not appended to your html body and use this to calculate the vintage effect. Sure, this is not elegant but it should work.
",NA,https://api.github.com/repos/rendro/vintageJS/issues/6/comments,https://github.com/rendro/vintageJS/issues/6#issuecomment-9975776,https://api.github.com/repos/rendro/vintageJS/issues/6
rendro,vintageJS,5299222,https://api.github.com/repos/rendro/vintageJS/issues/comments/6607302,6607302,MDEyOklzc3VlQ29tbWVudDY2MDczMDI=,422168,2012-06-27T15:54:14Z,2012-06-27T15:54:14Z,OWNER,"Thanks!
",NA,https://api.github.com/repos/rendro/vintageJS/issues/4/comments,https://github.com/rendro/vintageJS/pull/4#issuecomment-6607302,https://api.github.com/repos/rendro/vintageJS/issues/4
rendro,vintageJS,5299190,https://api.github.com/repos/rendro/vintageJS/issues/comments/6606763,6606763,MDEyOklzc3VlQ29tbWVudDY2MDY3NjM=,422168,2012-06-27T15:36:12Z,2012-06-27T15:36:12Z,OWNER,"Thank you for the fix
",NA,https://api.github.com/repos/rendro/vintageJS/issues/3/comments,https://github.com/rendro/vintageJS/pull/3#issuecomment-6606763,https://api.github.com/repos/rendro/vintageJS/issues/3
rendro,vintageJS,4343568,https://api.github.com/repos/rendro/vintageJS/issues/comments/5409351,5409351,MDEyOklzc3VlQ29tbWVudDU0MDkzNTE=,422168,2012-04-29T22:26:57Z,2012-04-29T22:26:57Z,OWNER,"Hey, feel free to use it. You can find the license note [here](https://github.com/rendro/vintageJS/blob/master/src/vintage.js#L4-5)
",NA,https://api.github.com/repos/rendro/vintageJS/issues/2/comments,https://github.com/rendro/vintageJS/issues/2#issuecomment-5409351,https://api.github.com/repos/rendro/vintageJS/issues/2
rendro,vintageJS,4343568,https://api.github.com/repos/rendro/vintageJS/issues/comments/5411449,5411449,MDEyOklzc3VlQ29tbWVudDU0MTE0NDk=,323190,2012-04-30T03:53:24Z,2012-04-30T03:53:24Z,NONE,"thx, great
",NA,https://api.github.com/repos/rendro/vintageJS/issues/2/comments,https://github.com/rendro/vintageJS/issues/2#issuecomment-5411449,https://api.github.com/repos/rendro/vintageJS/issues/2
rendro,vintageJS,4157312,https://api.github.com/repos/rendro/vintageJS/issues/comments/5218363,5218363,MDEyOklzc3VlQ29tbWVudDUyMTgzNjM=,422168,2012-04-19T10:05:22Z,2012-04-19T10:05:22Z,OWNER,"You can reset an image by setting it's source to the original file path again. Change [this part](https://github.com/rendro/vintageJS/blob/master/src/vintage.js#L154-163) and change Line 161 to reset the image to the origin path.
",NA,https://api.github.com/repos/rendro/vintageJS/issues/1/comments,https://github.com/rendro/vintageJS/issues/1#issuecomment-5218363,https://api.github.com/repos/rendro/vintageJS/issues/1
rendro,vintageJS,4157312,https://api.github.com/repos/rendro/vintageJS/issues/comments/5218481,5218481,MDEyOklzc3VlQ29tbWVudDUyMTg0ODE=,16512,2012-04-19T10:14:01Z,2012-04-19T10:14:01Z,NONE,"Did a little differently: add data-scr attribute to img and use it like source (i change line 200 to image.src = obj.attr('data-src'); )

Thank you for the excellent plugin!
",NA,https://api.github.com/repos/rendro/vintageJS/issues/1/comments,https://github.com/rendro/vintageJS/issues/1#issuecomment-5218481,https://api.github.com/repos/rendro/vintageJS/issues/1
rendro,vintageJS,4157312,https://api.github.com/repos/rendro/vintageJS/issues/comments/5218988,5218988,MDEyOklzc3VlQ29tbWVudDUyMTg5ODg=,422168,2012-04-19T10:51:49Z,2012-04-19T10:51:49Z,OWNER,"Great. Have fun using it ;)
",NA,https://api.github.com/repos/rendro/vintageJS/issues/1/comments,https://github.com/rendro/vintageJS/issues/1#issuecomment-5218988,https://api.github.com/repos/rendro/vintageJS/issues/1
soumith,convnet-benchmarks,339687132,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/410304505,410304505,MDEyOklzc3VlQ29tbWVudDQxMDMwNDUwNQ==,18536658,2018-08-03T16:20:58Z,2018-08-03T16:20:58Z,NONE,"Same here.
Not working for tensorflow 1.9 too.

OS: Ubuntu
Python: 3.5.2
Tensorflow: 1.9

![screenshot from 2018-08-03 12-19-25](https://user-images.githubusercontent.com/18536658/43653897-80edcb00-9717-11e8-8529-2b4940b2dc38.png)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/138/comments,https://github.com/soumith/convnet-benchmarks/issues/138#issuecomment-410304505,https://api.github.com/repos/soumith/convnet-benchmarks/issues/138
soumith,convnet-benchmarks,339687132,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/413586046,413586046,MDEyOklzc3VlQ29tbWVudDQxMzU4NjA0Ng==,4832280,2018-08-16T15:28:57Z,2018-08-16T15:28:57Z,NONE, res = session.run(target_op) to  res = session.run(target) seems to help.,NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/138/comments,https://github.com/soumith/convnet-benchmarks/issues/138#issuecomment-413586046,https://api.github.com/repos/soumith/convnet-benchmarks/issues/138
soumith,convnet-benchmarks,277035960,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/347205700,347205700,MDEyOklzc3VlQ29tbWVudDM0NzIwNTcwMA==,31092310,2017-11-27T14:52:45Z,2017-11-27T14:52:45Z,NONE,"seems that I went to wrong place, I meant to go to  https://github.com/mitmul/convnet-benchmarks. sorry.",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/136/comments,https://github.com/soumith/convnet-benchmarks/issues/136#issuecomment-347205700,https://api.github.com/repos/soumith/convnet-benchmarks/issues/136
soumith,convnet-benchmarks,277035960,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/347497459,347497459,MDEyOklzc3VlQ29tbWVudDM0NzQ5NzQ1OQ==,31092310,2017-11-28T11:39:08Z,2017-11-28T11:39:08Z,NONE,Seems that the convenet benchmark performance turns up to normal after we upgrade cupy to '3.0.0a1'. ,NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/136/comments,https://github.com/soumith/convnet-benchmarks/issues/136#issuecomment-347497459,https://api.github.com/repos/soumith/convnet-benchmarks/issues/136
soumith,convnet-benchmarks,245644702,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/402869847,402869847,MDEyOklzc3VlQ29tbWVudDQwMjg2OTg0Nw==,3193196,2018-07-05T22:22:15Z,2018-07-05T22:22:15Z,NONE,Caffe doesnt support Tensorcores right? So its useless anyway,NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/130/comments,https://github.com/soumith/convnet-benchmarks/issues/130#issuecomment-402869847,https://api.github.com/repos/soumith/convnet-benchmarks/issues/130
soumith,convnet-benchmarks,221814098,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/294178116,294178116,MDEyOklzc3VlQ29tbWVudDI5NDE3ODExNg==,1310570,2017-04-14T15:49:49Z,2017-04-14T15:49:49Z,OWNER,thanks!,NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/127/comments,https://github.com/soumith/convnet-benchmarks/pull/127#issuecomment-294178116,https://api.github.com/repos/soumith/convnet-benchmarks/issues/127
soumith,convnet-benchmarks,221357387,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/293678099,293678099,MDEyOklzc3VlQ29tbWVudDI5MzY3ODA5OQ==,1310570,2017-04-12T19:10:16Z,2017-04-12T19:10:16Z,OWNER,"i dont have plans to benchmark / maintain these tables. If you send a PR with README tables updated for all the frameworks, i'm happy to merge.",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/126/comments,https://github.com/soumith/convnet-benchmarks/issues/126#issuecomment-293678099,https://api.github.com/repos/soumith/convnet-benchmarks/issues/126
soumith,convnet-benchmarks,219359480,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/293467547,293467547,MDEyOklzc3VlQ29tbWVudDI5MzQ2NzU0Nw==,13167247,2017-04-12T03:56:33Z,2017-04-12T03:56:33Z,NONE,"This might not help, but worth a try. I've seen this in a bunch of the models from the tensorflow/models repo after upgrading to tensorflow-v1.0, but it looks like you're using 0.12 anyways. For me, I was seeing this Shapes(n,m,1,1) and (1,1) are incompatible. It seemed to me to be related changes to argument ordering made in tensorflow-v1.0 to some of the routines, including loss and one of the softmax routines. So I'm immediately suspicious of that tf.concat() call needing the arguments swapped or something added/removed. I had to rearrange arguments for a few routines if I recall.

Where did you get your benchmark_alexnet.py? If was built with a tf-v1.0 instead of 0.12, this might explain it. I believe the tensorflow guys  have a script for making other python scripts v1.0 compatible. You could try upgrading to tensorflow-v1 and see if this fixes the problem.

In benchmark_alexnet.py, try changing the order of the loss arguments, or use slim (I've had luck with this approach): 

import tensorflow.contrib.slim as slim

See available functions here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim

Good luck",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/125/comments,https://github.com/soumith/convnet-benchmarks/issues/125#issuecomment-293467547,https://api.github.com/repos/soumith/convnet-benchmarks/issues/125
soumith,convnet-benchmarks,208909833,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/284555624,284555624,MDEyOklzc3VlQ29tbWVudDI4NDU1NTYyNA==,6690599,2017-03-06T22:31:04Z,2017-03-06T22:31:04Z,NONE,Could you post your benchmark code?,NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/122/comments,https://github.com/soumith/convnet-benchmarks/issues/122#issuecomment-284555624,https://api.github.com/repos/soumith/convnet-benchmarks/issues/122
soumith,convnet-benchmarks,208909833,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/284635018,284635018,MDEyOklzc3VlQ29tbWVudDI4NDYzNTAxOA==,3028543,2017-03-07T06:34:24Z,2017-03-07T06:34:24Z,NONE,"```python
  config = tf.ConfigProto()
  
  # Turns on XLA JIT compilation.
  config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1
  run_metadata = tf.RunMetadata()
  sess = tf.Session(config=config)
  tf.global_variables_initializer().run(session=sess)
```

I've added theses lines to enable XLA",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/122/comments,https://github.com/soumith/convnet-benchmarks/issues/122#issuecomment-284635018,https://api.github.com/repos/soumith/convnet-benchmarks/issues/122
soumith,convnet-benchmarks,208339039,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/281120567,281120567,MDEyOklzc3VlQ29tbWVudDI4MTEyMDU2Nw==,5204400,2017-02-20T16:16:42Z,2017-02-20T16:16:42Z,NONE,"My device specs: NVIDIA GTX 1060, Cuda 8.0, Cudnn: v5

I downloaded some folders from Imagenet. Loaded pre-trained alexnet model and pass images through the network with BatchSize=128. I use inbuilt torch dataloader to load jpeg images.

I am getting 7ms as forward pass which is very fast. I am not sure what I am calculating is correct forward pass time. Please help ! 

Please look at the code and logs here: https://github.com/HarshaVardhanP/Random",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/121/comments,https://github.com/soumith/convnet-benchmarks/issues/121#issuecomment-281120567,https://api.github.com/repos/soumith/convnet-benchmarks/issues/121
soumith,convnet-benchmarks,208339039,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/281139710,281139710,MDEyOklzc3VlQ29tbWVudDI4MTEzOTcxMA==,5204400,2017-02-20T17:34:38Z,2017-02-20T17:35:29Z,NONE,"Using PyTorch example (https://github.com/pytorch/examples/blob/master/imagenet/main.py) with pretrained alexnet and evaluate mode using same data (Batchsize=128). Logs look like these:
 
=> using pre-trained model 'alexnet'
Test: [0/82]	Time 1.680 (1.680)	Loss 13.5178 (13.5178)	Prec@1 0.000 (0.000)	Prec@5 0.000 (0.000)
Test: [10/82]	Time 0.279 (0.288)	Loss 13.4128 (13.2985)	Prec@1 0.000 (0.000)	Prec@5 0.000 (0.000)
Test: [20/82]	Time 0.048 (0.248)	Loss 11.2954 (12.6275)	Prec@1 0.000 (0.000)	Prec@5 0.000 (0.000)
Test: [30/82]	Time 0.048 (0.235)	Loss 11.2725 (12.2873)	Prec@1 0.000 (0.000)	Prec@5 0.781 (0.101)
Test: [40/82]	Time 0.136 (0.233)	Loss 13.2035 (12.2026)	Prec@1 0.000 (0.019)	Prec@5 0.000 (0.171)
Test: [50/82]	Time 0.457 (0.239)	Loss 13.4102 (12.4278)	Prec@1 0.000 (0.015)	Prec@5 0.000 (0.138)
Test: [60/82]	Time 0.067 (0.230)	Loss 14.2645 (12.7405)	Prec@1 0.000 (0.013)	Prec@5 0.000 (0.115)
Test: [70/82]	Time 0.334 (0.228)	Loss 20.0007 (13.6347)	Prec@1 0.000 (0.011)	Prec@5 0.000 (0.099)
Test: [80/82]	Time 0.236 (0.229)	Loss 15.3086 (13.8212)	Prec@1 0.000 (0.010)	Prec@5 0.000 (0.106)
 * Prec@1 0.010 Prec@5 0.115

I can observe, this code includes data loading time as well while calculating batch time. So, it is 200ms per batch which is slower compared to TF. Please share how to estimate correct forward pass time in PyTorch? 
Thanks


",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/121/comments,https://github.com/soumith/convnet-benchmarks/issues/121#issuecomment-281139710,https://api.github.com/repos/soumith/convnet-benchmarks/issues/121
soumith,convnet-benchmarks,208339039,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/281889800,281889800,MDEyOklzc3VlQ29tbWVudDI4MTg4OTgwMA==,4228021,2017-02-23T04:14:34Z,2017-02-23T04:16:05Z,NONE,"@HarshaVardhanP I am running a Titan X pascal. Running the script you linked to (https://github.com/HarshaVardhanP/Random) I see an average forward time of around 88ms on pytorch+alexnet. I did increase num_workers to 12 and let it run for more then 20 steps.

The second script you link to also has a backprop + optimization step... that is probably why you see an increased step time.",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/121/comments,https://github.com/soumith/convnet-benchmarks/issues/121#issuecomment-281889800,https://api.github.com/repos/soumith/convnet-benchmarks/issues/121
soumith,convnet-benchmarks,208339039,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/282004685,282004685,MDEyOklzc3VlQ29tbWVudDI4MjAwNDY4NQ==,5204400,2017-02-23T14:26:13Z,2017-02-23T14:26:13Z,NONE,"@lolz0r  Thanks for your comment. 

1. Yeah, I am getting 58ms as avg forward pass when I setup 12 workers. Look at https://github.com/HarshaVardhanP/Random/blob/master/output2.log. You can observe first batch takes lot of time and rest are very fast. Would you please post your logs ?
By the way, isn't 88ms very slow for Alexnet on Titan X Pascal with 12 workers ? 

2. I am running that script on pretrained and eval mode. There is no backprop in the test step I guess. What would you say ?  




",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/121/comments,https://github.com/soumith/convnet-benchmarks/issues/121#issuecomment-282004685,https://api.github.com/repos/soumith/convnet-benchmarks/issues/121
soumith,convnet-benchmarks,208339039,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/282062140,282062140,MDEyOklzc3VlQ29tbWVudDI4MjA2MjE0MA==,1310570,2017-02-23T17:30:46Z,2017-02-23T17:30:46Z,OWNER,"you need to add two lines to the near top of your script:

```
import torch.backends.cudnn as cudnn
cudnn.benchmark = True
```
That will turn on the cudnn autotuner that selects efficient algorithms.

Secondly, convnet-benchmarks itself is based on synthetic data, so if you want to simulate that you simply use dummy data. Here's is your script partly modified to do so.

```
def main():
	t = time.time()
	global args
    	args = parser.parse_args()
	data_, target_ = torch.randn(128, 3, 224, 224).cuda(). torch.range(1, 128).long().cuda()
	net = models.alexnet() # no need to load pre-trained weights for dummy data
	net.cuda()
	net.eval()
	print(net)

	#passing images thru network
	n = 1
        batch_avgtime=0
	for data, target in val_loader:
		td = time.time()
		data, target = Variable(data_, volatile=True), Variable(target_)
		p = net(data)
                batch_time = time.time()-td	
                batch_avgtime=batch_avgtime+batch_time
                if n==20:
                    break
                n=n+1
```",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/121/comments,https://github.com/soumith/convnet-benchmarks/issues/121#issuecomment-282062140,https://api.github.com/repos/soumith/convnet-benchmarks/issues/121
soumith,convnet-benchmarks,193877551,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/265275684,265275684,MDEyOklzc3VlQ29tbWVudDI2NTI3NTY4NA==,180987,2016-12-06T21:19:54Z,2016-12-06T21:19:54Z,CONTRIBUTOR,"Doing benchmark without cudnn isn't useful. Can you install it? It could
also fix your problem.

If not, I have the impression that it get killed by the OS. Are you testing
it on a cluster? Did you request enough CPU memory? If not, many job
scheduler could kill your job with that message.

Le 6 dÃ©c. 2016 21:37, ""hpourreza"" <notifications@github.com> a Ã©crit :

I can run the Theano benchmark on K80 using Cuda 8.0 and Python/2.7.12 but
when I try to run it on older GPUs (K20 or M2090) using cuda 7.5 and python
2.7.9 the program gets killed:

$ SKIP=legacy python pylearn2_benchmark.py
Using gpu device 0: Tesla M2090 (CNMeM is disabled, cuDNN not available)
Note: cuDNN not available

CONFIG: input = 3 x 128 x 128 * ker = 3 x 96 x 11 x 11 ( bs = 128 , stride = 1 )
Killed

Any idea why this is happening?

â€”
You are receiving this because you are subscribed to this thread.
Reply to this email directly, view it on GitHub
<https://github.com/soumith/convnet-benchmarks/issues/119>, or mute the
thread
<https://github.com/notifications/unsubscribe-auth/AALC-05uwS7fA_VL8lXj-4xWxTYvZ-knks5rFceXgaJpZM4LF2gW>
.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/119/comments,https://github.com/soumith/convnet-benchmarks/issues/119#issuecomment-265275684,https://api.github.com/repos/soumith/convnet-benchmarks/issues/119
soumith,convnet-benchmarks,193877551,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/265288606,265288606,MDEyOklzc3VlQ29tbWVudDI2NTI4ODYwNg==,10135217,2016-12-06T22:08:28Z,2016-12-07T16:03:34Z,NONE,"Thanks @nouiz. I was asking for 2GB of memory but apparently it was not enough. I increased it and the code started running.
Unfortunately, `cuDNN` requires compute capability of 3 or more that M2090 does not support. ",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/119/comments,https://github.com/soumith/convnet-benchmarks/issues/119#issuecomment-265288606,https://api.github.com/repos/soumith/convnet-benchmarks/issues/119
soumith,convnet-benchmarks,186176426,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/257206941,257206941,MDEyOklzc3VlQ29tbWVudDI1NzIwNjk0MQ==,1310570,2016-10-31T03:28:11Z,2016-10-31T03:28:11Z,OWNER,"Thank you!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/118/comments,https://github.com/soumith/convnet-benchmarks/pull/118#issuecomment-257206941,https://api.github.com/repos/soumith/convnet-benchmarks/issues/118
soumith,convnet-benchmarks,183498192,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/254300380,254300380,MDEyOklzc3VlQ29tbWVudDI1NDMwMDM4MA==,1310570,2016-10-17T18:59:10Z,2016-10-17T18:59:10Z,OWNER,"Thanks Anna!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/117/comments,https://github.com/soumith/convnet-benchmarks/pull/117#issuecomment-254300380,https://api.github.com/repos/soumith/convnet-benchmarks/issues/117
soumith,convnet-benchmarks,183498192,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/254301010,254301010,MDEyOklzc3VlQ29tbWVudDI1NDMwMTAxMA==,22060313,2016-10-17T19:01:31Z,2016-10-17T19:01:31Z,CONTRIBUTOR,"Thank you for the quick merge!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/117/comments,https://github.com/soumith/convnet-benchmarks/pull/117#issuecomment-254301010,https://api.github.com/repos/soumith/convnet-benchmarks/issues/117
soumith,convnet-benchmarks,182211833,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/307307251,307307251,MDEyOklzc3VlQ29tbWVudDMwNzMwNzI1MQ==,22289078,2017-06-09T06:30:52Z,2017-06-09T06:30:52Z,NONE,@sjlee7748  how to you resolve the situation without GPU?,NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/116/comments,https://github.com/soumith/convnet-benchmarks/issues/116#issuecomment-307307251,https://api.github.com/repos/soumith/convnet-benchmarks/issues/116
soumith,convnet-benchmarks,174196457,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/247492833,247492833,MDEyOklzc3VlQ29tbWVudDI0NzQ5MjgzMw==,14060629,2016-09-16T00:43:41Z,2016-09-16T00:48:42Z,CONTRIBUTOR,"Because it's Alexnet from the ""One weird trick"" paper: https://arxiv.org/abs/1404.5997. Classic AlexNet 2-column topology was caused by him using 2 GPUs with 3GB RAM each (maximum at the time). As GPUs with more RAM appeared, it was replaced by simpler and better performing 1-column Alexnet. There is really no reason to benchmark 2-column classic Alexnet, except to preserve historic continuity, which often [accidentally] happens when people run Caffenet (since it's included with Caffe. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/114/comments,https://github.com/soumith/convnet-benchmarks/issues/114#issuecomment-247492833,https://api.github.com/repos/soumith/convnet-benchmarks/issues/114
soumith,convnet-benchmarks,172137784,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/241054959,241054959,MDEyOklzc3VlQ29tbWVudDI0MTA1NDk1OQ==,1310570,2016-08-19T15:45:51Z,2016-08-19T15:45:51Z,OWNER,"Thanks!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/113/comments,https://github.com/soumith/convnet-benchmarks/pull/113#issuecomment-241054959,https://api.github.com/repos/soumith/convnet-benchmarks/issues/113
soumith,convnet-benchmarks,170189671,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/238605544,238605544,MDEyOklzc3VlQ29tbWVudDIzODYwNTU0NA==,1310570,2016-08-09T16:16:19Z,2016-08-09T16:16:19Z,OWNER,"Thanks!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/112/comments,https://github.com/soumith/convnet-benchmarks/pull/112#issuecomment-238605544,https://api.github.com/repos/soumith/convnet-benchmarks/issues/112
soumith,convnet-benchmarks,170097655,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/238600509,238600509,MDEyOklzc3VlQ29tbWVudDIzODYwMDUwOQ==,1310570,2016-08-09T15:59:56Z,2016-08-09T15:59:56Z,OWNER,"Thanks!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/111/comments,https://github.com/soumith/convnet-benchmarks/pull/111#issuecomment-238600509,https://api.github.com/repos/soumith/convnet-benchmarks/issues/111
soumith,convnet-benchmarks,166356574,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/234097019,234097019,MDEyOklzc3VlQ29tbWVudDIzNDA5NzAxOQ==,8460517,2016-07-20T21:59:45Z,2016-07-21T03:53:07Z,NONE,"Hi @nomi-wei , just a clarification: our fast convnet algorithms use [Winograd's _convolution_ algorithms](https://www.encyclopediaofmath.org/index.php/Winograd_small_convolution_algorithm). But the same Shmuel Winograd did co-author the [Coppersmith-Winograd fast matrix multiplication algorithm](https://en.wikipedia.org/wiki/Coppersmith%E2%80%93Winograd_algorithm), so the confusion is understandable (I probably should not even mention that Winograd also devised [fast DFT algorithms](http://www.ams.org/journals/mcom/1978-32-141/S0025-5718-1978-0468306-4/S0025-5718-1978-0468306-4.pdf) ;-)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/110/comments,https://github.com/soumith/convnet-benchmarks/issues/110#issuecomment-234097019,https://api.github.com/repos/soumith/convnet-benchmarks/issues/110
soumith,convnet-benchmarks,166356574,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/234296794,234296794,MDEyOklzc3VlQ29tbWVudDIzNDI5Njc5NA==,3054232,2016-07-21T15:49:44Z,2016-07-21T15:49:44Z,NONE,"@andravin Ha-ha, my bad. Thanks for your clarification. It's really really helpful. ;-)
I didn't got the book you referenced, so I thought you might use winograd's famous mm method for this.  LoL. No wonder I still find it hard to understand your approach, after I learned these mm algorithms from scratch these few days.

Thanks again!
BTW, Andrew, I wonder if you're still working on improving this conv kernel stuff? If yes, that would be awesome.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/110/comments,https://github.com/soumith/convnet-benchmarks/issues/110#issuecomment-234296794,https://api.github.com/repos/soumith/convnet-benchmarks/issues/110
soumith,convnet-benchmarks,166356574,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/234442025,234442025,MDEyOklzc3VlQ29tbWVudDIzNDQ0MjAyNQ==,123560,2016-07-22T02:52:48Z,2016-07-22T02:52:48Z,CONTRIBUTOR,"> I think in theory 8-bit is enough to carry the information with quantization. 

googling for dp4a reaches a thread with Scott Gray in as first hit :-)  https://devtalk.nvidia.com/default/topic/934562/cuda-programming-and-performance/nvidia-pascal-geforce-gtx-1080-amp-gtx-1070/post/4889687/  So I would say he's aware of it :-)

I was actually pondering dabbling with ints way back in 2014 http://computer-go.org/pipermail/computer-go/2014-December/007105.html  ... but it's just one of many things that never survived contact with finite-hours-in-the-day :-)

Considering the effort involved in making gpus work, and work quickly, I would think the first thing to do might be to demonstrate using normal cpu code that you can get ok results?  You could just fire up torch, and create `torch.ByteTensor`s.

A few questions which occur:
- how will you deal with overflows?  It's one thing to have multiplications truncated to some maximum value, it's another thing for them to overflow into the opposite sign...
- 8-bits means there are 256 different values.  How will you deal with, well, gradients and stuff?

Hmmm, I'm simply reciting back to you the questions that were stated to me when I mentioned the idea myself :-)  http://computer-go.org/pipermail/computer-go/2014-December/007106.html
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/110/comments,https://github.com/soumith/convnet-benchmarks/issues/110#issuecomment-234442025,https://api.github.com/repos/soumith/convnet-benchmarks/issues/110
soumith,convnet-benchmarks,162020152,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/228182485,228182485,MDEyOklzc3VlQ29tbWVudDIyODE4MjQ4NQ==,1310570,2016-06-23T21:01:13Z,2016-06-23T21:01:13Z,OWNER,"Run it from the imagenet_winners directory
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/109/comments,https://github.com/soumith/convnet-benchmarks/issues/109#issuecomment-228182485,https://api.github.com/repos/soumith/convnet-benchmarks/issues/109
soumith,convnet-benchmarks,162020152,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/228201142,228201142,MDEyOklzc3VlQ29tbWVudDIyODIwMTE0Mg==,15735891,2016-06-23T22:12:07Z,2016-06-23T22:13:01Z,NONE,"Thanks for the swift response Soumith!  That fixed my issue.

I tried to run the fbnn implementation by commenting line 17 and uncommenting line 8 + 18, and it produced this error: ""~/torch/install/share/lua/5.1/trepl/init.lua:384: module 'fb.torch.async_rng' not found:No LuaRocks module found for fb.torch.async_rng"" 

I have installed all the pre-req libraries according to the readme. Can you provide some advice please?

Thanks,
Weidong
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/109/comments,https://github.com/soumith/convnet-benchmarks/issues/109#issuecomment-228201142,https://api.github.com/repos/soumith/convnet-benchmarks/issues/109
soumith,convnet-benchmarks,162020152,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/228202392,228202392,MDEyOklzc3VlQ29tbWVudDIyODIwMjM5Mg==,1310570,2016-06-23T22:17:26Z,2016-06-23T22:17:26Z,OWNER,"fbnn is a bit redundant now, just use cudnn and forget fbnn stuff ever existed, all fft stuff is available in cudnn now.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/109/comments,https://github.com/soumith/convnet-benchmarks/issues/109#issuecomment-228202392,https://api.github.com/repos/soumith/convnet-benchmarks/issues/109
soumith,convnet-benchmarks,162020152,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/230762397,230762397,MDEyOklzc3VlQ29tbWVudDIzMDc2MjM5Nw==,4562268,2016-07-06T12:53:59Z,2016-07-06T12:53:59Z,NONE,"Hi soumith, 

Could you explain this a bit more? What you mean by all fft stuff is in cudnn now. From the latest cudnn document, I didn't see they mention fft is implemented already in cudnn.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/109/comments,https://github.com/soumith/convnet-benchmarks/issues/109#issuecomment-230762397,https://api.github.com/repos/soumith/convnet-benchmarks/issues/109
soumith,convnet-benchmarks,162020152,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/230763319,230763319,MDEyOklzc3VlQ29tbWVudDIzMDc2MzMxOQ==,1310570,2016-07-06T12:57:54Z,2016-07-06T12:57:54Z,OWNER,"Search for FFT in the CuDNN Manual, you will find it.
https://github.com/soumith/cudnn.torch/blob/master/ffi.lua#L395-L403
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/109/comments,https://github.com/soumith/convnet-benchmarks/issues/109#issuecomment-230763319,https://api.github.com/repos/soumith/convnet-benchmarks/issues/109
soumith,convnet-benchmarks,153007675,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/218049204,218049204,MDEyOklzc3VlQ29tbWVudDIxODA0OTIwNA==,1310570,2016-05-10T03:19:04Z,2016-05-10T03:19:04Z,OWNER,"Thanks Sergey!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/105/comments,https://github.com/soumith/convnet-benchmarks/pull/105#issuecomment-218049204,https://api.github.com/repos/soumith/convnet-benchmarks/issues/105
soumith,convnet-benchmarks,152882435,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/216677451,216677451,MDEyOklzc3VlQ29tbWVudDIxNjY3NzQ1MQ==,1310570,2016-05-03T21:58:37Z,2016-05-03T21:58:37Z,OWNER,"Torch's MM based convolutions on CPU use a lot more memory, and the shapes probably are not as optimized for OpenBLAS-ARM (as it unfolds all mini-batches and does a single MM call, rather than doing per-batch unfold + gemm in caffe). I'd suggest trying out:
https://github.com/mvitez/OpenBLAS-conv
https://github.com/mvitez/thnets

This very old fork of torch also has optimized assembly NEON based convolutions in there, but only for 32-bit ARM: https://github.com/soumith/torch-android/commit/af6dc1ed85eb9a37b0bd96b89cdc27bf68990176
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/104/comments,https://github.com/soumith/convnet-benchmarks/issues/104#issuecomment-216677451,https://api.github.com/repos/soumith/convnet-benchmarks/issues/104
soumith,convnet-benchmarks,152882435,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/216679732,216679732,MDEyOklzc3VlQ29tbWVudDIxNjY3OTczMg==,1310570,2016-05-03T22:09:05Z,2016-05-03T22:09:05Z,OWNER,"that being said, 20x seems hugely suspect, as they are both calling gemm.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/104/comments,https://github.com/soumith/convnet-benchmarks/issues/104#issuecomment-216679732,https://api.github.com/repos/soumith/convnet-benchmarks/issues/104
soumith,convnet-benchmarks,152882435,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/216681514,216681514,MDEyOklzc3VlQ29tbWVudDIxNjY4MTUxNA==,10137,2016-05-03T22:17:25Z,2016-05-03T22:17:25Z,NONE,"@soumith Thanks for the pointers! I'll try out the codes you've linked and post back here. If it is indeed a difference in unfolding all batches vs. per-batch unfolds (in Caffe), then it should make a huge difference. Thanks!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/104/comments,https://github.com/soumith/convnet-benchmarks/issues/104#issuecomment-216681514,https://api.github.com/repos/soumith/convnet-benchmarks/issues/104
soumith,convnet-benchmarks,152882435,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/216706282,216706282,MDEyOklzc3VlQ29tbWVudDIxNjcwNjI4Mg==,10137,2016-05-04T00:23:12Z,2016-05-04T00:23:12Z,NONE,"@soumith I've validated AlexNet using thnets (https://github.com/mvitez/thnets) and the TX1's ARM A57 CPU is now within 18% of the Caffe implementation (4.54 FPS on thnets vs. 5.3 FPS on Caffe) for a batch_size = 4 on thnets. I attribute the speedup to assembly-level intrinsics + highly optimized openBLAS kernels for the ARM platform.

I couldn't verify your claim that Torch unfolds all batches and performs a single MM call (and that Caffe unfolds per batch and performs multiple MM calls. Running ~/tegrastats to monitor memory usage, it appears that Torch (for my original Torch benchmark) actually uses _less_ memory than Caffe.

Anyways, you solved my problem :) thanks man.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/104/comments,https://github.com/soumith/convnet-benchmarks/issues/104#issuecomment-216706282,https://api.github.com/repos/soumith/convnet-benchmarks/issues/104
soumith,convnet-benchmarks,152882435,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/221003525,221003525,MDEyOklzc3VlQ29tbWVudDIyMTAwMzUyNQ==,18326981,2016-05-23T14:54:31Z,2016-05-23T14:54:31Z,NONE,"Hi, these numbers seems very bad... Just if it helps let me say that I am developing my on toolkit for academic purposes:

https://github.com/RParedesPalacios/Layers

(i have still to upload src code)

And AlexNet with batch=100 and only forward (inference) it takes 2 secs approx. I use lowering and all the batch unfolded. I will try to upload the code to try it.

regards
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/104/comments,https://github.com/soumith/convnet-benchmarks/issues/104#issuecomment-221003525,https://api.github.com/repos/soumith/convnet-benchmarks/issues/104
soumith,convnet-benchmarks,152882435,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/221044649,221044649,MDEyOklzc3VlQ29tbWVudDIyMTA0NDY0OQ==,3846198,2016-05-23T17:51:00Z,2016-05-23T17:51:00Z,NONE,"@RParedesPalacios, are we talking about inference on the TX1's ARM CPU? If so, isn't 50 images per second unreasonable?

Say it's 720 MFLOP to perform a single forward pass for one image [1]. 50 images per second would roughly translate to 36 GFLOPS (720 MFLOP \* 50 images/s). I suspect the peak performance of the TX1's ARM CPUs cannot surpass 10 GFLOPS even with NEON and multithreading enabled.

[1] https://groups.google.com/forum/#!topic/caffe-users/cUD3IF5NMOk
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/104/comments,https://github.com/soumith/convnet-benchmarks/issues/104#issuecomment-221044649,https://api.github.com/repos/soumith/convnet-benchmarks/issues/104
soumith,convnet-benchmarks,152882435,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/221061570,221061570,MDEyOklzc3VlQ29tbWVudDIyMTA2MTU3MA==,18326981,2016-05-23T18:53:30Z,2016-05-23T18:53:30Z,NONE,"@carlodelmundo, hi i misunderstood the point.. I read, ""... Intel Xeon E5-2637 CPU ..."" and i thought that the following numbers refer to that.. but i read again and i understand that all the numbers refer to the ARM CPU!. 

sorry for that ,regards!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/104/comments,https://github.com/soumith/convnet-benchmarks/issues/104#issuecomment-221061570,https://api.github.com/repos/soumith/convnet-benchmarks/issues/104
soumith,convnet-benchmarks,148570455,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210313921,210313921,MDEyOklzc3VlQ29tbWVudDIxMDMxMzkyMQ==,1310570,2016-04-15T06:53:41Z,2016-04-15T06:53:41Z,OWNER,"Those two seem well qualified to be working on squeezing perf ;)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/102/comments,https://github.com/soumith/convnet-benchmarks/pull/102#issuecomment-210313921,https://api.github.com/repos/soumith/convnet-benchmarks/issues/102
soumith,convnet-benchmarks,148570455,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210316664,210316664,MDEyOklzc3VlQ29tbWVudDIxMDMxNjY2NA==,463737,2016-04-15T07:01:01Z,2016-04-15T07:06:45Z,CONTRIBUTOR,"Indeed :).

(I actually messed up benchmarking the vgg and overfeat, they both got better too:

88ms/271ms for overfeat
157ms/550ms for vgg 
)

vgg can be as low as 533ms for forwards/backwards but I think thermal limits kick in and slow things down when the benchmark runs too long.  An interesting thing to think about for DeepMark...
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/102/comments,https://github.com/soumith/convnet-benchmarks/pull/102#issuecomment-210316664,https://api.github.com/repos/soumith/convnet-benchmarks/issues/102
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210205485,210205485,MDEyOklzc3VlQ29tbWVudDIxMDIwNTQ4NQ==,1310570,2016-04-14T23:41:48Z,2016-04-14T23:41:48Z,OWNER,"Oh lastly, a good timeline for this would be to get an initial round of benchmarks by **June 15th** (since I only gave some of you a heads-up right now)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-210205485,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210208111,210208111,MDEyOklzc3VlQ29tbWVudDIxMDIwODExMQ==,1241240,2016-04-14T23:54:08Z,2016-04-14T23:54:08Z,NONE,"So awesome and useful. What are the data sets one should benchmark on? ImageNet, CIFAR10? It would also be nice to compare the accuracy of current implementations for each framework (although that would probably be a lot of work). 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-210208111,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210237045,210237045,MDEyOklzc3VlQ29tbWVudDIxMDIzNzA0NQ==,32325,2016-04-15T01:26:15Z,2016-04-15T01:26:15Z,NONE,"For text, I'd hope to expand beyond just RNN character generation. It doesn't capture many of the complexities of other models, such as variable sequence lengths or bidirectional RNNs.

The [Attention Sum Reader](https://www.semanticscholar.org/paper/Text-Understanding-with-the-Attention-Sum-Reader-Kadlec-Schmid/54fb788650fdbaeb5ab1401800f5d2e1f5c9b0d0) is a simple architecture (bidirectional GRU + dot product) that currently has SotA and could allow for optimizing sequences of different lengths, a major issue in RNNs. The model also has four different dataset sizes, small (Children's Book Test), medium (CNN), and large (Daily Mail), which are publicly available.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-210237045,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210263200,210263200,MDEyOklzc3VlQ29tbWVudDIxMDI2MzIwMA==,463737,2016-04-15T02:57:39Z,2016-04-15T03:18:10Z,CONTRIBUTOR,"This is great, thanks for organizing this!  One thing I've also been thinking about like @daviddao is how to validate that the models are actually computing the same thing -- I've seen some benchmarks elsewhere that have raised personal doubts that the benchmark code written in different frameworks are computing the same function.  As part of the benchmark framework, maybe the API could include a way to validate that given specified initialization and input, the outputs (forward and backward) are approximately equal.  Open to thoughts :).  Cheers!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-210263200,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210268181,210268181,MDEyOklzc3VlQ29tbWVudDIxMDI2ODE4MQ==,417568,2016-04-15T03:18:24Z,2016-04-15T03:20:31Z,NONE,"Nice!  I am very excited for this.

I have https://github.com/craffel/lstm_benchmarks, which is an out-of-date benchmark of Theano vs. rnnlib vs. currennt (which, at the time I wrote the benchmarks, were essentially the only options for LSTM).  The task was CHIME noisy speech recognition, which has pretty limited adoption so I would not strongly advocate for it being added as a task.  And I assume that rnnlib and current shouldn't be included in these benchmarks are they are RNN-only, right?

I'll be happy to contribute to some of the Theano RNN benchmarks once it becomes appropriate to do so.

> This is great, thanks for organizing this! One thing I've also been thinking about like @daviddao is how to validate that the models are actually computing the same thing -- I've seen some benchmarks elsewhere that have raised personal doubts that the frameworks are computing the same function.

This would be very cool, but from my own experience with the LSTM benchmark it can be very difficult - you have to make sure literally every hyperparameter is identical, and you effectively can't use any RNGs.  Not to say it's impossible, but it would add a lot of overhead to implementing new benchmarks.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-210268181,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210359284,210359284,MDEyOklzc3VlQ29tbWVudDIxMDM1OTI4NA==,123560,2016-04-15T08:31:15Z,2016-04-15T08:31:15Z,CONTRIBUTOR,"Caveat: per Paul Graham, better to go deep, do something very well, than kind of blur one's 'focus' over many things.  I worry gently that if too many benchmarks then:
- each benchmark less well maintained
- more confusing to read
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-210359284,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210397829,210397829,MDEyOklzc3VlQ29tbWVudDIxMDM5NzgyOQ==,629706,2016-04-15T10:03:11Z,2016-04-15T10:03:11Z,CONTRIBUTOR,":+1:

> What are the data sets one should benchmark on? ImageNet, CIFAR10?

Training on something like ImageNet would move away the focus from pure computation to fast dataset iteration -- this would be interesting as well, but should probably become a separate benchmark since not all frameworks actually provide any tools for this. The other extreme would be training on random dummy data (like sampled from a Gaussian), but this makes sense only if we can guarantee the running time does not depend on the input data. So probably we should have some realistic set of inputs for each task, just large enough to fill two batches or so?

> As part of the benchmark framework, maybe the API could include a way to validate that given specified initialization and input, the outputs (forward and backward) are approximately equal.

This seems useful. It requires initial model parameters to be dumped in some format and loaded into each framework, but it would help to ensure that all implementations are the same.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-210397829,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210464525,210464525,MDEyOklzc3VlQ29tbWVudDIxMDQ2NDUyNQ==,123560,2016-04-15T13:30:00Z,2016-04-15T13:30:56Z,CONTRIBUTOR,"> This seems useful. It requires initial model parameters to be dumped in some format and loaded into each framework, but it would help to ensure that all implementations are the same.

In the strongest case, weight initialization could be defined precisely as:
- a precise order of initialization, eg by layer, then by infeatureplane, then by outfeatureplane, then by height, then by width (for example)
- a precise random function to use (eg mt19937)
- the exact seed to use
- (edit, and of course the exact function to use, eg `sqrt(numberinputs) * 0.1`, or similar)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-210464525,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210469275,210469275,MDEyOklzc3VlQ29tbWVudDIxMDQ2OTI3NQ==,629706,2016-04-15T13:44:21Z,2016-04-15T13:44:21Z,CONTRIBUTOR,"> In the strongest case, weight initialization could be defined precisely as [...]

I guess getting the same stream of pseudo-random values in all different frameworks is more difficult than importing a set of tensors into all different frameworks. We wouldn't want to exclude candidates from being benchmarked because they fail to implement the same RNG.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-210469275,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210478943,210478943,MDEyOklzc3VlQ29tbWVudDIxMDQ3ODk0Mw==,123560,2016-04-15T14:15:41Z,2016-04-15T14:16:10Z,CONTRIBUTOR,"> I guess getting the same stream of pseudo-random values in all different frameworks is more difficult than importing a set of tensors into all different frameworks. We wouldn't want to exclude candidates from being benchmarked because they fail to implement the same RNG.

Having gone through the exact same process, to compare DeepCL with convnetjs, I found it significantly easier to make convnetjs use the exact same weight generator as DeepCL, than to load weights from a file https://github.com/hughperkins/DeepCL/blob/master/prototyping/convnetjs-reference/testconvnet2.js#L143-L201 .  It was a long time ago, so I dont remember why.  I do remember I initially tried writing weights to a file though, and I couldnt get it to work as easily as syncing weight generators, for some reason.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-210478943,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210524985,210524985,MDEyOklzc3VlQ29tbWVudDIxMDUyNDk4NQ==,629706,2016-04-15T16:13:16Z,2016-04-15T16:13:16Z,CONTRIBUTOR,"> I found it significantly easier to make convnetjs use the exact same weight generator as DeepCL, than to load weights from a file

If that's the case, one could of course create the initial weights in a reproducible way _and_ save them to files, so implementers for the different benchmarked frameworks can choose whatever is easiest.
(Note that loading from files has the additional benefit of documenting how to load foreign model parameters into each framework.)
Umm... are we supposed to discuss such details here or over at the deepmark repository?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-210524985,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210529187,210529187,MDEyOklzc3VlQ29tbWVudDIxMDUyOTE4Nw==,1310570,2016-04-15T16:24:44Z,2016-04-15T16:24:44Z,OWNER,"@daviddao @vrv @hughperkins @f0k for V1, I thought we should just go with synthetic data. It's very hard to setup to-convergence benchmarks, as there are very fine details wrt convergence being guaranteed, for example: some of the models (like googlenetv3) have taken a year to reproduce outside of the paper.

@Smerity In terms of evaluating perf, we can add a bidirectional RNN too. In fact, DeepSpeech2 has bidirectional-RNNs, so that should be sufficient?

@vrv definitely a great idea, but very very hard and takes a ton of resources. I feel like atleast for V1, we should just go with code review + synthetic data.

@craffel Awesome! At the moment I dont have a Point of Contact for Theano, maybe a combination of you, @f0k and @benanne could work (especially if they're implemented in Lasagne).
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-210529187,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210534604,210534604,MDEyOklzc3VlQ29tbWVudDIxMDUzNDYwNA==,417568,2016-04-15T16:34:47Z,2016-04-15T16:34:47Z,NONE,"> (especially if they're implemented in Lasagne).

That would be nice :) though I am personally interested in which of the Theano-based libraries manage to eek out the most performance, since their implementations are nonidentical.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-210534604,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210540783,210540783,MDEyOklzc3VlQ29tbWVudDIxMDU0MDc4Mw==,551151,2016-04-15T16:42:44Z,2016-04-15T17:06:45Z,CONTRIBUTOR,"Thanks @soumith for organizing this effort! I think this would definitely help us advance the field to the next level.

I am also very interested in benchmarking not only the training pipeline, but a more wide range of evaluation criteria. The reason is as follows: if I may make a bold claim, I believe that all frameworks will again very quickly converge to the same performance, because there is no fundamental difference between them. What we saw at convnet-benchmark is that almost everyone is using the same underlying library, and we are effectively benchmarking framework overheads, something that is good to know of course, but seems to be overwhelmed by other factors, such as ease to use etc.

Given the wide attention of this benchmark, I think it would be great if we can draw attention to some of the more practical issues, such as small batch sizes in deployment time - several frameworks (including some non-open-source production systems I've worked on) have historically ignored this, and I think it is worthwhile to invite people to invest more on this direction.

I have not got a perfece idea on this, of course. One thing we can do is to simply benchmark different batch sizes, but a more complex, and potentially useful, way is probably to set up a harness that can simulate requests generated from a Poisson distribution and comes with latency requirements, and see whether frameworks can address that in an optimal fashion - this might be too application specific, though. Just my 2 cents.

(Also adding @ajtulloch to the conversation. Andrew first raised this point when we were discussing offline.)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-210540783,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210550872,210550872,MDEyOklzc3VlQ29tbWVudDIxMDU1MDg3Mg==,5012379,2016-04-15T17:13:12Z,2016-04-15T17:13:12Z,NONE,"How about per-pixel scene labelling and optical flow?

Regards

-David
On 15 Apr 2016 00:41, ""Soumith Chintala"" notifications@github.com wrote:

> Hi all,
> 
> The reason I've been slow on convnet-benchmarks these days is because i've
> been working on the side on DeepMark.
> 
> I initially wrote _convnet-benchmarks_ to increase competition among
> frameworks so that we can work towards faster ConvNets, and they served
> their purpose well. After the release of convnet-benchmarks, multiple
> frameworks pulled up their socks to speedup convnets, with a deep sense of
> prestige for being on top of these benchmarks. In these two years, we as a
> community accelerated GPU ConvNets across all frameworks between 4x to 10x,
> efficiently implementing tricks such as FFT, Winograd, and powered by
> faster hardware. Alex Khrizevsky, Yangqing Jia, Scott Gray, Nicolas
> Vasilache, Sander Dieleman, Michael Mathieu, Julien Denmouth and many other
> human compilers helped make this a reality -- looking at the diversity in
> terms of where each of us work(ed) professionally shows that this kind of
> acceleration was truly a community effort with a ton of openness, something
> that is plain awesome! :)
> I've also enjoyed reading the deeply technical discussions that take place
> on convnet-benchmarks (my favorites: #93
> https://github.com/soumith/convnet-benchmarks/issues/93 , #66
> https://github.com/soumith/convnet-benchmarks/issues/66 , #59
> https://github.com/soumith/convnet-benchmarks/issues/59 in recent times
> ).< /p>
> 
> Moving on, _convnet-benchmarks_ do not accurately capture everything we
> think of when we say _deep learning_. We don't have Recurrent Nets, we
> don't have video use-cases, speech, NLP etc. There is a need for such
> comprehensive benchmarks, especially as the space is getting ready for
> dedicated hardware chips, multi-GPU and multi-machine frameworks and more
> complex use-cases.
> 
> I've sat down with a few of you at NIPS and GTC to discuss and freeze the
> initial round of benchmarks for what I am calling _DeepMark_. My initial
> plan was to work on the initial set of benchmark scripts by myself and
> cover the most popular frameworks, and then let the direction and
> maintenance of the benchmarks be community-driven. But the breadth of this
> effort has been overwhelming to say the least. After careful thought, I've
> decided that I'll just ask everyone to pitch in for their part of the
> benchmarks with making scripts etc., especially as many of you were very
> receptive to the idea offline.
> 
> Here are the initial set of use-cases we want to cover:
> Networks Images
> - InceptionV3-batchnorm (http://arxiv.org/abs/1512.00567 ,
>   https://github.com/Moodstocks/inception-v3.torch)
> - Alexnet-OWT
> - VGG
> - ResNet-50 ( http://arxiv.org/abs/1512.03385 ,
>   https://github.com/facebook/fb.resnet.torch )
> 
> Video
> - C3D - A vgg-style 3D net ( http://vlg.cs.dartmouth.edu/c3d/ )
> 
> Audio
> - DeepSpeech2 - Convnet + RNN + FC ( http://arxiv.org/abs/1512.02595 )
> - MSR's 5 layer FC net ( https://github.com/Alexey-Kamenev/Benchmarks )
> 
> Text
> - Small RNN LSTM (
>   https://github.com/karpathy/char-rnn/blob/master/train.lua#L38-L48 )
> - Large RNN LSTM ( BIG-LSTM in http://arxiv.org/abs/1602.02410 )
> 
> Platform
> - Initially multi-GPU with (1 to 4 titan-X cards)
> - However, multi-machine, custom hardware, other GPU cards such as
>   AMD, CPUs etc. can and should be accommodated, we will work this out after
>   the initial push.
> 
> Metrics
> - Round-trip time for 1 epoch of training (will define an epoch size
>   separately for each network)
> - Maximum batch-size that fits (to show and focus on the extra memory
>   consumption that the framework uses)
> 
> Frameworks
> 
> Everyone who wants to join-in, but I thought an initial set that is
> important to cover would be:
> - Caffe
> - Chainer
> - MXNet
> - Neon
> - Theano
> - TensorFlow
> - Torch
> 
> Scripts format
> - Emit JSON output (so that the README -- or jekyll website can be
>   auto-generated, similar to http://autumnai.com/deep-learning-benchmarks
>   )
> 
> Guarantees
> - I will personally to the best of my abilities make sure that the
>   benchmarking is fair and unbiased. The hope is that the community at large
>   will watch these and point-out / fix mistakes.
> 
> Governance
> - The benchmarks will be placed at https://github.com/DeepMark/deepmark
>   and other key community members / organizations who want ownership will be
>   welcome to join in proposing new benchmarks that get relevant as the field
>   progresses.
> 
> My hope is that these new set of benchmarks will not only increase
> competition but will also be beneficial in other ways to the community,
> serving as common examples to get started, etc.
> 
> Let me know what you think :)
> Soumith
> 
> cc: @hughperkins https://github.com/hughperkins @f0k
> https://github.com/f0k @scott-gray https://github.com/scott-gray
> @rajatmonga https://github.com/rajatmonga @vrv https://github.com/vrv
> @bennane @nouiz https://github.com/nouiz @Yangqing
> https://github.com/Yangqing @tqchen https://github.com/tqchen
> @unnonouno https://github.com/unnonouno
> 
> â€”
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/issues/101
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-210550872,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210551464,210551464,MDEyOklzc3VlQ29tbWVudDIxMDU1MTQ2NA==,2577440,2016-04-15T17:14:53Z,2016-04-15T17:14:53Z,NONE,"I agree that evaluate more aspects. Some of them is already covered in this proposal, for example
- Memory consumption
- Parallelization and Scheduling overhead

One of the most important factor, tradeoff between ease of use and optimization, was unfortunately not easy to benchmark as each people have their own taste.

What @Yangqing suggested is more on measuring perf for production and serving pipeline, which could be a whole area of new directions. As this benchmark was primarily on training. One alternative could be making a deep-serving benchmark on DeepMark organization that dedicate to this topic.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-210551464,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210551522,210551522,MDEyOklzc3VlQ29tbWVudDIxMDU1MTUyMg==,5012379,2016-04-15T17:14:59Z,2016-04-15T17:14:59Z,NONE,"It would be interesting to log the power dissipation in each testcase, as
well as fps, memory BW, FLOPS etc.
On 15 Apr 2016 00:41, ""Soumith Chintala"" notifications@github.com wrote:

> Hi all,
> 
> The reason I've been slow on convnet-benchmarks these days is because i've
> been working on the side on DeepMark.
> 
> I initially wrote _convnet-benchmarks_ to increase competition among
> frameworks so that we can work towards faster ConvNets, and they served
> their purpose well. After the release of convnet-benchmarks, multiple
> frameworks pulled up their socks to speedup convnets, with a deep sense of
> prestige for being on top of these benchmarks. In these two years, we as a
> community accelerated GPU ConvNets across all frameworks between 4x to 10x,
> efficiently implementing tricks such as FFT, Winograd, and powered by
> faster hardware. Alex Khrizevsky, Yangqing Jia, Scott Gray, Nicolas
> Vasilache, Sander Dieleman, Michael Mathieu, Julien Denmouth and many other
> human compilers helped make this a reality -- looking at the diversity in
> terms of where each of us work(ed) professionally shows that this kind of
> acceleration was truly a community effort with a ton of openness, something
> that is plain awesome! :)
> I've also enjoyed reading the deeply technical discussions that take place
> on convnet-benchmarks (my favorites: #93
> https://github.com/soumith/convnet-benchmarks/issues/93 , #66
> https://github.com/soumith/convnet-benchmarks/issues/66 , #59
> https://github.com/soumith/convnet-benchmarks/issues/59 in recent times
> ).< /p>
> 
> Moving on, _convnet-benchmarks_ do not accurately capture everything we
> think of when we say _deep learning_. We don't have Recurrent Nets, we
> don't have video use-cases, speech, NLP etc. There is a need for such
> comprehensive benchmarks, especially as the space is getting ready for
> dedicated hardware chips, multi-GPU and multi-machine frameworks and more
> complex use-cases.
> 
> I've sat down with a few of you at NIPS and GTC to discuss and freeze the
> initial round of benchmarks for what I am calling _DeepMark_. My initial
> plan was to work on the initial set of benchmark scripts by myself and
> cover the most popular frameworks, and then let the direction and
> maintenance of the benchmarks be community-driven. But the breadth of this
> effort has been overwhelming to say the least. After careful thought, I've
> decided that I'll just ask everyone to pitch in for their part of the
> benchmarks with making scripts etc., especially as many of you were very
> receptive to the idea offline.
> 
> Here are the initial set of use-cases we want to cover:
> Networks Images
> - InceptionV3-batchnorm (http://arxiv.org/abs/1512.00567 ,
>   https://github.com/Moodstocks/inception-v3.torch)
> - Alexnet-OWT
> - VGG
> - ResNet-50 ( http://arxiv.org/abs/1512.03385 ,
>   https://github.com/facebook/fb.resnet.torch )
> 
> Video
> - C3D - A vgg-style 3D net ( http://vlg.cs.dartmouth.edu/c3d/ )
> 
> Audio
> - DeepSpeech2 - Convnet + RNN + FC ( http://arxiv.org/abs/1512.02595 )
> - MSR's 5 layer FC net ( https://github.com/Alexey-Kamenev/Benchmarks )
> 
> Text
> - Small RNN LSTM (
>   https://github.com/karpathy/char-rnn/blob/master/train.lua#L38-L48 )
> - Large RNN LSTM ( BIG-LSTM in http://arxiv.org/abs/1602.02410 )
> 
> Platform
> - Initially multi-GPU with (1 to 4 titan-X cards)
> - However, multi-machine, custom hardware, other GPU cards such as
>   AMD, CPUs etc. can and should be accommodated, we will work this out after
>   the initial push.
> 
> Metrics
> - Round-trip time for 1 epoch of training (will define an epoch size
>   separately for each network)
> - Maximum batch-size that fits (to show and focus on the extra memory
>   consumption that the framework uses)
> 
> Frameworks
> 
> Everyone who wants to join-in, but I thought an initial set that is
> important to cover would be:
> - Caffe
> - Chainer
> - MXNet
> - Neon
> - Theano
> - TensorFlow
> - Torch
> 
> Scripts format
> - Emit JSON output (so that the README -- or jekyll website can be
>   auto-generated, similar to http://autumnai.com/deep-learning-benchmarks
>   )
> 
> Guarantees
> - I will personally to the best of my abilities make sure that the
>   benchmarking is fair and unbiased. The hope is that the community at large
>   will watch these and point-out / fix mistakes.
> 
> Governance
> - The benchmarks will be placed at https://github.com/DeepMark/deepmark
>   and other key community members / organizations who want ownership will be
>   welcome to join in proposing new benchmarks that get relevant as the field
>   progresses.
> 
> My hope is that these new set of benchmarks will not only increase
> competition but will also be beneficial in other ways to the community,
> serving as common examples to get started, etc.
> 
> Let me know what you think :)
> Soumith
> 
> cc: @hughperkins https://github.com/hughperkins @f0k
> https://github.com/f0k @scott-gray https://github.com/scott-gray
> @rajatmonga https://github.com/rajatmonga @vrv https://github.com/vrv
> @bennane @nouiz https://github.com/nouiz @Yangqing
> https://github.com/Yangqing @tqchen https://github.com/tqchen
> @unnonouno https://github.com/unnonouno
> 
> â€”
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/issues/101
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-210551522,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210564678,210564678,MDEyOklzc3VlQ29tbWVudDIxMDU2NDY3OA==,123560,2016-04-15T17:55:15Z,2016-04-15T17:55:15Z,CONTRIBUTOR,"> It would be interesting to log the power dissipation in each testcase

I like this idea.  A Titan draws 250watts peak (I think?).  24 hours a day for a year, 250watts is about ~600usd, which is in the same order of magnitude as the purchase price.

And power dissipation is going to become the main bottleneck plausibly in years to come. (""And over here we have our farm of 1000 Titan 2026s, and over there is the 20MW pebble bed we are using to power them"" :-) )
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-210564678,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210694677,210694677,MDEyOklzc3VlQ29tbWVudDIxMDY5NDY3Nw==,2020010,2016-04-16T00:19:09Z,2016-04-16T00:19:09Z,NONE,"@Yangqing 

> ""if I may make a bold claim, I believe that all frameworks will again very quickly converge to the same performance, because there is no fundamental difference between them.""

Agreed. Soumith's current benchmarks are useful, but they mainly evaluate ""who can make the thinnest wrapper around cuDNN, Neon, or similar?""

It would be useful to benchmark implementations of emerging algorithms for which tuned libraries may not yet exist -- certain versions of LSTMs and RNNs, for instance.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-210694677,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210695054,210695054,MDEyOklzc3VlQ29tbWVudDIxMDY5NTA1NA==,1310570,2016-04-16T00:20:34Z,2016-04-16T00:20:34Z,OWNER,"@forresti yea, for historical context it used to be not like that, but it is like that now. I think for LSTMs and RNNs, a lot of perf is still up for grabs.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-210695054,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210696718,210696718,MDEyOklzc3VlQ29tbWVudDIxMDY5NjcxOA==,123560,2016-04-16T00:24:39Z,2016-04-16T00:26:11Z,CONTRIBUTOR,"> Agreed. Soumith's current benchmarks are useful, but they mainly evaluate ""who can make the thinnest wrapper around cuDNN, Neon, or similar?""

To be fair, cudnn, neon are competing with each other.  The opencl implementations mostly copy the caffe cuda implementation of im2col as far as I know :-D  but have different performance from cudnn.  There is also 16-bit vs 32-bit.

(Edit, by the way, the cudnn vs neon comparison is exactly what comes to mind about power consumption.  I dont know if it's still the case, but as  far as I know it used to be the case that cudnn ran cooler than neon, and it'd be useful to be able to see this in the results)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-210696718,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210701872,210701872,MDEyOklzc3VlQ29tbWVudDIxMDcwMTg3Mg==,2020010,2016-04-16T01:06:36Z,2016-04-16T01:06:36Z,NONE,"@hughperkins Good point. I didn't mean to imply that there isn't a diverse array of low-level computational libraries for DNNs.

To tune up my comment a bit: ""When doing speed/efficiency benchmarks, it's hard to avoid conflating low-level computational libraries (cuDNN, Neon, various OpenCL efforts, ...) and higher-level frameworks (Caffe, Torch, Tensorflow, Theano, ...)."" 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-210701872,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210705590,210705590,MDEyOklzc3VlQ29tbWVudDIxMDcwNTU5MA==,9326960,2016-04-16T01:32:13Z,2016-04-16T01:32:13Z,NONE,"I would say that convolution is far from a solved problem.  I still have a long list of optimizations I want to make.  The biggest area to explore is how to best leverage lower precision without sacrificing accuracy.  The obvious target there would be xnor nets but maybe a bit more precision is best for the highest levels of accuracy.  The 4x int8 performance that Pascal will soon have (unfortunately not in P100 though) is a tantalizing format to target.  And also obviously the native fp16 support.

Another area is better efficiency at smaller batch sizes.  I have some brand new work there that I'd like to show off.  This is important for both inference and scaling to many nodes.

Power comparisons are useful but only when looking at implementations that have the same computational throughput.  Or just use some kind of flops/watt metric.  With my newer kernels I'm getting very good at squeezing the most out of cache utilization and hence I'm hitting and maintaining higher boost clocks (while using smaller and more versatile tiles).

As for the frameworks, the big area to focus on is graph based optimizations.  Maximizing data locality (compounding), memory allocation vs compute trade-offs, auto-parallelizing independent work across streams and gpus, and lots of other creative things computational graphs greatly simplify.

As for synthetic vs real data and parameters.. In fp32 I think only the distribution matters for performance comparisons.  But in lower precision like fp16 it's very easy to saturate or underflow with synthetic data which leads to far higher performance than is warranted.  At the very least you want to account for the fan-in when setting weight magnitudes (Kaiming, Xavier, etc).  Batch norm helps a lot here too.  Basically you should be able to prove that you can train with the params you benchmark with.

At the end of the day we care about speed and usability.   I think these benchmarks should make both pretty clear.  For usability you'll be able to inspect the script to see who has the cleanest syntax and solves the most problems for you without extra steps.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-210705590,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210727222,210727222,MDEyOklzc3VlQ29tbWVudDIxMDcyNzIyMg==,2020010,2016-04-16T03:48:31Z,2016-04-16T03:48:31Z,NONE,"@scott-gray That all sounds great. :)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-210727222,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210778421,210778421,MDEyOklzc3VlQ29tbWVudDIxMDc3ODQyMQ==,123560,2016-04-16T09:07:42Z,2016-04-16T09:07:42Z,CONTRIBUTOR,"> just use some kind of flops/watt metric

Well, the ideal would be joules per batch.  But I think this will be tricky to measure.  Might need some specialized hardware device, that sits on the power bus?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-210778421,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210782546,210782546,MDEyOklzc3VlQ29tbWVudDIxMDc4MjU0Ng==,9326960,2016-04-16T09:44:06Z,2016-04-16T09:44:57Z,NONE,"Maybe it wouldn't be quite so tricky.  You'd just need to collect some running average of the on chip power stats during the execution of the epoch.  Something like this would give you realtime stats:

`nvidia-smi -i 1 --loop-ms=333 --format=csv,noheader --query-gpu=power.draw,clocks.gr,temperature.gpu,fan.speed,clocks_throttle_reasons.sw_power_cap,clocks_throttle_reasons.hw_slowdown`

Or even better tie your benchmark script directly into NVML queries: 
https://developer.nvidia.com/nvidia-management-library-nvml

But I guess you'd want to be running these queries continuously so maybe as a separate process would be better.  You'd just need to synchronize the collection with the execution of the network.  Just a small bit of shell scripting should achieve this.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-210782546,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210783850,210783850,MDEyOklzc3VlQ29tbWVudDIxMDc4Mzg1MA==,123560,2016-04-16T09:54:32Z,2016-04-16T09:54:32Z,CONTRIBUTOR,"> Or even better tie your benchmark script directly into NVML queries:
> https://developer.nvidia.com/nvidia-management-library-nvml

Interesting.  Seems it's just a c-interface, so accessible using ffi etc.

```
nvmlDeviceGetPowerUsage(nvmlDevice_t device, unsigned int *power);
```
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-210783850,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210784139,210784139,MDEyOklzc3VlQ29tbWVudDIxMDc4NDEzOQ==,9326960,2016-04-16T09:58:53Z,2016-04-16T09:58:53Z,NONE,"And python bindings can be found here:
https://pypi.python.org/pypi/nvidia-ml-py
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-210784139,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210786173,210786173,MDEyOklzc3VlQ29tbWVudDIxMDc4NjE3Mw==,9326960,2016-04-16T10:23:06Z,2016-04-16T10:23:06Z,NONE,"But, it's worth pointing out that the boost clock is already tightly coupled with these real-time power and temperature measurements so the overall timings should be reflective of this.  So perhaps it's not worth the effort.  
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-210786173,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/211065747,211065747,MDEyOklzc3VlQ29tbWVudDIxMTA2NTc0Nw==,1710528,2016-04-17T17:24:47Z,2016-04-17T17:24:47Z,NONE,"/cc @naibaf7
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-211065747,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/211066934,211066934,MDEyOklzc3VlQ29tbWVudDIxMTA2NjkzNA==,5577650,2016-04-17T17:38:06Z,2016-04-17T17:48:45Z,NONE,"@soumith 
Great! I'll make sure to finish my OpenCL cuDNN replacement before initial benchmarking.
Can I ask to include also AMD and Intel GPU (+CPU) benchmarks (where applicable and possible)?

It is also important to me to benchmark not only networks that saturate the GPU load with a big minibatch size but also those that manage to fill GPU memory and exhaust them with only a batch size of N=1. A good example to use here is the U-Net.

Here a GFLOPs profile of a typical U-Net implementation:
![layerefficiency_unet-1](https://cloud.githubusercontent.com/assets/5577650/14588880/e4420b12-04d4-11e6-9052-4446b5457b1e.png)

Implementation here is Caffe, im2col (no cuDNN),
CUDA card: GTX 980, similar to Titan X
AMD card: W9100
Intel CPU: i7 4790K

It currently only works on Caffe, but TensorFlow will also support it once Olaf Ronneberger makes it available.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-211066934,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/211067215,211067215,MDEyOklzc3VlQ29tbWVudDIxMTA2NzIxNQ==,1310570,2016-04-17T17:42:40Z,2016-04-17T17:42:40Z,OWNER,"@naibaf7 some of the benchmarks wont be able to run on the AMD cards which right now seem to be limited to 4GB variants. I have a Fury Nano that I've started playing with. Are Intel GPU benchmarks even relevant? I can run Intel GPU benchmarks on like a macbook pro or something. Intel CPU, def can run.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-211067215,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/211068909,211068909,MDEyOklzc3VlQ29tbWVudDIxMTA2ODkwOQ==,1710528,2016-04-17T17:47:04Z,2016-04-17T17:47:04Z,NONE,"/cc @michaellarabel
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-211068909,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/211069098,211069098,MDEyOklzc3VlQ29tbWVudDIxMTA2OTA5OA==,5577650,2016-04-17T17:47:37Z,2016-04-17T17:50:34Z,NONE,"@soumith 
A single buffer can't be bigger than 1/4th of the GPUs memory on current AMD GPUs. I think the Fury Nano is not the right candidate to benchmark with.

I have a set of W9100 cards with 16 GB memory that would be more suitable to benchmark AMD.

@soumith please see my updated post above for additional information :)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-211069098,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/211071931,211071931,MDEyOklzc3VlQ29tbWVudDIxMTA3MTkzMQ==,5577650,2016-04-17T17:55:20Z,2016-04-17T17:55:20Z,NONE,"My idea to include a U-Net like architecture also ties in with @moloned 's suggestion.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-211071931,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/211072075,211072075,MDEyOklzc3VlQ29tbWVudDIxMTA3MjA3NQ==,123560,2016-04-17T17:55:37Z,2016-04-17T17:55:37Z,CONTRIBUTOR,"@naibaf7 

W9100s are 2500usd each.  You'd need performance to be ~2.5 times faster than Titan X for the price/performance ratio to be attractive.  R9 Nanos are 500usd a pop, so you could get two for the price of one Titan X, giving you 8GB total memory, and then use alexnet-style parallelism over these?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-211072075,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/211072776,211072776,MDEyOklzc3VlQ29tbWVudDIxMTA3Mjc3Ng==,5577650,2016-04-17T17:57:39Z,2016-04-17T18:10:39Z,NONE,"@hughperkins 
If we limit the memory usage to 8 GB the W9100 should perform similarly to R9 390X, which have even higher clock speeds than a W9100 and are clocking in at only 450usd each, roughly 5x less than a W9100. That would be fine at even 2-3x slower than a Titan X (not regarding power consumption).
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-211072776,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/211072837,211072837,MDEyOklzc3VlQ29tbWVudDIxMTA3MjgzNw==,123560,2016-04-17T17:58:42Z,2016-04-17T18:08:24Z,CONTRIBUTOR,"Yes. r9-390x are nice :-)

(Edit: it would be really nice if someone could make cloud-available r9-390x's.  Since every other cloud offering right now is CUDA, they would at least be unique.)

Edit2: I think it'd be totally fair to use two r9-390x gpus, in a comparison against one Titan X.  By the way, Amazon shows r9-390x at 450usd, not 250usd http://www.amazon.com/s?ie=UTF8&page=1&rh=i%3Aaps%2Ck%3Ar9%20390x , but that still means two of them are 100usd cheaper than one Titan X.  (or even 3 r9-390x for that matter, Amazon is showing Titan X for around ~1500usd http://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=titan+X&rh=i%3Aaps%2Ck%3Atitan+X   )  (edited edit2 a bunch for typos)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-211072837,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/211074285,211074285,MDEyOklzc3VlQ29tbWVudDIxMTA3NDI4NQ==,5577650,2016-04-17T18:08:53Z,2016-04-17T18:08:53Z,NONE,"@soumith 
Oh right. My fault on the price conversion. Corrected it above.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-211074285,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/211074803,211074803,MDEyOklzc3VlQ29tbWVudDIxMTA3NDgwMw==,123560,2016-04-17T18:14:00Z,2016-04-17T18:15:15Z,CONTRIBUTOR,"> re: power consumption

Ah, hmmm, thats a good point. looks like one r9-390x uses about the same power as a titan x http://www.guru3d.com/articles-pages/msi-radeon-r9-390x-gaming-8g-oc-review,8.html  Anyway, seems like a nice card to benchmark on, as long as the benchmarks are tweaked to fit in memory (most of them will run in 8GB I think, with slightly smaller batch sizes?)  (edit: corrected link addrss)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-211074803,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/211619941,211619941,MDEyOklzc3VlQ29tbWVudDIxMTYxOTk0MQ==,8671412,2016-04-18T22:57:18Z,2016-04-18T22:57:18Z,NONE,"Do you have a reference to the paper for MSR's 5 layer FC net?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-211619941,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/211702398,211702398,MDEyOklzc3VlQ29tbWVudDIxMTcwMjM5OA==,1121581,2016-04-19T02:43:37Z,2016-04-19T02:43:37Z,NONE," AFAIK it's just a model for benchmarking, probably derived from some of their fully-connected speech recognition nets. It's similar to the net MSR used in the 1-bit SGD paper for speech recognition (http://research.microsoft.com/pubs/230137/IS140694.PDF), but not identical.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-211702398,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/214765889,214765889,MDEyOklzc3VlQ29tbWVudDIxNDc2NTg4OQ==,123560,2016-04-26T14:35:41Z,2016-04-26T14:35:41Z,CONTRIBUTOR,"Observations:
- I reckon it'd be cool if the benchmarks are automated, so we can submit a github repo, and a branch name, and push changes to that, and the results will either be generated automatically, or eg once a week.
- Personally, I doubt I'm going to be able to support so many benchmarks (at least initially), so I think it'd be nice if eg a framework could submit initially only for eg image recognition benchmarks, and then could elect at some later date to submit to one or more additional domains.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-214765889,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/214773392,214773392,MDEyOklzc3VlQ29tbWVudDIxNDc3MzM5Mg==,8753078,2016-04-26T14:58:43Z,2016-04-26T14:58:43Z,NONE,"> but from my own experience with the LSTM benchmark it can be very difficult - you have to make sure literally every hyperparameter is identical, and you effectively can't use any RNGs. 

@craffel, couldn't get what do you meant by this. If we are not worried about convergence and look at only the speed, I don't see a real issue with hyperparameters as long as all nets are implemented based on the same pseudocode. Also, I think over time many frameworks will shift to using cuDNN RNN primitives.

I hope there can be some way of comparing the extent of support for recurrence offered by the framework - like presence of `scan(...)`, handling variable length sequences etc

About comparing usability: I  think its better not to compare frameworks wrt usability as there is no simple way to quantify the variations among frameworks. Syntax could look clean because of a lot of in-built APIs for standard things **now** but for ""research"", these may not help much. Mental overhead while writing these scripts also may have to be considered. So, it is not easy to capture and convey all of these aspects in a meaningful and an unbiased way. In the list of frameworks above, all but Caffe have a numpy-like API for manipulating Tensors in the most common ways and a mention of it somewhere should suffice.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-214773392,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/214774506,214774506,MDEyOklzc3VlQ29tbWVudDIxNDc3NDUwNg==,417568,2016-04-26T15:02:04Z,2016-04-26T15:03:27Z,NONE,"> @craffel, couldn't get what do you meant by this

This was in response to

> One thing I've also been thinking about like @daviddao is how to validate that the models are actually computing the same thing -- I've seen some benchmarks elsewhere that have raised personal doubts that the frameworks are computing the same function.

I was saying that in order to determine that the networks are computing _exactly_ the same thing, the initialization must be _exactly_ the same, etc., which was taking this point to the extreme.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-214774506,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/214776919,214776919,MDEyOklzc3VlQ29tbWVudDIxNDc3NjkxOQ==,8753078,2016-04-26T15:08:15Z,2016-04-26T15:08:15Z,NONE,"@craffel, I thought you were saying something specific about LSTM, sorry, my bad.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-214776919,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/215262456,215262456,MDEyOklzc3VlQ29tbWVudDIxNTI2MjQ1Ng==,8460517,2016-04-27T23:25:05Z,2016-04-27T23:25:05Z,NONE,"@soumith thank you for continuing to lead this valuable benchmark project, which benefits the entire machine learning community.

+1 for Intel GPUs. They are potentially very relevant for inference. In a few weeks you will be able to buy an Intel Skull Canyon NUC system for $999 that has 2.3 TFLOPS fp16 on the GPU and integration with a 4-core CPU, at 45 Watts, with 6MB on-chip coherent cache, 128MB on-package last level cache, 16 GB DDR4, and 256GB SSD [1][2][3]. I would guess the total system power is under 150 Watts, as it is intended to compete with game consoles. This platform could in theory compare favorably with a system equipped with a Tesla M4 GPU [4], both in terms of inference throughput and total system power. Of course Skull Canyon was not designed for the data center, but neither was the GTX 980. We can look past this to the next generation of AMD Zen APUs which according to PS4K and Xbox rumors should raise the performance bar quite a bit higher.

[1] http://www.anandtech.com/show/10152/intels-skull-canyon-nuc-is-officia
[2] http://ark.intel.com/products/93341/Intel-Core-i7-6770HQ-Processor-6M-Cache-up-to-3_50-GHz
[3] http://wccftech.com/intel-iris-pro-graphics-gamers/
[4] http://images.nvidia.com/content/tesla/pdf/nvidia-tesla-m4-datasheet.pdf
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-215262456,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/215310684,215310684,MDEyOklzc3VlQ29tbWVudDIxNTMxMDY4NA==,1310570,2016-04-28T05:02:53Z,2016-04-28T05:02:53Z,OWNER,"After talking to Rajat from the TensorFlow team, we thought that an automated Jenkins server will go a long way in maintaining these benchmarks, as well as benchmarking on new hardware. Rajat and Martin Wicke are helping set this stuff up here :)
After the initial setup gets going on a digits box, we can figure out how to add donated machines with different configurations from the community as slaves to the jenkins server.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-215310684,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/215310722,215310722,MDEyOklzc3VlQ29tbWVudDIxNTMxMDcyMg==,15679194,2016-04-28T05:03:17Z,2016-04-28T05:03:17Z,NONE,"@hughperkins : @martinwicke is working with @soumith to set something like this up through our Jenkins setup. The idea would be to run these tests automatically every X days or so.

@andravin @naibaf7  I like the idea of running on other relevant hardware e.g. Intel and AMD GPUs that make sense. Not all libraries may support every vendor, but will be useful to have baselines from even a few.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-215310722,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/217732001,217732001,MDEyOklzc3VlQ29tbWVudDIxNzczMjAwMQ==,8460517,2016-05-08T16:45:04Z,2016-05-08T16:45:04Z,NONE,"@soumith Are accuracy / convergence tests back on the table? Per layer numeric accuracy and iterations to / accuracy of convergence are both informative measures. We do not really understand in detail how the former affects the latter, so both would be interesting.

All floating point calculations are approximations. fp16 quantization is an explicit approximation. Fast algorithms have different accuracy than naive algorithms. Bugs are very unstable approximations. ;-)

I have considered submitting a closed source ""fp0"" kernel to convnet-benchmarks. Only after results are released will I reveal that it just fills the output buffer with zeros. Then maybe everybody would appreciate the importance of accuracy tests.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-217732001,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/217732394,217732394,MDEyOklzc3VlQ29tbWVudDIxNzczMjM5NA==,1310570,2016-05-08T16:48:14Z,2016-05-08T16:48:14Z,OWNER,"@andravin accuracy / convergence tests are off the table atleast for the first deepmark release. There's simply not enough bandwidth / man-power to pull it off.

> I have considered submitting a closed source ""fp0"" kernel to convnet-benchmarks. Only after results are released will I reveal that it just fills the output buffer with zeros. Then maybe everybody would appreciate the importance of accuracy tests.

Convnet-benchmarks and deepmark are done in good faith and really are a tool for the community built by the community. There are several things that can be done to show an adversarial setting, but that would be pointless. Each of these frameworks are also used widely in other settings, making them have an implicit accuracy test.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-217732394,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/217734719,217734719,MDEyOklzc3VlQ29tbWVudDIxNzczNDcxOQ==,123560,2016-05-08T17:23:02Z,2016-05-08T17:23:02Z,CONTRIBUTOR,"@andravin 

If DeepMark works like convnet-benchmark worked, and I have no reason to think that it wont, then all the scripts are publicly available, in a relatively standard-ish format.  There is no reason why you couldnt set up a parallel repository, which runs accuracy tests against each library, and reports the results.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-217734719,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/217734949,217734949,MDEyOklzc3VlQ29tbWVudDIxNzczNDk0OQ==,123560,2016-05-08T17:27:47Z,2016-05-08T17:28:16Z,CONTRIBUTOR,"(actually, in general, in my opinion, it would be better to factorize deepmark into per-domain repos, like images, nlp, video, etc, and get them working one by one.  Otherwise it risks becoming a bit of a Doom3 :-D  Just my opinion though.... ).

(Edit: do I mean Doom3?  Do I mean Duke Nukem Forever?  Anyway, either way...)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-217734949,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/217739917,217739917,MDEyOklzc3VlQ29tbWVudDIxNzczOTkxNw==,8460517,2016-05-08T19:06:02Z,2016-05-08T19:06:02Z,NONE,"@soumith I have been a software engineer for too long to believe that good intentions are enough to ensure accuracy. ;-) I do believe that rewarding frameworks for speed without regard for accuracy creates a moral hazard.

In order to believe that the community is taking care of convergence testing, I would want to see links to some learning curves for recent Imagenet winners (not Alexnet, a recent very deep model from the last year) using fp16, FFT, and Winograd kernels.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-217739917,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/217740215,217740215,MDEyOklzc3VlQ29tbWVudDIxNzc0MDIxNQ==,1710528,2016-05-08T19:11:01Z,2016-05-08T19:11:01Z,NONE,"I think that also a miracle for a standard definition of a network specification and its weights could really help the whole ecosystem. Tensorflow is in a position that could try to keep the responsibility of the leadership for a standard with versioning but probably everybody want to eat its own food.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-217740215,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/217740818,217740818,MDEyOklzc3VlQ29tbWVudDIxNzc0MDgxOA==,123560,2016-05-08T19:22:57Z,2016-05-08T19:22:57Z,CONTRIBUTOR,"> I think that also a miracle for a standard definition of a network specification and its weights could really help the whole ecosystem.

Just as for andravin's request, this could also be done by an individual contributor, orthogonally to all other efforts: simply pick one format, or make up a new one (obligatory xckd :-D https://m.xkcd.com/927/ ), and create converters to/from the most important frameworks. But ... isnt caffe zoo kind of already a de facto standard for this?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-217740818,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/217741134,217741134,MDEyOklzc3VlQ29tbWVudDIxNzc0MTEzNA==,1710528,2016-05-08T19:29:27Z,2016-05-08T19:29:27Z,NONE,"So protobuf is the de facto standard? I believe only in something more formal where all major framework are involved by design without to/from converters (or only optional for back compatibility). Cudnn API in some way is the only de facto standard.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-217741134,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/217741314,217741314,MDEyOklzc3VlQ29tbWVudDIxNzc0MTMxNA==,9326960,2016-05-08T19:32:53Z,2016-05-08T19:32:53Z,NONE,"For training with existing fp16 kernels you'll likely need a few tricks.  To allow weight updates to proceed there needs to be enough overlap in mantissa and the weight for the summation to have any effect.   Batch norm helps a lot here.  You can also use stochastic rounding, which allows weight updates even when there is no effective overlap.  But probably the best way is to leverage the fact that the filter tensors are relatively small and just keep them in fp32.  You can quantize just prior to the conv operation.  If you're already doing some kind of transform or dimshuffle you can just bake that quantization into that at no cost.

Batch norm also helps a lot with keeping the activations and delta's bounded within the range that fp16 can represent.  Weight initialization that factors in the fan-in also helps here.  Winograd transforms tend to widen the dynamic range of the activations/deltas so that's something to look out for too.

On upcoming pascal hardware we'll really need to fully explore the limits of fp16 accumulate.  I'm guessing some of the faster winograd transforms might break down here.  It's a shame that nvidia didn't make that fp16x2 instruction an inner product accumulate like the new int8x4 instruction will be.  With that you could multiply two sets of fp16 values and accumulate to a single fp32 register.

Anyway, I think with a bit of care you can generally achieve exactly the same accuracy with fp16 as you can with fp32.  I've even seen the reduced precision act as a regularizer to increase accuracy.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-217741314,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/217742271,217742271,MDEyOklzc3VlQ29tbWVudDIxNzc0MjI3MQ==,123560,2016-05-08T19:51:54Z,2016-05-08T19:51:54Z,CONTRIBUTOR,"> So protobuf is the de facto standard? I believe only in something more formal where all major framework are involved by design without to/from converters

Well, I'm not sure about the 'without to/form converters' part, but torch [already supports protobuf](https://github.com/szagoruyko/loadcaffe).  As does caffe.  mxnet https://github.com/dmlc/mxnet/blob/master/tools/caffe_converter/README.md tensorflow https://github.com/ethereon/caffe-tensorflow
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-217742271,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/217742729,217742729,MDEyOklzc3VlQ29tbWVudDIxNzc0MjcyOQ==,1710528,2016-05-08T20:00:02Z,2016-05-08T20:00:02Z,NONE,"Yes but every framework write whatever they want with protobuf.
Converters don't avoid framework feature disaligment. Actually in production seems that cudnn versions defines what it is really important to introduce in API. It could be really better that with a common format this could be defined by design by frameworks i.e. [like in containers domain](https://www.opencontainers.org)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-217742729,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/217743289,217743289,MDEyOklzc3VlQ29tbWVudDIxNzc0MzI4OQ==,123560,2016-05-08T20:11:26Z,2016-05-08T20:11:26Z,CONTRIBUTOR,"> Actually in production seems that cudnn versions defines what it is really important to introduce in API. It could be really better that with a common format this could be defined by design by frameworks i.e. like in containers domain

Without having looked at it (because I dont want to be exposed to the IP, if I can avoid it), I would think that the cudnn interfaces are very good.  However, note that to see their api, as far as I know, you have to click through a licensing agreement saying you understand it is their IP and so on, so I'm not sure that one could use the cudnn API as a standard?  (unless you could get agreement from NVIDIA of course).  By the way, you are hiding behind a pseudonym, I am curious what is your real name :-)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-217743289,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/217744107,217744107,MDEyOklzc3VlQ29tbWVudDIxNzc0NDEwNw==,1710528,2016-05-08T20:26:12Z,2016-05-08T20:26:12Z,NONE,"I will like a common roundtable like for containers format hosted by linux foundation.
I'm sure that this format need to be always a little bit behind state of the art but still very helpful in production. 
Probably @openai could has the ""neutral status"" to host a multi stakeholders design and maintenance.

P.s. I'm a simple GSoC 2016 mentor for OpenCV foundation :)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-217744107,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/217744904,217744904,MDEyOklzc3VlQ29tbWVudDIxNzc0NDkwNA==,123560,2016-05-08T20:42:36Z,2016-05-08T20:42:36Z,CONTRIBUTOR,"> Probably @openai could has the ""neutral status"" to host a multi stakeholders design and maintenance.

Interesting
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-217744904,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/217745038,217745038,MDEyOklzc3VlQ29tbWVudDIxNzc0NTAzOA==,577277,2016-05-08T20:45:10Z,2016-05-08T20:45:10Z,NONE,"I am doubtful that the sourceforge (sic, really?) thing is related to
openai (https://openai.com)
On Sun, May 8, 2016 at 13:42 Hugh Perkins notifications@github.com wrote:

> Probably @openai https://github.com/openai could has the ""neutral
> status"" to host a multi stakeholders design and maintenance.
> 
> Interesting
> 
> â€”
> 
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-217744904
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-217745038,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/217745062,217745062,MDEyOklzc3VlQ29tbWVudDIxNzc0NTA2Mg==,123560,2016-05-08T20:45:34Z,2016-05-08T20:45:34Z,CONTRIBUTOR,"> I am doubtful that the sourceforge (sic, really?) thing is related to

yeah, hence why I deleted my original comment :-D
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-217745062,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/217745137,217745137,MDEyOklzc3VlQ29tbWVudDIxNzc0NTEzNw==,577277,2016-05-08T20:47:00Z,2016-05-08T20:47:00Z,NONE,"Sorry. Working off emails, didn't see. :)
On Sun, May 8, 2016 at 13:45 Hugh Perkins notifications@github.com wrote:

> I am doubtful that the sourceforge (sic, really?) thing is related to
> 
> yeah, hence why I deleted my original comment :-D
> 
> â€”
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-217745062
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-217745137,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/217745469,217745469,MDEyOklzc3VlQ29tbWVudDIxNzc0NTQ2OQ==,8460517,2016-05-08T20:53:53Z,2016-05-08T20:53:53Z,NONE,"@scott-gray I am sure you know the difference between an idea that seems like it should work and an experimental result that is publicly available.

> In order to believe that the community is taking care of convergence testing, I would want to see links to some learning curves for recent Imagenet winners (not Alexnet, a recent very deep model from the last year) using fp16, FFT, and Winograd kernels.

I am a _little_ bit surprised that nobody addressed this point.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-217745469,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/217745847,217745847,MDEyOklzc3VlQ29tbWVudDIxNzc0NTg0Nw==,9326960,2016-05-08T21:01:10Z,2016-05-08T21:01:10Z,NONE,"@andravin I agree that we need this.   It's just hard to make it a priority over other things.  But I think with pascal coming out, there will be a real push to make fp16 work since the potential speedups will be so much more dramatic than they are now.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-217745847,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/217746846,217746846,MDEyOklzc3VlQ29tbWVudDIxNzc0Njg0Ng==,2020010,2016-05-08T21:20:18Z,2016-05-08T21:20:18Z,NONE,"In terms of a standard format for representing DNN architectures and weights:

There is a tension between research and standardization. As mentioned above, there is some native support and/or conversion scripts for protobuf representations among caffe, torch, tensorflow, and mxnet. But, as research charges ahead, people in each framework/community are trying different types of new layers, solvers, weight initialization schemes, etc. Convolutions and SGD are very popular, but the finer points are still in flux. It may be too early to be worth trying to rigidly standardize all the frameworks.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-217746846,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/217747408,217747408,MDEyOklzc3VlQ29tbWVudDIxNzc0NzQwOA==,1710528,2016-05-08T21:31:27Z,2016-05-08T21:31:27Z,NONE,"@forresti Exactly what I have told. Standard need to stay behind state of the art. Is cudnn really cutting edge (excluding the performance point of view)?
No, arxiv is cutting edge, cudnn is behind and probably production. See also [Openvx at Kronos](https://www.khronos.org/openvx/) for CV.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-217747408,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/217750850,217750850,MDEyOklzc3VlQ29tbWVudDIxNzc1MDg1MA==,2577440,2016-05-08T22:47:13Z,2016-05-08T22:47:13Z,NONE,"echo @bhack I have some thoughts in turns of standardize the frameworks. It is fundamentally hard for all frameworks to be forced into one. 

However, we might have enough idea on what are the common interface in a system that could be shared across frameworks, CuDNN is a perfect example of these. The possible things I can think of includes:
- Operator interface for registering kernels on different types of devices.
- Dependency scheduler to schedule things across devices.
- Data loading module to keep up with pre-processing pipeline.
- Representation of computation graph.
- Storage format of tensors.

I can see the finer we can break them, there is a better chance to see chance of sharing of some modules between frameworks, and move the field as a whole. I would love to see such discussion happen, and possibly in a neutral way
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-217750850,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/217751468,217751468,MDEyOklzc3VlQ29tbWVudDIxNzc1MTQ2OA==,123560,2016-05-08T23:00:53Z,2016-05-08T23:01:53Z,CONTRIBUTOR,"> However, we might have enough idea on what are the common interface in a system that could be shared across frameworks, CuDNN is a perfect example of these.

On a somewhat related note, I was thinking about how we could get portable dnn libraries, across multiple hardware, not just cuda hardware. We already have portable implementations of caffe, torch, maybe some others, by writing the kernels in OpenCL. However, the performance of the convolutions is arguably not as fast as one might achieve by using the hardware-level optimizations in winograd, and perhaps cudnn.

I'm thinking that it might be useful to have an api at exactly the cudnn level, so that libraries will automatically call cudnn on cuda platforms, or eg greentea on amd platforms, or some other convolutional library on eg Intel platforms. I made a pretty slide of this here :-)

![openclconv1c](https://cloud.githubusercontent.com/assets/123560/15100939/91e10284-1581-11e6-80b5-c841ce045458.png)

And note that this architecture would say nothing about who writes the hw-level implementation, eg one could imagine that the end-user could install winograd kernels on their machine, and caffe, mxnet, torch etc would automatically make use of them, without the dnn library authors even needing to know about this, or compile-time link with them:

![openclconv2](https://cloud.githubusercontent.com/assets/123560/15100940/990e3810-1581-11e6-8bbf-e1abde7c0442.png)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-217751468,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/217751775,217751775,MDEyOklzc3VlQ29tbWVudDIxNzc1MTc3NQ==,5577650,2016-05-08T23:08:15Z,2016-05-08T23:08:15Z,NONE,"@hughperkins 
Thanks for including my efforts :) not quite there yet with performance (writing the autotuning code at this instant).

I think this is a very nice idea. At least the convolutions should be somewhat standardized as they are the most performance critical part and about the same in every framework.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-217751775,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/217752170,217752170,MDEyOklzc3VlQ29tbWVudDIxNzc1MjE3MA==,178249,2016-05-08T23:18:15Z,2016-05-08T23:18:15Z,NONE,"> Well, I'm not sure about the 'without to/form converters' part, but torch [already supports protobuf](https://github.com/szagoruyko/loadcaffe).  As does caffe.  mxnet https://github.com/dmlc/mxnet/blob/master/tools/caffe_converter/README.md tensorflow https://github.com/ethereon/caffe-tensorflow

[sklearn-theano](https://github.com/sklearn-theano/sklearn-theano) also has a [converter](https://github.com/sklearn-theano/sklearn-theano/blob/master/sklearn_theano/feature_extraction/caffe/caffemodel.py).

## 

Pascal
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-217752170,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/217940603,217940603,MDEyOklzc3VlQ29tbWVudDIxNzk0MDYwMw==,8460517,2016-05-09T18:01:56Z,2016-05-09T18:01:56Z,NONE,"While I find the discussion of standardization interesting and important, I really do not need any of that to do basic convergence testing.

For example, if I had Neon configuration files for Resnet or Inception that were known to give state of the art results when trained with fp32 direct convolution kernels, and access to a multi-GPU machine, then I could easily substitute the Winograd fp32 F(2x2,3x3) kernels for all the convolutions and compare the learning curve to the original. As simple as this experiment is, nobody has ever reported these results. Then you run the experiment again with F(4x4,3x3) kernels.

As @scott-gray mentioned, switching to fp16 weights might need some extra tricks. But all of this could be done within Neon.

Likewise the comparison of cuDNN direct convolution fp32, direct convolution fp16, and FFT should be possible entirely within TensorFlow.

Comparison of results between different frameworks might be a bit of an apples to oranges comparison, but within-framework comparison of different algorithms gives us a measure of the usefulness of state of the art kernels. Also it would force us to nail down the tricks that make them perform best.

I would think it is in the best interest of framework maintainers to facilitate these experiments.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-217940603,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/218800333,218800333,MDEyOklzc3VlQ29tbWVudDIxODgwMDMzMw==,1710528,2016-05-12T15:50:27Z,2016-05-12T15:50:27Z,NONE,"This @vrv [comment](https://github.com/tensorflow/tensorflow/pull/2002#issuecomment-218783825) it is quite auto explicative of some of the standardization posts in this thread 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-218800333,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/218802843,218802843,MDEyOklzc3VlQ29tbWVudDIxODgwMjg0Mw==,463737,2016-05-12T15:58:37Z,2016-05-12T15:58:37Z,CONTRIBUTOR,"1) eh, software is tricky, nothing to do with standardization
2) this conversation has ratholed from its original intent to discuss benchmarks, though the discussion is interesting.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-218802843,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/218822492,218822492,MDEyOklzc3VlQ29tbWVudDIxODgyMjQ5Mg==,1710528,2016-05-12T17:08:53Z,2016-05-12T18:19:16Z,NONE,"1) Yes but why all the frameworks need to do it on cudnn? :)
2) Sure but there is no space for this cause frameworks still don't has started a discussion on this topic.
3) One hardware vendor it is doing a standard de facto at some level.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-218822492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/218937969,218937969,MDEyOklzc3VlQ29tbWVudDIxODkzNzk2OQ==,8460517,2016-05-13T02:33:51Z,2016-05-13T02:33:51Z,NONE,"@vrv Convergence speed vs. accuracy tests _are_ benchmarks. But the idea has no traction here so I will do it somewhere else.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-218937969,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/218938179,218938179,MDEyOklzc3VlQ29tbWVudDIxODkzODE3OQ==,1310570,2016-05-13T02:35:45Z,2016-05-13T02:35:45Z,OWNER,"@andravin they ARE benchmarks, and actually my early versions of deepmark iterated on convergence speed. But this requires way more effort than is available, so i converged onto something a bit cheaper overall...
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-218938179,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/218938416,218938416,MDEyOklzc3VlQ29tbWVudDIxODkzODQxNg==,463737,2016-05-13T02:37:56Z,2016-05-13T02:37:56Z,CONTRIBUTOR,"My comment on ratholing was about standardization of APIs and frameworks -- I do think convergence speed and accuracy tests are important and we should eventually have them.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-218938416,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/218977082,218977082,MDEyOklzc3VlQ29tbWVudDIxODk3NzA4Mg==,1710528,2016-05-13T08:01:29Z,2016-05-13T08:01:29Z,NONE,"Yes we have diverted the discussion at some point. I hope that the framework with more starts on github on day could restart that topic in a more appropriate context than this one.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-218977082,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/219149740,219149740,MDEyOklzc3VlQ29tbWVudDIxOTE0OTc0MA==,8460517,2016-05-13T20:22:51Z,2016-05-13T20:22:51Z,NONE,"@soumith yes convergence tests of course require a lot of compute time. can we at least incorporate numeric accuracy tests with synthetic data, as I think @vrv mentioned earlier?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-219149740,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/219152473,219152473,MDEyOklzc3VlQ29tbWVudDIxOTE1MjQ3Mw==,1310570,2016-05-13T20:35:07Z,2016-05-13T20:35:07Z,OWNER,"@andravin no. not in V1.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-219152473,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/219154758,219154758,MDEyOklzc3VlQ29tbWVudDIxOTE1NDc1OA==,9326960,2016-05-13T20:45:53Z,2016-05-13T20:45:53Z,NONE,"One point about synthetic accuracy tests is that it doesn't necessarily correspond to final test accuracy.  As I was saying earlier, low precision can sometimes produce better results.  I'll quote the conclusion of this [paper](http://arxiv.org/pdf/1510.03009v3.pdf) here:

> Low precision prevents the optimizer from finding solutions that require a lot of precision, which correspond to very thin (high curvature) critical points, and these minima are more likely to correspond to overfitted solutions then broad minima (there are more functions that are compatible with such solutions, corresponding to a smaller description length and thus better generalization).
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-219154758,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/219155930,219155930,MDEyOklzc3VlQ29tbWVudDIxOTE1NTkzMA==,463737,2016-05-13T20:51:18Z,2016-05-13T20:51:18Z,CONTRIBUTOR,"Step 1: define important models / benchmarks
Step 2: make sure people's definitions of models match so the numbers are actually comparable
Step 3: care and compare multi-step convergence

I don't think you can have 2 without 1, and 3 without 2.  So the order @soumith is pursuing this seems fine.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-219155930,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/219173509,219173509,MDEyOklzc3VlQ29tbWVudDIxOTE3MzUwOQ==,8460517,2016-05-13T22:22:18Z,2016-05-13T22:22:50Z,NONE,"The approach I would take is:

A) kernel efficiency versus numeric accuracy testing
B) whole network convergence A/B testing (vary one of 1. kernel, 2. precision, 3. batch size, etc...). 
C) framework profiling (measure time spent in low level kernels and compare with total run time).

I think that will give us a more detailed picture of the factors that contribute to neural network performance.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-219173509,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/219175620,219175620,MDEyOklzc3VlQ29tbWVudDIxOTE3NTYyMA==,1310570,2016-05-13T22:36:13Z,2016-05-19T18:18:27Z,OWNER,"So, getting back to the discussion. 
I have volunteers from a few frameworks, looking for some.

# Volunteers available

Torch: Soumith, @SeanNaren , Shubo Sengupta (Baidu)
TensorFlow: @vrv and @martinwicke 
Neon: Evren Tumer @nervetumer
Chainer: @delta2323 and team
MXNet: Edit: @antinucleon will do it
Theano / Lasagne / Keras: @f0k and @pranv (and maybe @craffel )
Caffe / NVIDIA-Caffe: [Need volunteers] @thatguymike and team at NVIDIA

# Volunteers needed

N/A

If I dont find volunteers for some frameworks, I'll (without guarantee) try to do them myself.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-219175620,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/219177998,219177998,MDEyOklzc3VlQ29tbWVudDIxOTE3Nzk5OA==,1258974,2016-05-13T22:53:18Z,2016-05-13T22:53:18Z,NONE,"@soumith OK. I will do it
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-219177998,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/219190708,219190708,MDEyOklzc3VlQ29tbWVudDIxOTE5MDcwOA==,8460517,2016-05-14T00:53:46Z,2016-05-14T02:46:42Z,NONE,"The 3 different benchmarks I outlined A) kernel speed/accuracy, B) network convergence, C) framework profiling are independent of each other, can be done in any order, and can even be separate projects.

convnet-benchmarks and the proposed deepmark are a very coarse grained version of C) framework profiling (they just measure the whole network forward/backward iteration time). You could get more information out of the benchmarks if you drilled down to per layer timings, and even better, drill down to kernel timings. Then we would know why one framework is faster than another, which the current tests do not tell us.

A good graph computation framework would profile itself continuously under normal operation. I do not know the extent to which TensorFlow and Neon do this already, but it is not hard to instrument code so that all layers and kernel calls are timed and then generate summary statistics. Of course memory use could also be measured.

So I see a good framework profiling benchmark:

1) assisting framework maintainers to instrument their code for auto-benchmarking 
2) providing standard synthetic data generators for each framework to wrap as an input layer
3) creating per-framework configuration files for each test
4) running tests and collecting the benchmark data logged by the framework
5) generating reports from benchmark data

Instrumenting frameworks for auto-benchmarking would also help users tune networks for fast operation, and help framework maintainers identify what to optimize. As such the deepmark profiling report would be a tool used not just for benchmark contests, but part of the everyday machine learning workflow.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-219190708,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/219211497,219211497,MDEyOklzc3VlQ29tbWVudDIxOTIxMTQ5Nw==,123560,2016-05-14T09:52:42Z,2016-05-14T09:52:42Z,CONTRIBUTOR,"Andrew,

No-one here is seeing that we dont want correctness.  I think it's fair to say we would all be happy to see tests for correctness.  I think it's also fair to say that most frameworks already have unit tests and other tests for correctness.  I think it's fair to say that anything that hasnt been tested generally doesnt work, so I would be very surprised to see a framework without any kind of testing :-)

I think the only question is: to add correctness tests alongside the exact models in this benchmark, and to do it in a way so you can show numerical equivalence, down to say 4 significant figures, across all weights and activations, is a phenomenal amount of work, that goes far beyond what is already in place for our correctness checking.  If you are saying that you are going to handle this effort, and all we have to do is sit back and look at the results, then I dont think there is any discussion required :-)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-219211497,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/219218618,219218618,MDEyOklzc3VlQ29tbWVudDIxOTIxODYxOA==,629706,2016-05-14T12:48:29Z,2016-05-14T12:48:29Z,CONTRIBUTOR,"You can count me in again for Theano (either vanilla or using Lasagne), but I'd prefer not to be the only one.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-219218618,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/219226874,219226874,MDEyOklzc3VlQ29tbWVudDIxOTIyNjg3NA==,8753078,2016-05-14T15:39:36Z,2016-05-14T15:39:36Z,NONE,"I can help @f0k with Theano, but only after 23rd.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-219226874,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/219227640,219227640,MDEyOklzc3VlQ29tbWVudDIxOTIyNzY0MA==,8460517,2016-05-14T15:54:11Z,2016-05-14T15:54:11Z,NONE,"@hughperkins please read my posts more carefully. I wrote that deepmark could be a framework profiling tool rather than just a forward/backward iteration timing tool.

TensorFlow is now instrumented for profiling: https://github.com/tensorflow/tensorflow/issues/1824

Numeric accuracy tests, on the other hand, can be a separate project and should be at the level of individual kernels.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-219227640,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/219231006,219231006,MDEyOklzc3VlQ29tbWVudDIxOTIzMTAwNg==,123560,2016-05-14T16:56:14Z,2016-05-14T16:56:14Z,CONTRIBUTOR,"> @hughperkins please read my posts more carefully.  I wrote that deepmark could be a framework profiling tool rather than just a forward/backward iteration timing tool.

Hi Andrew, fair point :-D
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-219231006,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/219248910,219248910,MDEyOklzc3VlQ29tbWVudDIxOTI0ODkxMA==,1710528,2016-05-14T20:08:10Z,2016-05-14T20:09:09Z,NONE,"/cc @nyanp if he is interested for tinycnn. But I don't know if could be ready for this with full models coverage and GPU backend support.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-219248910,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/219266615,219266615,MDEyOklzc3VlQ29tbWVudDIxOTI2NjYxNQ==,13167247,2016-05-15T05:04:39Z,2016-05-15T05:04:39Z,NONE,"I apologize if my comment further derails this thread but this seems like an appropriate time to bring it up.   I wonder if anyone is interested in strong scaling the time to convergence mentioned earlier? This would require what andravin calls tasks A and B above. It will soon be required to demonstrate convergence and the resulting accuracy at various network/dataset scales.

I'm less worried about functional profiling since external tools exist to do this fairly well and the algorithmic bottlenecks in sgd are a good first order place to optimize. It's also good to see mentioned the effectiveness of quantization (1bit and 8bit sgd) as I suspect these will become important features in frameworks to come especially with regard to scaling. 

I've been working with CNTK on many node systems recently and have become interested in how other platform's model parallelism scales in terms of time to solution (which is a tricky thing the community needs to define sooner or later). I really like the idea of having a common number to compare across frameworks (samples per second / epoch work well but don't capture wall time ) , which has inspired needed attention by developers on performance at the particular scale which is most used today. The main issue becomes larger and more complex models need to be used to demonstrate strong scaling as system size increases beyond a few gpus. I've run into this problem on large systems because there just isn't enough work to distribute using current problem sizes. 

That said, it would be incredibly useful to have a set of even two or three (small medium large) benchmarks (convolution + fully connected) which use the same training data but at distinct scales in terms of number of features, input dimension and hiddens such that we could measure and estimate how well the model/data parallelism preforms across MANY cpus and gpus and it would help us pin down performance bottlenecks as a side effect. The convergence and accuracy criteria then become important though, as well as going beyond the current memory requirements. 

This is a dream list of criteria to meet and no small task but I wonder if anyone else would be interested in such an effort or perhaps it's already going on somewhere? 

I think the profiling and benchmark efforts here are great and incredibly beneficial in creating standards / best practices across frameworks going forward. Thanks for all the work and for a great thread to read at the least!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-219266615,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/219527504,219527504,MDEyOklzc3VlQ29tbWVudDIxOTUyNzUwNA==,1310570,2016-05-16T19:50:27Z,2016-05-16T19:50:27Z,OWNER,"Excellent @pranv and @f0k . @craffel was interested in partly doing Theano benchmarking as well.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-219527504,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/219786690,219786690,MDEyOklzc3VlQ29tbWVudDIxOTc4NjY5MA==,19168148,2016-05-17T17:10:26Z,2016-05-17T17:10:26Z,NONE,"I'd hope this also supports CPU training/testing. 
My mojo-cnn project seems to be relatively fast on CPU for smaller problems and i know intel is still trying to push CPU use with their DNN wrappers for MKL to better compete with GPUs.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-219786690,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/220241263,220241263,MDEyOklzc3VlQ29tbWVudDIyMDI0MTI2Mw==,8460517,2016-05-19T06:46:36Z,2016-05-19T06:46:36Z,NONE,"@jbalma yes I find strong scaling the training problem to be very interesting, and of course that is an active area of research. I hope you find a good forum for pursuing it. Let me know if I can help.

I would also point out with regards to profiling, an external profiling tool will of course not be able to correlate timings with the graph structure of the network, so it cannot group statistics by layer, which is essential for understanding the performance of the network. I think all successful machine learning frameworks will be instrumented for profiling eventually, and tensorflow appears to be taking the lead there. Now imagine the power of being able to compare network performance graphs across different frameworks, because they all were designed to output profiler stats in the same format.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-220241263,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/220283712,220283712,MDEyOklzc3VlQ29tbWVudDIyMDI4MzcxMg==,123560,2016-05-19T10:15:15Z,2016-05-19T10:16:03Z,CONTRIBUTOR,"> I think all successful machine learning frameworks will be instrumented for profiling eventually, and tensorflow appears to be taking the lead there

[cltorch profiling and instrumentation](https://github.com/hughperkins/cltorch#profiling-tools) :-)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-220283712,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/220301202,220301202,MDEyOklzc3VlQ29tbWVudDIyMDMwMTIwMg==,629706,2016-05-19T11:46:39Z,2016-05-19T11:46:39Z,CONTRIBUTOR,"> > I think all successful machine learning frameworks will be instrumented for profiling eventually, and tensorflow appears to be taking the lead there
> 
> [cltorch profiling and instrumentation](https://github.com/hughperkins/cltorch#profiling-tools) :-)

`THEANO_FLAGS=profile=1` works since [at least 2013](https://github.com/Theano/Theano/commits/master/doc/tutorial/profiling.txt). Caveat: It profiles both CPU and GPU operations, so it times things ""from outside"", so it needs to be combined with `CUDA_LAUNCH_BLOCKING=1` to make GPU operations synchronous, so it cannot profile a real-life program. We could [time kernels with CUDA events](https://devblogs.nvidia.com/parallelforall/how-implement-performance-metrics-cuda-cc/) instead, but that would limit timings to things happening in a CUDA stream.

> I would also point out with regards to profiling, an external profiling tool will of course not be able to correlate timings with the graph structure of the network, so it cannot group statistics by layer, which is essential for understanding the performance of the network.

If we want metrics per layer (not per operation, or per operation type), we'll need to figure out how/where to introduce timing checkpoints in the graph without hindering optimization. That's a fundamental problem for frameworks based on computation graphs: They are free to rearrange operations across layer boundaries, so you cannot necessarily correlate the optimized graph with the network layers at all.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-220301202,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/220303921,220303921,MDEyOklzc3VlQ29tbWVudDIyMDMwMzkyMQ==,180987,2016-05-19T12:00:54Z,2016-05-19T12:00:54Z,CONTRIBUTOR,"On Thu, May 19, 2016 at 7:46 AM, Jan SchlÃ¼ter notifications@github.com
wrote:

> I think all successful machine learning frameworks will be instrumented
> for profiling eventually, and tensorflow appears to be taking the lead there
> 
> cltorch profiling and instrumentation
> https://github.com/hughperkins/cltorch#profiling-tools :-)
> 
> THEANO_FLAGS=profile=1 works since at least 2013
> https://github.com/Theano/Theano/commits/master/doc/tutorial/profiling.txt.
> Caveat: It profiles both CPU and GPU operations, so it times things ""from
> outside"", so it needs to be combined with CUDA_LAUNCH_BLOCKING=1 to make
> GPU operations synchronous, so it cannot profile a real-life program. We
> could time kernels with CUDA events
> https://devblogs.nvidia.com/parallelforall/how-implement-performance-metrics-cuda-cc/
> instead, but that would limit timings to things happening in a CUDA stream.
> 
> As the CPU node are already blocking, we could just add this as a second
> timing columns that would be non-0 only for the new gpu-backend.
> 
> I would also point out with regards to profiling, an external profiling
> tool will of course not be able to correlate timings with the graph
> structure of the network, so it cannot group statistics by layer, which is
> essential for understanding the performance of the network.
> 
> If we want metrics per layer (not per operation, or per operation type),
> we'll need to figure out how/where to introduce timing checkpoints in the
> graph without hindering optimization. That's a fundamental problem for
> frameworks based on computation graphs: They are free to rearrange
> operations across layer boundaries, so you cannot necessarily correlate the
> optimized graph with the network layers at all.
> 
> We could reject optimization that merge node from 2 layers to have exact
> per layer timing. Otherwise we could just let layer to be merged in the
> profiler.

As always, we can always do better!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-220303921,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/220317971,220317971,MDEyOklzc3VlQ29tbWVudDIyMDMxNzk3MQ==,123560,2016-05-19T13:05:21Z,2016-05-19T13:05:35Z,CONTRIBUTOR,"or, we could use the existing per-layer timings method, which seems to me to be a reasonably rigorous way of doing it, and avoids fudge-factors based on differing opinions on how to do in-situ timings.  like:
- do we call synchronize() between layers?
- wallclock timings?  kernel profiling timings?

I'm sure there are a bunch more of such questions....
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-220317971,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/220331261,220331261,MDEyOklzc3VlQ29tbWVudDIyMDMzMTI2MQ==,629706,2016-05-19T13:55:06Z,2016-05-19T13:55:06Z,CONTRIBUTOR,"> We could reject optimization that merge node from 2 layers to have exact per layer timing.

The event-based timing could be realized as Ops that insert events into the CUDA stream. If inserted between layers, these Ops would naturally block any optimizations across layers if existing optimizers are not adapted to ignore them. But again, this would prevent real in-situ timings, because the graph might not be fully optimized.

> I'm sure there are a bunch more of such questions....

Yes... in-situ timings would be the best, but depending on the framework, we cannot get per-layer timings without changing the process, and maybe not even distinguish the forward and backward pass, just training (fw+bw) and inference (fw only). For the start, we should probably have per-epoch or per-batch timings only.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-220331261,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/220409348,220409348,MDEyOklzc3VlQ29tbWVudDIyMDQwOTM0OA==,1310570,2016-05-19T18:19:21Z,2016-05-19T18:19:21Z,OWNER,"On the Caffe side, Mike Houston and team at NVIDIA have agreed to do the benchmark scripts. So that concludes all the volunteers for each framework :)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-220409348,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/220412962,220412962,MDEyOklzc3VlQ29tbWVudDIyMDQxMjk2Mg==,5577650,2016-05-19T18:32:18Z,2016-05-19T18:32:18Z,NONE,"@soumith 
I might do some adaptions to the Caffe scripts for OpenCL/Greentea-libDNN if required :)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-220412962,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/220414051,220414051,MDEyOklzc3VlQ29tbWVudDIyMDQxNDA1MQ==,1310570,2016-05-19T18:35:55Z,2016-05-19T18:35:55Z,OWNER,"Sure :)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-220414051,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/221752054,221752054,MDEyOklzc3VlQ29tbWVudDIyMTc1MjA1NA==,5577650,2016-05-26T01:09:58Z,2016-05-26T01:09:58Z,NONE,"@soumith 
I know we're on the edge of switching to a new benchmark system, but it would be great if you could give this a shot: https://github.com/soumith/convnet-benchmarks/pull/106
Thanks :)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-221752054,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/221844693,221844693,MDEyOklzc3VlQ29tbWVudDIyMTg0NDY5Mw==,123560,2016-05-26T11:20:14Z,2016-05-26T11:20:26Z,CONTRIBUTOR,"Opinion: the new system should be a collection of specialized repos, each with their own repo, being:
- images: owner Soumith, basically this one
- video: owner: ???
- nlp: owner: ???

... etc.  Therefore, I see no reason for this repo disappearing in any way, shape or form, personally :-)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-221844693,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/222547169,222547169,MDEyOklzc3VlQ29tbWVudDIyMjU0NzE2OQ==,123560,2016-05-30T19:54:45Z,2016-05-30T19:55:23Z,CONTRIBUTOR,"forresti wrote:

> Agreed. Soumith's current benchmarks are useful, but they mainly evaluate ""who can make the thinnest wrapper around cuDNN, Neon, or similar?""

On a somewhat related note, since GEMM seems to be at the heart of convolution.  It's used by fft, winograd, im2col, and presumably also implicit gemm.  So, could it be worth having some GEMM benchmarks?  For OpenCL, there are at least 3 GEMM implementations I know of, ie: clBLAS, clBLAST, ViennaCL.

(Edit: and for CUDA, there's at least: cublas, and the sass gemm implementation that is part of neon)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-222547169,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/222549643,222549643,MDEyOklzc3VlQ29tbWVudDIyMjU0OTY0Mw==,1710528,2016-05-30T20:17:50Z,2016-05-30T20:17:50Z,NONE,"@hughperkins Do you mean something like https://github.com/dividiti/gemmbench?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-222549643,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/222550678,222550678,MDEyOklzc3VlQ29tbWVudDIyMjU1MDY3OA==,123560,2016-05-30T20:27:25Z,2016-05-30T20:27:25Z,CONTRIBUTOR,"@bhack 

Possibly.  I'm not sure what I mean to be honest.  I'm not sure that's quite exactly what I was thinking of.  I was thinking of something more like the simple tables in convnet-benchmarks, but comparing these 5 or so GEMM implementations.  Presumably, the actual workloads, if we are targeting convolution, should be workloads sampled by running a forwards-backwards batch through a few common convolutional models, such as those currently in convnet-benchmarks.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-222550678,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/222556448,222556448,MDEyOklzc3VlQ29tbWVudDIyMjU1NjQ0OA==,5577650,2016-05-30T21:26:52Z,2016-05-30T21:26:52Z,NONE,"@hughperkins 
I actually think GEMM will be a bit less at the heart of convolutions as we move forward. At least not pure GEMM as implemented in BLAS libraries.
There are many nice optimization and memory coalescing possibilities when implementing kernel fusion, that the GEMM only remains at the core (register blocked GEMM, at most shared memory GEMM).
I am just right now doing these optimizations on libDNN to get closer to cuDNN scores.

Also, what makes benchmarking the GEMMs more difficult is that their performance differs a lot on a variety of devices. And then the GEMMs can also be autotuned, which works more or less depending on what BLAS and architecture combination is used.

I think benchmarking on the network and layer level is enough for the DeepMark project.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-222556448,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/222557607,222557607,MDEyOklzc3VlQ29tbWVudDIyMjU1NzYwNw==,123560,2016-05-30T21:40:35Z,2016-05-30T21:40:35Z,CONTRIBUTOR,"> There are many nice optimization and memory coalescing possibilities when implementing kernel fusion, that the GEMM only remains at the core (register blocked GEMM, at most shared memory GEMM).
> I am just right now doing these optimizations on libDNN to get closer to cuDNN scores.

Ah, sounds good :-)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-222557607,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/226045355,226045355,MDEyOklzc3VlQ29tbWVudDIyNjA0NTM1NQ==,123560,2016-06-14T23:20:43Z,2016-06-21T04:32:29Z,CONTRIBUTOR,"Hi Andrew, So ... guess what?   I wrote a correctness checking script :-)  And ... ironically enough... it targets Neon :-D  Because I need it for testing the OpenCL port.

It outputs for each layer:
- average forward time
- average backwards time
- average delta between each output value and a cpu-calculated reference value, assumed correct, since python
  uses float64 (I think?)
- ditto for weights gradient
- ditto for input gradient

Example [results](https://github.com/hughperkins/neon-benchmarks/tree/master/results), for neon on Titan X, using [vgga](https://github.com/hughperkins/neon-benchmarks/blob/master/models/vgga.py) model:

[Maxwell kernels, Winograd, SASS](https://github.com/hughperkins/neon-benchmarks/blob/master/results/neonbase_maxwell_vgga.txt):

```
Layer 0: fprop=0.004 bprop=0.036 eps_O=3e-07 eps_gradW=2e-03 eps_gradI=4e-05 (note: direct, not winograd)
Layer 1: fprop=0.012 bprop=0.032 eps_O=1e-05 eps_gradW=1e-02 eps_gradI=3e-05
Layer 2: fprop=0.009 bprop=0.021 eps_O=5e-05 eps_gradW=2e-03 eps_gradI=6e-05
Layer 3: fprop=0.017 bprop=0.036 eps_O=1e-04 eps_gradW=3e-03 eps_gradI=8e-05
Layer 4: fprop=0.007 bprop=0.016 eps_O=8e-05 eps_gradW=6e-04 eps_gradI=2e-04
Layer 5: fprop=0.015 bprop=0.031 eps_O=9e-05 eps_gradW=8e-04 eps_gradI=1e-04
Layer 6: fprop=0.005 bprop=0.010 eps_O=8e-05 eps_gradW=4e-04 eps_gradI=9e-05
Layer 7: fprop=0.005 bprop=0.010 eps_O=8e-05 eps_gradW=4e-04 eps_gradI=9e-05
```

[Kepler kernels, Direct, CUDA](https://github.com/hughperkins/neon-benchmarks/blob/master/results/neonbase_kepler_vgga.txt):

```
Layer 0: SKIPPING
Layer 1: fprop=0.032 bprop=0.158 eps_O=9e-06 eps_gradW=1e-03 eps_gradI=2e-05
Layer 2: fprop=0.033 bprop=0.110 eps_O=2e-05 eps_gradW=3e-04 eps_gradI=2e-05
Layer 3: fprop=0.067 bprop=0.222 eps_O=2e-05 eps_gradW=3e-04 eps_gradI=3e-05
Layer 4: fprop=0.033 bprop=0.111 eps_O=3e-05 eps_gradW=1e-04 eps_gradI=9e-05
Layer 5: fprop=0.066 bprop=0.222 eps_O=4e-05 eps_gradW=9e-05 eps_gradI=5e-05
Layer 6: fprop=0.016 bprop=0.053 eps_O=4e-05 eps_gradW=2e-05 eps_gradI=3e-05
Layer 7: fprop=0.016 bprop=0.053 eps_O=4e-05 eps_gradW=2e-05 eps_gradI=3e-05
```

[Kepler kernels, Direct, OpenCL](https://github.com/hughperkins/neon-benchmarks/blob/master/results/winogradcl_vgga.txt):

```
Layer 0: SKIPPING
Layer 1: fprop=0.039 bprop=0.173 eps_O=1e-05 eps_gradW=8e-04 eps_gradI=1e-05
Layer 2: fprop=0.039 bprop=0.124 eps_O=1e-05 eps_gradW=4e-04 eps_gradI=2e-05
Layer 3: fprop=0.073 bprop=0.237 eps_O=3e-05 eps_gradW=2e-04 eps_gradI=4e-05
Layer 4: fprop=0.038 bprop=0.125 eps_O=2e-05 eps_gradW=5e-05 eps_gradI=2e-05
Layer 5: fprop=0.073 bprop=0.238 eps_O=3e-05 eps_gradW=1e-04 eps_gradI=4e-05
Layer 6: fprop=0.022 bprop=0.069 eps_O=5e-05 eps_gradW=2e-05 eps_gradI=7e-05
Layer 7: fprop=0.021 bprop=0.067 eps_O=5e-05 eps_gradW=2e-05 eps_gradI=7e-05
```

https://github.com/hughperkins/neon-benchmarks

(Edited with the layer 0 results for Maxwell CUDA kernels, summary page at https://github.com/hughperkins/neon-benchmarks/blob/master/results/vgga_summary.md )
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-226045355,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/227323427,227323427,MDEyOklzc3VlQ29tbWVudDIyNzMyMzQyNw==,123560,2016-06-21T02:22:03Z,2016-06-21T04:18:13Z,CONTRIBUTOR,"For the benchmarks with correctness checker, added stride and padding, so it can handle eg alexnet now:

https://github.com/hughperkins/neon-benchmarks/blob/master/results/alexnet_summary.md

_Nervana Neon CUDA/SASS Winograd kernels for Maxwell_

[neon_maxwell](backends/neon_maxwell.py)

```
Layer 0: fprop=0.003 bprop=0.012 eps_O=4e-06 eps_gradW=5e-04 eps_gradI=2e-06 (note: direct, not winograd)
Layer 1: fprop=0.009 bprop=0.020 eps_O=9e-06 eps_gradW=4e-04 eps_gradI=3e-05 (note: direct, not winograd)
Layer 2: fprop=0.003 bprop=0.006 eps_O=5e-05 eps_gradW=3e-04 eps_gradI=8e-05
Layer 3: fprop=0.004 bprop=0.007 eps_O=5e-05 eps_gradW=7e-04 eps_gradI=1e-04
Layer 4: fprop=0.003 bprop=0.005 eps_O=1e-04 eps_gradW=3e-04 eps_gradI=6e-05
```

_Nervana Neon Kepler direct kernels, in CUDA_

[neon_kepler](backends/neon_kepler.py)

```
Layer 0: SKIPPED
Layer 1: fprop=0.015 bprop=0.046 eps_O=9e-06 eps_gradW=2e-04 eps_gradI=3e-05
Layer 2: fprop=0.008 bprop=0.023 eps_O=2e-05 eps_gradW=3e-05 eps_gradI=5e-05
Layer 3: fprop=0.010 bprop=0.030 eps_O=4e-05 eps_gradW=3e-05 eps_gradI=2e-05
Layer 4: fprop=0.007 bprop=0.020 eps_O=3e-05 eps_gradW=2e-05 eps_gradI=2e-05
```

_OpenCL port of Nervana Neon Kepler direct kernels_

[neoncl_direct](backends/neoncl_direct.py)

```
Layer 0: SKIPPED
Layer 1: fprop=0.016 bprop=0.049 eps_O=1e-05 eps_gradW=1e-04 eps_gradI=6e-05
Layer 2: fprop=0.008 bprop=0.024 eps_O=2e-05 eps_gradW=3e-05 eps_gradI=3e-05
Layer 3: fprop=0.010 bprop=0.031 eps_O=4e-05 eps_gradW=3e-05 eps_gradI=3e-05
Layer 4: fprop=0.007 bprop=0.021 eps_O=2e-05 eps_gradW=2e-05 eps_gradI=2e-05
```

However, there's still a couple of things we'd want, if we wanted to generalize this to other networks:
- some way of running the other frameworks from python, and / or
- some way of transmitting the tensors to the other frameworks

Actually, torch can [run from python](https://github.com/hughperkins/pytorch), as can theano, tensorflow, mxnet (I think?), caffe, DeepCL, chainer.  So maybe python is all that is needed???
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-227323427,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/227687870,227687870,MDEyOklzc3VlQ29tbWVudDIyNzY4Nzg3MA==,123560,2016-06-22T09:15:39Z,2016-06-22T09:16:26Z,CONTRIBUTOR,"Andrew, hmmm, just noticed, this is figure 4 in your paper.  Hadnt noticed that before :-D  Well, I hadnt read the paper earlier...   I see that you are using element max though.  I think this might be sensitive to outliers, and also larger for layers with more output values?  Maybe median or average, possibly also with standard deviation, is good?  (edited to spell median with a 'd' and an 'i' ...)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-227687870,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/242013635,242013635,MDEyOklzc3VlQ29tbWVudDI0MjAxMzYzNQ==,629706,2016-08-24T09:57:08Z,2016-08-24T09:57:08Z,CONTRIBUTOR,"Possibly relevant: [Fathom: Reference Workloads for Modern Deep Learning Methods](http://arxiv.org/abs/1608.06581)

> Consequently, we have assembled Fathom: a collection of eight archetypal deep learning workloads for study. Each of these models comes from a seminal work in the deep learning community, ranging from the familiar deep convolutional neural network of Krizhevsky et al., to the more exotic memory networks from Facebook's AI research group. Fathom has been released online, and this paper focuses on understanding the fundamental performance characteristics of each model. We use a set of application-level modeling tools built around the TensorFlow deep learning framework in order to analyze the behavior of the Fathom workloads. We present a breakdown of where time is spent, the similarities between the performance profiles of our models, an analysis of behavior in inference and training, and the effects of parallelism on scaling.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-242013635,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/249804880,249804880,MDEyOklzc3VlQ29tbWVudDI0OTgwNDg4MA==,1710528,2016-09-27T08:49:36Z,2016-09-27T08:49:36Z,NONE,"See also Baidu Research [DeepBench](https://svail.github.io/DeepBench/)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-249804880,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/282883074,282883074,MDEyOklzc3VlQ29tbWVudDI4Mjg4MzA3NA==,847743,2017-02-27T22:51:08Z,2017-02-27T22:51:08Z,NONE,"Hi everyone, 
we will soon be benchmarking a NVIDIA DGX-1 and we are currently looking for ways to do it effectively. So if you have ever wondered how well the DGX-1 would perform on a specific task, let me know which. We will release all results online. 

For now, we have run some classic benchmarks with a K40/Maxwell Titan X/GTX 1080/ Pascal Titan X, using various architectures and frameworks. Results are available [here](http://add-for.com/blog/nvidia-dgx-1-supercomputer-join-our-community-based-deep-learning-benchmark/).
Again,  all suggestions on how to benchmark the DGX-1 are welcome. Thanks",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-282883074,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/282948834,282948834,MDEyOklzc3VlQ29tbWVudDI4Mjk0ODgzNA==,551151,2017-02-28T05:49:01Z,2017-02-28T05:49:01Z,CONTRIBUTOR,"@pedropgusmao from the curve it seems that you basically need to increase the workspace limit in Caffe:

https://github.com/BVLC/caffe/blob/80f44100e19fd371ff55beb3ec2ad5919fb6ac43/src/caffe/layers/cudnn_conv_layer.cpp#L113

The value was in order to support all platforms (old and new) but it usually comes slow for most recent cudnn with most recent hardware.",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-282948834,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/282955355,282955355,MDEyOklzc3VlQ29tbWVudDI4Mjk1NTM1NQ==,847743,2017-02-28T06:34:58Z,2017-02-28T06:34:58Z,NONE,"@Yangqing , thank you very much. Do you have any suggested values for workspace_limit_bytes considering both those GPUs and the DGX-1?  Again, thanks.",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-282955355,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/282958886,282958886,MDEyOklzc3VlQ29tbWVudDI4Mjk1ODg4Ng==,463737,2017-02-28T06:58:19Z,2017-02-28T06:58:19Z,CONTRIBUTOR,"@pedropgusmao take a look at https://www.tensorflow.org/performance/performance_guide with TF 1.0.0.  I believe we are also working on benchmarks for some of these models, so you'll have comparison code at some point soon.",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-282958886,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,148512920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/282969866,282969866,MDEyOklzc3VlQ29tbWVudDI4Mjk2OTg2Ng==,847743,2017-02-28T08:03:37Z,2017-02-28T08:03:37Z,NONE,"@vrv, thanks a lot! We will modify our code to follow those suggestions. We look forward to see your results.   ",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101/comments,https://github.com/soumith/convnet-benchmarks/issues/101#issuecomment-282969866,https://api.github.com/repos/soumith/convnet-benchmarks/issues/101
soumith,convnet-benchmarks,146193532,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/206389601,206389601,MDEyOklzc3VlQ29tbWVudDIwNjM4OTYwMQ==,1310570,2016-04-06T14:09:30Z,2016-04-06T14:09:30Z,OWNER,"Done via https://github.com/soumith/convnet-benchmarks/commit/79627ebdb37c14cebac0f2302b8dbfc23f6e2ce7
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/100/comments,https://github.com/soumith/convnet-benchmarks/issues/100#issuecomment-206389601,https://api.github.com/repos/soumith/convnet-benchmarks/issues/100
soumith,convnet-benchmarks,143787157,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/288067826,288067826,MDEyOklzc3VlQ29tbWVudDI4ODA2NzgyNg==,22461308,2017-03-21T12:47:57Z,2017-03-21T12:47:57Z,NONE,"FYI, there is a benchmark with Theano for convnet-benchmark.

+1 for integration of Theaon on the table in this repo.

https://github.com/intel/Theano/blob/dev/README.md
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/97/comments,https://github.com/soumith/convnet-benchmarks/issues/97#issuecomment-288067826,https://api.github.com/repos/soumith/convnet-benchmarks/issues/97
soumith,convnet-benchmarks,143787157,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/288069126,288069126,MDEyOklzc3VlQ29tbWVudDI4ODA2OTEyNg==,180987,2017-03-21T12:53:37Z,2017-03-21T12:53:37Z,CONTRIBUTOR,"There is imagenet Theano benchmark, check in this folder:

https://github.com/soumith/convnet-benchmarks/blob/master/theano/alexnet.py

I just want to point that th URL point to Intel Theano version, that speed
up Theano on CPU. Currently all the benchmark was done on GPU. So this is
another issue. If you are interrested in that, I would suggest you to open
a new issue to run the benchmark on CPU, and make a PR to modify the
current benchmark to run on CPU too. Probably soumith don't have time for
this, so helping him is probably the only way you can get that.

On Tue, Mar 21, 2017 at 8:47 AM PatricZhao <notifications@github.com> wrote:

> FYI, there is a benchmark with Theano for convnet-benchmark.
>
> +1 for integration of Theaon on the table in this repo.
>
> https://github.com/intel/Theano/blob/dev/README.md
>
> â€”
> You are receiving this because you are subscribed to this thread.
>
> Reply to this email directly, view it on GitHub
> <https://github.com/soumith/convnet-benchmarks/issues/97#issuecomment-288067826>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AALC-z8i60Kwj3yrKF2YzZB4x0u7Tfb_ks5rn8b-gaJpZM4H5aeR>
> .
>
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/97/comments,https://github.com/soumith/convnet-benchmarks/issues/97#issuecomment-288069126,https://api.github.com/repos/soumith/convnet-benchmarks/issues/97
soumith,convnet-benchmarks,143787157,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/288076359,288076359,MDEyOklzc3VlQ29tbWVudDI4ODA3NjM1OQ==,12723067,2017-03-21T13:23:19Z,2017-03-21T13:23:19Z,NONE,"You guys just replied a one-year-ago issue. It's not a problem now. Anyway, thanks for your reply, best wishes!",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/97/comments,https://github.com/soumith/convnet-benchmarks/issues/97#issuecomment-288076359,https://api.github.com/repos/soumith/convnet-benchmarks/issues/97
soumith,convnet-benchmarks,140754873,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/196454282,196454282,MDEyOklzc3VlQ29tbWVudDE5NjQ1NDI4Mg==,1310570,2016-03-14T18:22:36Z,2016-03-14T18:22:36Z,OWNER,"XQ, this is excellent. Thanks.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/96/comments,https://github.com/soumith/convnet-benchmarks/pull/96#issuecomment-196454282,https://api.github.com/repos/soumith/convnet-benchmarks/issues/96
soumith,convnet-benchmarks,139481923,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/194124761,194124761,MDEyOklzc3VlQ29tbWVudDE5NDEyNDc2MQ==,1310570,2016-03-09T05:48:07Z,2016-03-09T05:48:07Z,OWNER,"ask this on nvidia/caffe. wrong repo.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/95/comments,https://github.com/soumith/convnet-benchmarks/issues/95#issuecomment-194124761,https://api.github.com/repos/soumith/convnet-benchmarks/issues/95
soumith,convnet-benchmarks,139481923,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/292839375,292839375,MDEyOklzc3VlQ29tbWVudDI5MjgzOTM3NQ==,11991566,2017-04-10T03:08:02Z,2017-04-10T03:08:02Z,NONE,"I got the same situation here. Same prototxt and caffemodel worked well on my server,  but on TX1, killed. 

Maybe the problem comes from the storage space ?",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/95/comments,https://github.com/soumith/convnet-benchmarks/issues/95#issuecomment-292839375,https://api.github.com/repos/soumith/convnet-benchmarks/issues/95
soumith,convnet-benchmarks,139421818,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/209573337,209573337,MDEyOklzc3VlQ29tbWVudDIwOTU3MzMzNw==,10819534,2016-04-13T18:10:43Z,2016-04-13T18:10:43Z,CONTRIBUTOR,"Sorry, did I miss a point why theano benchmarks are irrelevant compare to torch and tensorflow?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/94/comments,https://github.com/soumith/convnet-benchmarks/pull/94#issuecomment-209573337,https://api.github.com/repos/soumith/convnet-benchmarks/issues/94
soumith,convnet-benchmarks,139421818,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/209575254,209575254,MDEyOklzc3VlQ29tbWVudDIwOTU3NTI1NA==,1310570,2016-04-13T18:14:19Z,2016-04-13T18:14:19Z,OWNER,"Hi Konstantin, no you did not, it is plainly my fault for missing the notification -- terribly sorry for the delay.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/94/comments,https://github.com/soumith/convnet-benchmarks/pull/94#issuecomment-209575254,https://api.github.com/repos/soumith/convnet-benchmarks/issues/94
soumith,convnet-benchmarks,139421818,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/209575410,209575410,MDEyOklzc3VlQ29tbWVudDIwOTU3NTQxMA==,1310570,2016-04-13T18:14:30Z,2016-04-13T18:14:30Z,OWNER,"What are the numbers like on your machine?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/94/comments,https://github.com/soumith/convnet-benchmarks/pull/94#issuecomment-209575410,https://api.github.com/repos/soumith/convnet-benchmarks/issues/94
soumith,convnet-benchmarks,139421818,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/209578442,209578442,MDEyOklzc3VlQ29tbWVudDIwOTU3ODQ0Mg==,10819534,2016-04-13T18:21:41Z,2016-04-13T18:21:41Z,CONTRIBUTOR,"I didn't run them on GPU, only on CPU to be sure everything works. Hardware setup I have access to is quite different from yours to have comparable numbers.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/94/comments,https://github.com/soumith/convnet-benchmarks/pull/94#issuecomment-209578442,https://api.github.com/repos/soumith/convnet-benchmarks/issues/94
soumith,convnet-benchmarks,139421818,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/209579833,209579833,MDEyOklzc3VlQ29tbWVudDIwOTU3OTgzMw==,1310570,2016-04-13T18:25:00Z,2016-04-13T18:25:00Z,OWNER,"okay, are there best practices wrt Theano flags and command line arguments I have to use to make sure I get maximum performance? Does theano enable cudnn by default or do I need any flags?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/94/comments,https://github.com/soumith/convnet-benchmarks/pull/94#issuecomment-209579833,https://api.github.com/repos/soumith/convnet-benchmarks/issues/94
soumith,convnet-benchmarks,139421818,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/209585222,209585222,MDEyOklzc3VlQ29tbWVudDIwOTU4NTIyMg==,10819534,2016-04-13T18:32:48Z,2016-04-13T18:32:48Z,CONTRIBUTOR,"It should detect cudnn automatically and be reasonably fast by default. Although I suggest following `~/.theanorc` config:

```
[global]
floatX = float32
mode=FAST_RUN
device=gpu0
force_device=True

[dnn]
enabled=True

[cuda]
root=/path/to/cuda

[lib]
cnmem=0.9

[nvcc]
fastmath=True
```

It makes theano fail if it can't attach to gpu or find cudnn.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/94/comments,https://github.com/soumith/convnet-benchmarks/pull/94#issuecomment-209585222,https://api.github.com/repos/soumith/convnet-benchmarks/issues/94
soumith,convnet-benchmarks,139421818,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/209624924,209624924,MDEyOklzc3VlQ29tbWVudDIwOTYyNDkyNA==,180987,2016-04-13T20:08:22Z,2016-04-13T20:08:22Z,CONTRIBUTOR,"Which cudnn algo is used by other framework? They can be changed by a few
Theano flags. If you want Theano to time each (via cudnn) and pick the
fastest, you can use this Theano flags:

dnn.conv.algo_fwd=time_once
dnn.conv.algo_bwd_filter=time_once
dnn.conv.algo_bwd_data=time_once

(add that in your .theanorc)

[dnn.conv]
algo_fwd=time_once
algo_bwd_filter=time_once
algo_bwd_data=time_once

That flag also allow to use swap the default algo used by cudnn to other
values.

On Wed, Apr 13, 2016 at 2:32 PM, Konstantin Shmelkov <
notifications@github.com> wrote:

> It should detect cudnn automatically and be reasonably fast by default.
> Although I suggest following ~/.theanorc config:
> 
> [global]
> floatX = float32
> mode=FAST_RUN
> device=gpu0
> force_device=True
> 
> [dnn]
> enabled=True
> 
> [cuda]
> root=/path/to/cuda
> 
> [lib]
> cnmem=0.9
> 
> [nvcc]
> fastmath=True
> 
> It makes theano fail if it can't attach to gpu or find cudnn.
> 
> â€”
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/pull/94#issuecomment-209585222
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/94/comments,https://github.com/soumith/convnet-benchmarks/pull/94#issuecomment-209624924,https://api.github.com/repos/soumith/convnet-benchmarks/issues/94
soumith,convnet-benchmarks,139421818,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/210152179,210152179,MDEyOklzc3VlQ29tbWVudDIxMDE1MjE3OQ==,10819534,2016-04-14T21:17:14Z,2016-04-14T21:17:14Z,CONTRIBUTOR,"I managed to get Titan X today. Here are numbers I got with suggested `time_once` option:

```
$ python benchmark_imagenet.py -a alexnet -B 128
Using gpu device 0: GeForce GTX TITAN X (CNMeM is enabled with initial size: 90.0% of memory, cuDNN 4007)
Forward across 100 steps, 0.054 +/- 0.001 sec / batch
Forward-Backward across 100 steps, 0.115 +/- 0.001 sec / batch

$ python benchmark_imagenet.py -a overfeat -B 128
Using gpu device 0: GeForce GTX TITAN X (CNMeM is enabled with initial size: 90.0% of memory, cuDNN 4007)
Forward across 100 steps, 0.137 +/- 0.001 sec / batch
Forward-Backward across 100 steps, 0.400 +/- 0.003 sec / batch

$ python benchmark_imagenet.py -a googlenet -B 128
Using gpu device 0: GeForce GTX TITAN X (CNMeM is enabled with initial size: 90.0% of memory, cuDNN 4007)
Forward across 100 steps, 0.170 +/- 0.001 sec / batch
Forward-Backward across 100 steps, 0.536 +/- 0.003 sec / batch

$ python benchmark_imagenet.py -a vgg -B 64
Using gpu device 0: GeForce GTX TITAN X (CNMeM is enabled with initial size: 90.0% of memory, cuDNN 4007)
Forward across 100 steps, 0.212 +/- 0.001 sec / batch
Forward-Backward across 100 steps, 0.684 +/- 0.003 sec / batch
```
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/94/comments,https://github.com/soumith/convnet-benchmarks/pull/94#issuecomment-210152179,https://api.github.com/repos/soumith/convnet-benchmarks/issues/94
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192067570,192067570,MDEyOklzc3VlQ29tbWVudDE5MjA2NzU3MA==,9326960,2016-03-04T02:30:15Z,2016-03-04T02:30:15Z,NONE,"Sorry, the full blog post isn't quite done yet, so we put up a quick performance overview post instead.  But do feel free to download the new neon and try out the kernels (and even browse the source if you're curious).    I'm almost done with a new Winograd kernel that should speed things up quite a bit more for smaller 3x3 layers (like in googlenet).
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-192067570,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192089206,192089206,MDEyOklzc3VlQ29tbWVudDE5MjA4OTIwNg==,8460517,2016-03-04T04:01:19Z,2016-03-04T04:01:19Z,NONE,"@soumith Why have the cuDNN R4 Googlenet-1 numbers changed?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-192089206,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192089382,192089382,MDEyOklzc3VlQ29tbWVudDE5MjA4OTM4Mg==,1310570,2016-03-04T04:02:50Z,2016-03-04T04:02:50Z,OWNER,"@andravin my copy-paste screwup. fixed.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-192089382,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192090521,192090521,MDEyOklzc3VlQ29tbWVudDE5MjA5MDUyMQ==,8460517,2016-03-04T04:09:59Z,2016-03-04T04:10:12Z,NONE,"OK, thanks, I wasn't expecting the Neon Googlenet-1 speedup to decrease. ;-) So these numbers make more sense.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-192090521,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192093844,192093844,MDEyOklzc3VlQ29tbWVudDE5MjA5Mzg0NA==,9326960,2016-03-04T04:21:45Z,2016-03-04T04:21:45Z,NONE,"I should point out that Andrew here not only worked through all the math for Winograd but our long discussions were pretty integral to the successful design of these kernels.

Oh and while we're giving credit we were just discussing that these two guys probably deserve as much as Shmuel Winograd for working out the original math:

https://en.wikipedia.org/wiki/Andrei_Toom
https://en.wikipedia.org/wiki/Stephen_Cook

I'll go into a bit more detail on this in the full blog.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-192093844,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192150350,192150350,MDEyOklzc3VlQ29tbWVudDE5MjE1MDM1MA==,1792006,2016-03-04T07:14:58Z,2016-03-04T07:14:58Z,NONE,"Thanks Scott and Andrew. We also see good gains for 3x3 with Winograd. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-192150350,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192413304,192413304,MDEyOklzc3VlQ29tbWVudDE5MjQxMzMwNA==,8460517,2016-03-04T19:03:06Z,2016-03-04T19:03:06Z,NONE,"That's great, Julien. Can't wait to see Winograd/Cook/Toom in cuDNN. :-) 

It has been almost a year since I discovered this new approach to convnet acceleration, and it is great to see these ideas having a real impact on performance now. Everybody should check out Scott's F(4x4,3x3) implementation, it is extremely clever. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-192413304,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192421658,192421658,MDEyOklzc3VlQ29tbWVudDE5MjQyMTY1OA==,1792006,2016-03-04T19:14:20Z,2016-03-04T19:14:20Z,NONE,"It is indeed amazing that you were able to get F(4x4, 3x3) to work. I'm really impressed because I know for a fact that F(2x2, 3x3) is already super hard :). I am really looking forward to making it work in cuDNN.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-192421658,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192603109,192603109,MDEyOklzc3VlQ29tbWVudDE5MjYwMzEwOQ==,1792006,2016-03-05T07:57:42Z,2016-03-05T07:57:42Z,NONE,"@scott-gray Awesome work! The way you specialize warps in the F(4x4,3x3) kernel is just brilliant! I'm super excited and it's going to be fun to implement such a scheme for cuDNN :) and bring that speedup to the different frameworks.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-192603109,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192614751,192614751,MDEyOklzc3VlQ29tbWVudDE5MjYxNDc1MQ==,8460517,2016-03-05T09:51:34Z,2016-03-05T09:51:34Z,NONE,"Now if you guys could put your heads together and figure out a way to end the NCHW vs CHWN vs NHWC wars. There must be some way to equip these kernels with pluggable front-ends and back-ends that understand the tensor order for load / store, and leaves the computation pipeline unchanged.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-192614751,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192621350,192621350,MDEyOklzc3VlQ29tbWVudDE5MjYyMTM1MA==,9326960,2016-03-05T10:51:32Z,2016-03-05T10:51:32Z,NONE,"I already have a fully fused version of that kernel that I should finish debugging this weekend.  I'm  hoping it will bring fprop/bprop performance for fp32 much closer to the fp16 level, as well as perform much more consistently with the size of the C/K dimensions.  On the weight update side, fusion probably isn't possible due to the extremely strided memory access pattern required and no shared memory left for mitigating that. But 2 out of 3 fast operations isn't bad.  In NHWC it would be the update operation that is fast and the other two slower.

I guess there's a chance in NCHW that the overlaps in the super-tiling might make full fusion possible in update, but on the downside you're slower on fprop/bprop for smallish HW because your effective tile size needs to be much bigger and you end up with a lot of zero overlap.  At very small N NCHW and CHWN are pretty equivalent.  But, I'm confident that CHWN is fastest overall for Winograd.

For direct conv, I'm starting to think that NHWC might be best for good performance across all minibatch sizes.  There's plenty of shared memory around to efficiently transpose in place at no cost and having C as the inner dimension means that you minimize the slicing logic for all values of N and not just larger ones.  But CHWN is just as good for N bigger than about 8 or so.

Also, having HWN contiguous means that you can do 1x1 conv super efficiently in a basic gemm kernel.

If I had to pick one, I'd stick with what I have: CHWN.  But longer term it probably makes sense to have them all implemented to best suit the needs of the task.

Speaking of longer term, it would be nice if the community migrated to a fully open sourced implementation for all of this.  This stuff is just too important to the progress of the field for it to be locked away in proprietary implementations.   The more people working together on this the better for everyone.  There's plenty of room to compete on the hardware implementation side.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-192621350,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192625162,192625162,MDEyOklzc3VlQ29tbWVudDE5MjYyNTE2Mg==,43829,2016-03-05T11:40:44Z,2016-03-05T11:40:44Z,NONE,"> Also, having HWN contiguous means that you can do 1x1 conv super efficiently in a basic gemm kernel.

The same goes for NHWC though, right? If I'm not mistaken, the order of these dimensions doesn't matter as long as they are contiguous. This is the TensorFlow default, I don't know if any other frameworks use it though. I think CHWN might not get adopted very easily, because everyone is used to having the leading dimension be the batch dimension nowadays (the only established framework I know of that deviates from this is cuda-convnet, which isn't used much anymore).
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-192625162,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192627065,192627065,MDEyOklzc3VlQ29tbWVudDE5MjYyNzA2NQ==,1792006,2016-03-05T12:05:33Z,2016-03-05T12:05:33Z,NONE,"@benanne: You're right, NHWC works fine with 1x1 (as does CHWN). The issue we're having with cuDNN with 1x1 is that NCHW has the C ""in the middle"". Today, our direct convolution is similar to 3x3 or 5x5 for 1x1 and we are having a complex logic that we could simplify for 1x1. Scott's CHWN is ""easier"" to deal with in many cases. We also suffer from the fact that our filters are KCRS when CRSK (used by Scott) would be better.

On paper, NHWC has advantages over NCHW thanks to the fact that data is partly contiguous in memory. I'm only worried about the fact that NHWC could have a bad impact on the behavior of the TEX cache as fetching 8xFP16 (8 is the unrolling factor of the main loop - except for Scott's new F(4x4,3x3)) is only 16B and it's not so great with respect to cache line size.

@andravin, @scott-gray: Indeed, I think we should sit together and find a way to get the awesome performance of Scott's implementations for CHWN available to popular frameworks. We'll all be at GTC, for example. Making it open-source is a long discussion ;)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-192627065,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192627323,192627323,MDEyOklzc3VlQ29tbWVudDE5MjYyNzMyMw==,9326960,2016-03-05T12:06:23Z,2016-03-05T12:06:23Z,NONE,"Right, I just meant grouped.  So that's a plus for both NHWC and CHWN.  You're right in that there is a lot of cuda code written for the cuDNN layout, and migrating away from that will likely be painful.  But for some writing fresh code to a different layout might be a good option if they know they'll get a bit more speed. As I said, ideally you have the option for any layout.    

Anyway, if you guys are happy sticking with NCHW, then we'll be happy to continue topping you on the benchmarks :)   The neon framework isn't burdened by any legacy code and everything is being built from the ground up for speed.  And with the new graph backend we're working on hopefully we can substantially improve on the ease of use as well (not that it's too bad right now).
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-192627323,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192628232,192628232,MDEyOklzc3VlQ29tbWVudDE5MjYyODIzMg==,9326960,2016-03-05T12:15:13Z,2016-03-05T12:15:13Z,NONE,"@jdemouth You're not thinking creatively enough with leveraging shared memory to read deeper than 8 lines.  You can cast any gemm or conv operation as a batched gemm and sum the results prior to writing out.  The batch dimension in this case is just alternating groups of 8 rows.

Happy to chat at GTC.  I was looking forward to attending your talk.  And I guess I should advertise my own talk here.  I was scheduled for an hour but that was mysteriously shortened to just 25 minutes.  So I wont be able to go into as much depth as I'd like.  But on the other hand it makes preparing for it a lot easier, which means more time to be writing kernels.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-192628232,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192636494,192636494,MDEyOklzc3VlQ29tbWVudDE5MjYzNjQ5NA==,1792006,2016-03-05T12:34:00Z,2016-03-05T12:34:00Z,NONE,"@scott-gray: Funny that you just mentioned that because I was thinking along those lines when you posted your comment :). I already have batched GEMM code for some scenarios. Not to mention my Winograd implementation for NCHW. 

The DL track is pretty packed, that's the reason why your slot was shortened from 50 to 25 minutes. Like all the other talks. My talk was even cancelled. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-192636494,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192638889,192638889,MDEyOklzc3VlQ29tbWVudDE5MjYzODg4OQ==,9326960,2016-03-05T12:48:51Z,2016-03-05T12:48:51Z,NONE,"Yah, I thought of the technique while developing the first winograd kernel.  It's what I meant above in being able to leverage shared memory for in place transpose.  I actually already have some fp16 32x32 gemm tiles that use this that get over 5Tflops with a minibatch of 32.  The TN tile (col major) can even out perform the 128x128 tile because it halves the overall number of strided accesses to ddr at any one time.  

I haven't had a chance to release these yet since I need to still need to finish the complete set.  Hopefully I'll get to that in the next week or so.  I have new direct conv kernels I want to build first.  The cuDNN advantage on small minibatch (for non 3x3s1) will soon be going away :)  The goal is end to end training of convnets at very small minibatches at full utilization.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-192638889,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192639179,192639179,MDEyOklzc3VlQ29tbWVudDE5MjYzOTE3OQ==,43829,2016-03-05T12:54:44Z,2016-03-05T12:54:44Z,NONE,"> The issue we're having with cuDNN with 1x1 is that NCHW has the C ""in the middle"". Today, our direct convolution is similar to 3x3 or 5x5 for 1x1 and we are having a complex logic that we could simplify for 1x1. Scott's CHWN is ""easier"" to deal with in many cases. 

Right -- what I was saying is that, if NHWC is almost as good as CHWN, the former might be adopted much more quickly. Because TensorFlow already uses it, and because many people would find it ""more natural"" to have the batch size as the leading dimension.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-192639179,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192639628,192639628,MDEyOklzc3VlQ29tbWVudDE5MjYzOTYyOA==,9326960,2016-03-05T13:02:47Z,2016-03-05T13:02:47Z,NONE,"I actually really like NHWC a lot, but it means I can't use my fancy new fully fused fprop/bprop F(4x4,3x3) with it.  And I have a feeling the performance with it will be too good to throw away.  The current partially fused kernels are trivial to convert to any layout.  Just modify the external transform cuda-c code and then tweak a few lines in the batched gemm assembly for setting up the output pointers.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-192639628,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192640469,192640469,MDEyOklzc3VlQ29tbWVudDE5MjY0MDQ2OQ==,1792006,2016-03-05T13:10:15Z,2016-03-05T13:10:15Z,NONE,"@benanne: I agree with you... We want a layout which can be adopted by the community and which is good for performance. NCHW is widely adopted (and we are making it faster at each new release of cuDNN). CHWN is easier to deal with in many cases and Scott is pushing its performance to awesome levels but it is a somewhat weird layout. Maybe NHWC brings the best of both worlds together :). 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-192640469,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192640877,192640877,MDEyOklzc3VlQ29tbWVudDE5MjY0MDg3Nw==,1792006,2016-03-05T13:18:18Z,2016-03-05T13:18:18Z,NONE,"@scott-gray: What prevents you from using your new fused kernel with NHWC (except for the time to write it)? Is it a fetch issue due to your need for LDG.128?

Btw, were the numbers quoted in the benchmark all obtained using F(4x4,3x3) or do you use F(2x2,3x3) for some of the layers?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-192640877,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192641776,192641776,MDEyOklzc3VlQ29tbWVudDE5MjY0MTc3Ng==,43829,2016-03-05T13:25:36Z,2016-03-05T13:25:36Z,NONE,"Here's another approach for tackling this ""optimal layout"" issue in frameworks using the computational graph paradigm (such as Theano and TensorFlow): stick with the canonical NCHW or NHWC layout on the surface, but have optimizations that insert alternative implementations using more efficient layouts, as well as the necessary reshape operations at the input and output side. Since many convolution and pooling operations usually follow each other (and elementwise nonlinearities are not affected by the layout), spurious reshapes can then be eliminated quite easily.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-192641776,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192641828,192641828,MDEyOklzc3VlQ29tbWVudDE5MjY0MTgyOA==,8460517,2016-03-05T13:26:35Z,2016-03-05T13:26:35Z,NONE,"An API is also an important requirement for adoption. That is the real reason cuDNN has been so successful, it defined a low level C API for deep learning primitives, and nobody else did. cuDNN is both the standard API and its only implementation.

If Neon kernels were wrapped in the cuDNN API then it would be trivial to support them in your favorite framework (provided they are sane about allowing different tensor formats).

Maybe the cuDNN API is not ideal, maybe we could do better. But coding to a standard API is key to providing fast kernels that framework maintainers can actually use. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-192641828,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192643796,192643796,MDEyOklzc3VlQ29tbWVudDE5MjY0Mzc5Ng==,9326960,2016-03-05T13:33:45Z,2016-03-05T21:17:23Z,NONE,"@jdemouth: I pick the best of both.   Most of the time it's the 4x4 numbers.  But for small HWN the 2x2 can be faster.  Or I guess for small C/K too when the external transform isn't well amortized.  But the fully fused kernel will solve that.

The fused kernel has no available shared memory to do the in place transpose required of NHWC in fprop/bprop.  For update, the data is laid out fine, and that kernel could be fused instead.  But that's just 1 of 3 ops instead of 2/3.  Plus you want fprop to be the fastest for use in inference.  Also fusing update is much more problematic because there's a lot of predicates that need to be recomputed any time you change x or y.

@benanne That's basically what I recommended to the TF guys.  But you definitely want to avoid dimshuffles between every op.  But I guess your point is that the graph optimizer should be smart enough to eliminate dimshuffles that cancel each other out.

@andravin: I've wanted to put together an API but all of my time is devoted to writing kernels and just when I start to think things have stabilized enough to do this, someone comes along and asks you to implement some new fancy algorithm :)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-192643796,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192645401,192645401,MDEyOklzc3VlQ29tbWVudDE5MjY0NTQwMQ==,9326960,2016-03-05T13:40:29Z,2016-03-05T13:40:29Z,NONE,"To elaborate on NHWC in fprop/bprop, the 2 image load warps are making 32*36 loads to distinct addresses, only 2 channels deep.  That's way more than can fit in L1 so you end up fetching the same transaction 4 times and saturating both L2 and DDR traffic.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-192645401,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192646091,192646091,MDEyOklzc3VlQ29tbWVudDE5MjY0NjA5MQ==,8460517,2016-03-05T13:44:18Z,2016-03-05T13:44:18Z,NONE,"A standard API for deep learning primitives would also mean that frameworks would be able to support any GPU or hardware platform that implements the API. The fact that none of us are even thinking about that is another symptom of our dangerous monoculture.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-192646091,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192649769,192649769,MDEyOklzc3VlQ29tbWVudDE5MjY0OTc2OQ==,9326960,2016-03-05T13:54:03Z,2016-03-05T13:54:03Z,NONE,"An API has definitely been on my mind.. I just wanted to finish a complete set of kernels first.  The only problem is that I keep changing the definition of complete.  Anyway, I need to get some sleep.  It's been nice chatting with you guys.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-192649769,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192650997,192650997,MDEyOklzc3VlQ29tbWVudDE5MjY1MDk5Nw==,1792006,2016-03-05T14:05:56Z,2016-03-05T14:05:56Z,NONE,"Indeed, it was great chatting with all of you. Thanks.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-192650997,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192732728,192732728,MDEyOklzc3VlQ29tbWVudDE5MjczMjcyOA==,9326960,2016-03-05T20:49:39Z,2016-03-05T21:08:36Z,NONE,"Oh, and another interesting constraint is batch norm.   Reducing HWN is rather straight forward and fast with CHWN.   It's just a reshape(C,-1).sum(axis=1).  NCHW isn't too bad (but probably annoying).  NHWC is a bit trickier to optimize as axis=0 reductions lead to expensive strided memory access patterns if done naively.

Another interesting point on the Nervana kernels is that they all have the ""mean"" component of batchnorm optionally compounded directly inside of the conv kernel at not cost.  Currently this is done with atomics but I have a deterministic way I want to change it to that should be just as fast.  Many other common operations can be compounded inside of gemm/conv kernels.  Incidentally, all the kernels can now be run in full deterministic mode with virtually no change in performance.

Anyway, I'm looking forward to Soumith's new set of benchmarks.  There's a ton of optimizations that we've made in neon that the current set just don't expose. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-192732728,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192747077,192747077,MDEyOklzc3VlQ29tbWVudDE5Mjc0NzA3Nw==,9326960,2016-03-05T21:38:48Z,2016-03-05T21:45:53Z,NONE,"Oh, and another thought.  I recently wrote a very fast generalized [dimshuffle](https://github.com/NervanaSystems/neon/blob/master/neon/backends/nervanagpu.py#L2235) ([src](https://github.com/NervanaSystems/neon/blob/master/neon/backends/convolution.py#L981)) routine for neon that implements the full [numpy.transpose spec](http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.transpose.html).  So if there is some custom kernel you want to write that is more natural in one format over another, then it's now easy to get that.  And so long as you're not doing it on every layer there would be negligible impact to speed.  For example, ROI pooling for RCNN networks is far easier to implement with NHWC.  But even if you are using it a lot, it's about as fast as an fprop_relu op.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-192747077,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/194447374,194447374,MDEyOklzc3VlQ29tbWVudDE5NDQ0NzM3NA==,14060629,2016-03-09T18:48:33Z,2016-03-09T18:59:01Z,CONTRIBUTOR,"@scott-gray: Awesome, as always! What do you consider ""fully fused kernel"", compared to a ""partially fused kernel"", and what is its real advantages? Guarantee that the data is available in L1 or something else?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-194447374,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/194473347,194473347,MDEyOklzc3VlQ29tbWVudDE5NDQ3MzM0Nw==,9326960,2016-03-09T19:43:55Z,2016-03-09T19:43:55Z,NONE,"Right now the input transforms are handled with external cuda kernels.  Then the batched gemm kernel is able to fit all 36 tiles and do the output transform in place (fused).  I also now have an fprop/bprop kernel that does all transforms internally to one kernel.  This one performs well when HW is large (high external transform costs), but there are warp scheduling issues that I'm  not sure I'm going to be able to work around to make it faster overall.  If the fused kernel weren't having IPC issues it would have a huge advantage in L1/L2 utilization.  The transforms expand image and delta by 9/4 and filters by 4.  Plus you lose the overlap in image tiles after the transform.

The main advantage to the external transform is the ability to combine it with a transpose.  It's not possible to efficiently do transposes in place with a batched gemm kernel that consumes all your shared memory.  So in CHWN, the update operation will probably always have to use external transforms.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-194473347,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/194551307,194551307,MDEyOklzc3VlQ29tbWVudDE5NDU1MTMwNw==,14060629,2016-03-09T22:48:55Z,2016-03-09T22:48:55Z,CONTRIBUTOR,"Is ""input/output transform"" what the paper calls ""data/inverse transform"" or something else? For F(4x4, 3x3) those transforms do expand image and delta by 9/4 and filters by 4.

BTW, what is the breakdown of utilization for fprop + delta + update? From your comments in this thread and earlier, my best guess is (~300%+~200%+~100%)/3~=200%.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-194551307,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/194556519,194556519,MDEyOklzc3VlQ29tbWVudDE5NDU1NjUxOQ==,9326960,2016-03-09T23:00:40Z,2016-03-09T23:00:40Z,NONE,"For input/output I mean the transforms needed to be applied to the input/out of the batched gemm respectively.  Utilization is high for all 3 operations.  For fp32 and N=32 in VGG I get these speedups:
fprop:1.94x
bprop: 1.92x
update: 1.71x

I'll cover all this in more detail in the blog update.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-194556519,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/194973119,194973119,MDEyOklzc3VlQ29tbWVudDE5NDk3MzExOQ==,14060629,2016-03-10T17:45:27Z,2016-03-10T17:45:27Z,CONTRIBUTOR,"Thank you. Eagerly waiting for the blog update. I got confused by your earlier prediction of 3x on fprop with fp32 F(4x4, 3x3), and the following quote, I've misinterpreted. Slow due to impossible fusion, doesn't mean 1x, but ""only"" 1.71x:

> On the weight update side, fusion probably isn't possible due to the extremely strided memory access pattern required and no shared memory left for mitigating that. But 2 out of 3 fast operations isn't bad.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-194973119,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/194976521,194976521,MDEyOklzc3VlQ29tbWVudDE5NDk3NjUyMQ==,1792006,2016-03-10T17:55:31Z,2016-03-10T17:55:31Z,NONE,"Those speedups are already awesome. As soon as I'm done with a few other things, I'll work on the integration in cuDNN. ;)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-194976521,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/195017542,195017542,MDEyOklzc3VlQ29tbWVudDE5NTAxNzU0Mg==,9326960,2016-03-10T19:48:56Z,2016-03-10T19:48:56Z,NONE,"I'm guessing these fprop/bprop kernels wont be as fast in practice with the NCHW layout.  Frequently your HW dim is only moderate or small in size.  To get good contiguous memory access on the external transform you'll need to include multiple points of H and/or W in the 32 points of the outer product tile.  This will give you a larger effective tile size and hence the potential for more zero overlap. The amount of zero overlap is one of the biggest factors in determining the speedup.

In CHWN or NHWC you can load a single point of HW with maximal DDR efficiency, thereby minimizing your effective tile size.  

With small N you are forced to use multiple points of HW just to fill the gemm tile and I think all formats are about the same, perhaps with CHWN being slightly faster due to having potentially less overfetch and better cache utilization.

In the update operation you can skip over bad points of HW while reducing them and hence it performs more consistently with small N.  On the transform they're all probably just as fast.. but again NCHW has overfetch potential.  

It's just basically a bad idea to have your inner contiguous dimension to possibly be an odd (or non-power of 2) number.  This is going to make fp16x2 to be rather difficult to implement on Pascal.  And I predict cuDNN will be forced to switch at this point.  Both CHWN and NHWC are good options with CHWN being a bit faster for winograd, and NHWC being slightly better for direct conv.  CHWN will be faster for inference work.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-195017542,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/195038326,195038326,MDEyOklzc3VlQ29tbWVudDE5NTAzODMyNg==,1792006,2016-03-10T20:44:24Z,2016-03-10T20:44:24Z,NONE,"To say the least NCHW has some disadvantages ;)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-195038326,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/198671876,198671876,MDEyOklzc3VlQ29tbWVudDE5ODY3MTg3Ng==,1710528,2016-03-19T09:15:52Z,2016-03-19T09:15:52Z,NONE,"@scott-gray @andravin A vendor neutral API would really be a game changer. In the Vulkan era do you think that will be enougth to target SPIR-V? /cc @naibaf7 @keryell
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-198671876,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/198675261,198675261,MDEyOklzc3VlQ29tbWVudDE5ODY3NTI2MQ==,1792006,2016-03-19T09:44:30Z,2016-03-19T09:44:30Z,NONE,"@bhack Both Scott and us target our (NVIDIA) GPU arch using assembly for those very specific tasks (the rest of the code is written in high level/easy to use CUDA). So far compiler generated code has not reached the level of performance achieved by ninja programmers directly in assembly. The compiler has hard time doing perfect register allocation and instruction scheduling. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-198675261,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/198677013,198677013,MDEyOklzc3VlQ29tbWVudDE5ODY3NzAxMw==,1710528,2016-03-19T09:58:24Z,2016-03-19T09:58:24Z,NONE,"@jdemouth Yes i know that all are working at assembly level, a level that in AMD world we could call GCN. But I hoped that a common target like SPIR-V changed something on compiler optimization side. What about targeting LLVM? AMD started to release an interesting [GCN 3.0 llvm backend](http://llvm.org/docs/AMDGPUUsage.html)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-198677013,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/198681180,198681180,MDEyOklzc3VlQ29tbWVudDE5ODY4MTE4MA==,1792006,2016-03-19T10:29:21Z,2016-03-19T10:29:21Z,NONE,"Most of the optimization work is done at the level covered by the compiler backend (register allocation and instruction scheduling). It comes after IR generation so most (all?) the problem remains identical. The techniques we use seem very hard (impossible?) to implement in a compiler backend. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-198681180,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/198681694,198681694,MDEyOklzc3VlQ29tbWVudDE5ODY4MTY5NA==,1710528,2016-03-19T10:41:07Z,2016-03-19T10:41:07Z,NONE,"@LunarG has heavily experimented on a two step IR but on shaders. Probably can give us some feedback on backend limits based on LunarGlass experience.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-198681694,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/198681926,198681926,MDEyOklzc3VlQ29tbWVudDE5ODY4MTkyNg==,1792006,2016-03-19T10:47:01Z,2016-03-19T10:47:01Z,NONE,"Don't get me wrong. I'm not saying compilers do a bad job in general. Most of the time they do awesome. However, fast convolutions are extremely complicated and some of the techniques are even hard to express in a high level language.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-198681926,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/198682325,198682325,MDEyOklzc3VlQ29tbWVudDE5ODY4MjMyNQ==,1710528,2016-03-19T10:54:59Z,2016-03-19T10:54:59Z,NONE,"@jdemouth Yes I'm only guessing if exist and what is the vendor neutral ""lowest common denominator"". This could still give a margin for hardware vendors to compete but also give some chance for an unified API for developer interested in a vendor neutral solution.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-198682325,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/198683216,198683216,MDEyOklzc3VlQ29tbWVudDE5ODY4MzIxNg==,1792006,2016-03-19T11:05:00Z,2016-03-19T11:05:00Z,NONE,"I get your point. So far, I do not see the solution but having a higher level solution - if interesting for vendor neutrality - would surely help with development time and innovation.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-198683216,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/198686116,198686116,MDEyOklzc3VlQ29tbWVudDE5ODY4NjExNg==,1710528,2016-03-19T11:16:38Z,2016-03-19T12:13:05Z,NONE,"[OpenVX](https://www.khronos.org/openvx/) was a good example for interface collaboration that involved many stakeholders (Nvidia included). But it has totally lost the occasions to cover deep learning needs in the actual release. In the meantime Google is trying to push an llvm subgroup for stream executor with ""deep learning"" canned operations. See last messages in https://github.com/tensorflow/tensorflow/issues/22 and @henline bootstrap doc at https://github.com/henline/streamexecutordoc
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-198686116,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/198745652,198745652,MDEyOklzc3VlQ29tbWVudDE5ODc0NTY1Mg==,9326960,2016-03-19T16:33:57Z,2016-03-19T16:33:57Z,NONE,"It is simply not possible to develop efficient dense linear algebra kernels with the current intermediate representations available (like ptx).  That's not to say that an IR couldn't be developed that would make it possible.  Pascal will be largely binary compatible with the Maxwell ISA, but when Volta rolls around I may adapt my assembler to let you target both architectures with one language.  Though maybe it's not worth the effort since new hardware always frees up more resources making kernel design decisions very different.

I guess the real key would be to have an IR that can target both Nvidia and AMD.  I've read through the GCN spec and there's a lot of overlap, but again the differences are big enough to make a large impact in how you would design kernels.  But still, having a common language would make development for both targets much easier and perhaps allow some code sharing.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-198745652,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/198771664,198771664,MDEyOklzc3VlQ29tbWVudDE5ODc3MTY2NA==,1710528,2016-03-19T19:13:56Z,2016-03-19T21:57:20Z,NONE,"The common IR that target both NVIDIA and AMD is SPIR-V (and was co-designed). But seems that it is not enough for achieve this level of optimization. So if the biggest common multi stakeholders effort on a common IR is not enough I think that is better to extend standard API at higher level like pushing, in the next version of OpenVX, support for unfied tensor operations API that fits deep learning needs.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-198771664,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/198774152,198774152,MDEyOklzc3VlQ29tbWVudDE5ODc3NDE1Mg==,1792006,2016-03-19T19:25:42Z,2016-03-19T19:25:42Z,NONE,"Yes, the high level API approach looks more promising. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-198774152,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/198777263,198777263,MDEyOklzc3VlQ29tbWVudDE5ODc3NzI2Mw==,9326960,2016-03-19T19:55:32Z,2016-03-19T19:55:32Z,NONE,"I think there's still room for better abstractions at the lowest level.  Deep learning is currently somewhere between 60-90% dense linear algebra of some kind.  And the parts that aren't will soon largely be merged into the dense kernels for better efficiency.  That doesn't leave a lot left for higher level gpu languages.

Aside form simple operation compounding there may be other innovations we'll want to make with these dense kernels.  We know the brain's connections are far from the simple feed forward nets currently in use.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-198777263,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/198778356,198778356,MDEyOklzc3VlQ29tbWVudDE5ODc3ODM1Ng==,1710528,2016-03-19T20:08:37Z,2016-03-19T20:10:33Z,NONE,"@jdemouth If you like the idea you can try to talk about this with Thierry Lepley. He is the Nvidia representative in the Khronos OpenVX standardization group.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-198778356,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/198788334,198788334,MDEyOklzc3VlQ29tbWVudDE5ODc4ODMzNA==,1792006,2016-03-19T21:40:32Z,2016-03-19T21:40:32Z,NONE,"Sure. Thierry and I are both in France :)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-198788334,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/198946875,198946875,MDEyOklzc3VlQ29tbWVudDE5ODk0Njg3NQ==,1710528,2016-03-20T15:01:22Z,2016-03-20T15:01:22Z,NONE,"And I think that we could start to consider also the interesting results from [binarizzation](http://arxiv.org/abs/1603.05279)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-198946875,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/198982804,198982804,MDEyOklzc3VlQ29tbWVudDE5ODk4MjgwNA==,9326960,2016-03-20T18:31:52Z,2016-03-20T18:31:52Z,NONE,"I plan on implementing a set of fast binary gemm and conv kernels as soon as I'm done with the small minibatch floating point kernels I'm working on.  
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-198982804,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/198986729,198986729,MDEyOklzc3VlQ29tbWVudDE5ODk4NjcyOQ==,1710528,2016-03-20T18:46:57Z,2016-03-20T18:46:57Z,NONE,"This start to become interesting also on CPU. /cc @xianyi
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-198986729,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/200203030,200203030,MDEyOklzc3VlQ29tbWVudDIwMDIwMzAzMA==,5112846,2016-03-23T06:21:41Z,2016-03-23T06:21:41Z,NONE,"@scott-gray 
""I plan on implementing a set of fast binary gemm and conv kernels""
I am interested in this work are you planning for
     i)  real input + binary weights
or iI)  binary input + binary weights (XNOR net) ?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-200203030,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/200364717,200364717,MDEyOklzc3VlQ29tbWVudDIwMDM2NDcxNw==,1710528,2016-03-23T14:21:00Z,2016-03-23T14:21:09Z,NONE,"@xianyi On AVX-512 I think that there is a XNOR instruction __mmask16 _mm512_kxnor? Do you think that this kind of operators could be included in openblas?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-200364717,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/200514873,200514873,MDEyOklzc3VlQ29tbWVudDIwMDUxNDg3Mw==,9326960,2016-03-23T19:43:57Z,2016-03-23T19:43:57Z,NONE,"@hengck23 The XNOR/POPC variety.  Should be pretty easy to implement with the only real challenge being implementing padding in convolution.  It will all just be cuda-c.   I don't think there's a need for assembly optimization until we have a full throughput integrated xnor/popc/accumulate instruction
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-200514873,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/200515977,200515977,MDEyOklzc3VlQ29tbWVudDIwMDUxNTk3Nw==,1710528,2016-03-23T19:48:21Z,2016-03-23T19:48:21Z,NONE,"A kernel under BSD it is at https://github.com/MatthieuCourbariaux/BinaryNet/blob/master/Run-time/binary_kernels.cu
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-200515977,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/200518733,200518733,MDEyOklzc3VlQ29tbWVudDIwMDUxODczMw==,9326960,2016-03-23T19:57:18Z,2016-03-23T19:57:18Z,NONE,"@bhack yup, I'm aware of that and am actually working with Matthieu on a much faster and more flexible version of that code.  I just need to push through this other work first.  Which, btw, is working out pretty well.  I'm getting really good L1 cache utilization in CHWN for small N.  This keeps the power levels lower and let's the clock run faster.  Starting to think NWHC is really only good for N<4.  Below 4 your slicing logic starts to eat into compute without NHWC.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-200518733,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/221551944,221551944,MDEyOklzc3VlQ29tbWVudDIyMTU1MTk0NA==,123560,2016-05-25T11:59:54Z,2016-05-25T12:05:46Z,CONTRIBUTOR,"bhack wrote:

> The common IR that target both NVIDIA and AMD is SPIR-V (and was co-designed).

Interesting.  Seems I should take a look at this...

andrew wrote:

> An API is also an important requirement for adoption. That is the real reason cuDNN has been so successful, it defined a low level C API for deep learning primitives, and nobody else did. cuDNN is both the standard API and its only implementation.
> If Neon kernels were wrapped in the cuDNN API then it would be trivial to support them in your favorite framework (provided they are sane about allowing different tensor formats).
> Maybe the cuDNN API is not ideal, maybe we could do better. But coding to a standard API is key to providing fast kernels that framework maintainers can actually use.

Agree with these points.

Note: if you can allow me to pass a `cl_mem` to your api, I'd be happy to figure out a way to use neon convolutional kernels from my OpenCL frameworks, ie clTorch and DeepCL.  I'd load it in some pluggable way, so it would be used when running on a CUDA GPU.

Scott wrote:

> I recently wrote a very fast generalized dimshuffle (src) routine for neon that implements the full numpy.transpose spec. [...] it's about as fast as an fprop_relu op.

That sounds pretty fast.  So on the whole the layout doesnt matter too much, can just shuffle before calling the convolutional implementation, if necessary?

Scott wrote:

> It is simply not possible to develop efficient dense linear algebra kernels with the current intermediate representations available (like ptx).

Ok, you mean, for SPIR-V, SPIR-V plausibly targets the PTX level, rather than the SASS level, and therefore cant get results much better than current CUDA and OpenCL high-level languages?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-221551944,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/221559683,221559683,MDEyOklzc3VlQ29tbWVudDIyMTU1OTY4Mw==,1710528,2016-05-25T12:29:52Z,2016-05-25T12:29:52Z,NONE,"Kronos maintain a bidirectional [LLVM to SPIR-V translator](https://github.com/KhronosGroup/SPIRV-LLVM). There are also some efforts inside Kronos on the OpenVX roadmap to support dnn.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-221559683,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/221567181,221567181,MDEyOklzc3VlQ29tbWVudDIyMTU2NzE4MQ==,123560,2016-05-25T12:56:47Z,2016-05-25T12:56:47Z,CONTRIBUTOR,"bhack wrote:

> Kronos maintain a bidirectional LLVM to SPIR-V translator.

If I understand correctly, that will translate between OpenCL and SPIR-V?  What advantage(s) do you see to doing this?  At the moment, the source code I want to use is all written in [sass](https://github.com/NervanaSystems/neon/blob/master/neon/backends/kernels/sass/xconv_winograd_4x4_3x3_32x32.sass)  I was thinking of porting it to OpenCL (admittedly saying 'only 1300 lines of sass' is a bit like saying [nips only needs 8 pages](https://www.reddit.com/r/MachineLearning/comments/4k4j5z/what_can_i_do_right_now_to_get_into_nips/) ...), but maybe I should port it to SPIR-V instead???

Thoughts on how useable/mature/portable SPIR-V is right now?  I dont know much beyond the name, and a one-line description...
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-221567181,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/221571160,221571160,MDEyOklzc3VlQ29tbWVudDIyMTU3MTE2MA==,1710528,2016-05-25T13:11:27Z,2016-05-25T15:41:28Z,NONE,"It will translate also compatibile LLVM bytecode in SPIR-V. 
SPIR-V is also the machine model for Vulkan.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-221571160,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/221572891,221572891,MDEyOklzc3VlQ29tbWVudDIyMTU3Mjg5MQ==,1710528,2016-05-25T13:18:06Z,2016-05-25T13:25:27Z,NONE,"@hughperkins If you want to generally take an overview of these differents IR you can see https://www.linkedin.com/pulse/era-intermediate-languages-vincent-hindriksen. I think that sass it is more at gcn level in the AMD world. But the majority of opinions here is that we cannot go so fast with an IR cause GPU assembly ninjas are still better than compilers to optimize :smiley:
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-221572891,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/221573723,221573723,MDEyOklzc3VlQ29tbWVudDIyMTU3MzcyMw==,123560,2016-05-25T13:20:58Z,2016-05-25T13:20:58Z,CONTRIBUTOR,"> If you want to generally take an overview of these differents IR you can see https://www.linkedin.com/pulse/era-intermediate-languages-vincent-hindriksen. 

Good information.  Thanks!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-221573723,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,138356929,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/221576407,221576407,MDEyOklzc3VlQ29tbWVudDIyMTU3NjQwNw==,123560,2016-05-25T13:30:37Z,2016-05-25T13:31:16Z,CONTRIBUTOR,">  If you want to generally take an overview of these differents IR you can see https://www.linkedin.com/pulse/era-intermediate-languages-vincent-hindriksen. 

Probably I should target AMD IL or HSAIL.  However, I lack an AMD GPU.  So ...
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93/comments,https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-221576407,https://api.github.com/repos/soumith/convnet-benchmarks/issues/93
soumith,convnet-benchmarks,137210492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/190263592,190263592,MDEyOklzc3VlQ29tbWVudDE5MDI2MzU5Mg==,1310570,2016-02-29T15:47:21Z,2016-02-29T15:47:21Z,OWNER,"Merci!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/92/comments,https://github.com/soumith/convnet-benchmarks/pull/92#issuecomment-190263592,https://api.github.com/repos/soumith/convnet-benchmarks/issues/92
soumith,convnet-benchmarks,137141243,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/190031593,190031593,MDEyOklzc3VlQ29tbWVudDE5MDAzMTU5Mw==,1310570,2016-02-29T04:22:25Z,2016-02-29T04:22:25Z,OWNER,"@rryan i dont think the sparse softmax will improve the overall benchmarks, the time contributed by a 1000-way softmax is very little. I'll def change reduce_mean to reduce_sum + division, if you think it helps.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/91/comments,https://github.com/soumith/convnet-benchmarks/issues/91#issuecomment-190031593,https://api.github.com/repos/soumith/convnet-benchmarks/issues/91
soumith,convnet-benchmarks,137141243,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/190348183,190348183,MDEyOklzc3VlQ29tbWVudDE5MDM0ODE4Mw==,26527,2016-02-29T19:32:35Z,2016-02-29T19:32:35Z,NONE,"@soumith -- I can't say how much it would affect this benchmark. On a much simpler model -- [the CIFAR-10 multi-GPU example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py) -- replacing reduce_mean sped up my batches by 5% or so on a k20.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/91/comments,https://github.com/soumith/convnet-benchmarks/issues/91#issuecomment-190348183,https://api.github.com/repos/soumith/convnet-benchmarks/issues/91
soumith,convnet-benchmarks,137117686,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/189981105,189981105,MDEyOklzc3VlQ29tbWVudDE4OTk4MTEwNQ==,1310570,2016-02-29T00:41:43Z,2016-02-29T00:41:43Z,OWNER,"i do not think this will give us a lot more data points, but i am happy to do it. Caffe install is always a bit of a tightrope balancing act to get right, i'll do it in a few days.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/90/comments,https://github.com/soumith/convnet-benchmarks/issues/90#issuecomment-189981105,https://api.github.com/repos/soumith/convnet-benchmarks/issues/90
soumith,convnet-benchmarks,137117686,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/190013189,190013189,MDEyOklzc3VlQ29tbWVudDE5MDAxMzE4OQ==,1115209,2016-02-29T02:52:16Z,2016-02-29T02:52:16Z,NONE,"Thanks!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/90/comments,https://github.com/soumith/convnet-benchmarks/issues/90#issuecomment-190013189,https://api.github.com/repos/soumith/convnet-benchmarks/issues/90
soumith,convnet-benchmarks,137117686,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/190030708,190030708,MDEyOklzc3VlQ29tbWVudDE5MDAzMDcwOA==,1310570,2016-02-29T04:13:24Z,2016-02-29T04:20:15Z,OWNER,"I've ran the caffe numbers here:
https://github.com/soumith/convnet-benchmarks/commit/6f718dbcfdaefe1af6c04ab2be3927e0728b599e

It is strange, because the caffe numbers look to be quite off.
Alexnet:  128ms          vs 81ms   (Torch-fp32)
Overfeat: 430ms         vs 268ms (Torch-fp32) 
VGG-A: 680ms           vs 529ms (Torch-fp32)
Googlenet: 484ms.     vs 470ms (Torch-fp32) 

The only thing I can think of right now is that Torch enables the CuDNN autotuner (via a caching mechanism on sizes / strides ), and I suspect that Caffe does not enable it, and just uses cudnn heuristics, which are not always best perf.

In fact, now I am suspecting that maybe TF also does not enable autotuner.

The only network where Caffe looks close to Torch is Googlenet, and it seems to have serious perf regressions for the other 3. (though both are using the same code, i.e. CuDNN R4 + CuBLAS 7.5)

Should I add these numbers to the readme?
Considering how sensitive the benchmarks have become, I would want someone from the Caffe ecosystem to have a quick look at the prototxt files to see if there's any new settings I should add that were recent.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/90/comments,https://github.com/soumith/convnet-benchmarks/issues/90#issuecomment-190030708,https://api.github.com/repos/soumith/convnet-benchmarks/issues/90
soumith,convnet-benchmarks,137117686,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/190074188,190074188,MDEyOklzc3VlQ29tbWVudDE5MDA3NDE4OA==,3530657,2016-02-29T07:25:50Z,2016-02-29T07:25:50Z,CONTRIBUTOR,"Adding them with a slight warning containing your second paragraph seems a good thing to do... better than keeping with the 'native' bench IMO... Thanks for the great work.
I can take a look at the Caffe bench and `prototxt` files a bit later in the day if this helps.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/90/comments,https://github.com/soumith/convnet-benchmarks/issues/90#issuecomment-190074188,https://api.github.com/repos/soumith/convnet-benchmarks/issues/90
soumith,convnet-benchmarks,137117686,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/190082552,190082552,MDEyOklzc3VlQ29tbWVudDE5MDA4MjU1Mg==,3530657,2016-02-29T07:52:49Z,2016-02-29T07:53:08Z,CONTRIBUTOR,"OK, so quick remarks:
- the `.prototxt` files are in old format, not that it matters much I believe, but I could PR an update if you're interested
- `alexnet.prototxt` is missing the `ReLU` in between `fc6`,`fc7`,`fc8`, not sure whether this is on purpose ?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/90/comments,https://github.com/soumith/convnet-benchmarks/issues/90#issuecomment-190082552,https://api.github.com/repos/soumith/convnet-benchmarks/issues/90
soumith,convnet-benchmarks,137117686,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/190083526,190083526,MDEyOklzc3VlQ29tbWVudDE5MDA4MzUyNg==,1310570,2016-02-29T07:54:53Z,2016-02-29T07:54:53Z,OWNER,"@beniz def up for a PR to make it up to date. the missing ReLU are def an oversight, have to be added.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/90/comments,https://github.com/soumith/convnet-benchmarks/issues/90#issuecomment-190083526,https://api.github.com/repos/soumith/convnet-benchmarks/issues/90
soumith,convnet-benchmarks,137117686,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/190093096,190093096,MDEyOklzc3VlQ29tbWVudDE5MDA5MzA5Ng==,601283,2016-02-29T08:24:25Z,2016-02-29T08:24:25Z,NONE,"I recently looked into the performance of Caffe when bringing our framework Leaf up to speed and I can confirm that the biggest speed hit comes from not using the autotuner. Caffe is also loosing a bit of time (IIRC 2-3ms) because it reshapes its layers on every forward pass, where it reallocates some cuDNN descriptors.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/90/comments,https://github.com/soumith/convnet-benchmarks/issues/90#issuecomment-190093096,https://api.github.com/repos/soumith/convnet-benchmarks/issues/90
soumith,convnet-benchmarks,135836957,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/189979235,189979235,MDEyOklzc3VlQ29tbWVudDE4OTk3OTIzNQ==,1310570,2016-02-29T00:23:36Z,2016-02-29T00:23:36Z,OWNER,"i dont mind benchmarking, but i dont have the time to extend convnet-benchmarks to this.
I am working on a bigger fuller benchmark suite that spans all of deep learning, so all my benchmark time is occupied by writing scripts for those.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/88/comments,https://github.com/soumith/convnet-benchmarks/issues/88#issuecomment-189979235,https://api.github.com/repos/soumith/convnet-benchmarks/issues/88
soumith,convnet-benchmarks,135836957,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/189980278,189980278,MDEyOklzc3VlQ29tbWVudDE4OTk4MDI3OA==,1710528,2016-02-29T00:31:13Z,2016-02-29T00:31:13Z,NONE,"Do you have an idea of when this will be published?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/88/comments,https://github.com/soumith/convnet-benchmarks/issues/88#issuecomment-189980278,https://api.github.com/repos/soumith/convnet-benchmarks/issues/88
soumith,convnet-benchmarks,135836957,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/189980308,189980308,MDEyOklzc3VlQ29tbWVudDE4OTk4MDMwOA==,1310570,2016-02-29T00:31:42Z,2016-02-29T00:31:42Z,OWNER,"Target date is April 1st, around the time of GTC
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/88/comments,https://github.com/soumith/convnet-benchmarks/issues/88#issuecomment-189980308,https://api.github.com/repos/soumith/convnet-benchmarks/issues/88
soumith,convnet-benchmarks,135836957,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/189980393,189980393,MDEyOklzc3VlQ29tbWVudDE4OTk4MDM5Mw==,1710528,2016-02-29T00:32:55Z,2016-02-29T00:32:55Z,NONE,"Nice so probably we will put a GSoC student to this new one. Thanks.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/88/comments,https://github.com/soumith/convnet-benchmarks/issues/88#issuecomment-189980393,https://api.github.com/repos/soumith/convnet-benchmarks/issues/88
soumith,convnet-benchmarks,134392661,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/189947846,189947846,MDEyOklzc3VlQ29tbWVudDE4OTk0Nzg0Ng==,1381301,2016-02-28T21:26:14Z,2016-02-28T23:41:27Z,NONE,"For people who're curious, I ran the benchmark for TensorFlow 0.7.0 with cuda7.5 and cudnnR4, on Quadro M6000 (very similar to TitanX). I saw a 20-40% speedup compared to the old numbers in this repo.
For alexnet/vgg where there're a lot more intense convolutions, the speedup is higher. For googlenet it's about 20% only. There is still an obvious gap compared to other libraries with cudnnR3.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/86/comments,https://github.com/soumith/convnet-benchmarks/issues/86#issuecomment-189947846,https://api.github.com/repos/soumith/convnet-benchmarks/issues/86
soumith,convnet-benchmarks,134392661,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/189947912,189947912,MDEyOklzc3VlQ29tbWVudDE4OTk0NzkxMg==,1310570,2016-02-28T21:27:06Z,2016-02-28T21:27:06Z,OWNER,"thanks. i'll run the numbers in a couple of hours.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/86/comments,https://github.com/soumith/convnet-benchmarks/issues/86#issuecomment-189947912,https://api.github.com/repos/soumith/convnet-benchmarks/issues/86
soumith,convnet-benchmarks,134392661,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/189978722,189978722,MDEyOklzc3VlQ29tbWVudDE4OTk3ODcyMg==,1310570,2016-02-29T00:22:33Z,2016-02-29T00:22:33Z,OWNER,"closed via https://github.com/soumith/convnet-benchmarks/commit/2888b23959190cefeee59cdd5e15f66a74031f8f
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/86/comments,https://github.com/soumith/convnet-benchmarks/issues/86#issuecomment-189978722,https://api.github.com/repos/soumith/convnet-benchmarks/issues/86
soumith,convnet-benchmarks,132768720,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/182517511,182517511,MDEyOklzc3VlQ29tbWVudDE4MjUxNzUxMQ==,1310570,2016-02-10T18:29:33Z,2016-02-10T18:29:33Z,OWNER,"use tensorflow support.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/84/comments,https://github.com/soumith/convnet-benchmarks/issues/84#issuecomment-182517511,https://api.github.com/repos/soumith/convnet-benchmarks/issues/84
soumith,convnet-benchmarks,131704614,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/180491406,180491406,MDEyOklzc3VlQ29tbWVudDE4MDQ5MTQwNg==,1248454,2016-02-05T18:37:21Z,2016-02-05T18:37:21Z,NONE,"Are you on the most recent version of MXNet and mshadow? I just ran mine off of master and it worked fine.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/82/comments,https://github.com/soumith/convnet-benchmarks/issues/82#issuecomment-180491406,https://api.github.com/repos/soumith/convnet-benchmarks/issues/82
soumith,convnet-benchmarks,131704614,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/239476249,239476249,MDEyOklzc3VlQ29tbWVudDIzOTQ3NjI0OQ==,13065142,2016-08-12T15:22:54Z,2016-08-12T15:22:54Z,NONE,"I have the same problem 
1. CPU only
2. simply excute https://github.com/soumith/convnet-benchmarks/blob/master/mxnet/gnetv1.py 
3. get above error on 16/08/12 (should add padding) and I found this error in Tag V0.7, Tag16016
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/82/comments,https://github.com/soumith/convnet-benchmarks/issues/82#issuecomment-239476249,https://api.github.com/repos/soumith/convnet-benchmarks/issues/82
soumith,convnet-benchmarks,131704614,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/243562401,243562401,MDEyOklzc3VlQ29tbWVudDI0MzU2MjQwMQ==,3160803,2016-08-30T20:05:59Z,2016-08-30T20:05:59Z,NONE,"I have a different error:
[15:03:52] /root/DL/mxnet/dmlc-core/include/dmlc/logging.h:235: [15:03:52] src/operator/./pooling-inl.h:200: Check failed: param_.kernel[0] <= dshape[2] + 2 \* param_.pad[0] && param_.kernel[1] <= dshape[3] + 2 \* param_.pad[1] kernel size exceed input
Traceback (most recent call last):
  File ""gnetv1.py"", line 88, in <module>
    g_exec = loss3_classifier.simple_bind(ctx=dev, grad_req=""write"", data=dshape)
  File ""/usr/lib/python2.7/site-packages/mxnet-0.7.0-py2.7.egg/mxnet/symbol.py"", line 671, in simple_bind
    arg_shapes, _, aux_shapes = self.infer_shape(*_kwargs)
  File ""/usr/lib/python2.7/site-packages/mxnet-0.7.0-py2.7.egg/mxnet/symbol.py"", line 453, in infer_shape
    return self._infer_shape_impl(False, *args, *_kwargs)
  File ""/usr/lib/python2.7/site-packages/mxnet-0.7.0-py2.7.egg/mxnet/symbol.py"", line 513, in _infer_shape_impl
    ctypes.byref(complete)))
  File ""/usr/lib/python2.7/site-packages/mxnet-0.7.0-py2.7.egg/mxnet/base.py"", line 77, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: InferShape Error in pooling4: [15:03:52] src/operator/./pooling-inl.h:200: Check failed: param_.kernel[0] <= dshape[2] + 2 \* param_.pad[0] && param_.kernel[1] <= dshape[3] + 2 \* param_.pad[1] kernel size exceed input

Anyone knows how to solve it?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/82/comments,https://github.com/soumith/convnet-benchmarks/issues/82#issuecomment-243562401,https://api.github.com/repos/soumith/convnet-benchmarks/issues/82
soumith,convnet-benchmarks,129285542,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/175888788,175888788,MDEyOklzc3VlQ29tbWVudDE3NTg4ODc4OA==,1310570,2016-01-27T22:15:57Z,2016-01-27T22:15:57Z,OWNER,"that looks very interesting. I'll have to get my hands on one of those Fiji cards first.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/81/comments,https://github.com/soumith/convnet-benchmarks/issues/81#issuecomment-175888788,https://api.github.com/repos/soumith/convnet-benchmarks/issues/81
soumith,convnet-benchmarks,129285542,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/175891731,175891731,MDEyOklzc3VlQ29tbWVudDE3NTg5MTczMQ==,1710528,2016-01-27T22:24:20Z,2016-01-27T22:24:20Z,NONE,"@soumith I think you can get a partnership of your benchmark with @michaellarabel of phoronix. He has a plenty of hardware. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/81/comments,https://github.com/soumith/convnet-benchmarks/issues/81#issuecomment-175891731,https://api.github.com/repos/soumith/convnet-benchmarks/issues/81
soumith,convnet-benchmarks,129285542,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/175892505,175892505,MDEyOklzc3VlQ29tbWVudDE3NTg5MjUwNQ==,6035554,2016-01-27T22:27:33Z,2016-01-27T22:27:33Z,NONE,"Is there any complete script anywhere for deploying the HcCaffe benchmark? If I can get it automated for PTS, can run a large comparison on many different GPUs.

My current test script for Caffe is http://openbenchmarking.org/innhold/622aa95f22029911b8cfc99e9ddc292d237e8571 if anyone wants to adapt it for HcCaffe as well, patches welcome.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/81/comments,https://github.com/soumith/convnet-benchmarks/issues/81#issuecomment-175892505,https://api.github.com/repos/soumith/convnet-benchmarks/issues/81
soumith,convnet-benchmarks,128387155,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/175849681,175849681,MDEyOklzc3VlQ29tbWVudDE3NTg0OTY4MQ==,1310570,2016-01-27T20:59:45Z,2016-01-27T20:59:45Z,OWNER,"Usually, I warm up the GPU by running the benchmarks a hundred times-ish (depends on which one and how long it is).
I get memory utilization from nvidia-smi in MB, such as: 927MiB / 12284MiB
I've also tried ever increasing batch sizes to find the breaking point where things go out of memory. 
GPU-Utilization varies between framework to framework, I dont really look at it much.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/79/comments,https://github.com/soumith/convnet-benchmarks/issues/79#issuecomment-175849681,https://api.github.com/repos/soumith/convnet-benchmarks/issues/79
soumith,convnet-benchmarks,128387155,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/175916474,175916474,MDEyOklzc3VlQ29tbWVudDE3NTkxNjQ3NA==,1889878,2016-01-27T23:57:35Z,2016-01-27T23:57:35Z,NONE,"Hmm, that is interesting. For TensorFlow this does not seem to work as it seems they allocate all of the free memory beforehead. For instance I get `11729MiB / 12287MiB` in-respectable of the size of the model I run. Have you tried Tf by any chance and got sensible result?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/79/comments,https://github.com/soumith/convnet-benchmarks/issues/79#issuecomment-175916474,https://api.github.com/repos/soumith/convnet-benchmarks/issues/79
soumith,convnet-benchmarks,128387155,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/176129753,176129753,MDEyOklzc3VlQ29tbWVudDE3NjEyOTc1Mw==,1310570,2016-01-28T11:19:55Z,2016-01-28T11:19:55Z,OWNER,"@Botev for TensorFlow, it maintains it pre-allocs and then manages it's own memory, ever-increasing batch sizes are your best bet. TF probably has some debugging tools for memory tracking, I just dont know enough about it.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/79/comments,https://github.com/soumith/convnet-benchmarks/issues/79#issuecomment-176129753,https://api.github.com/repos/soumith/convnet-benchmarks/issues/79
soumith,convnet-benchmarks,125301348,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/169517498,169517498,MDEyOklzc3VlQ29tbWVudDE2OTUxNzQ5OA==,1310570,2016-01-07T01:27:39Z,2016-01-07T01:27:39Z,OWNER,"It is indeed the case. I just added a note to the main README to reflect this:
https://github.com/soumith/convnet-benchmarks/blob/master/README.md#one-small-note
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/77/comments,https://github.com/soumith/convnet-benchmarks/issues/77#issuecomment-169517498,https://api.github.com/repos/soumith/convnet-benchmarks/issues/77
soumith,convnet-benchmarks,125301348,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/169518082,169518082,MDEyOklzc3VlQ29tbWVudDE2OTUxODA4Mg==,392019,2016-01-07T01:30:46Z,2016-01-07T01:30:46Z,NONE,"Thank you very much!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/77/comments,https://github.com/soumith/convnet-benchmarks/issues/77#issuecomment-169518082,https://api.github.com/repos/soumith/convnet-benchmarks/issues/77
soumith,convnet-benchmarks,123042728,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/165921171,165921171,MDEyOklzc3VlQ29tbWVudDE2NTkyMTE3MQ==,1310570,2015-12-18T23:38:27Z,2015-12-18T23:38:27Z,OWNER,"Thanks!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/76/comments,https://github.com/soumith/convnet-benchmarks/pull/76#issuecomment-165921171,https://api.github.com/repos/soumith/convnet-benchmarks/issues/76
soumith,convnet-benchmarks,121591982,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/163771648,163771648,MDEyOklzc3VlQ29tbWVudDE2Mzc3MTY0OA==,1310570,2015-12-10T22:34:17Z,2015-12-10T22:34:17Z,OWNER,"Thanks Yangqing.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/75/comments,https://github.com/soumith/convnet-benchmarks/pull/75#issuecomment-163771648,https://api.github.com/repos/soumith/convnet-benchmarks/issues/75
soumith,convnet-benchmarks,121543458,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/163735949,163735949,MDEyOklzc3VlQ29tbWVudDE2MzczNTk0OQ==,1310570,2015-12-10T20:07:56Z,2015-12-10T20:07:56Z,OWNER,"Hey Alexey, the CUDNN error messages are quite cryptic. Do you have the latest CuDNN, and CUDA 7.0 or above, as well as an NVIDIA driver of 352.xx or more...
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/74/comments,https://github.com/soumith/convnet-benchmarks/issues/74#issuecomment-163735949,https://api.github.com/repos/soumith/convnet-benchmarks/issues/74
soumith,convnet-benchmarks,121543458,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/163742080,163742080,MDEyOklzc3VlQ29tbWVudDE2Mzc0MjA4MA==,8118464,2015-12-10T20:35:26Z,2015-12-10T20:35:26Z,NONE,"Hi Soumith,
I have cuDNN v2, v3 as well as v4 on that machine, how to tell Torch which one to use (and how to determine which one it actually uses)?
As for the drivers: according to nvidia-smi, I have  346.72, however, other toolkits that use cuDNN (versions from v2 to v4) are working fine.
I can certainly update the drivers but I thought it might be related to Torch not able to find the correct version?
Thanks.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/74/comments,https://github.com/soumith/convnet-benchmarks/issues/74#issuecomment-163742080,https://api.github.com/repos/soumith/convnet-benchmarks/issues/74
soumith,convnet-benchmarks,121543458,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/163742411,163742411,MDEyOklzc3VlQ29tbWVudDE2Mzc0MjQxMQ==,1310570,2015-12-10T20:36:58Z,2015-12-10T20:36:58Z,OWNER,"@Alexey-Kamenev let's discuss here: https://github.com/soumith/cudnn.torch/issues/75
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/74/comments,https://github.com/soumith/convnet-benchmarks/issues/74#issuecomment-163742411,https://api.github.com/repos/soumith/convnet-benchmarks/issues/74
soumith,convnet-benchmarks,120354877,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/161983701,161983701,MDEyOklzc3VlQ29tbWVudDE2MTk4MzcwMQ==,123560,2015-12-04T14:45:56Z,2015-12-04T14:45:56Z,CONTRIBUTOR,"This is not my repo, I'm not affiliated with it, and I reckon such questions are best in the forums, eg the torch forums https://groups.google.com/forum/#!forum/torch7 but...

Each model is essentially a network design, like 'First layer is 32 5x5 convolutional filters, then a max pooling with stride 2, and pooling size, and then ... (etc...)'.

Such a network design is implementation-independent.  The code supplied with papers is one particular instantiation of the model, but many models can typically be instantiated using any of many possible libraries, eg torch, caffe, theano, mxnet, ...

And so, in the benchmarks, each model is instantiated for that library.  You can see the instantiations in the relevant sub-directory, eg for caffe, they are in https://github.com/soumith/convnet-benchmarks/tree/master/caffe  tensorflow is at https://github.com/soumith/convnet-benchmarks/tree/master/tensorflow , etc ...
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/72/comments,https://github.com/soumith/convnet-benchmarks/issues/72#issuecomment-161983701,https://api.github.com/repos/soumith/convnet-benchmarks/issues/72
soumith,convnet-benchmarks,120354877,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/162379893,162379893,MDEyOklzc3VlQ29tbWVudDE2MjM3OTg5Mw==,16148457,2015-12-07T00:26:25Z,2015-12-07T00:26:25Z,NONE,"Thank you ! :)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/72/comments,https://github.com/soumith/convnet-benchmarks/issues/72#issuecomment-162379893,https://api.github.com/repos/soumith/convnet-benchmarks/issues/72
soumith,convnet-benchmarks,120229488,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/161773046,161773046,MDEyOklzc3VlQ29tbWVudDE2MTc3MzA0Ng==,9326960,2015-12-03T20:27:25Z,2015-12-03T20:27:25Z,NONE,"If you're looking to benchmark performance of a long running kernel, turning off the boost clock will give you the most accurate results:

```
sudo nvidia-smi -i 1 --auto-boost-default=0
```

You can also adjust clocks more directly with:

```
sudo nvidia-smi -i 1 -ac 3505,1392
sudo nvidia-smi -i 1 -ac 3505,1000
```

This lets you run cuda at the full memory clock (with and without boost), but I'm not sure if that's wise without ECC.

But benchmarking with autoboost enabled is still useful as you can see exactly where the kernel becomes power limited.  The factor most important for power limits is the amount of DDR access.  So the more you can keep data in L2 or below, the less power you'll draw (and the easier the chip will be to cool).  

I like to set the power limit at 275 with full clocks and benchmark while running this:

```
sudo nvidia-smi -i 1 -pl 275
nvidia-smi -i 1 --loop-ms=333 --format=csv,noheader --query-gpu=power.draw,clocks.gr,temperature.gpu,fan.speed,clocks_throttle_reasons.sw_power_cap,clocks_throttle_reasons.hw_slowdown
```

This gives me a really good sense of the power profile of the kernel.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71/comments,https://github.com/soumith/convnet-benchmarks/issues/71#issuecomment-161773046,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71
soumith,convnet-benchmarks,120229488,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/162651384,162651384,MDEyOklzc3VlQ29tbWVudDE2MjY1MTM4NA==,14060629,2015-12-07T20:29:36Z,2015-12-07T20:37:38Z,CONTRIBUTOR,"> I like to set the power limit at 275

But Titan-X doesn't allow setting power limits (275W is default). The only way I found to meaningfully benchmark Titan-X, is to watch temperature and if it reaches 84 degrees, crank up fan manually (through a GUI, command-line has a bug and doesn't work), since stock BIOS inexplicably wouldn't do it. A hacked BIOS is probably the way to go for multiple Titan-X cards in server(s).

Overclocking my Titan-X, I was able to get 10-14% faster timings that reported by Soumith. Without overclocking, I reproduce his timings to within 1%.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71/comments,https://github.com/soumith/convnet-benchmarks/issues/71#issuecomment-162651384,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71
soumith,convnet-benchmarks,120229488,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/162656493,162656493,MDEyOklzc3VlQ29tbWVudDE2MjY1NjQ5Mw==,14060629,2015-12-07T20:45:16Z,2015-12-07T21:14:31Z,CONTRIBUTOR,"BTW, see commit https://github.com/soumith/convnet-benchmarks/commit/d6177f97e61da0d98a528f355086eb2fc05fe7b8 for how Soumith is doing warmup for both nervana and cudnn and its effect
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71/comments,https://github.com/soumith/convnet-benchmarks/issues/71#issuecomment-162656493,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71
soumith,convnet-benchmarks,120229488,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/162663730,162663730,MDEyOklzc3VlQ29tbWVudDE2MjY2MzczMA==,1310570,2015-12-07T21:09:21Z,2015-12-07T21:09:21Z,OWNER,"The thermal behavior on these GPUs is very very interesting.

While benchmarking, I constantly monitor the card to make sure it is at a stable clock rate (and the same clock all across).

Some more interesting things that one would want to know are that:
- Nervana's Neon kernels cant sustain boost clock over training time. They actually clock down if you run them for long enough. These kernels push the GPU to an absolute extreme. 
- CuDNN Kernels dont push the GPUs this hard overall. They went for lower power draw + FFT instead, to get the same performance.
- There seem to be power and speed optimizations depending on special cases of zero. If you send in a uniformly distributed input, it will run slightly slower than an all-zero input. @scott-gray observed the same. This is quite interesting, especially more in the context of ReLU nets.

Also, a quote from @scott-gray while we were discussing the benchmarks in an email thread (I wanted to make sure I was doing things right).

> The GPU has a very active power sensor and dynamically changes the clock depending on power draw (independent of temperature).  This happens on the millisecond time scale.  My fprop and bprop kernels run at 7.2 TFlops when the input is all 1 ones (or any other low entropy data).  Switching to random data they top out at 6.6 Tflops or so.  One of the reasons that fp16 is faster (aside from reduced bandwidth) is that after converting to fp32 for compute only 10 bits of the mantissa are populated.  You can compare the difference if you truncate the inputs to fp16 and then convert back again to fp32 prior to sending the data to the fp32 kernels.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71/comments,https://github.com/soumith/convnet-benchmarks/issues/71#issuecomment-162663730,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71
soumith,convnet-benchmarks,120229488,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/162668733,162668733,MDEyOklzc3VlQ29tbWVudDE2MjY2ODczMw==,14060629,2015-12-07T21:26:26Z,2015-12-07T21:26:26Z,CONTRIBUTOR,"> There seem to be power and speed optimizations depending on special cases of zero.

very interesting. This must be in hardware. I wonder if the hardware automatically uses less power when many bits are zero, or it's an explicit hardware optimization. Maybe all mantissa bits must be zero for that?

> My fprop and bprop kernels run at 7.2 TFlops when the input is all 1 ones (or any other low entropy data). 

Really, any other low entropy data? Or data with lots (or even all) zero bits? What if bits are all 1? Maybe it'll be even lower than random?

> Switching to random data they top out at 6.6 Tflops or so.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71/comments,https://github.com/soumith/convnet-benchmarks/issues/71#issuecomment-162668733,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71
soumith,convnet-benchmarks,120229488,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/162706657,162706657,MDEyOklzc3VlQ29tbWVudDE2MjcwNjY1Nw==,14060629,2015-12-07T23:42:07Z,2015-12-08T00:47:54Z,CONTRIBUTOR,"FWIW, the following shows data-dependent (all-zero vs all-one) 7% difference of power consumption on integer matrix multiplication on AVR. Titan-X is likely to have the same effect, IMO, even without explicit hardware optimizations, if they exist at all.

""Data dependent energy modelling for worst case energy consumption analysis"" (2015) Pallister et al 
http://arxiv.org/abs/1505.03374
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71/comments,https://github.com/soumith/convnet-benchmarks/issues/71#issuecomment-162706657,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71
soumith,convnet-benchmarks,120229488,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/162768012,162768012,MDEyOklzc3VlQ29tbWVudDE2Mjc2ODAxMg==,9326960,2015-12-08T05:11:19Z,2015-12-08T05:11:19Z,NONE,"My TitanX defaults to a power limit of 250.  For power draw it's the toggling of bits that matters (particularly over long wires).  So all ones will be almost as fast as all zeros.  It's more random data patterns that draw the most power.

One thing I want to point out is that I'll have a completely new set of kernels out soonish, and these do a much better job of keeping data in L2 and using larger tiles when possible.  This keeps the power levels significantly lower allowing the clock to run at full boost.  I'll also have everything working at small minibatches across the board.  This should make them much easier to scale with multiple gpus.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71/comments,https://github.com/soumith/convnet-benchmarks/issues/71#issuecomment-162768012,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71
soumith,convnet-benchmarks,120229488,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/162876522,162876522,MDEyOklzc3VlQ29tbWVudDE2Mjg3NjUyMg==,123560,2015-12-08T13:17:37Z,2015-12-08T13:17:37Z,CONTRIBUTOR,"> My TitanX defaults to a power limit of 250. For power draw it's the toggling of bits that matters (particularly over long wires). So all ones will be almost as fast as all zeros. It's more random data patterns that draw the most power.

You mean, because all 1s, or all 0s, will basically just run dc along the cables, but alternating 1s and 0s will start radiating em-radiation?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71/comments,https://github.com/soumith/convnet-benchmarks/issues/71#issuecomment-162876522,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71
soumith,convnet-benchmarks,120229488,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/162933355,162933355,MDEyOklzc3VlQ29tbWVudDE2MjkzMzM1NQ==,9326960,2015-12-08T16:18:14Z,2015-12-09T15:38:24Z,NONE,"I'm not an expert on these matters, but it's clear the more things are toggling on the chip the more power it draws.  It's also possible that additional power is saved when an all zero condition is met.  Portions of the logic might be dynamically disabled.  No idea if the gpu does this.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71/comments,https://github.com/soumith/convnet-benchmarks/issues/71#issuecomment-162933355,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71
soumith,convnet-benchmarks,120229488,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/163006428,163006428,MDEyOklzc3VlQ29tbWVudDE2MzAwNjQyOA==,14060629,2015-12-08T20:23:44Z,2015-12-08T20:23:44Z,CONTRIBUTOR,"> One thing I want to point out is that I'll have a completely new set of kernels out soonish, and these do a much better job of keeping data in L2 and using larger tiles when possible. This keeps the power levels significantly lower allowing the clock to run at full boost. I'll also have everything working at small minibatches across the board. This should make them much easier to scale with multiple gpus.

Super awesome.

> My TitanX defaults to a power limit of 250.

Oops, sorry, my mistake. 250W is the default, can be raised to max 275W. Here is how I overclock my Titan-X:

#increase â€œapplication clockâ€ and power
nvidia-smi -i 0 -pm 1
nvidia-smi -i 0 -ac 3505,1392 âˆ’âˆ’powerâˆ’limit=275

#enable coolbits
user@dnn1:~$ cat /etc/X11/xorg.conf 
[â€¦]
Section ""Device""
    Identifier ""nvidia""
    Driver ""nvidia""
    BusID ""PCI:1@0:0:0""
    Option ""ConstrainCursor"" ""off""
    Option         ""Coolbits"" ""31""
EndSection
[â€¦]

#set PowerMizer mode to â€œPrefer Maximum performanceâ€
DISPLAY=:0 nvidia-settings -a [gpu:0]/GPUPowerMizerMode=1

#overvolt
DISPLAY=:0 nvidia-settings -a [gpu:0]/GPUOverVoltageOffset=112399

#from nvidia-settings GUI (canâ€™t do it from command-line) set â€œGraphics clock Offsetâ€ to 300 Mhz, â€œMemory Transfer Rate Offsetâ€ to 800
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71/comments,https://github.com/soumith/convnet-benchmarks/issues/71#issuecomment-163006428,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71
soumith,convnet-benchmarks,120229488,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/163008806,163008806,MDEyOklzc3VlQ29tbWVudDE2MzAwODgwNg==,87486,2015-12-08T20:31:26Z,2015-12-08T20:31:26Z,NONE,"Thanks Scott Gray and Soumith for sharing the info. It is very interesting
H/W behavior.

On Mon, Dec 7, 2015 at 9:11 PM, Scott Gray notifications@github.com wrote:

> My TitanX defaults to a power limit of 250. For power draw it's the
> toggling of bits that matters (particularly over long wires). So all ones
> will be almost as fast as all zeros. It's more random data patterns that
> draw the most power.
> 
> One thing I want to point out is that I'll have a completely new set of
> kernels out soonish, and these do a much better job of keeping data in L2
> and using larger tiles when possible. This keeps the power levels
> significantly lower allowing the clock to run at full boost. I'll also have
> everything working at small minibatches across the board. This should make
> them much easier to scale with multiple gpus.
> 
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/issues/71#issuecomment-162768012
> .

## 

---

Junli Gu--è°·ä¿Šä¸½
Coordinated Science Lab
University of Illinois at Urbana-Champaign

---
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71/comments,https://github.com/soumith/convnet-benchmarks/issues/71#issuecomment-163008806,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71
soumith,convnet-benchmarks,120229488,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/163009128,163009128,MDEyOklzc3VlQ29tbWVudDE2MzAwOTEyOA==,14060629,2015-12-08T20:32:28Z,2015-12-08T20:32:28Z,CONTRIBUTOR,"> t's clear the more things are toggling on the chip the more power it draws. It's also possible that additional power is saved when an all zero condition is met. Portions of the logic might be dynamically disabled. No idea if they gpu does this.

I don't know what NVIDIA does, but a chip can indeed detect 0.0, 1.0, etc, situation and turn portions off, like described here:
https://en.wikipedia.org/wiki/Clock_gating
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71/comments,https://github.com/soumith/convnet-benchmarks/issues/71#issuecomment-163009128,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71
soumith,convnet-benchmarks,120229488,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/181426534,181426534,MDEyOklzc3VlQ29tbWVudDE4MTQyNjUzNA==,123560,2016-02-08T15:36:07Z,2016-02-08T15:36:07Z,CONTRIBUTOR,"Presumably we need to water cool the GPUs, so they can dissipate 250-300 watts under continuous operation?  What is common practice for doing this?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71/comments,https://github.com/soumith/convnet-benchmarks/issues/71#issuecomment-181426534,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71
soumith,convnet-benchmarks,120229488,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/181509573,181509573,MDEyOklzc3VlQ29tbWVudDE4MTUwOTU3Mw==,14060629,2016-02-08T18:23:09Z,2016-02-08T18:23:09Z,CONTRIBUTOR,"In a workstation, it's a standard thing you can find in the gamers' forums and magazines, for example, see http://www.maximumpc.com/a-beginners-guide-to-liquid-cooling/. In a standard rack-mounted server, there is no room for it. NVIDIA DIGITS workstation is air-cooled.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71/comments,https://github.com/soumith/convnet-benchmarks/issues/71#issuecomment-181509573,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71
soumith,convnet-benchmarks,120229488,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/181521357,181521357,MDEyOklzc3VlQ29tbWVudDE4MTUyMTM1Nw==,123560,2016-02-08T19:02:29Z,2016-02-08T19:02:29Z,CONTRIBUTOR,"Nice link and pictures.  Thanks! :-)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71/comments,https://github.com/soumith/convnet-benchmarks/issues/71#issuecomment-181521357,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71
soumith,convnet-benchmarks,120229488,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/222320347,222320347,MDEyOklzc3VlQ29tbWVudDIyMjMyMDM0Nw==,123560,2016-05-28T17:38:24Z,2016-05-28T17:38:24Z,CONTRIBUTOR,"Out of curiosity, I had a play with looking at the effect of load on the nimbix instances. I ran https://gist.github.com/hughperkins/6194efd67ad7fcbf5678b1285cc45327 with no arguments (except `-gpu 1` for one of the gpus) on a dual Titan X insance, with one process on one gpu, and the other on the other.  It just runs vgg model 'a' forward, with a batchsize of 128, on cudnnv4, with `cudnn.fastest = true` set.

When cold, the forward time was ~0.524

After running for ~10 minutes or so:
- GPU 1 was stable at 67C, forward time 0.539
- GPU 2 was stable at 75C, forward time 0.535

In other words:
- the difference in perf, on these GPUs, between cold and hot was ~2.8% for GPU1, and ~2% for GPU2, which seems fairly small, compared to the differences in benchmark resluts that we're mostly concerned with
- these GPUs are running pretty cold, nowhere near 85 celsius

![nimbixnvidiasmi](https://cloud.githubusercontent.com/assets/123560/15628820/39a92d9e-2503-11e6-9020-65340de0067a.png)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71/comments,https://github.com/soumith/convnet-benchmarks/issues/71#issuecomment-222320347,https://api.github.com/repos/soumith/convnet-benchmarks/issues/71
soumith,convnet-benchmarks,120174358,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/161675807,161675807,MDEyOklzc3VlQ29tbWVudDE2MTY3NTgwNw==,5577650,2015-12-03T15:28:00Z,2015-12-03T15:29:05Z,NONE,"@NH89 
Greentea/OpenCL is really slow for CNNs with batched data because of overhead and inefficiency in the Matrix-Matrix multiplications used for convolutions, especially when they are smaller. This benchmark also uses the ViennaCL library, the clBLAS AMD library could be a bit faster as well (can be selected at compile time).

However, to be really up to speed, there need to be vendor and hardware specific convolution libraries such as cuDNN.

AMD has an OpenCL branch (https://github.com/amd/OpenCL-caffe) where they are alternatively unwrapping the batch into one large Matrix-Matrix multiplication. Very memory inefficient compared to cuDNN, but almost as fast.

In the same technical report you can read up that with interleaved, pixelwise classification data (which causes large matrix-matrix multiplications and thus a higher efficiency and no batches) are comparably fast to CUDA.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/70/comments,https://github.com/soumith/convnet-benchmarks/issues/70#issuecomment-161675807,https://api.github.com/repos/soumith/convnet-benchmarks/issues/70
soumith,convnet-benchmarks,120174358,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/161767389,161767389,MDEyOklzc3VlQ29tbWVudDE2MTc2NzM4OQ==,810993,2015-12-03T20:04:04Z,2015-12-03T20:04:04Z,NONE,"@naibaf7 Thanks, you saved me from making an expensive error :-) Thank you also for creating Greentea. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/70/comments,https://github.com/soumith/convnet-benchmarks/issues/70#issuecomment-161767389,https://api.github.com/repos/soumith/convnet-benchmarks/issues/70
soumith,convnet-benchmarks,120174358,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/161772288,161772288,MDEyOklzc3VlQ29tbWVudDE2MTc3MjI4OA==,5577650,2015-12-03T20:23:56Z,2015-12-03T20:23:56Z,NONE,"@NH89

No problem. Probably the OpenCL approaches will catch up with CUDA solutions during Q2/3 next year, as major developments are going on by both AMD and Intel.

For my projects in biomedical image segmentation, the OpenCL solution is already speed competitive though.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/70/comments,https://github.com/soumith/convnet-benchmarks/issues/70#issuecomment-161772288,https://api.github.com/repos/soumith/convnet-benchmarks/issues/70
soumith,convnet-benchmarks,116481615,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156002580,156002580,MDEyOklzc3VlQ29tbWVudDE1NjAwMjU4MA==,1310570,2015-11-12T05:22:17Z,2015-11-12T05:22:17Z,OWNER,"@222464 folks at AMD, including @gujunli said that they are working on benchmarks on AMD cards. I dont have a machine with an AMD card.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/69/comments,https://github.com/soumith/convnet-benchmarks/issues/69#issuecomment-156002580,https://api.github.com/repos/soumith/convnet-benchmarks/issues/69
soumith,convnet-benchmarks,116481615,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156002736,156002736,MDEyOklzc3VlQ29tbWVudDE1NjAwMjczNg==,1310570,2015-11-12T05:22:46Z,2015-11-12T05:22:46Z,OWNER,"fwiw, I clearly state my hardware, so there's no misrepresentation.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/69/comments,https://github.com/soumith/convnet-benchmarks/issues/69#issuecomment-156002736,https://api.github.com/repos/soumith/convnet-benchmarks/issues/69
soumith,convnet-benchmarks,116481615,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156003940,156003940,MDEyOklzc3VlQ29tbWVudDE1NjAwMzk0MA==,123560,2015-11-12T05:25:48Z,2015-11-12T05:25:48Z,CONTRIBUTOR,"> As far as I can tell, the OpenCL based benchmarks are also done on the Nvidia GPU. Nvidia purposely nerfed their OpenCL drivers to promote CUDA (AMD is on OpenCL 2.0 and Nvidia is on 1.1, and then are not equal in performance).

Well... yes and no.  I've run my own code on AMD gpus, and the results were ... well... anyway... AMD has forked Soumith's benchmarks at https://github.com/amd/convnet-benchmarks , so you could push them to publish some results I guess :-)  The scripts that soumith uses are all opensource, publically available, so anyone can run them against any GPU they want, and publish the results.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/69/comments,https://github.com/soumith/convnet-benchmarks/issues/69#issuecomment-156003940,https://api.github.com/repos/soumith/convnet-benchmarks/issues/69
soumith,convnet-benchmarks,116481615,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156005160,156005160,MDEyOklzc3VlQ29tbWVudDE1NjAwNTE2MA==,9326960,2015-11-12T05:32:10Z,2015-11-12T05:32:10Z,NONE,"Keep in mind that my kernels are not written in cuda-c.  AMD has very graciously posted their complete ISA here: http://amd-dev.wpengine.netdna-cdn.com/wordpress/media/2013/07/AMD_GCN3_Instruction_Set_Architecture.pdf

So if you really want to compare hardware to hardware that's now achievable.  I personally don't think it's possible to efficiently implement these dense linear algebra kernels in a higher level language.  That's particularly true of my new winograd kernels which require an incredibly well crafted register budget along with very carefully scheduled instructions.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/69/comments,https://github.com/soumith/convnet-benchmarks/issues/69#issuecomment-156005160,https://api.github.com/repos/soumith/convnet-benchmarks/issues/69
soumith,convnet-benchmarks,116481615,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156009545,156009545,MDEyOklzc3VlQ29tbWVudDE1NjAwOTU0NQ==,1609713,2015-11-12T05:53:32Z,2015-11-12T05:55:29Z,NONE,"Thanks for the responses. Yes, it's fine that you stated the hardware, but I just felt like many people draw the wrong conclusions. I used both CUDA and OpenCL, I found them to be about the same speed for similar hardware. Adobe products also have comparisons of the two, which show that they are about the same. The only comparison that can be made is really cross-hardware I am afraid.

Edit: It's great to hear that some OpenCL benchmarks are being made!

Thank you!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/69/comments,https://github.com/soumith/convnet-benchmarks/issues/69#issuecomment-156009545,https://api.github.com/repos/soumith/convnet-benchmarks/issues/69
soumith,convnet-benchmarks,116481615,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156059480,156059480,MDEyOklzc3VlQ29tbWVudDE1NjA1OTQ4MA==,123560,2015-11-12T10:20:04Z,2015-11-12T10:20:04Z,CONTRIBUTOR,"Hi 222464, I'm interested in the benchmarking you tried, however informal.
- what library(s) did you use?
- what cards?
- what benchmarking test(s)?
- what were the results?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/69/comments,https://github.com/soumith/convnet-benchmarks/issues/69#issuecomment-156059480,https://api.github.com/repos/soumith/convnet-benchmarks/issues/69
soumith,convnet-benchmarks,116480870,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156000559,156000559,MDEyOklzc3VlQ29tbWVudDE1NjAwMDU1OQ==,127987,2015-11-12T05:11:11Z,2015-11-12T05:11:11Z,CONTRIBUTOR,"<3
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68/comments,https://github.com/soumith/convnet-benchmarks/issues/68#issuecomment-156000559,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68
soumith,convnet-benchmarks,116480870,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156730699,156730699,MDEyOklzc3VlQ29tbWVudDE1NjczMDY5OQ==,9004594,2015-11-14T18:27:03Z,2015-11-14T18:27:03Z,NONE,"An unofficial preliminary benchmark result is presented in https://github.com/dmlc/mxnet/issues/378#issuecomment-156730363. It is very surprising that MXNet is much faster than Caffe.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68/comments,https://github.com/soumith/convnet-benchmarks/issues/68#issuecomment-156730699,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68
soumith,convnet-benchmarks,116480870,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156730830,156730830,MDEyOklzc3VlQ29tbWVudDE1NjczMDgzMA==,2577440,2015-11-14T18:29:39Z,2015-11-14T18:29:39Z,NONE,"This was due to incorrect timing on async API... there is no magic to run much faster than others using CuDNN..
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68/comments,https://github.com/soumith/convnet-benchmarks/issues/68#issuecomment-156730830,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68
soumith,convnet-benchmarks,116480870,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156734520,156734520,MDEyOklzc3VlQ29tbWVudDE1NjczNDUyMA==,1258974,2015-11-14T19:08:33Z,2015-11-14T19:08:33Z,NONE,"@futurely Also I notice you are using simple factory instead of correct inception factory. I am working on Conv/LSTM timing. Sorry for delay because I am traveling these days.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68/comments,https://github.com/soumith/convnet-benchmarks/issues/68#issuecomment-156734520,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68
soumith,convnet-benchmarks,116480870,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156738333,156738333,MDEyOklzc3VlQ29tbWVudDE1NjczODMzMw==,9004594,2015-11-14T19:26:03Z,2015-11-14T19:26:03Z,NONE,"The simple factory's results are not presented there. To make sure models for both libraries are exactly the same, only the GoogLeNet and VGG 16 models from the Caffe model zoo and their conversions to the MXNet format are used. Expect your aync API timing.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68/comments,https://github.com/soumith/convnet-benchmarks/issues/68#issuecomment-156738333,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68
soumith,convnet-benchmarks,116480870,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/157192069,157192069,MDEyOklzc3VlQ29tbWVudDE1NzE5MjA2OQ==,1310570,2015-11-16T22:30:53Z,2015-11-16T22:30:53Z,OWNER,"Both MXNet and Chainer scripts are ready, thanks to Kentaa Oono and @antinucleon . As some of you might know, ICLR deadline is on Thursday, a bit too busy with that, will benchmark over the coming weekend.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68/comments,https://github.com/soumith/convnet-benchmarks/issues/68#issuecomment-157192069,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68
soumith,convnet-benchmarks,116480870,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/159004773,159004773,MDEyOklzc3VlQ29tbWVudDE1OTAwNDc3Mw==,5196895,2015-11-23T17:32:03Z,2015-11-23T17:32:03Z,NONE,"Any luck with results?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68/comments,https://github.com/soumith/convnet-benchmarks/issues/68#issuecomment-159004773,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68
soumith,convnet-benchmarks,116480870,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/163512924,163512924,MDEyOklzc3VlQ29tbWVudDE2MzUxMjkyNA==,2974951,2015-12-10T06:21:12Z,2015-12-10T06:21:12Z,NONE,"+1
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68/comments,https://github.com/soumith/convnet-benchmarks/issues/68#issuecomment-163512924,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68
soumith,convnet-benchmarks,116480870,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/166488780,166488780,MDEyOklzc3VlQ29tbWVudDE2NjQ4ODc4MA==,7547121,2015-12-22T02:52:04Z,2015-12-22T02:52:04Z,NONE,"+1
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68/comments,https://github.com/soumith/convnet-benchmarks/issues/68#issuecomment-166488780,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68
soumith,convnet-benchmarks,116480870,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/166519339,166519339,MDEyOklzc3VlQ29tbWVudDE2NjUxOTMzOQ==,9958446,2015-12-22T05:56:20Z,2015-12-22T05:56:20Z,NONE,"+1. How is it going by now?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68/comments,https://github.com/soumith/convnet-benchmarks/issues/68#issuecomment-166519339,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68
soumith,convnet-benchmarks,116480870,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/166532422,166532422,MDEyOklzc3VlQ29tbWVudDE2NjUzMjQyMg==,123560,2015-12-22T06:56:13Z,2015-12-22T06:56:31Z,CONTRIBUTOR,"Guys, rather than hassling Soumith, who does have a full-time job and stuff to do ;-) perhaps you might consider creating a pull request, with a script, so Soumith simply has to do `git pull`, and run your script :-)  You can see that this is what Fabian did https://github.com/soumith/convnet-benchmarks/pull/49 , and myself https://github.com/soumith/convnet-benchmarks/pull/47 , for our own libraries, for example.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68/comments,https://github.com/soumith/convnet-benchmarks/issues/68#issuecomment-166532422,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68
soumith,convnet-benchmarks,116480870,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/166646945,166646945,MDEyOklzc3VlQ29tbWVudDE2NjY0Njk0NQ==,1310570,2015-12-22T15:29:40Z,2015-12-22T15:29:40Z,OWNER,"To be fair to both Chainer and MXNet folks, they gave me scripts to benchmark. I put it off because of NIPS / ICLR, and their libs have changed APIs, so I am stuck fixing the scripts for chainer. As always, working on it, at my own pace.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68/comments,https://github.com/soumith/convnet-benchmarks/issues/68#issuecomment-166646945,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68
soumith,convnet-benchmarks,116480870,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/166726569,166726569,MDEyOklzc3VlQ29tbWVudDE2NjcyNjU2OQ==,1310570,2015-12-22T20:51:09Z,2015-12-22T20:51:09Z,OWNER,"Just finished Chainer. Working on MXNet ...
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68/comments,https://github.com/soumith/convnet-benchmarks/issues/68#issuecomment-166726569,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68
soumith,convnet-benchmarks,116480870,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/168936974,168936974,MDEyOklzc3VlQ29tbWVudDE2ODkzNjk3NA==,1310570,2016-01-05T08:52:27Z,2016-01-05T08:52:27Z,OWNER,"I committed MXNet AlexNet + Googlenet scripts that @antinucleon had given me. I wanted to get some experience with MXNet before I benchmarked it, because it can use multiple threads etc. Hence the delay (didn't find time to read the docs, get familiar etc.). 
If anyone who wants to see the MXNet benchmarks finishes the vgg and fixes the error in the googlenet script, I can run them on the Titan-X cards and report the numbers. 
Chainer logs btw are all checked in via: https://github.com/soumith/convnet-benchmarks/commit/c4dfa528cd7f2abd2e9abd91b294f91d01146c42
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68/comments,https://github.com/soumith/convnet-benchmarks/issues/68#issuecomment-168936974,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68
soumith,convnet-benchmarks,116480870,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/194600282,194600282,MDEyOklzc3VlQ29tbWVudDE5NDYwMDI4Mg==,3055719,2016-03-10T01:06:59Z,2016-03-10T01:06:59Z,NONE,"It looks like there's a bug in the [benchmark of Chainer](https://github.com/soumith/convnet-benchmarks/blob/master/chainer/train_imagenet.py#L98).

They computed the averaged time to be `total  / niter-1` instead of `total  / (niter-1)`.

![image](https://cloud.githubusercontent.com/assets/3055719/13655871/7446059c-e617-11e5-8d0a-74e1aa091537.png)

---

Another thing that I noticed is that the script uses `cuda.Event()` to measure the time for the backward pass while using the standard Python `time()` to measure the time for the forward pass. Does `cuda.get_elapsed_time(cudaStartEvent, cudaEndEvent)` measure time computational time in the backward pass before CUDA kernel launch? I'm asking because Chainer apparently does [a lot of stuffs in Python (potentially negligible) before passing to libcudnn](https://github.com/pfnet/chainer/blob/master/chainer/functions/connection/convolution_2d.py#L154) for each call of forward and backward.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68/comments,https://github.com/soumith/convnet-benchmarks/issues/68#issuecomment-194600282,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68
soumith,convnet-benchmarks,116480870,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/251478671,251478671,MDEyOklzc3VlQ29tbWVudDI1MTQ3ODY3MQ==,4495212,2016-10-04T18:51:51Z,2016-10-04T18:52:03Z,NONE,"+1 
want to know which one is faster!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68/comments,https://github.com/soumith/convnet-benchmarks/issues/68#issuecomment-251478671,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68
soumith,convnet-benchmarks,116480870,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/281298227,281298227,MDEyOklzc3VlQ29tbWVudDI4MTI5ODIyNw==,7012124,2017-02-21T10:02:42Z,2017-02-21T10:02:42Z,NONE,"+1
I'm looking for it.",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68/comments,https://github.com/soumith/convnet-benchmarks/issues/68#issuecomment-281298227,https://api.github.com/repos/soumith/convnet-benchmarks/issues/68
soumith,convnet-benchmarks,116413730,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155899700,155899700,MDEyOklzc3VlQ29tbWVudDE1NTg5OTcwMA==,1310570,2015-11-11T20:22:21Z,2015-11-11T20:22:21Z,OWNER,"Thanks. trying it now.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/67/comments,https://github.com/soumith/convnet-benchmarks/pull/67#issuecomment-155899700,https://api.github.com/repos/soumith/convnet-benchmarks/issues/67
soumith,convnet-benchmarks,116413730,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155906365,155906365,MDEyOklzc3VlQ29tbWVudDE1NTkwNjM2NQ==,1310570,2015-11-11T20:52:13Z,2015-11-11T20:52:13Z,OWNER,"Your fixes improved results a little bit, but for the VGG_A to not go OOM, I had to show 2 Titan-X cards via CUDA_VISIBLE_DEVIES, it was going OOM if I showed only 1 Titan-X card. However, it was not using two cards for compute (I checked via nvidia-smi ). Updating the main post now. The raw logs are checked in. Thanks Yangqing
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/67/comments,https://github.com/soumith/convnet-benchmarks/pull/67#issuecomment-155906365,https://api.github.com/repos/soumith/convnet-benchmarks/issues/67
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155702952,155702952,MDEyOklzc3VlQ29tbWVudDE1NTcwMjk1Mg==,1310570,2015-11-11T08:32:05Z,2015-11-11T08:32:05Z,OWNER,"The benchmark scripts and raw outputs are located here: https://github.com/soumith/convnet-benchmarks/tree/master/tensorflow
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155702952,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155712069,155712069,MDEyOklzc3VlQ29tbWVudDE1NTcxMjA2OQ==,9326960,2015-11-11T09:21:29Z,2015-11-11T09:21:29Z,NONE,"The lack of in place operations is rather surprising.  Once you have the full DAG it should be rather easy to apply a liveness algorithm to it to optimize tensor allocations.  For an example see this: http://www.diku.dk/hjemmesider/ansatte/torbenm/ICD/Register.pdf (just replace register with tensor).

I'm kind of curious if there's any support for automatically compounding operations together or of leveraging kernels that have some compounding built in (like the alpha/beta params of gemm).  I'm pretty close to maximizing the amount of compounding that's possible in my benchmark networks.  And because I write all my own kernels I can further compound things that aren't possible with closed source libraries like cuDNN.  For example, I'm now able to compute the mean along the PQN dimension inside the conv and gemm kernels at no cost.  This cuts down the bandwidth required by batch norm in fprop by a third.

Though on the whole I think TensorFlow seems like a great platform to build on.  I'd say there's a good chance my kernels will make their way there sooner rather than later.  You can find new benchmarks of my latest winograd kernels in the updated paper here: http://arxiv.org/abs/1509.09308  

What I'll be working on next is basically going to be taking a lot of what I learned implementing winograd and refreshing all of my conv/pooling/gemm kernels to support very small minibatches at near full utilization.  This should have a big impact on the level at which you can scale these networks and the speed at which they converge.  Here's a great paper exploring this: http://arxiv.org/abs/1509.04210
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155712069,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155775347,155775347,MDEyOklzc3VlQ29tbWVudDE1NTc3NTM0Nw==,9126588,2015-11-11T12:55:32Z,2015-11-11T12:55:32Z,NONE,"Hi, I strongly recommand to add mxnet https://github.com/dmlc/mxnet into comparision which in my opinion may be the fastest DL library :)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155775347,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155788671,155788671,MDEyOklzc3VlQ29tbWVudDE1NTc4ODY3MQ==,1367713,2015-11-11T13:48:19Z,2015-11-11T13:48:19Z,NONE,"+1 for benchmarking mxnet, the fastest now.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155788671,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155797513,155797513,MDEyOklzc3VlQ29tbWVudDE1NTc5NzUxMw==,13505911,2015-11-11T14:30:35Z,2015-11-11T14:30:35Z,NONE,"+1 for benchmarking mxnet
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155797513,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155814941,155814941,MDEyOklzc3VlQ29tbWVudDE1NTgxNDk0MQ==,4639261,2015-11-11T15:23:53Z,2015-11-11T15:23:53Z,NONE,"I would also love to see a comparison with Theano http://deeplearning.net/software/theano/ as it is another widely adopted deep learning library. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155814941,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155816021,155816021,MDEyOklzc3VlQ29tbWVudDE1NTgxNjAyMQ==,6900572,2015-11-11T15:28:18Z,2015-11-11T15:28:18Z,NONE,"Thanks for benchmarking!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155816021,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155826188,155826188,MDEyOklzc3VlQ29tbWVudDE1NTgyNjE4OA==,647529,2015-11-11T15:59:37Z,2015-11-11T15:59:37Z,NONE,"+1 would love to see tensorflow benchmarked against mxnet, Theano, Autograd for Torch, and Caffe. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155826188,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155826594,155826594,MDEyOklzc3VlQ29tbWVudDE1NTgyNjU5NA==,15737127,2015-11-11T16:01:05Z,2015-11-11T16:01:05Z,NONE,"Thanks @soumith! Yes, our only launch criterion for convnets was 'GoogLeNet within distance from CuDNN[R2]', and we've punted on a lot of performance work, including upgrading CuDNN, until after the initial release. Expect a lot of movement on that front in the coming weeks.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155826594,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155834804,155834804,MDEyOklzc3VlQ29tbWVudDE1NTgzNDgwNA==,1310570,2015-11-11T16:26:02Z,2015-11-11T16:26:02Z,OWNER,"@aaronwro @fvisin it's already benchmarked against Torch, Theano, Caffe. Look at the readme on the main page ( https://github.com/soumith/convnet-benchmarks/blob/master/README.md ). 
I definitely need to pull my socks up and benchmark MXNet and Chainer.

@vincentvanhoucke thanks for your response. I assumed that you'll fix it over the next weeks / months :)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155834804,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155835770,155835770,MDEyOklzc3VlQ29tbWVudDE1NTgzNTc3MA==,15737127,2015-11-11T16:29:43Z,2015-11-11T16:29:43Z,NONE,"@scott-gray let us know if you need help with compounding / graph rewriting. The graph representation is meant to make these kinds of operations possible, and the common subexpression elimination that TF currently uses is also meant as a demonstration of that. I suspect we might need to do a bit more to provide good APIs to make it easier to bake in compound kernels.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155835770,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155836664,155836664,MDEyOklzc3VlQ29tbWVudDE1NTgzNjY2NA==,1310570,2015-11-11T16:33:17Z,2015-11-11T16:33:17Z,OWNER,"there seems to be some misinterpretation by random people in social media that because I work for Facebook, I'm attacking TensorFlow. That seems super weird, because I love the vision of TensorFlow, and there's no competition (one can write a XXX frontend for a TensorFlow backend).

My benchmarks have always been independently run, and completely neutral, I've been running them forever now, sad that people misinterpret the slightest of things.
cc: @vincentvanhoucke 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155836664,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155837136,155837136,MDEyOklzc3VlQ29tbWVudDE1NTgzNzEzNg==,188959,2015-11-11T16:35:18Z,2015-11-11T16:35:18Z,COLLABORATOR,"will defend Soumith on this one â€“ he has indeed been running these
benchmarks for quite some time, and complete neutrality.

On Wed, Nov 11, 2015 at 11:33 AM, Soumith Chintala <notifications@github.com

> wrote:
> 
> there seems to be some misinterpretation by random people in social media
> that because I work for Facebook, I'm attacking TensorFlow. That seems
> super weird, because I love the vision of TensorFlow, and there's no
> competition (one can write a XXX frontend for a TensorFlow backend).
> 
> My benchmarks have always been independently run, and completely neutral,
> I've been running them forever now, sad that people misinterpret the
> slightest of things.
> cc: @vincentvanhoucke https://github.com/vincentvanhoucke
> 
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155836664
> .
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155837136,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155837306,155837306,MDEyOklzc3VlQ29tbWVudDE1NTgzNzMwNg==,4639261,2015-11-11T16:35:56Z,2015-11-11T16:35:56Z,NONE,"@soumith Excellent, thank you!!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155837306,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155837602,155837602,MDEyOklzc3VlQ29tbWVudDE1NTgzNzYwMg==,15737127,2015-11-11T16:37:02Z,2015-11-11T16:37:02Z,NONE,"@soumith no good deed goes unpunished ;) Please don't let this deter you from providing this valuable service to the community!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155837602,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155837696,155837696,MDEyOklzc3VlQ29tbWVudDE1NTgzNzY5Ng==,551151,2015-11-11T16:37:27Z,2015-11-11T16:37:27Z,CONTRIBUTOR,"@soumith , I am sorry that some people interpreted things that way. I've always appreciated your benchmark, which creates a great atmosphere for us to look at bottlenecks and push forward the field as a whole community. We all owe you a big debt of gratitude.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155837696,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155837772,155837772,MDEyOklzc3VlQ29tbWVudDE1NTgzNzc3Mg==,647529,2015-11-11T16:37:46Z,2015-11-11T16:37:46Z,NONE,"@soumith thanks! 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155837772,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155843946,155843946,MDEyOklzc3VlQ29tbWVudDE1NTg0Mzk0Ng==,1792006,2015-11-11T16:52:02Z,2015-11-11T16:52:02Z,NONE,"As always, that's super interesting. Thanks for pushing all of us toward more performance. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155843946,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155852051,155852051,MDEyOklzc3VlQ29tbWVudDE1NTg1MjA1MQ==,2577440,2015-11-11T17:22:49Z,2015-11-11T17:23:07Z,NONE,"For memory optimizations, what we have found is that inplace optimization does not matter that much, if the allocator is smart enough to do a static allocation before running the graph(as opposed to relying on a dynamic allocator). We have detailed what can be done here 

https://mxnet.readthedocs.org/en/latest/developer-guide/note_memory.html

Which I assume applies to computation graph frameworks such as TF, caffe2 and CGT.
@vincentvanhoucke @Yangqing 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155852051,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155852768,155852768,MDEyOklzc3VlQ29tbWVudDE1NTg1Mjc2OA==,2577440,2015-11-11T17:25:00Z,2015-11-11T17:25:00Z,NONE,"The general idea is not only to share memory of same shape(i.e. inplace) , but also different shapes and size
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155852768,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155854840,155854840,MDEyOklzc3VlQ29tbWVudDE1NTg1NDg0MA==,15679194,2015-11-11T17:29:47Z,2015-11-11T17:29:47Z,NONE,"@soumith Thanks for running the benchmarks! As @vincentvanhoucke noted in this thread, our goal was to get an early release out so users can start playing with it and provide feedback on what they care about. We are committed to making TensorFlow fast and are actively working on the performance issues you highlight here.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155854840,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155858031,155858031,MDEyOklzc3VlQ29tbWVudDE1NTg1ODAzMQ==,161935,2015-11-11T17:42:06Z,2015-11-11T17:42:06Z,NONE,"@soumith You're doing a good deed! Haters gonna hate.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155858031,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155870309,155870309,MDEyOklzc3VlQ29tbWVudDE1NTg3MDMwOQ==,568948,2015-11-11T18:35:21Z,2015-11-11T18:35:21Z,NONE,"I'm a little confused by the number. 1300 samples/sec seems too fast even for alexnet on single TitanX. Is this standard training, e.g. io+forward+backward+update, or just forward+backward?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155870309,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155870325,155870325,MDEyOklzc3VlQ29tbWVudDE1NTg3MDMyNQ==,15558201,2015-11-11T18:35:26Z,2015-11-11T18:35:26Z,NONE,"Nice work.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155870325,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155873763,155873763,MDEyOklzc3VlQ29tbWVudDE1NTg3Mzc2Mw==,1258974,2015-11-11T18:44:59Z,2015-11-13T08:46:06Z,NONE,"@piiswrong I will help @soumith make the benchmark script. 

Anyway we opened everything since beginning. The main purpose is learning from each other but not advertise boring number.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155873763,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155875997,155875997,MDEyOklzc3VlQ29tbWVudDE1NTg3NTk5Nw==,820731,2015-11-11T18:53:49Z,2015-11-11T18:53:49Z,NONE,"I will also add my support to Soumith. He has been running these benchmarks for sometime with complete transparency and neutrality.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155875997,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155886855,155886855,MDEyOklzc3VlQ29tbWVudDE1NTg4Njg1NQ==,1074537,2015-11-11T19:30:11Z,2015-11-11T19:30:11Z,NONE,"@koraykv +1, thanks Soumith!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155886855,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155897093,155897093,MDEyOklzc3VlQ29tbWVudDE1NTg5NzA5Mw==,1310570,2015-11-11T20:09:49Z,2015-11-11T20:09:49Z,OWNER,"Someone on reddit suggested that I build tensorflow from source, to fix speed issues. That did not help, It produces the same numbers as the pip version on my [alexnet script](https://github.com/soumith/convnet-benchmarks/blob/master/tensorflow/benchmark_alexnet.py) :

https://gist.github.com/soumith/11acc2f0dbc5212ea372
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155897093,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155907479,155907479,MDEyOklzc3VlQ29tbWVudDE1NTkwNzQ3OQ==,1310570,2015-11-11T20:57:20Z,2015-11-11T20:59:19Z,OWNER,"FWIW, [Yangqing's fix to avoid CPU-GPU](https://github.com/soumith/convnet-benchmarks/pull/67) transfers improved results across the board by ~20%. (I've updated the tables above). The memory issues are unchanged. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155907479,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155927485,155927485,MDEyOklzc3VlQ29tbWVudDE1NTkyNzQ4NQ==,3773185,2015-11-11T22:19:00Z,2015-11-11T22:19:00Z,NONE,"+1 for mxnet! Thanks. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155927485,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155929014,155929014,MDEyOklzc3VlQ29tbWVudDE1NTkyOTAxNA==,14301668,2015-11-11T22:25:59Z,2015-11-11T22:25:59Z,NONE,"+1 for mxnet.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155929014,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155929736,155929736,MDEyOklzc3VlQ29tbWVudDE1NTkyOTczNg==,87486,2015-11-11T22:29:19Z,2015-11-11T22:29:19Z,NONE,"@soumith I have a naive question, is the Tensor Flow's result based on c++ code or cuDNN v2? I would guess if you run on Titanx tensor flow will rely on some CUDA library? 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155929736,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155930423,155930423,MDEyOklzc3VlQ29tbWVudDE1NTkzMDQyMw==,1310570,2015-11-11T22:32:33Z,2015-11-11T22:32:33Z,OWNER,"@gujunli it's based on CuDNN V2.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155930423,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155931041,155931041,MDEyOklzc3VlQ29tbWVudDE1NTkzMTA0MQ==,1458824,2015-11-11T22:35:30Z,2015-11-11T22:35:30Z,NONE,"@soumith thanks for running and maintaining these benchmarks; they're always thorough and informative!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155931041,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155931632,155931632,MDEyOklzc3VlQ29tbWVudDE1NTkzMTYzMg==,87486,2015-11-11T22:38:38Z,2015-11-11T22:38:38Z,NONE,"@soumith Then I don't understand Why Tensor Flow with cuDNN v2 ends up being so slow? Can you share some of your understanding? I will guess TF still calls cuDNN v2 for the conv/pool/relu/FC layers. Remember from your earlier AlexNet results, cuDNN v2 is 231=70+161,  Caffe (native)  ConvolutionLayer    324=121+203. However Tensor flow is 326=96+230. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155931632,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155932667,155932667,MDEyOklzc3VlQ29tbWVudDE1NTkzMjY2Nw==,9326960,2015-11-11T22:42:44Z,2015-11-11T22:42:44Z,NONE,"Running the network under nvvp (nvidia visual profiler) should be pretty informative.  A well tuned network timeline should just be a solid block of kernel calls with no gaps.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155932667,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155933713,155933713,MDEyOklzc3VlQ29tbWVudDE1NTkzMzcxMw==,87486,2015-11-11T22:48:14Z,2015-11-11T22:48:14Z,NONE,"@scott-gray so you think TF scheduling may not be efficient? I need to read TF whitepaper to understand how it works. Any one understands? 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155933713,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155935071,155935071,MDEyOklzc3VlQ29tbWVudDE1NTkzNTA3MQ==,9326960,2015-11-11T22:54:07Z,2015-11-11T22:54:07Z,NONE,"@gujunli I'm just saying if they're just using stock cuDNNv2 then the only reason it would be slower is if there were gaps in the timeline.  Seeing where those gaps occur and any extra host/device memcpy traffic would give you a clearer picture of what's going wrong.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155935071,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155937519,155937519,MDEyOklzc3VlQ29tbWVudDE1NTkzNzUxOQ==,831751,2015-11-11T23:04:34Z,2015-11-11T23:04:34Z,NONE,"@soumith Thanks for this and all the other previous benchmark you took the time to create.

+1 for MxNet
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155937519,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155938280,155938280,MDEyOklzc3VlQ29tbWVudDE1NTkzODI4MA==,14264652,2015-11-11T23:08:30Z,2015-11-11T23:08:30Z,NONE,"+1 for mxnet! Thank you so much!!!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155938280,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155938969,155938969,MDEyOklzc3VlQ29tbWVudDE1NTkzODk2OQ==,551151,2015-11-11T23:12:32Z,2015-11-11T23:12:32Z,CONTRIBUTOR,"@gujunli @scott-gray To provide some historical perspective: this is mostly due to legacy choices. Historically, Google Brain has been using the NHWC storage order and a slightly different padding scheme (""SAME/VALID"" instead of an explicit padding number). CuDNN, as well as Caffe, uses NCHW order. Note that CuDNN support NHWC interface-wise, but some underlying paths are not implemented, like NHWC convolution backward.

As a result, when calling cuDNN, there are some code that generates intermediate padded and order-switched intermediate tensors. The code was written with Eigen and did not interact very well with nvcc, causing a nontrivial overhead (you can observe that by running the benchmark in an nvvp session as Scott suggested).

We are having people looking into this and the performance should be brought to cuDNN-level.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155938969,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155944875,155944875,MDEyOklzc3VlQ29tbWVudDE1NTk0NDg3NQ==,9326960,2015-11-11T23:38:29Z,2015-11-12T00:56:01Z,NONE,"Gah, everyone's using different tensor layouts still.  You all need to turn from the dark side and see the speed benefits to using CHWN.  Though NHWC is probably better than NCHW at least.  You want that inner dimension to be a nice even number to facilitate cleaner alligned memory access, leading to less over-fetch.  CHWN gets you better contiguous memory access over all.  In recurrent networks with model parallelism having N as the outer dim definitely helps, but most distributed convnets are data parallel where it doesn't matter.

I have some very fast shared memory dimshuffle code if you want it.  I use it to make this operation on the filters:

```
# C <=> K and mirror R,S
F = np.transpose(F[:,::-1,::-1,:], (3,1,2,0))
```

Turns out a kernel for fprop_conv can work with very little change (or no change if padding and striding are symmetric) to be a kernel for bprop_conv.  There's almost no overhead in the dimshuffle since the filters are so small and you completely avoid any atomic adds.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155944875,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155954861,155954861,MDEyOklzc3VlQ29tbWVudDE1NTk1NDg2MQ==,3529132,2015-11-12T00:16:41Z,2015-11-12T00:16:41Z,NONE,"krizhevsky first demonstrated the benefits of using the CHWN layout in cuda-convnet.  In addition to being advantageous for convolutional kernels, it's very beneficial for models like GoogLeNet where inception modules concatenate activations along feature map depth.  Using CHWN allows you to write directly into an output buffer in the layout that the subsequent layer will consume (C1 + C2 + C3)HWN.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155954861,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155965163,155965163,MDEyOklzc3VlQ29tbWVudDE1NTk2NTE2Mw==,551151,2015-11-12T01:07:48Z,2015-11-12T01:07:48Z,CONTRIBUTOR,"Thanks @scott-gray - having the dim shuffle kernels to improve performance will be great.

One potential issue with CHWN is that during inference time N is often small, so there are two different sets of optimizations to be carried out for large N and small N. NCHW/NHWC usually makes things a bit batch-agnostic, but that's not always true of course.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155965163,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155965666,155965666,MDEyOklzc3VlQ29tbWVudDE1NTk2NTY2Ng==,551151,2015-11-12T01:11:12Z,2015-11-12T01:11:12Z,CONTRIBUTOR,"@soumith Regarding the memory issue, we found that if one turns on the best-fit GPU allocator, you would be able to run VGG on batch sizes of 64. I did a quick change if you would like to build and try from source:

``` shell
git clone https://github.com/Yangqing/tensorflow.git
cd tensorflow
git checkout bfc
```

There will be more fixes to be submitted by @vrv to enable it more easily (such as during a session creation time) down the road.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155965666,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155970724,155970724,MDEyOklzc3VlQ29tbWVudDE1NTk3MDcyNA==,9326960,2015-11-12T01:46:14Z,2015-11-12T01:46:14Z,NONE,"@Yangqing The shuffle code is here (note that this does not do the RS mirror operation): 
https://github.com/NervanaSystems/neon/blob/master/neon/backends/float_ew.py#L1481

It uses magic numbers for fast integer division.  Here's the code that sets up the kernel params:
https://github.com/NervanaSystems/neon/blob/master/neon/backends/layer_gpu.py#L504

The code is adapted from here (the diagram will be helpful):
http://devblogs.nvidia.com/parallelforall/efficient-matrix-transpose-cuda-cc/

It's on my list of things to do to generalize it and make it available as a flexible backend operation.  But I haven't gotten to it.  Theano may also have some good dimshuffle code you can borrow.

Also, most of the code in that float_ew file is devoted to automatically generating extremely efficient compound elementwise/reduction/broadcast/transpose/take operations.  It allows you to write complex numpy expressions and have them compile to a single cuda kernel.  It even does common sub-expression removal, but sounds like you already have that.  This all works off of little optrees that exist in layer code.  But I've been meaning to find a way to collect the full program DAG in a clean way.  Seems like you guys solved that and that's why I'm interested in TensorFlow.  There's so much burden you can shift from the programmer and have automatically optimized via graph traversals.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155970724,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155980389,155980389,MDEyOklzc3VlQ29tbWVudDE1NTk4MDM4OQ==,1132769,2015-11-12T02:42:43Z,2015-11-12T02:42:43Z,NONE,"+1 for mxnet. Dynamic GPU memory allocation does have a big impact on performance. A simple memory allocator can significantly reduce the overhead. A smarter allocator which reuses blocks with best-fit can almost eliminate the overhead completely.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155980389,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155983063,155983063,MDEyOklzc3VlQ29tbWVudDE1NTk4MzA2Mw==,463737,2015-11-12T02:57:56Z,2015-11-12T02:57:56Z,CONTRIBUTOR,"@soumith I just pushed https://github.com/tensorflow/tensorflow/commit/1d76583411038767f673a0c96174c80eaf9ff42f, which should allow you to use our best-fit-with-coalescing allocator via the ConfigProto.

Example usage here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/alexnet/alexnet_benchmark.py#L201

We were able to get some of the larger batch sizes working with the BFC Allocator, so probably worth a try.

(We plan to make the BFC allocator the default soon, but it's not fully ready yet to be the default).
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155983063,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155984507,155984507,MDEyOklzc3VlQ29tbWVudDE1NTk4NDUwNw==,4626592,2015-11-12T03:12:21Z,2015-11-12T03:12:21Z,NONE,"Thanks a lot @soumith for the numbers, super useful!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155984507,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155985623,155985623,MDEyOklzc3VlQ29tbWVudDE1NTk4NTYyMw==,9004594,2015-11-12T03:23:44Z,2015-11-12T03:23:44Z,NONE,"The creators of cuDNN [1] may help with the performance optimization. @BryanCatanzaro

[1] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran, B. Catanzaro, and E. Shelhamer. cuDNN: Efficient Primitives for Deep Learning. arXiv preprint arXiv:1410.0759, 2014.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-155985623,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156000165,156000165,MDEyOklzc3VlQ29tbWVudDE1NjAwMDE2NQ==,123560,2015-11-12T05:06:22Z,2015-11-12T05:20:21Z,CONTRIBUTOR,"hjk41 wrote:

> +1 for mxnet. Dynamic GPU memory allocation does have a big impact on performance. A simple memory allocator can significantly reduce the overhead. A smarter allocator which reuses blocks with best-fit can almost eliminate the overhead completely.

Hi mxnet guys, the title of this thread is 'Benchmark TensorFlow' ;-)  I think you could create a new issue to request mxnet, using https://github.com/soumith/convnet-benchmarks/issues/new

[edit: looks like Soumith has created an issue for mxnet here:  https://github.com/soumith/convnet-benchmarks/issues/68 ]
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-156000165,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156012742,156012742,MDEyOklzc3VlQ29tbWVudDE1NjAxMjc0Mg==,15811008,2015-11-12T06:17:30Z,2015-11-12T07:20:31Z,NONE,"I was curious about the performance of Tensorflow using CUDA 7.5 and CUDNN 7.0. I modified the build to use them and rebuilt source. I then ran @soumith benchmark scripts for alexnet and overfeat.
My PC is getting old (Intel Core 2 quad), 16 GB RAM, NVIDIA Titan X, Ubuntu 15.04 x86_64

Alexnet: forward/backward 290ms, forward 78ms. (1.12x improvement for f/b)
Overfeat: forward/backward 1040ms, forward 264ms (1.04x improvement for f/b).

So not much of a speedup by only swapping the CUDA libraries.

@soumith, one thing I did notice is that your benchmarks for Caffe are quite different from what I got using CUDA 7.5. I think the benchmarks  you used are with CUDA 7.0, right?. When I ran your ""run_imagenet.sh"" script on my setup, I got much better results.
Alexnet: forward/backward 171ms, forward 41ms ( 1.89x improvement for F/B)
Overfeat: forward/backward 601ms, forward 133ms ( 1.37x improvement)
Googlenet: F/B 624ms, F 174ms ( 3.1x improvement)

It's not clear to me if CUDA 7.5 is supported for Caffe, but in https://github.com/BVLC/caffe/wiki/Installation, they provide 7.0 and 7.5 Docker images. However, in the main installation instructions they say to use 7.0

I attached the log files for his benchmarks running on my PC.

Tensorflow:

[tensorflow_alexnet.txt](https://github.com/soumith/convnet-benchmarks/files/32603/tensorflow_alexnet.txt)
[tensorflow_overfeat.txt](https://github.com/soumith/convnet-benchmarks/files/32604/tensorflow_overfeat.txt)

Caffe:

[output_alexnet.txt](https://github.com/soumith/convnet-benchmarks/files/32591/output_alexnet.txt)
[output_googlenet.txt](https://github.com/soumith/convnet-benchmarks/files/32590/output_googlenet.txt)
[output_overfeat.txt](https://github.com/soumith/convnet-benchmarks/files/32592/output_overfeat.txt)
[output_vgg_a.txt](https://github.com/soumith/convnet-benchmarks/files/32593/output_vgg_a.txt)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-156012742,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156051570,156051570,MDEyOklzc3VlQ29tbWVudDE1NjA1MTU3MA==,1490131,2015-11-12T09:47:15Z,2015-11-12T09:47:15Z,NONE,"@soumith Thanks for an invaluable community service!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-156051570,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156115737,156115737,MDEyOklzc3VlQ29tbWVudDE1NjExNTczNw==,8429860,2015-11-12T14:27:54Z,2015-11-12T14:27:54Z,NONE,"Nice!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-156115737,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156125375,156125375,MDEyOklzc3VlQ29tbWVudDE1NjEyNTM3NQ==,4028684,2015-11-12T14:51:15Z,2015-11-12T14:51:15Z,NONE,"@soumith Thanks you.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-156125375,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156130792,156130792,MDEyOklzc3VlQ29tbWVudDE1NjEzMDc5Mg==,5692966,2015-11-12T15:00:45Z,2015-11-12T15:00:45Z,NONE,"@soumith Thank you for benchmarking!!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-156130792,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156158598,156158598,MDEyOklzc3VlQ29tbWVudDE1NjE1ODU5OA==,1911637,2015-11-12T16:35:14Z,2015-11-12T16:35:14Z,NONE,"@scott-gray Sorry to spam @soumith's TF discussions, but when I last played with integer division via magic number mul-and-shift on GPU, the performance I got (on K40 though) was about the same as straightforward division by unsigned int32; the compiler seemed to have strength reductions that it performed in this case. However, there was lower register usage, so using this technique in a kernel that actually does other things (like transposition) would probably help. This was at the SASS level on CUDA 6.5? though.

https://github.com/facebook/fbcuda/commit/d5c8b38b4071b0151b27293a67b27c3868a0f948
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-156158598,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156163958,156163958,MDEyOklzc3VlQ29tbWVudDE1NjE2Mzk1OA==,1792006,2015-11-12T16:53:15Z,2015-11-12T16:53:15Z,NONE,"The compiler does a good job when the constant is known in advance. Was it your case? 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-156163958,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156206622,156206622,MDEyOklzc3VlQ29tbWVudDE1NjIwNjYyMg==,1399717,2015-11-12T19:18:49Z,2015-11-12T19:18:49Z,NONE,"Nice! Thank you @soumith 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-156206622,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156227405,156227405,MDEyOklzc3VlQ29tbWVudDE1NjIyNzQwNQ==,9326960,2015-11-12T20:40:01Z,2015-11-12T21:26:55Z,NONE,"@wickedfoo The advantage of calculating the magic numbers manually is that the divisor is typically parameterized and so the compiler can't compute magic numbers ahead of time.  So it then falls back to to using the floating point rcp operator and doing a bunch of corrections to make up for the potentially shorter mantissa of float (23 vs 32 bits).

To do an integer division and modulus with magic numbers reduces to just this code:

```
// j   = jrst / RST
// rst = jrst % RST
int j   = jrst * magic_RST; j >>= shift_RST;
int rst = jrst - j * RST;
```

If you know all those numbers fit in 16 bits you can use vmad from ptx or sass.  That looks like this:

```
VMAD.U16.U16 j, jrst, magic_RST, RZ;
SHR.U32      j, j, shift_RST;
VMAD.U16.U16 rst, -j, RST, jrst;
```

Otherwise your multiplications are going to expand out to 3 XMADS each, regardless of the datatype used.  It would be nice if the compiler was a little smarter about multiplication by using the minimal number of instructions for the given data types.

For larger values that might require 64 bit math, I use something like this:

```
      MOV  magicPQ,    param_magic_PQ;
      IADD negPQ, RZ, -param_grid_PQ;

      ISETP.NE.AND P1, PT, magicPQ, 1, PT;

      // m = blkMPQ / PQ
  @P1 XMAD     div1, blkMPQ,    magicPQ,    RZ;
  @P1 XMAD     div2, blkMPQ,    magicPQ.H1, RZ;
  @P1 XMAD     div3, blkMPQ.H1, magicPQ.H1, RZ;
  @P1 XMAD.CHI div1, blkMPQ.H1, magicPQ,    div1;
  @P1 IADD3.RS m, div1, div2, div3;
  @P1 SHR.U32  m, m,      param_shift_PQ;
 @!P1 SHR.U32  m, blkMPQ, param_shift_PQ;

      // pq = blkMPQ % PQ
      XMAD pq, negPQ, m, blkMPQ;
      XMAD.PSL pq, negPQ.H1, m, pq;
```

Integer division is essential for these multi dimensional tensors where you cant fit everything in just 3 block coordinates.  For more advanced uses, you can leverage it to pack all your coordinates into a single blockIdx.x value, then completely remap the order in which the indexes are scheduled.  I'm able to achieve 95% L2 hit rates using this in my winograd kernels.  This is essential for good performance as the small 32x32 batched gemm tile is pretty high bandwidth.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-156227405,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156238591,156238591,MDEyOklzc3VlQ29tbWVudDE1NjIzODU5MQ==,3587616,2015-11-12T21:22:22Z,2015-11-12T21:22:22Z,NONE,"Is there any benchmark showing the predictive performance? Computing fast but inaccurate predictions does not seem useful.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-156238591,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156287071,156287071,MDEyOklzc3VlQ29tbWVudDE1NjI4NzA3MQ==,6969686,2015-11-13T01:12:23Z,2015-11-13T01:12:23Z,NONE,"@scott-gray TensorFlow already uses fast integer division using the code in http://github.com/tensorflow/tensorflow/blob/master/third_party/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorIntDiv.h. One of the issues is that a lot of the TensorFlow kernels use 64 bit integers to index tensors, which ends up slowing things down on GPU. This is being fixed.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-156287071,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156322041,156322041,MDEyOklzc3VlQ29tbWVudDE1NjMyMjA0MQ==,123560,2015-11-13T05:01:31Z,2015-11-13T05:22:48Z,CONTRIBUTOR,"karenyyng wrote:

> Is there any benchmark showing the predictive performance? Computing fast but inaccurate predictions does not seem useful.

Ideally, they are all learning the exact same model, so the outputs should be identical (to within the bounds of rounding accuracy).  A correctness check is not a bad idea though.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-156322041,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156341020,156341020,MDEyOklzc3VlQ29tbWVudDE1NjM0MTAyMA==,1310570,2015-11-13T06:41:58Z,2015-11-13T06:41:58Z,OWNER,"@vrv thanks a lot, trying out the BFC allocator now for vgg and googlenet models.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-156341020,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156355934,156355934,MDEyOklzc3VlQ29tbWVudDE1NjM1NTkzNA==,1310570,2015-11-13T07:47:21Z,2015-11-13T07:47:21Z,OWNER,"@ces-bertino the numbers I have with Caffe are with Caffe's native kernels (that's why the entry is marked as Caffe (native) "", I presume you have CuDNN, and hence have the speedups. To compare your entries, look at the entry marked as CuDNN
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-156355934,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156357178,156357178,MDEyOklzc3VlQ29tbWVudDE1NjM1NzE3OA==,1310570,2015-11-13T07:58:15Z,2015-11-13T07:58:15Z,OWNER,"@vrv updated the table for VGG. 
Googlenet still goes OOM at batch size 128, but if my memory is right, it's really tight on space to do a batch size of 128 Googlenet in 12GB, and one needs in-place ops for sure.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-156357178,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156578574,156578574,MDEyOklzc3VlQ29tbWVudDE1NjU3ODU3NA==,1121581,2015-11-13T22:34:00Z,2015-11-13T22:34:00Z,NONE,"@Yangqing, @benoitsteiner  do you have a sense for how performance for these benchmarks depends on nvcc vs gpucc? Are the 10%-50% numbers in http://llvm.org/devmtg/2015-10/slides/Wu-OptimizingLLVMforGPGPU.pdf  for ic1/ic2 applicable here?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-156578574,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156591468,156591468,MDEyOklzc3VlQ29tbWVudDE1NjU5MTQ2OA==,15679194,2015-11-13T23:41:31Z,2015-11-13T23:41:31Z,NONE,"@ajtulloch gpucc does hide these latency issues vs nvcc as it seems to do a much better job at optimization. Using gpucc brings TensorFlow pretty close to the cuDNN[R2] numbers for AlexNet.
We are working on bridging that gap for nvcc by addressing a number of specific issues that @benoitsteiner and @Yangqing mentioned earlier.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-156591468,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156596902,156596902,MDEyOklzc3VlQ29tbWVudDE1NjU5NjkwMg==,6969686,2015-11-14T00:20:46Z,2015-11-14T00:20:46Z,NONE,"@ajtulloch The 2 main reasons why gpucc can generate faster code than nvcc are:
- The fact that gpucc can replace 64 bit integer divisions with 32 bit divisions if the values stored in the 64 bit integers can actually fit in 32 bit. As we update the TensorFlow convolution kernels to use 32 bit indices the performance of the code generated by nvcc will start to approach that of the code generated by gpucc.
- The fact that clang supports c++11 constant expressions much better than nvcc. Constant expressions allow us to generate much more efficient CUDA kernels. Unfortunately for the time being we have to disable this feature since the corresponding code doesn't compile with nvcc. I am rewritting the corresponding code to make it compatible with nvcc 7.5, and hopefully with nvcc 7.0 as well.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-156596902,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156614308,156614308,MDEyOklzc3VlQ29tbWVudDE1NjYxNDMwOA==,1121581,2015-11-14T02:51:16Z,2015-11-14T02:51:16Z,NONE,"@rajatmonga, @benoitsteiner that makes sense, thanks for that. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-156614308,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156663653,156663653,MDEyOklzc3VlQ29tbWVudDE1NjY2MzY1Mw==,9326960,2015-11-14T07:42:03Z,2015-11-14T07:42:03Z,NONE,"@benoitsteiner I'm curious how you guys are using integer division in your implementation.  The only places I find a need to use it are in custom kernels where I'm unpacking multiple coordinates from a compound index.

On a related note, I should mention that I also have another simple technique I developed for when you don't know ahead of time the value of the divisor.  It looks something like this:

```
// rcpRST = 1 / RST
I2F.F32.S32 rcpRST, RST;
MUFU.RCP rcpRST, rcpRST;

// c = crst / RST
I2F.F32.S32 crst, crst;
FMUL c, crst, rcpRST;
FFMA c, c, 5.9604644775390625e-08, c;
F2I.S32.F32.TRUNC c, c;

// rst = crst % RST
VMAD.U16.U16 rst, -c, RST, crst;
```

For most values the floating point reciprocal gets you the correct value.  It's just when the numerator and denominator are very close that you need to correct for the missing precision in float32.  This is a lot less code than the compiler would generate and is accurate for the range of values I need it for.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-156663653,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156664170,156664170,MDEyOklzc3VlQ29tbWVudDE1NjY2NDE3MA==,123560,2015-11-14T07:46:11Z,2015-11-14T08:00:15Z,CONTRIBUTOR,"> are in custom kernels where I'm unpacking multiple coordinates from a compound index

Somewhat related: question that I've been wondering about somewhat, and never quite got around to measuring: if we have two 8-bit integers, is it faster to store in separate registers/variables, or faster to pack into one register/variable, using bit-shifting?  (Edit: I suppose this is a bit vague really, since it entirely depends on how they're being used ... but I guess the trade-off I'm thinking about is: packing multiple values into a few registers will reduce register pressure, but maybe the increase in processing time from all the bit-shifting offsets any benefti?) (Edit2: I suppose what I mean actually is, are there any best-practices/guidelines as far as this goes?)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-156664170,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156665432,156665432,MDEyOklzc3VlQ29tbWVudDE1NjY2NTQzMg==,1792006,2015-11-14T08:00:35Z,2015-11-14T08:00:35Z,NONE,"If you have enough registers, do not pack the 8-bit numbers and use one register per element. Now, how do we define ""enough registers""? Well, if the occupancy you get allows you to have enough warp parallelism (together with enough instruction level parallelism) to cover the latencies, you are good. In general, unless you have a clear use case, do not pack. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-156665432,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156666194,156666194,MDEyOklzc3VlQ29tbWVudDE1NjY2NjE5NA==,9326960,2015-11-14T08:05:24Z,2015-11-14T08:05:24Z,NONE,"That would completely depend on the context in which you are using them. If you're short on register space, packing them might avoid some register spilling.  Otherwise it's probably better to keep them separate.  I'd also take a look at the video instructions like VMAD, VADD, VABSDIFF, etc.  These can operate directly on packed 8 bit values.  But in this mode these instructions are unfortunately only half throughput.  Maybe this isn't a big deal for your application but, if you wanted to write a super efficient 8 bit gemm core, they're not ideal.  These instructions are full throughput with packed 16 bit values and that is very interesting.. at least until Pascal rolls out with native fp16 support (or if you get a hold of an sm_53 X1)

Looks like @jdemouth beat me to it.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-156666194,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156670206,156670206,MDEyOklzc3VlQ29tbWVudDE1NjY3MDIwNg==,123560,2015-11-14T08:37:17Z,2015-11-14T08:37:17Z,CONTRIBUTOR,"Thanks! :-)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-156670206,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156713051,156713051,MDEyOklzc3VlQ29tbWVudDE1NjcxMzA1MQ==,6969686,2015-11-14T15:46:04Z,2015-11-14T15:46:04Z,NONE,"@scott-gray We use integer division in order to extract the individual coordinates of a tensor coefficient from its compound index. We often use compound indices for 2 reasons:
- they are independent from the rank and the shape of the tensor. This simplifies the fusion of primitive tensor operations. For example, if you reshape a 4D tensor into a 3D tensor all the coordinates need to be adjusted, but the compound indices remain the same.
- they save registers compared to using individual coordinates. This often makes a significant difference on CPUs which don't have nearly as many registers as GPUs.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-156713051,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/156738308,156738308,MDEyOklzc3VlQ29tbWVudDE1NjczODMwOA==,9326960,2015-11-14T19:25:26Z,2015-11-15T04:22:42Z,NONE,"@benoitsteiner Ok, that makes sense now.  For basic elementwise operations our backend just automatically reshapes all tensors involved in the kernel to the most efficient 2d shape.  For broadcast/reduction/take/transpose type operations, it only currently supports those in 2d and requires the user to reshape things prior to performing those ops.  This covers 99% of the use cases we've encountered but it sometimes does place a little extra burden on the user.  On the other hand it is extremely fast.  Sounds like you guys are shooting for much more general ndarray support in which case what you're doing sounds ideal.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-156738308,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/158204634,158204634,MDEyOklzc3VlQ29tbWVudDE1ODIwNDYzNA==,1288060,2015-11-19T21:34:18Z,2015-11-19T21:34:18Z,NONE,"Disclaimer: I am totally new to tensorflow and cudnn so I may not know what I am doing but very keen :)

So I built from source then realised that I already had R3 installed; I did what any other sensible person would do and replaced all R2 references with R3 and all seems well as far as running the models included.

@soumith @Yangqing, am I setting myself for trouble here? one word will suffice :)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-158204634,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/158229358,158229358,MDEyOklzc3VlQ29tbWVudDE1ODIyOTM1OA==,551151,2015-11-19T23:10:00Z,2015-11-19T23:10:00Z,CONTRIBUTOR,"@milijan You should be running fine. R3 seems to be binary compatible in the sense that most of the functions in R2 still exists in R3. I think R4 may break such hack because it will deprecate a few functions.

In case you are wondering, the reason you are not seeing any speedup by going to R3 may be as follows: in Tensorflow we hard code the cudnn algorithm to be NO_WORKSPACE, so some faster convolution paths are not being selected for now. Upcoming changes should further speed things up.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-158229358,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/158231888,158231888,MDEyOklzc3VlQ29tbWVudDE1ODIzMTg4OA==,1288060,2015-11-19T23:22:35Z,2015-11-19T23:22:35Z,NONE,"@Yangqing thanks! :+1: 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-158231888,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/159489396,159489396,MDEyOklzc3VlQ29tbWVudDE1OTQ4OTM5Ng==,606565,2015-11-25T04:34:51Z,2015-11-25T04:38:28Z,NONE,"A question with GoogleNet batch size. 

> Googlenet with batchsize 128 goes Out of Memory. The largest batch-size I could fit is 16 (tried 16, 32, 64, 128)

I can use up to 640 images per batch, using the graph from the tensorflow android example: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android 

Why tensorflow can not survive 32 images in this benchmark? 

My setup:
1. up to date tensorflow (9c3043ff3bf31a6a81810b4ce9e87ef936f1f529), compiled from scratch
2. K80 GPU with 12 GB memory

Here is the code to load the inception graph:

```
INPUT_SIZE = 224
OUTPUT_SIZE = 1024

# input should be: BS x INPUT_SIZE x INPUT_SIZE x 3 tensor
# output: BS x OUTPUT_SIZE
def inferences(images):
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(open('./tensorflow_inception_graph.pb').read())
    for n in graph_def.node:
        # control device from caller
        n.device = ''
    tf.import_graph_def(graph_def, input_map = {'input:0': images}, name = name)
    graph = tf.get_default_graph()
    output = graph.get_tensor_by_name(name + '/avgpool0:0')
    return tf.squeeze(output)
```

Given the big difference between 640 and 32, there must be something wrong. Either mine or this benchmark. Because tensorflow pre-allocating all memory, I don't know how much memory consumed exactly.

@soumith  @Yangqing Please help!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-159489396,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/159493197,159493197,MDEyOklzc3VlQ29tbWVudDE1OTQ5MzE5Nw==,463737,2015-11-25T05:05:41Z,2015-11-25T05:06:48Z,CONTRIBUTOR,"@raingo: when training we keep the activations for the lower layers to compute the gradients, so a lot of intermediate memory is used during each training step.  When doing inference, you only need the activations around to compute the next operation(s), and then they can be freed, so a lot less intermediate state is needed.

Also, based on the comment in https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-156357178, it sounds like GoogleNet training with TF might now work for up to batch 64, but not batch 128.  (I'd be surprised if batch 32 doesn't work at HEAD, for sure.)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-159493197,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/159629987,159629987,MDEyOklzc3VlQ29tbWVudDE1OTYyOTk4Nw==,606565,2015-11-25T14:49:00Z,2015-11-25T14:49:00Z,NONE,"@vrv Gotta. Thanks!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-159629987,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/159937924,159937924,MDEyOklzc3VlQ29tbWVudDE1OTkzNzkyNA==,243115,2015-11-26T15:26:30Z,2015-11-26T15:26:30Z,NONE,"@soumith They made some changes to tensorflow [TensorFlow: Improve performance of Alexnet](https://github.com/tensorflow/tensorflow/commit/9c3043ff3bf31a6a81810b4ce9e87ef936f1f529). can you update the benchmark for Alexnet
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-159937924,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/160766752,160766752,MDEyOklzc3VlQ29tbWVudDE2MDc2Njc1Mg==,14060629,2015-11-30T21:26:36Z,2015-11-30T21:26:36Z,CONTRIBUTOR,"@soumith  

> Googlenet with batchsize 128 goes Out of Memory. The largest batch-size I could fit is 16 (tried 16, 32, 64, 128) [...] if my memory is right, it's really tight on space to do a batch size of 128 Googlenet in 12GB, and one needs in-place ops for sure.

For comparison, here are my measurements of approximate peak memory usage with Torch/cuDNNv3 on Titan-X:

AlexNet (128): 3 GB
OverFeat (128): 5 GB
VGG Model-A (128): OOM
GoogLeNet(128): 9G

VGG Model-A-11 (64): 8 G
VGG Model-B-13(64): 12 G (I think this may fall back on slower algos due to tight memory)
VGG Model-D-16 (64): 12 G (I think this may fall back on slower algos due to tight memory)
VGG Model-E-19 (64): 12 G (I think this may fall back on slower algos due to tight memory)

VGG Model-A-11 (96): 11 G
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-160766752,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/163724264,163724264,MDEyOklzc3VlQ29tbWVudDE2MzcyNDI2NA==,15474222,2015-12-10T19:21:16Z,2015-12-10T19:21:16Z,NONE,"@soumith Since its release I've seen pretty dramatic improvements in tensorflow's memory management and performance. I think it may be time to benchmark 0.6.0.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-163724264,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/163736764,163736764,MDEyOklzc3VlQ29tbWVudDE2MzczNjc2NA==,1310570,2015-12-10T20:11:34Z,2015-12-10T20:11:34Z,OWNER,"@alexatknit will do. i will take some time one of these days to do MXNet, Chainer and TF 0.6. Have been a bit busy lately with wrapping up research.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-163736764,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/168931600,168931600,MDEyOklzc3VlQ29tbWVudDE2ODkzMTYwMA==,716138,2016-01-05T08:14:43Z,2016-01-05T08:14:43Z,NONE,"I am looking forward to the updated comparison, have you found time to look into it?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-168931600,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/168936385,168936385,MDEyOklzc3VlQ29tbWVudDE2ODkzNjM4NQ==,1310570,2016-01-05T08:48:40Z,2016-01-05T08:49:24Z,OWNER,"TensorFlow Trunk as of 1 hour ago (post 0.6 release) numbers:

**[AlexNet (One Weird Trick paper)](https://code.google.com/p/cuda-convnet2/source/browse/layers/layers-imagenet-1gpu.cfg)** - Input 128x3x224x224

| Library | Time (ms) | forward (ms) | backward (ms) |
| :-: | --: | --: | --: |
| CuDNN-R3 (Torch) | 96 | 32 | 64 |
| Nervana (Neon) | 101 | 32 | 69 |
| CuDNN-R2 (Torch) | 231 | 70 | 161 |
| **TensorFlow 0.5** | 326 | 96 | 230 |
| **TensorFlow 0.6+** | 292 | 70 | 222 |

**[Overfeat [fast]](http://arxiv.org/abs/1312.6229)** - Input 128x3x231x231

| Library | Time (ms) | forward (ms) | backward (ms) |
| :-: | --: | --: | --: |
| CuDNN-R3 (Torch) | 326 | 113 | 213 |
| fbfft (Torch) | 342 | 114 | 227 |
| CuDNN-R2 (Torch) | 810 | 234 | 576 |
| TensorFlow 0.5 | 1084 | 316 | 768 |
| TensorFlow 0.6+ | 856 | 204 | 652 |

**[OxfordNet [Model-A]](http://arxiv.org/abs/1409.1556/)** - Input 64x3x224x224

| Library | Time (ms) | forward (ms) | backward (ms) |
| :-: | --: | --: | --: |
| Nervana | 590 | 180 | 410 |
| CuDNN-R3 (Torch) | 615 | 196 | 418 |
| CuDNN-R2 (Torch) | 1099 | 342 | 757 |
| TensorFlow  0.5 | 1840 | 545 | 1295 |
| TensorFlow  0.6+ | 1656 | 347 | 1309 |

**[GoogleNet V1](http://research.google.com/pubs/pub43022.html)** - Input **128**x3x224x224

| Library | Time (ms) | forward (ms) | backward (ms) |
| :-: | --: | --: | --: |
| CuDNN-R3 (Torch) | 431 | 117 | 313 |
| TensorFlow 0.5 | OOM | OOM | OOM |
| TensorFlow 0.6+ | 1237 | 246 | 991 |

There you go.
The new logs are all checked in.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-168936385,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/170825600,170825600,MDEyOklzc3VlQ29tbWVudDE3MDgyNTYwMA==,15679194,2016-01-12T07:37:48Z,2016-01-12T07:37:48Z,NONE,"@soumith Thanks for running the numbers again. I know you have been asked to do this a number of times lately and it takes you away from your research. Having these benchmarks have been greatly useful for everyone.

After your run we realized we seem to have regressed in performance since the 0.6.0 release (mostly from our switch over to the public Eigen branch) and over the last few days @zheng-xq and @benoitsteiner along with others have made improvements to get back the performance. When running the benchmarks again at [commit d1b8333](https://github.com/tensorflow/tensorflow/commit/d1b8333), we get the following numbers:

| Model | Total (ms) | Forward (ms) | Backward (ms) |
| --- | --- | --- | --- |
| AlexNet | 229 | 69 | 160 |
| Overfeat [fast] | 839 | 203 | 636 |
| OxfordNet | 1216 | 329 | 887 |
| GoogleNet V1 - Input 128x3x224x224 | 815 | 234 | 581 |
- This is measured on an unsuperclocked Titan-X with the default power-limit 250W. 
- For consistency, between each run, we wait for a few minutes for GPU to cool down to room temperature. 

These results are also in line with what we see at 0.6.0 release.

We are also looking into setting up performance benchmarks with the builds so we don't hit such performance regressions.

Again, Thanks for all your updates.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-170825600,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/174478539,174478539,MDEyOklzc3VlQ29tbWVudDE3NDQ3ODUzOQ==,3648713,2016-01-25T11:30:00Z,2016-01-25T11:30:00Z,NONE,"Does anyone has experiences and/or comparisons with DL4J (http://deeplearning4j.org) ?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-174478539,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/175050036,175050036,MDEyOklzc3VlQ29tbWVudDE3NTA1MDAzNg==,1310570,2016-01-26T14:39:22Z,2016-01-26T14:39:22Z,OWNER,"@rajatmonga just got back from vacay. It's cool that you guys are setting up contbuilds for perf regressions.

However, I dont get the numbers that you seem to be getting on the tensorflow as of yesterday ( a27d844e05447e65aa279ae5269a2d75590f46f6 ). The numbers are slightly better but not quite the improvement that you are seeing.

Look here for the new numbers: https://github.com/soumith/convnet-benchmarks/commit/1f09e1e3b2841a2f58b83aa1d078a830bca4508f
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-175050036,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/175327608,175327608,MDEyOklzc3VlQ29tbWVudDE3NTMyNzYwOA==,15679194,2016-01-27T01:18:39Z,2016-01-27T01:18:39Z,NONE,"@soumith Thanks for running the benchmarks again. It is possible there are some memory related regressions that are hurting performance again. What you have right now is good, lets not worry about this.

We are working on getting cuDNN R4 fully supported and will address the remaining performance issues in that context. May ping this thread once we have a full release with R4, and it will be worthwhile rerunning benchmarks - likely for many of the libraries.

Also, let me know if we can help you with this project in any way - it is very useful to the community, but I am sure it takes a lot of your time as well. Thanks for keeping this going!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-175327608,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/180036384,180036384,MDEyOklzc3VlQ29tbWVudDE4MDAzNjM4NA==,15679194,2016-02-04T20:31:43Z,2016-02-04T20:31:43Z,NONE,"Yes, That is in our list of tasks and is quite important to make sure we
don't have performance regressions. We haven't been able to get to it yet.

On Thu, Feb 4, 2016 at 9:11 AM Madder notifications@github.com wrote:

> Has anyone thought of running these benchmarks periodically as part of
> tensorflow's CI for instance?
> 
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-179950846
> .
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-180036384,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/184911136,184911136,MDEyOklzc3VlQ29tbWVudDE4NDkxMTEzNg==,11093686,2016-02-16T23:02:49Z,2016-02-16T23:02:49Z,NONE,"Tf 0.7.0 released!
Looking forward to the updated benchmarks.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-184911136,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/185393104,185393104,MDEyOklzc3VlQ29tbWVudDE4NTM5MzEwNA==,953399,2016-02-17T20:37:20Z,2016-02-17T20:37:20Z,NONE,":+1: +1:
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-185393104,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/187919685,187919685,MDEyOklzc3VlQ29tbWVudDE4NzkxOTY4NQ==,6997335,2016-02-23T21:28:53Z,2016-02-23T21:28:53Z,NONE,"Great results :+1:  :+1:  :+1: 

Looking forward to the results with cuDNN v4
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-187919685,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/187962829,187962829,MDEyOklzc3VlQ29tbWVudDE4Nzk2MjgyOQ==,324300,2016-02-23T23:25:46Z,2016-02-23T23:25:46Z,NONE,"+1

On Tue, Feb 23, 2016 at 10:29 PM, Ronghang Hu notifications@github.com
wrote:

> Great results [image: :+1:] [image: :+1:] [image: :+1:]
> 
> Looking forward to the results with cuDNN v4
> 
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-187919685
> .
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-187962829,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/189980061,189980061,MDEyOklzc3VlQ29tbWVudDE4OTk4MDA2MQ==,1310570,2016-02-29T00:28:02Z,2016-02-29T00:28:02Z,OWNER,"As requested, TF 0.7 + CuDNN R4 has been benchmarked. CuDNN R4 + Torch has also been benchmarked as a baseline.

Within the span of Nervana's Neon, Torch + CuDNN4, TensorFlow + CuDNN4 (and Caffe + CuDNN is likely in the same ballpark as torch), TensorFlow ( at commit https://github.com/tensorflow/tensorflow/commit/1d4f00da15a886916cd7a62ddf119b0b460c850c ) still lags behind the others by 2x to 3x performance on Alexnet, VGG and Googlenet. It is within 1.5x of Overfeat.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-189980061,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/189980223,189980223,MDEyOklzc3VlQ29tbWVudDE4OTk4MDIyMw==,1310570,2016-02-29T00:30:22Z,2016-02-29T00:30:22Z,OWNER,"For full details, see the main README.md: https://github.com/soumith/convnet-benchmarks/blob/master/README.md and the raw logs are located here: https://github.com/soumith/convnet-benchmarks/commit/2888b23959190cefeee59cdd5e15f66a74031f8f
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-189980223,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/189980365,189980365,MDEyOklzc3VlQ29tbWVudDE4OTk4MDM2NQ==,1310570,2016-02-29T00:32:29Z,2016-02-29T00:32:29Z,OWNER,"i have not changed the benchmark scripts in any way, so if the TF benchmark scripts need any change (such as new allocator settings etc.), I welcome the TF folks to let me know.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-189980365,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/190010122,190010122,MDEyOklzc3VlQ29tbWVudDE5MDAxMDEyMg==,15679194,2016-02-29T02:39:12Z,2016-02-29T02:39:12Z,NONE,"Thanks Soumith@, this isn't quite where we had seen our numbers at, but we
will look at the tests again and ping you if we notice something.

Thanks again for running these benchmarks!

On Sun, Feb 28, 2016, 4:32 PM Soumith Chintala notifications@github.com
wrote:

> i have not changed the benchmark scripts in any way, so if the TF
> benchmark scripts need any change (such as new allocator settings etc.), I
> welcome the TF folks to let me know.
> 
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-189980365
> .
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-190010122,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/190010537,190010537,MDEyOklzc3VlQ29tbWVudDE5MDAxMDUzNw==,1310570,2016-02-29T02:40:30Z,2016-02-29T02:40:30Z,OWNER,"Thanks Rajat, happy to investigate further. I built TF from source, and configured it with CUDA 7.5 + CuDNN-4, if that helps. The commit is https://github.com/tensorflow/tensorflow/commit/1d4f00da15a886916cd7a62ddf119b0b460c850c
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-190010537,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/190082597,190082597,MDEyOklzc3VlQ29tbWVudDE5MDA4MjU5Nw==,716377,2016-02-29T07:52:55Z,2016-02-29T07:52:55Z,NONE,"I've had similar numbers using CUDA 7.0, cuDNN v4, and https://github.com/tensorflow/tensorflow/commit/b88971051fbc49fa1e0b91ec1b0b60defa11697e on a Titan X. Tried fiddling with device placement and the session config, but it made no material difference in the results. @rajatmonga, out of curiosity are you using cuDNN and nvcc internally, or gpucc?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-190082597,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/191085006,191085006,MDEyOklzc3VlQ29tbWVudDE5MTA4NTAwNg==,1310570,2016-03-02T06:23:01Z,2016-03-02T06:23:01Z,OWNER,"@nryant Thanks for the additional data point. I am honestly very nervous whenever I have to deliver any negative news on convnet-benchmarks. fwiw, @spezzer on reddit also confirmed that it was a data layout thing as well https://www.reddit.com/r/MachineLearning/comments/487fmo/convnetbenchmarks_updated_with_numbers_for/d0i7ord .
I'm closing this issue now, as we have benchmarked tensorflow across multiple versions and given it enough time and data. Will of course keep updating it over time as appropriate.
Thanks all.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-191085006,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/191096830,191096830,MDEyOklzc3VlQ29tbWVudDE5MTA5NjgzMA==,463737,2016-03-02T06:52:26Z,2016-03-02T06:52:26Z,CONTRIBUTOR,"@soumith: I think in this case it's a combination of layout and some Eigen improvements that hadn't made its way upstream -- we're looking at both of these actively.  Thanks again for your effort -- we'll let you know when it makes sense to update the numbers (and provide our own for comparison).
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-191096830,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192806640,192806640,MDEyOklzc3VlQ29tbWVudDE5MjgwNjY0MA==,66073,2016-03-06T05:08:46Z,2016-03-06T05:08:46Z,NONE,"A recent commit adds NCHW support for BiasAdd, which results in about 40% speed up.

https://github.com/tensorflow/tensorflow/commit/d6f3ebfdfc1d5b5df1f6ae73466abe2ec5721b5b
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-192806640,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192809336,192809336,MDEyOklzc3VlQ29tbWVudDE5MjgwOTMzNg==,463737,2016-03-06T05:32:23Z,2016-03-06T05:32:23Z,CONTRIBUTOR,"@thinxer: we'll let @soumith know when to update the numbers, but thanks for noticing :)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-192809336,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192809431,192809431,MDEyOklzc3VlQ29tbWVudDE5MjgwOTQzMQ==,1310570,2016-03-06T05:35:18Z,2016-03-06T05:35:18Z,OWNER,"That's really cool, thanks for letting me know. I'm doing a new, complete set of benchmarks for deep learning, not just convnets, will cover this commit in them
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-192809431,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/192824189,192824189,MDEyOklzc3VlQ29tbWVudDE5MjgyNDE4OQ==,15679194,2016-03-06T07:50:21Z,2016-03-06T07:50:21Z,NONE,"Thanks @soumith! No rush though.

We have most of the pieces together to support NCHW and expect to see more
gains once we update the models to use that. Will ping you once that is
ready as well. This commit helps quite a bit (was another regression on our
part). Of course the layout changes will mostly help convnets and not other
kinds of models.

On Sat, Mar 5, 2016 at 9:35 PM Soumith Chintala notifications@github.com
wrote:

> That's really cool, thanks for letting me know. I'm doing a new, complete
> set of benchmarks for deep learning, not just convnets, will cover this
> commit in them
> 
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-192809431
> .
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-192824189,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/197199165,197199165,MDEyOklzc3VlQ29tbWVudDE5NzE5OTE2NQ==,4180295,2016-03-16T07:43:53Z,2016-03-16T07:43:53Z,NONE,"How about tensorflow 0.7ï¼Ÿ
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-197199165,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,116287942,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/198289829,198289829,MDEyOklzc3VlQ29tbWVudDE5ODI4OTgyOQ==,10137,2016-03-18T10:08:50Z,2016-03-18T10:08:50Z,NONE,"Thanks for the benchmark @soumith . Looking forward for new updated TensorFlow.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66/comments,https://github.com/soumith/convnet-benchmarks/issues/66#issuecomment-198289829,https://api.github.com/repos/soumith/convnet-benchmarks/issues/66
soumith,convnet-benchmarks,115907655,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155107108,155107108,MDEyOklzc3VlQ29tbWVudDE1NTEwNzEwOA==,1310570,2015-11-09T16:05:11Z,2015-11-09T16:05:11Z,OWNER,"Thanks. On it right away!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/65/comments,https://github.com/soumith/convnet-benchmarks/issues/65#issuecomment-155107108,https://api.github.com/repos/soumith/convnet-benchmarks/issues/65
soumith,convnet-benchmarks,115907655,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155113156,155113156,MDEyOklzc3VlQ29tbWVudDE1NTExMzE1Ng==,43829,2015-11-09T16:23:13Z,2015-11-09T16:23:13Z,NONE,"It seems to only support CuDNN v2, so I'm curious to see if it manages to be competitive.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/65/comments,https://github.com/soumith/convnet-benchmarks/issues/65#issuecomment-155113156,https://api.github.com/repos/soumith/convnet-benchmarks/issues/65
soumith,convnet-benchmarks,115907655,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155278880,155278880,MDEyOklzc3VlQ29tbWVudDE1NTI3ODg4MA==,2306281,2015-11-10T04:06:33Z,2015-11-10T04:06:33Z,NONE,"Very curious about its performance, as Jeff Dean hasn't mentioned much about its speed.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/65/comments,https://github.com/soumith/convnet-benchmarks/issues/65#issuecomment-155278880,https://api.github.com/repos/soumith/convnet-benchmarks/issues/65
soumith,convnet-benchmarks,115907655,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155680190,155680190,MDEyOklzc3VlQ29tbWVudDE1NTY4MDE5MA==,2974951,2015-11-11T06:23:56Z,2015-11-11T06:45:29Z,NONE,"Would this be same condition?
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/alexnet/alexnet_benchmark.py

Done experiment on Tesla K-80 and showed around 180ms forward and 540ms forward-backward.

[edited out to avoid confusion]
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/65/comments,https://github.com/soumith/convnet-benchmarks/issues/65#issuecomment-155680190,https://api.github.com/repos/soumith/convnet-benchmarks/issues/65
soumith,convnet-benchmarks,115907655,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155680323,155680323,MDEyOklzc3VlQ29tbWVudDE1NTY4MDMyMw==,1310570,2015-11-11T06:25:13Z,2015-11-11T06:25:13Z,OWNER,"@adamist521 That benchmark is incomplete. It does not have FC layers, loss is different etc. 
I should have numbers in about 1-2 hours. Finished AlexNet, Overfeat, working on VGG, Googlenet. Stay tuned.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/65/comments,https://github.com/soumith/convnet-benchmarks/issues/65#issuecomment-155680323,https://api.github.com/repos/soumith/convnet-benchmarks/issues/65
soumith,convnet-benchmarks,115907655,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155680351,155680351,MDEyOklzc3VlQ29tbWVudDE1NTY4MDM1MQ==,2974951,2015-11-11T06:25:36Z,2015-11-11T06:25:36Z,NONE,"@soumith Awesome!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/65/comments,https://github.com/soumith/convnet-benchmarks/issues/65#issuecomment-155680351,https://api.github.com/repos/soumith/convnet-benchmarks/issues/65
soumith,convnet-benchmarks,115907655,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155692608,155692608,MDEyOklzc3VlQ29tbWVudDE1NTY5MjYwOA==,1710528,2015-11-11T07:29:30Z,2015-11-11T07:29:30Z,NONE,":+1:
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/65/comments,https://github.com/soumith/convnet-benchmarks/issues/65#issuecomment-155692608,https://api.github.com/repos/soumith/convnet-benchmarks/issues/65
soumith,convnet-benchmarks,115907655,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155692843,155692843,MDEyOklzc3VlQ29tbWVudDE1NTY5Mjg0Mw==,1710528,2015-11-11T07:31:37Z,2015-11-11T07:31:37Z,NONE,"@soumith take a look at https://github.com/tensorflow/tensorflow/pull/113
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/65/comments,https://github.com/soumith/convnet-benchmarks/issues/65#issuecomment-155692843,https://api.github.com/repos/soumith/convnet-benchmarks/issues/65
soumith,convnet-benchmarks,115907655,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155702042,155702042,MDEyOklzc3VlQ29tbWVudDE1NTcwMjA0Mg==,1310570,2015-11-11T08:26:01Z,2015-11-11T08:26:01Z,OWNER,"closing this. discuss here: https://github.com/soumith/convnet-benchmarks/issues/66
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/65/comments,https://github.com/soumith/convnet-benchmarks/issues/65#issuecomment-155702042,https://api.github.com/repos/soumith/convnet-benchmarks/issues/65
soumith,convnet-benchmarks,113558343,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/151711812,151711812,MDEyOklzc3VlQ29tbWVudDE1MTcxMTgxMg==,1310570,2015-10-28T03:35:25Z,2015-10-28T03:35:25Z,OWNER,"Thanks a lot Kenta!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/64/comments,https://github.com/soumith/convnet-benchmarks/pull/64#issuecomment-151711812,https://api.github.com/repos/soumith/convnet-benchmarks/issues/64
soumith,convnet-benchmarks,112226484,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/149355040,149355040,MDEyOklzc3VlQ29tbWVudDE0OTM1NTA0MA==,1310570,2015-10-19T21:38:34Z,2015-10-19T21:38:34Z,OWNER,"Thanks Baran.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/63/comments,https://github.com/soumith/convnet-benchmarks/pull/63#issuecomment-149355040,https://api.github.com/repos/soumith/convnet-benchmarks/issues/63
soumith,convnet-benchmarks,111935063,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/149310959,149310959,MDEyOklzc3VlQ29tbWVudDE0OTMxMDk1OQ==,1310570,2015-10-19T18:51:22Z,2015-10-19T18:51:22Z,OWNER,"Thank you!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/62/comments,https://github.com/soumith/convnet-benchmarks/pull/62#issuecomment-149310959,https://api.github.com/repos/soumith/convnet-benchmarks/issues/62
soumith,convnet-benchmarks,111763969,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148615472,148615472,MDEyOklzc3VlQ29tbWVudDE0ODYxNTQ3Mg==,1310570,2015-10-16T05:40:21Z,2015-10-16T05:40:21Z,OWNER,"not sure why I have to....
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/61/comments,https://github.com/soumith/convnet-benchmarks/pull/61#issuecomment-148615472,https://api.github.com/repos/soumith/convnet-benchmarks/issues/61
soumith,convnet-benchmarks,111763969,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148616017,148616017,MDEyOklzc3VlQ29tbWVudDE0ODYxNjAxNw==,14060629,2015-10-16T05:46:12Z,2015-10-16T05:46:12Z,CONTRIBUTOR,"caffe/install.sh script fails, because 'dev' branch was deprecated and deleted. Now all development in Caffe is done against master branch.

$ git clone https://github.com/BVLC/caffe.git
Cloning into 'caffe'...
remote: Counting objects: 23687, done.
remote: Compressing objects: 100% (16/16), done.
remote: Total 23687 (delta 3), reused 0 (delta 0), pack-reused 23669
Receiving objects: 100% (23687/23687), 32.49 MiB | 12.96 MiB/s, done.
Resolving deltas: 100% (15071/15071), done.
Checking connectivity... done.

$ cd caffe/
~/caffe$ git checkout dev
error: pathspec 'dev' did not match any file(s) known to git.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/61/comments,https://github.com/soumith/convnet-benchmarks/pull/61#issuecomment-148616017,https://api.github.com/repos/soumith/convnet-benchmarks/issues/61
soumith,convnet-benchmarks,111763969,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148616101,148616101,MDEyOklzc3VlQ29tbWVudDE0ODYxNjEwMQ==,1310570,2015-10-16T05:47:04Z,2015-10-16T05:47:04Z,OWNER,"oh well then....
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/61/comments,https://github.com/soumith/convnet-benchmarks/pull/61#issuecomment-148616101,https://api.github.com/repos/soumith/convnet-benchmarks/issues/61
soumith,convnet-benchmarks,111663183,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148496452,148496452,MDEyOklzc3VlQ29tbWVudDE0ODQ5NjQ1Mg==,14060629,2015-10-15T19:24:09Z,2015-10-15T19:24:09Z,CONTRIBUTOR,"Only the first commit ""Add missing padding"" was meant to go into this PR. I am trying to figure out how to remove the other two commits from this PR.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/60/comments,https://github.com/soumith/convnet-benchmarks/pull/60#issuecomment-148496452,https://api.github.com/repos/soumith/convnet-benchmarks/issues/60
soumith,convnet-benchmarks,111663183,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148499941,148499941,MDEyOklzc3VlQ29tbWVudDE0ODQ5OTk0MQ==,14060629,2015-10-15T19:40:23Z,2015-10-15T19:40:23Z,CONTRIBUTOR,"Removed the other two commits. Now this PR contains exactly one correct commit.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/60/comments,https://github.com/soumith/convnet-benchmarks/pull/60#issuecomment-148499941,https://api.github.com/repos/soumith/convnet-benchmarks/issues/60
soumith,convnet-benchmarks,111663183,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148500103,148500103,MDEyOklzc3VlQ29tbWVudDE0ODUwMDEwMw==,1310570,2015-10-15T19:41:07Z,2015-10-15T19:41:07Z,OWNER,"thanks!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/60/comments,https://github.com/soumith/convnet-benchmarks/pull/60#issuecomment-148500103,https://api.github.com/repos/soumith/convnet-benchmarks/issues/60
soumith,convnet-benchmarks,111663183,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148617281,148617281,MDEyOklzc3VlQ29tbWVudDE0ODYxNzI4MQ==,14060629,2015-10-16T05:57:04Z,2015-10-16T05:57:04Z,CONTRIBUTOR,"Now greentea directory is out-of sync with caffe directory, due to files duplicated in greentea directory (vgg_a.prototxt, alexnet.prototxt, etc). I can submit a PR removing those and other duplicate files, and using files in ../caffe instead, but I am not sure if @naibaf7 plans on changing the copy of the files in the future with greentea-specific things.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/60/comments,https://github.com/soumith/convnet-benchmarks/pull/60#issuecomment-148617281,https://api.github.com/repos/soumith/convnet-benchmarks/issues/60
soumith,convnet-benchmarks,111663183,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148848576,148848576,MDEyOklzc3VlQ29tbWVudDE0ODg0ODU3Ng==,5577650,2015-10-16T22:14:55Z,2015-10-16T22:14:55Z,NONE,"@ozabluda 
for the close future it is safe to assume Greentea can always use the same protobuf files as original Caffe.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/60/comments,https://github.com/soumith/convnet-benchmarks/pull/60#issuecomment-148848576,https://api.github.com/repos/soumith/convnet-benchmarks/issues/60
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148242847,148242847,MDEyOklzc3VlQ29tbWVudDE0ODI0Mjg0Nw==,1310570,2015-10-15T00:47:26Z,2015-10-15T00:47:26Z,OWNER,"For comparison, here's the log of Caffe + OpenBLAS numbers on the same machine (It's the Digits box ;-) )
https://github.com/soumith/convnet-benchmarks/blob/cpu/caffe/output_alexnet.log
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148242847,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148252001,148252001,MDEyOklzc3VlQ29tbWVudDE0ODI1MjAwMQ==,1310570,2015-10-15T01:44:51Z,2015-10-15T01:44:51Z,OWNER,"More info is in the CPU branch:
https://github.com/soumith/convnet-benchmarks/tree/cpu

The alexnet-owt protobuf, with the same architecture I use for the GPU versions is here:
https://github.com/soumith/convnet-benchmarks/blob/cpu/caffe/imagenet_winners/alexnet.prototxt

The intel-adapted version is here:
https://github.com/soumith/convnet-benchmarks/blob/cpu/intel_optimized_technical_preview_for_multinode_caffe_1.0/models/intel_alexnet/alexnet.prototxt
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148252001,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148264019,148264019,MDEyOklzc3VlQ29tbWVudDE0ODI2NDAxOQ==,6487731,2015-10-15T02:45:54Z,2015-10-15T02:45:54Z,NONE,"well, assuming i didn't mess up the analysis, and used the right inputs/etc, a runtime of 0.146s on the (non-intel) alexnet-owl prototxt you linked above, for a batch of 128 forward and backward, implies 3.77TF/s.

AFAIK, haswell can do at most 32FLOPs/cycle/core. for your 6-core cpu @ 3.5 GHZ, that would be 672GF/s peak.

so, i guess that seems pretty fishy overall (i.e. perf ~6X peak). i might suspect benchmarking error, such as accidentally running in GPU mode with who-knows-what backend (i.e BLAS, cudnn v?, i dunno). it's not clear that intel themselves was claiming perf anything like that in thier blog post, but i didn't try to runs the #s on their post.

then again, i have no idea what the intel code might be doing (got scared off by the license, so didn't dig into it), but if there are some algorithmic changes and/or anything that means they're not doing the same set of FLOPS, then all bets are off. but of course such improvement might port to GPUs as well. or not; i'd believe there are algorithms that are more suited to CPUs that trade uniformity/complexity for doing less raw FLOPS.

for ref, here's the #s i'm working from:

```
moskewcz@maaya:~/git_work/boda/run/tr1$ boda cnet_ana  --in-model=alexnet_owl --print-ops=1 --in-sz=227 && python ../../pysrc/flops.py  --per-layer=1 --backward 1 --num-imgs=128 --runtime=.164

conv1 FWD 18.7GF 182MB  --- BACK_GRAD 18.7GF  --- BACK_DIFF 18.7GF  BACKWARD_BYTES 261MB 
conv2/5x5_s1 FWD 61.7GF 104MB  --- BACK_GRAD 61.7GF  --- BACK_DIFF 61.7GF  BACKWARD_BYTES 131MB 
conv3/3x3_s1 FWD 33.3GF 60.5MB  --- BACK_GRAD 33.3GF  --- BACK_DIFF 33.3GF  BACKWARD_BYTES 82.4MB 
conv4/3x3_s1 FWD 44.4GF 67.8MB  --- BACK_GRAD 44.4GF  --- BACK_DIFF 44.4GF  BACKWARD_BYTES 110MB 
conv5/3x3_s1 FWD 29.6GF 53.7MB  --- BACK_GRAD 29.6GF  --- BACK_DIFF 29.6GF  BACKWARD_BYTES 81.8MB 
fc6 FWD 13.2GF 214MB  --- BACK_GRAD 13.2GF  --- BACK_DIFF 13.2GF  BACKWARD_BYTES 426MB 
fc7 FWD 4.29GF 71.3MB  --- BACK_GRAD 4.29GF  --- BACK_DIFF 4.29GF  BACKWARD_BYTES 141MB 
fc8 FWD 1.05GF 19.0MB  --- BACK_GRAD 1.05GF  --- BACK_DIFF 1.05GF  BACKWARD_BYTES 37.5MB 
total _inxp time:  0s
-- INPUT: NUM_IMGS=128 --
-- INPUT: RUNTIME=0.164s --
-- INPUT: POWER=200W --
--- FWD TOTALS ---
618GF 3.77TF/s
2.04GB 12.5GB/s AI=303F/B
32.8J 18.8GF/s/W
moskewcz@maaya:~/git_work/boda/run/tr1$ 
```
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148264019,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148265239,148265239,MDEyOklzc3VlQ29tbWVudDE0ODI2NTIzOQ==,1310570,2015-10-15T02:51:00Z,2015-10-15T02:51:00Z,OWNER,"@moskewcz 3.77TF/s doesn't hold true if you switch to FFT or Winograd based convolutions.

References:
https://en.wikipedia.org/wiki/Convolution_theorem
http://arxiv.org/abs/1509.09308
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148265239,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148271787,148271787,MDEyOklzc3VlQ29tbWVudDE0ODI3MTc4Nw==,8460517,2015-10-15T03:29:34Z,2015-10-15T03:32:40Z,NONE,"""With these optimizations time to train AlexNet\* network on full ILSVRC-2012 dataset to 80% top5 accuracy reduces from 58 days to about 5 days.""

The benchmark used dual E5-2699-v3 CPUs, which have 18 cores at 2.3 GHz => 2x18x32FLOPs/cyclex2.3Ghz=2.65TFLOPs

Sounds about right.

TitanX running Nervanagpu probably about 1 day?

I would guess Intel just implemented a more efficient direct convolution for many-core Intel CPUs. I do not see any indication they are using fast algorithns.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148271787,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148274464,148274464,MDEyOklzc3VlQ29tbWVudDE0ODI3NDQ2NA==,8460517,2015-10-15T03:56:13Z,2015-10-15T03:56:29Z,NONE,"So anyway the numbers Intel reported sound plausible, but your numbers don't. :-)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148274464,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148275088,148275088,MDEyOklzc3VlQ29tbWVudDE0ODI3NTA4OA==,6487731,2015-10-15T03:59:45Z,2015-10-15T03:59:45Z,NONE,"again, if i got my #s right, if we assume 70M images (~65 epochs \* 1.1M images/epoch, not sure if that's a good value or not) in 5 days to train alexnet_owl as per the blog post, that implies 783GF/s -- given the peak #s that andravin gave above, that would be ~35% efficiency, which is perhaps pretty impressive but believable. but it'd be good to know the actual # of epochs/images/etc to get a real value, i could easily be off by quite a bit on those guesses. corrections welcome.

mwm

```
moskewcz@maaya:~/git_work/boda/run/tr1$ boda cnet_ana  --in-model=alexnet_owl --print-ops=1 --in-sz=227 && python ../../pysrc/flops.py  --per-layer=1 --backward 1 --num-imgs=70000000 --runtime=432000
conv1 FWD 10.2PF 99.5TB  --- BACK_GRAD 10.2PF  --- BACK_DIFF 10.2PF  BACKWARD_BYTES 143TB 
conv2/5x5_s1 FWD 33.7PF 56.2TB  --- BACK_GRAD 33.7PF  --- BACK_DIFF 33.7PF  BACKWARD_BYTES 70.2TB 
conv3/3x3_s1 FWD 18.2PF 31.6TB  --- BACK_GRAD 18.2PF  --- BACK_DIFF 18.2PF  BACKWARD_BYTES 42.1TB 
conv4/3x3_s1 FWD 24.3PF 35.1TB  --- BACK_GRAD 24.3PF  --- BACK_DIFF 24.3PF  BACKWARD_BYTES 56.2TB 
conv5/3x3_s1 FWD 16.2PF 28.1TB  --- BACK_GRAD 16.2PF  --- BACK_DIFF 16.2PF  BACKWARD_BYTES 42.1TB 
fc6 FWD 7.19PF 4.66TB  --- BACK_GRAD 7.19PF  --- BACK_DIFF 7.19PF  BACKWARD_BYTES 8.17TB 
fc7 FWD 2.35PF 2.29TB  --- BACK_GRAD 2.35PF  --- BACK_DIFF 2.35PF  BACKWARD_BYTES 3.44TB 
fc8 FWD 573TF 1.43TB  --- BACK_GRAD 573TF  --- BACK_DIFF 573TF  BACKWARD_BYTES 2.57TB 
total _inxp time:  0s
-- INPUT: NUM_IMGS=70000000 --
-- INPUT: RUNTIME=432000.0s --
-- INPUT: POWER=200W --
--- FWD TOTALS ---
338PF 783GF/s
627TB 1.45GB/s AI=540F/B
86.4MJ 3.91GF/s/W
moskewcz@maaya:~/git_work/boda/run/tr1$
```
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148275088,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148275345,148275345,MDEyOklzc3VlQ29tbWVudDE0ODI3NTM0NQ==,8460517,2015-10-15T04:01:53Z,2015-10-15T04:01:53Z,NONE,".. and having looked a bit at Caffe's CPU implementation, im2col is single-threaded, and will be a pretty nasty bottleneck in a 36-core system.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148275345,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148276633,148276633,MDEyOklzc3VlQ29tbWVudDE0ODI3NjYzMw==,8460517,2015-10-15T04:18:16Z,2015-10-15T04:18:16Z,NONE,"@moskewcz your numbers sound plausible to me.. and so Intel's post really points to what a disaster out of the box Caffe performance must be on many-core CPUs.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148276633,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148276926,148276926,MDEyOklzc3VlQ29tbWVudDE0ODI3NjkyNg==,1310570,2015-10-15T04:22:17Z,2015-10-15T04:22:17Z,OWNER,"@andravin @moskewcz thanks. I'm going to investigate a bit on why the numbers are much more fluffier on my machine. For a start, I'll probably start an end-to-end training and see what happens....
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148276926,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148286924,148286924,MDEyOklzc3VlQ29tbWVudDE0ODI4NjkyNA==,6487731,2015-10-15T05:40:30Z,2015-10-15T05:40:30Z,NONE,"sounds like a plan. make sure you fire up nvidia-smi while you're running it ... ;)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148286924,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148401660,148401660,MDEyOklzc3VlQ29tbWVudDE0ODQwMTY2MA==,1310570,2015-10-15T14:26:29Z,2015-10-15T14:26:29Z,OWNER,"@moskewcz I've already verified that it's running on CPU and using intel code-paths, simply by collecting samples from the stack and looking at hotspots.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148401660,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148410812,148410812,MDEyOklzc3VlQ29tbWVudDE0ODQxMDgxMg==,6487731,2015-10-15T14:55:02Z,2015-10-15T14:55:02Z,NONE,"hmm, well, i was mostly joking and i mostly believe you. however, i'm not sure that what you say precludes the GPU being active. in fact, if, say, the new intel layers were running on the CPU, but all/some conv layers were on the GPU, you'd probably see perf similar to what you reported. and if you look at the CPU usage/stack, it'll be pegged at 100%, and it'll always be inside the intel code if you stop it ...

i'm really just suggesting that, given the fishiness of the #s, some form(s) of sanity checking are in order. in particular, for example, did you compile in CPU only mode? again, i don't really think that's the issue, but if (for example) intel ran/compiled on boxes without GPUs, then maybe something unexpected happens with their code/build on a box that has GPUs. 

but i'm not really fixated on the maybe-running-on-GPU idea, there are plenty of other places for errors. batch size issues, shared library wackiness, straight-up user error, etc ...

on a side note, thanks for all your hard work running these benchmarks!

mwm
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148410812,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148486227,148486227,MDEyOklzc3VlQ29tbWVudDE0ODQ4NjIyNw==,1310570,2015-10-15T18:49:17Z,2015-10-15T18:49:17Z,OWNER,"caffe is getting no access to the GPUs, I disabled it at the driver level.
I just fixed the protobuf to force itself to do the backward phase (it was conveniently deciding that it doesn't need to do the backward). That brought the backward times up, and overall it stands at 268ms / mini-batch now. I'm working on training it fully with the imagenet lmdb. Let's see.
https://github.com/soumith/convnet-benchmarks/blob/cpu/intel_optimized_technical_preview_for_multinode_caffe_1.0/output_alexnet.log#L329-L365
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148486227,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148776416,148776416,MDEyOklzc3VlQ29tbWVudDE0ODc3NjQxNg==,14060629,2015-10-16T17:17:10Z,2015-10-16T19:48:43Z,CONTRIBUTOR,"2-columnn AlexNet Intel is benchmarking at the announcement (different from 1-col AlexNet ""One weird trick"" from Soumith's benchmark) has 1449 MFLOPs per image in the forward pass and 2x that in the backward pass, ignoring biases, LRN, activations, pooling, and loss. Taking numbers from Intel's announcement we have:

Forward pass: 1449 MFLOP \* 731images/1sec=1.059 TFLOP/s
Forw+Backw pass: 3 \* 1449 MFLOP \* 271 images/1sec=1.187 TFLOP/s

which is easily believable (exact max FLOPs on those Intel CPUs to be posted later).
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148776416,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148803382,148803382,MDEyOklzc3VlQ29tbWVudDE0ODgwMzM4Mg==,14060629,2015-10-16T18:44:28Z,2015-10-16T20:47:21Z,CONTRIBUTOR,"@soumith>A full [forward + backward] on AlexNet on a Desktop 6-core Intel(R) Core(TM) i7-5930K CPU @ 3.50GHz takes an average of 164ms EDIT: 268 ms. [...] I need a couple more sanity checks before I can believe this result. Look at how little time they are spending in the convolution layers, even the biggest ones: https://github.com/soumith/convnet-benchmarks/blob/cpu/intel_optimized_technical_preview_for_multinode_caffe_1.0/output_alexnet.log#L329-L365

i7-5930K AVX2 clock  is smaller than 3.50 GHz base clock. I don't recall exact value, but it seems to be ~3.2 GHz. It can issue 2 AVX256 (8 operand) SP MAD (=2FLOP) per clock, for the total of 2 \* 8 \* 2=32 FLOP/clock. 

32 FLOP/clock \* 3.0GHz \* 6core=576 GFLOP/s.

Your numbers at the url above (output from Intel's Caffe) seem to be per image for conv and per minibatch for fc) and are comfortably below that (except for fc6 backward, which must be an artifact of Caffe timing), so they are totally believable. In fact, there is a lot of room for improvement. In fact, they are not that much better than your numbers for OpenBLAS (except for conv1)

| layer | MFLOP/image | ms/image | Intel GFLOP/s | OpenBLAS GFLOP/s |
| --- | --- | --- | --- | --- |
| conv1 forward: | 141 | 0.726 | 194 | 59 |
| conv1 backward | 282 | 0.672 | 420 | 67 |
| conv2 forward: | 448 | 3.722 | 120 | 159 |
| conv2 backward: | 896 | 6.22 | 144 | 167 |
| conv3 forward: | 224 | 2.323 | 96 | 112 |
| conv3 backward: | 448 | 3.604 | 124 | 148 |
| conv4 forward: | 299 | 3.851 | 78 | 90 |
| conv4 backward: | 598 | 6.344 | 94 | 116 |
| conv5 forward: | 199 | 2.621 | 76 | 90 |
| conv5 backward: | 398 | 4.375 | 91 | 119 |
| fc6 forward: | 75 | 38.597/mb | 250 | 232 |
| fc6 backward: | 151 | 32.152/mb | 601 | 243 |
| fc7 forward: | 34 | 18.549/mb | 232 | 231 |
| fc7 backward: | 67 | 15.504/mb | 554 | 293 |
| fc8 forward: | 8 | 4.967/mb | 211 | 249 |
| fc8 backward: | 16 | 3.932/mb | 533 | 278 |
| Forward: | 1428 | 90.621 | 104 | 94 |
| Backward: | 2856 | 72.961 | 132 | 123 |
| Forward-Backward: | 4231 | 1684 | 121 | 112 |
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148803382,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148819523,148819523,MDEyOklzc3VlQ29tbWVudDE0ODgxOTUyMw==,6487731,2015-10-16T19:57:51Z,2015-10-16T19:57:51Z,NONE,"@ozabluda i think your analysis of the intel #s looks good and is believable. as per an above comment, we're guessing ~2.65TFLOPs peak for the dual-socket 36-core machine intel used for the announcement. so again it comes out to ~35% or so efficiency.

but, i think there are some issues with your per-layer analysis in your second comment. firstly, i don't think we can trust the per-layer #s from the caffe log too much; for example the pack+relu1 times are >> the conv1 time, so i'd assume there's some timing wonkiness there -- time and/or work being shifted among layers for example.

but, perhaps more importantly (and confusingly):
1) the 1684 ms is for _10_ iterations/batches. this is the value that got corrected to  ~2680ms, with a corresponding 268ms forward+backward per batch. confusingly, the other two #s for forward and backward (the ~73ms back / ~91ms fwd) are per single iteration/batch. the idea is that they are the 'min' batch times across interactions, and thus in theory more indicative of the steady-state per-batch performance (which does seem to be the case). so for your forward-backward line you probably want to add the times of the forward and backwards lines and ignore the overall combined time. alternately you could divide it by the iteration count which will yield a similar value.
 2) the 268ms is for a 128 image batch, not a single image. i believe your flop #s are for a single image (i have ~6.1GF for the no-groups regular alexnet per image, so i'd guess that your 4.2GF / image is right for the 'original' 2-groups version), so you're off by a factor of 128 in flops.

PS: using 268ms / batch, and 4.2GF / image, that yields a still-implausible ~2TF/s for the 6-core digits box, and again it seems to disagree with the more-reasonable intel announced #s, so i'm still assuming benchmarking error.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148819523,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148820011,148820011,MDEyOklzc3VlQ29tbWVudDE0ODgyMDAxMQ==,8460517,2015-10-16T20:00:43Z,2015-10-16T20:50:17Z,NONE,"There is no such thing as an AVX2 clock. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148820011,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148823190,148823190,MDEyOklzc3VlQ29tbWVudDE0ODgyMzE5MA==,14060629,2015-10-16T20:17:27Z,2015-10-16T20:18:42Z,CONTRIBUTOR,"@moskewcz I also noticed that Intel's Caffe seems to report timings for conv layers per image and for fc per minibatch. I corrected the table above (I also realized Soumith's numbers are for 1-col AlexNet, while Intel's are for 2-col AlexNet). Please check if it makes sense to you now.

AVX2 (32 SP ops/clock) can't run at the base clock frequency, so it throttles down to a lower ""AVX clock"". Although, maybe it is only true for AVX-512, which none of the CPUs in question have.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148823190,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148825669,148825669,MDEyOklzc3VlQ29tbWVudDE0ODgyNTY2OQ==,6487731,2015-10-16T20:28:13Z,2015-10-16T20:28:13Z,NONE,"@ozabluda hmm, i'm not sure what you changed, but i guess it looks more/differently wrong to me now, still as per my (1) and (2). AFAIK all the caffe timings are supposedly per batch/iteration, not per image (as per my comment section (2)). and in this case, they look like garbage, as per my comment section (1). FWIW it's been a while since i dug into the caffe timing code and it has changed over time but on the whole i've always found it hard to work with / understand; i'm mostly just looking at things here from the top level and using my own calculations, so i'm not the best one to comment on the details of the caffe reported #s.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148825669,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148829039,148829039,MDEyOklzc3VlQ29tbWVudDE0ODgyOTAzOQ==,14060629,2015-10-16T20:42:19Z,2015-10-16T20:44:12Z,CONTRIBUTOR,"@moskewcz Stock Caffe timings sure are per minibatch (like Soumith's OpenBLAS timings). Intel's port timings do look like garbage (say 0.726ms for conv1), unless they are per image (except for fc), in which case they totally make sense (and approximately equal to stock Caffe/OpenBLAS). See my table above.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148829039,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148837954,148837954,MDEyOklzc3VlQ29tbWVudDE0ODgzNzk1NA==,14060629,2015-10-16T21:11:55Z,2015-10-16T21:11:55Z,CONTRIBUTOR,"@andravin> The benchmark used dual E5-2699-v3 CPUs, which have 18 cores at 2.3 GHz => 2x18x32FLOPs/cyclex2.3Ghz=2.65TFLOPs

Actual AVX base clock is 1.9 Ghz (see quote below).

2 CPU \* 18 cores \* 32FLOPs/cycle \* 1.9Ghz =2.189 TFLOP/s

I am almost willing to bet that the scaling to the second CPU is extremely poor in this Intel's iteration. i.e. 2 CPUs are not that much faster than 1 CPU.

> To cope with the huge difference between the power consumption of Integer and AVX code, Intel is >introducing new base and Turbo Boost frequencies for all their SKUs; these are called AVX >base/Turbo. For example, the E5-2693 v3 will start from a base frequency of 2.3GHz and turbo up >to 3.3GHz when running non-AVX code. When it encounters AVX code however, it will not able to >boost its clock to more than 3GHz during a 1 ms window of time. If the CPU comes close to thermal >and TDP limits, clock speed will drop down to 1.9GHz, the ""AVX base clock"".
> http://www.anandtech.com/show/8423/intel-xeon-e5-version-3-up-to-18-haswell-ep-cores-/5
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148837954,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148840853,148840853,MDEyOklzc3VlQ29tbWVudDE0ODg0MDg1Mw==,8460517,2015-10-16T21:28:17Z,2015-10-16T21:28:17Z,NONE,"@ozabluda Ah, I did not know about this feature of Xeon processors, thanks. So it is Xeon only? soumith's Core(TM) i7-5930K will not have this? My i7-5775C seems to sustain AVX2 256-bit FMA instructions at regular turbo boost speed with liquid cooling.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148840853,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148845316,148845316,MDEyOklzc3VlQ29tbWVudDE0ODg0NTMxNg==,8460517,2015-10-16T21:54:15Z,2015-10-16T21:54:15Z,NONE,"I tracked down AVX base frequency specs for haswell e5 processors here: https://www.microway.com/knowledge-center-articles/detailed-specifications-intel-xeon-e5-2600v3-haswell-ep-processors/

Would be nice to find an official Intel source. I suspect this is only a feature of the big Xeon chips.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148845316,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148869861,148869861,MDEyOklzc3VlQ29tbWVudDE0ODg2OTg2MQ==,8460517,2015-10-17T00:18:53Z,2015-10-17T00:18:53Z,NONE,"@soumith What command line did you use? README.txt says:

```
For timing
#> ./build/tools/caffe time \
   -iterations <number of iterations> \
   --model=models/intel_alexnet/train_val.prototxt
```

When I run that on my 4-core i7-5775C I get:

```
I1016 16:39:40.395843 15816 caffe.cpp:333]      conv1   forward: 379.242 ms.
I1016 16:39:40.395848 15816 caffe.cpp:336]      conv1   backward: 354.405 ms.
[...]
I1016 16:39:40.396093 15816 caffe.cpp:341] Min Forward pass: 2879.16 ms.
I1016 16:39:40.396098 15816 caffe.cpp:343] Min Backward pass: 5410.64 ms.
I1016 16:39:40.396102 15816 caffe.cpp:345] Min Forward-Backward: 83316 ms.
I1016 16:39:40.396107 15816 caffe.cpp:347] Total Time: 83316 ms.
[...]
Total FP jobs:8192 jpt:2048 residue:0
Total BP jobs:106496
```

Most telling are the Total FP/BP jobs numbers, which are exactly equal to 256X the values in your log file. 256 is the batch size specified in train_val.prototxt.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148869861,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148881777,148881777,MDEyOklzc3VlQ29tbWVudDE0ODg4MTc3Nw==,8460517,2015-10-17T03:28:16Z,2015-10-17T03:28:16Z,NONE,"@soumith Oh I see now you are using your own prototxt file, not the one that was provided by Intel. Obviously there is something wrong that is causing your prototxt to use minibatch size 1.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148881777,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148885178,148885178,MDEyOklzc3VlQ29tbWVudDE0ODg4NTE3OA==,8460517,2015-10-17T04:33:38Z,2015-10-17T04:33:38Z,NONE,"Actually I get reasonable numbers using your alexnet.prototxt too. So I am not sure what is wrong with your setup.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148885178,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148886033,148886033,MDEyOklzc3VlQ29tbWVudDE0ODg4NjAzMw==,14060629,2015-10-17T04:56:56Z,2015-10-17T04:57:21Z,CONTRIBUTOR,"@andravin:

> Ah, I did not know about this feature of Xeon processors, thanks. So it is Xeon only? My i7-5775C seems to sustain AVX2 256-bit FMA instructions at regular turbo boost speed with liquid cooling.

I think all CPUs have it, if they overheat. Liquid cooling helps (I notice dthat with my liquid cooled Haswell as well. Can your CPU run AVX2 256-bit FMA instructions at regular turbo boost speed on all cores simultaneously or just one?

> I tracked down AVX base frequency specs for haswell e5 processors here: [microway]

This is awesome, thank you.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148886033,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148889830,148889830,MDEyOklzc3VlQ29tbWVudDE0ODg4OTgzMA==,14060629,2015-10-17T06:24:58Z,2015-10-17T06:53:55Z,CONTRIBUTOR,"@moskewcz:

> i think there are some issues with your per-layer analysis in your second comment. firstly, i don't think we can trust the per-layer #s from the caffe log too much; for example the pack+relu1 times are >> the conv1 time, so i'd assume there's some timing wonkiness there -- time and/or work being shifted among layers for example.

I think something caused conv layers to report time per image, while everything else is per minibatch.

> but, perhaps more importantly (and confusingly):
> 1) the 1684 ms is for 10 iterations/batches. this is the value that got corrected to ~2680ms, with a >corresponding 268ms forward+backward per batch. confusingly, the other two #s for forward and >backward (the ~73ms back / ~91ms fwd) are per single iteration/batch. 

My calculations are per-layer. Total Forward/Backward are also calculated from per-layer (reported numbers are all screwed up), exactly as you suggest.

> [...] so for your forward-backward line you probably want to add the times of the forward and >backwards lines and ignore the overall combined time. alternately you could divide it by the >iteration count which will yield a similar value.
> 2) the 268ms is for a 128 image batch, not a single image. 

I ignore 2680/268 number.

> i believe your flop #s are for a single image 

that's right.

> (i have ~6.1GF for the no-groups regular alexnet per image, so i'd guess that your 4.2GF / image is right for the 'original' 2-groups version), so you're off by a factor of 128 in flops.

I have 4.231 GF/image for the 'original' 2-groups version and 4.285 GF/image for the ""One weird trick"" 1-col version, ignoring biases, LRN, activations, pooling, and loss. Your 6.1 GF/image is probably the 'original' 2-groups version without groups, but it's not what 1-col version is (the number of filtermaps is different).

> PS: using 268ms / batch, and 4.2GF / image, that yields a still-implausible ~2TF/s for the 6-core >digits box, and again it seems to disagree with the more-reasonable intel announced #s, so i'm still >assuming benchmarking error.

My calculated ""total time"" conv*128+fc comes to 4524 ms/minibatch. I ignore 268, because it doesn't correspond to anything in the per-layer I can think of. 90ms and 72ms correspond to the sum, but is incorrect because conv is per image and everything else is per minibatch.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148889830,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148958264,148958264,MDEyOklzc3VlQ29tbWVudDE0ODk1ODI2NA==,1310570,2015-10-17T22:44:05Z,2015-10-17T22:44:05Z,OWNER,"@andravin thanks for the log on your side. I suppose doing pure-benchmarking instead of having that lmdb data layer before might be having side-effects on the intel caffe. I'll follow-up on Monday.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148958264,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148963280,148963280,MDEyOklzc3VlQ29tbWVudDE0ODk2MzI4MA==,8460517,2015-10-18T00:23:33Z,2015-10-18T00:24:17Z,NONE,"@ozabluda Here are official Intel documents about avx and frequencies for Xeon E5 v3, does not mention other processors, which of course leaves us wondering: http://www.intel.com/content/dam/www/public/us/en/documents/white-papers/performance-xeon-e5-v3-advanced-vector-extensions-paper.pdf
http://www.intel.com/content/dam/www/public/us/en/documents/specification-updates/xeon-e5-v3-spec-update.pdf

Still haven't found anything authoritative for i7. Probably have to ask Intel.

Also I want to make clear that I think @soumith 's log file indicates that the batch size was just 1 image.  Not sure why his alexnet.prototxt gives me batch size 128 and behaves differently for him.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148963280,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/148973811,148973811,MDEyOklzc3VlQ29tbWVudDE0ODk3MzgxMQ==,14060629,2015-10-18T03:10:48Z,2015-10-18T03:10:48Z,CONTRIBUTOR,"@andravin>Here are official Intel documents about avx and frequencies for Xeon E5 v3, does not mention other processors, which of course leaves us wondering:

Thank you. These are good. I think all Intel CPUs are in practice limited only by TDP (which liquid cooling helps with), even though Intel also lists current and power limits. Overclocked Intel CPUs are known to suck 350W on Prime95 without damage and 400W, maybe with long-term damage, and Intel CPUs don't prevent it, if cooled.

The doc says that AVX will never go over ""AVX Max All Core Turbo"" (even though the doc implies that it should only be true for AVX2.

> Also I want to make clear that I think @soumith 's log file indicates that the batch size was just 1 image. 

I don't think so. There is:

input_dim: 128
Top shape: 128 3 227 227 (19787136)

https://github.com/soumith/convnet-benchmarks/blob/cpu/intel_optimized_technical_preview_for_multinode_caffe_1.0/output_alexnet.log#L5
https://github.com/soumith/convnet-benchmarks/blob/cpu/intel_optimized_technical_preview_for_multinode_caffe_1.0/output_alexnet.log#L183

and timings for all non-conv layers (fc, relu, pool) look like they are for minibatch size=128. It looks more like only conv layer timings are per image for whatever reason.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-148973811,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/149372172,149372172,MDEyOklzc3VlQ29tbWVudDE0OTM3MjE3Mg==,14060629,2015-10-19T23:06:07Z,2015-10-19T23:06:07Z,CONTRIBUTOR,"@soumith:

> @moskewcz 3.77TF/s doesn't hold true if you switch to FFT or Winograd based convolutions.
> http://arxiv.org/abs/1509.09308

This is pretty awesome, @andravin 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-149372172,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/149429776,149429776,MDEyOklzc3VlQ29tbWVudDE0OTQyOTc3Ng==,8460517,2015-10-20T04:40:30Z,2015-10-20T04:51:47Z,NONE,"Thanks, @ozabluda I'm looking forward to the first truly efficient implementations of the fast Winograd convnet algorithms. The first draft of the paper was just a teaser. ;-)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-149429776,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/149736293,149736293,MDEyOklzc3VlQ29tbWVudDE0OTczNjI5Mw==,14060629,2015-10-20T23:43:03Z,2015-10-20T23:49:50Z,CONTRIBUTOR,"For one 4x4 block, F(2x2, 3x3), standard direct convolution uses (3 \* 3) \* (2 \* 2)=36 multiplications, and 6*4=24 additions, for the total 36+24=60 FLOP.

Ignoring amortized filter [1], and amortized inverse [2] @andravin's implementation of Winograd's convolutions uses 4*4=16 multiplications, and 80/3 amortized additions for the total of 16+80/3=42+2/3  FLOP.

Utilization is 60/(16+80/3)= 140.625%, which is how he gets results in Table 6 (max efficiency 134.0% on conv4.2).

I tried counting absolute minimum number of amortized additions, ignoring filter [1], inverse [2], assuming infinite image and infinite CPU registers.

I counted 24 additions for data per block.

This gives us 16+24=40 FLOP. Compared to standard direct 60 FLOP, we have
60/40= 150% max possible utilization.

[1] Paper says filter transform uses 28 FLOP per input channel. For conv4.2 image is 24x24, which makes filter FLOP negligible.

[2] Paper says inverse transform uses 24 additions, amortized over input channels, which is negligible for all layers, except conv1.2, but even there it's a win 60/(16+24+24/3)=1.25, not sure which is why it is not in Table 6.

| C^TdC= |  |  |  |
| --- | --- | --- | --- |
| d00âˆ’d20âˆ’d02+d22 | d20âˆ’d22+d10âˆ’d12 | d20âˆ’d22âˆ’d10+d12 | d10âˆ’d12âˆ’d30+d32 |
| d01âˆ’d21+d02âˆ’d22 | d21+d22+d11+d12 | d21+d22âˆ’d11âˆ’ d12 | d11+d12âˆ’d31âˆ’d32 |
| âˆ’d01+d21+d02âˆ’d22 | âˆ’d21+d22âˆ’d11+d12 | âˆ’d21+d22+d11âˆ’d12 | âˆ’d11+d12+d31âˆ’d32 |
| d01âˆ’d21âˆ’d03+d23 | d21âˆ’d23+d11âˆ’d13 | d21âˆ’d23âˆ’d11+d13 | d11âˆ’d13âˆ’d31+d33 |
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-149736293,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/149753067,149753067,MDEyOklzc3VlQ29tbWVudDE0OTc1MzA2Nw==,8460517,2015-10-21T01:31:13Z,2015-10-21T01:31:13Z,NONE,"@ozabluda Thanks, one thing I think you are missing is that transformed data can be re-used for convolution with every filter. So the data transform FLOPs can be amortized over the number of filters.

Anyway I don't want to hijack this issue, so please continue the conversation of Winograd convnet algorithms at https://www.reddit.com/r/MachineLearning/comments/3nocg5/fast_algorithms_for_convolutional_neural_networks/
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-149753067,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/149952828,149952828,MDEyOklzc3VlQ29tbWVudDE0OTk1MjgyOA==,14060629,2015-10-21T16:30:11Z,2015-10-21T16:30:11Z,CONTRIBUTOR,"@andravin Aha! This is why you keep referring to only the number of multiplications in ""arithmetic complexity reduction"".
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-149952828,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/150106424,150106424,MDEyOklzc3VlQ29tbWVudDE1MDEwNjQyNA==,1310570,2015-10-22T05:01:08Z,2015-10-22T05:23:24Z,OWNER,"Ok, so today I finally finished building my caffe lmdb for imagenet, and I ran the intel benchmarks with the lmdb data layer etc. etc. (just like how they want it to be).

The numbers are not as impressive anymore (as expected).
- **Caffe + MKL** - ~ **5100 ms**
- **IntelCaffe + MKL** - ~ **3052 ms**
- Speedup: **1.67x**

References:
Caffe + MKL: https://github.com/soumith/convnet-benchmarks/blob/cpu/caffe/output_alexnet_mkl.log#L320-L329

IntelCaffe + MKL: https://github.com/soumith/convnet-benchmarks/blob/cpu/intel_optimized_technical_preview_for_multinode_caffe_1.0/output_alexnet.log#L362-L371
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150106424,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/150111895,150111895,MDEyOklzc3VlQ29tbWVudDE1MDExMTg5NQ==,9326960,2015-10-22T05:28:04Z,2015-10-22T05:28:04Z,NONE,"In related news, I just finished the first winograd fprop/bprop fp32 kernel.  It is fully fused and requires no additional memory.  But the big news is that it runs fastest at a minibatch size of 8.  And by fast I mean close to 10 virtual Tflops.  It is full utilization and is primarily power limited.  The tile size is K32xN8 so it should be pretty versatile over a wide range of dimensions.  Even C=3 performance is pretty good at a 1.7 vTflops.

I have a fair amount of tuning I want to try with it to see if I can boost cache performance, then I'll move on to the grad weight update kernel which I already have sketched out.

Then after that I'll take a stab at the bigger and faster transforms.  I'm hoping to hit about 3x the performance of direct conv, and also at N=8.  But those will be much trickier to fit in a fully fused kernel.

Special thanks to @andravin for lots of fruitful discussion on bringing this about.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150111895,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/150112415,150112415,MDEyOklzc3VlQ29tbWVudDE1MDExMjQxNQ==,1310570,2015-10-22T05:34:13Z,2015-10-22T05:34:13Z,OWNER,"@scott-gray that sounds super exciting. Cant wait to bench it.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150112415,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/150146963,150146963,MDEyOklzc3VlQ29tbWVudDE1MDE0Njk2Mw==,43829,2015-10-22T08:31:31Z,2015-10-22T08:31:31Z,NONE,"I second that :) Can't wait to try this out! With the pervasiveness of 3x3 convolutions nowadays, this could be a game changer.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150146963,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/150212831,150212831,MDEyOklzc3VlQ29tbWVudDE1MDIxMjgzMQ==,15225631,2015-10-22T13:03:49Z,2015-10-22T13:03:49Z,NONE,"Hi, I'm one of the developers who worked on this package. I've looked at the [run.sh](https://github.com/soumith/convnet-benchmarks/blob/cpu/intel_optimized_technical_preview_for_multinode_caffe_1.0/run.sh) and the only suggestion I have is to enable OpenMP thread affinity by setting `KMP_AFFINITY=compact,granularity=fine` (assuming that the CPU has HyperThreading enabled). This probably should be done for the baseline run as well.

Looking at the logs, I see that the speedups for the convolution layers are not as high as we'd expect, but we never ran on a 6-core machine, so maybe our expectations are wrong. The CPU convolution layers often call tall and skinny SGEMMs which have limited scalability for a 2x18-core machine. But on a 6-core machine the gap between the SGEMM-based convolution and the approach we used may be much more narrow.

Also, it's weird that the fc layers run slower in the new package because we did not modify that code.

Here's a link to a [whitepaper](http://www.intel.com/content/dam/www/public/us/en/documents/white-papers/performance-xeon-e5-v3-advanced-vector-extensions-paper.pdf) explaining how CPU changes frequency when executing AVX2 instructions.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150212831,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/150278425,150278425,MDEyOklzc3VlQ29tbWVudDE1MDI3ODQyNQ==,8460517,2015-10-22T16:20:02Z,2015-10-22T16:54:30Z,NONE,"Thanks, @rsdubtso We found that whitepaper but it only explicitly mentions Xeon E5 v3 processors. Are other processors (eg i7) affected by AVX2 frequencies, if so where can we find documentation of the AVX2 frequencies for those processors? Anyway I opened an Intel forum ticket for it here: https://communities.intel.com/thread/87851
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150278425,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/150278775,150278775,MDEyOklzc3VlQ29tbWVudDE1MDI3ODc3NQ==,10148468,2015-10-22T16:21:33Z,2015-10-22T16:21:33Z,NONE,"Nice work Scott! Looking forward to playing with it 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150278775,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/150296106,150296106,MDEyOklzc3VlQ29tbWVudDE1MDI5NjEwNg==,8460517,2015-10-22T17:21:26Z,2015-10-22T17:21:26Z,NONE,"@scott-gray strikes again! Well done.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150296106,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/150330295,150330295,MDEyOklzc3VlQ29tbWVudDE1MDMzMDI5NQ==,14060629,2015-10-22T19:30:59Z,2015-10-22T19:30:59Z,CONTRIBUTOR,"@soumith @rsdubtso 

> The numbers are not as impressive anymore (as expected).
> Caffe + MKL - ~ 5100 ms
> IntelCaffe + MKL - ~ 3052 ms
> Speedup: 1.67x

Actually, there is tremendous improvement in the convolutional layers (still far from 614 GFLOP/s peak), even bigger improvements in pool and activation layers (don't matter much), huge regression in fc layers - should be easy to fix, and huge regression in data layer (even easier to fix). Caffe/MKL is also much faster that Caffe/OpenBLAS you benchmarked earlier. There is a timing bug in conv1 backward (implausible GFLOP/s). Also, this benchmark was run with minibatch=256, inconsistent with all others, where minibatch=128.

| i7-5930K (614 GF/s) |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- |
|  |  | OpenBLAS |  | CaffeMKL |  | IntelCaffe |  |
|  | MF | ms | GF/s | ms | GF/s | ms | GF/s |
| conv1 forward: | 141 | 304.795 | 59.2 | 357.069 | 101.1 | 88.196 | 409.3 |
| conv1 backward: | 282 | 536.807 | 67.2 | 330.767 | 218.3 | 93.893 | 768.9 |
| conv1/relu forward: |  | 21.8936 |  | 47.1887 |  | 7.79 |  |
| conv1/relu backward: |  | 28.5025 |  | 57.7991 |  | 12.544 |  |
| pool1/3x3_s2 forward: |  | 85.0495 |  | 216.237 |  | 8.542 |  |
| pool1/3x3_s2 backward: |  | 45.7551 |  | 92.3998 |  | 18.194 |  |
| conv2/5x5_s1 forward: | 448 | 361.393 | 158.7 | 533.456 | 215.0 | 251.792 | 455.5 |
| conv2/5x5_s1 backward: | 896 | 687.499 | 166.8 | 1007.32 | 227.7 | 684.775 | 335.0 |
| cpnv2/relu forward: |  | 15.8821 |  | 32.5319 |  | 5.629 |  |
| cpnv2/relu backward: |  | 20.6075 |  | 41.8427 |  | 9.129 |  |
| pool2/3x3_s2 forward: |  | 67.7179 |  | 138.228 |  | 6.104 |  |
| pool2/3x3_s2 backward: |  | 35.3347 |  | 71.5279 |  | 13.084 |  |
| conv3/3x3_s1 forward: | 224 | 254.672 | 112.6 | 207.55 | 276.3 | 126.165 | 454.5 |
| conv3/3x3_s1 backward: | 448 | 385.527 | 148.7 | 415.731 | 275.9 | 285.18 | 402.2 |
| conv3/relu forward: |  | 7.8402 |  | 15.1693 |  | 2.503 |  |
| conv3/relu backward: |  | 9.814 |  | 19.5894 |  | 4.08 |  |
| conv4/3x3_s1 forward: | 299 | 424.084 | 90.2 | 321.758 | 237.9 | 169.798 | 450.8 |
| conv4/3x3_s1 backward: | 598 | 660.748 | 115.8 | 658.584 | 232.5 | 382.091 | 400.7 |
| conv4/relu forward: |  | 5.3955 |  | 10.159 |  | 1.559 |  |
| conv4/relu backward: |  | 6.7793 |  | 13.0562 |  | 2.705 |  |
| conv5/3x3_s1 forward: | 199 | 282.846 | 90.1 | 218.284 | 233.4 | 113.286 | 449.7 |
| conv5/3x3_s1 backward: | 398 | 428.887 | 118.8 | 435.634 | 233.9 | 256.046 | 397.9 |
| conv5/relu forward: |  | 5.4022 |  | 10.1855 |  | 1.583 |  |
| conv5/relu backward: |  | 6.4006 |  | 12.9499 |  | 2.86 |  |
| pool5/3x3_s2 forward: |  | 34.1529 |  | 53.2655 |  | 1.958 |  |
| pool5/3x3_s2 backward: |  | 15.0692 |  | 31.1371 |  | 4.043 |  |
| fc6 forward: | 75 | 41.5847 | 232.4 | 42.6512 | 453.1 | 72.547 | 266.4 |
| fc6 backward: | 151 | 79.5084 | 243.1 | 77.2451 | 500.4 | 132.581 | 291.6 |
| fc7 forward: | 34 | 18.6208 | 230.7 | 20.1991 | 425.3 | 33.675 | 255.1 |
| fc7 backward: | 67 | 29.3293 | 292.9 | 37.3001 | 460.6 | 61.519 | 279.3 |
| fc8 forward: | 8 | 4.2152 | 248.8 | 5.7591 | 364.1 | 8.904 | 235.5 |
| fc8 backward: | 16 | 7.5515 | 277.7 | 9.0703 | 462.4 | 15.573 | 269.3 |
| Average Forward | 1428 | 1935.58 | 94.4 | 2259.9 | 161.8 | 1026.8 | 356.1 |
| Average Backward | 2856 | 2984.16 | 122.5 | 3312.14 | 220.8 | 1982.95 | 368.8 |
| Average Forward-Backward: | 4285 | 4919.8 | 111.5 | 5572.1 | 196.9 | 30845 | 355.6 |
| Total Time: |  |  |  | 55721 |  | 30845 |  |
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150330295,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/150333438,150333438,MDEyOklzc3VlQ29tbWVudDE1MDMzMzQzOA==,1310570,2015-10-22T19:37:58Z,2015-10-22T19:37:58Z,OWNER,"@ozabluda just note that IntelCaffe uses ""minimum time over all runs"" for the per-layer numbers, whereas regular Caffe uses ""average time over all runs"". That's one reason why I didn't do a per-layer breakdown.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150333438,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/150336649,150336649,MDEyOklzc3VlQ29tbWVudDE1MDMzNjY0OQ==,14060629,2015-10-22T19:50:23Z,2015-10-22T19:50:23Z,CONTRIBUTOR,"@soumith, does it really matter? Typically only first iteration differs much from others.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150336649,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/150345130,150345130,MDEyOklzc3VlQ29tbWVudDE1MDM0NTEzMA==,14060629,2015-10-22T20:20:53Z,2015-10-22T20:20:53Z,CONTRIBUTOR,"@scott-gray:

> In related news, I just finished the first winograd fprop/bprop fp32 kernel. It is fully fused and requires no additional memory. But the big news is that it runs fastest at a minibatch size of 8. And by fast I mean close to 10 virtual Tflops. It is full utilization and is primarily power limited. The tile size is K32xN8 so it should be pretty versatile over a wide range of dimensions. Even C=3 performance is pretty good at a 1.7 vTflops. I have a fair amount of tuning I want to try with it to see if I can boost cache performance, 

Awesome. Fastest at a minibatch size of N=8 is awesome, but weird (cache performance?), because, in addition to less work for GPU, you amortize filer transforms over smaller N. 

10 virtual Tflops/6.144 actual Tflops=**163%** ""utilization"" (using @andravin's terminology). Why so little? Gimme, gimme, gimme :-). Assuming you implemented F(2x2,3x3), max theoretic utilization [1] is (60+4)/(16+4)=**320%** [2]  For N=8, we can't neglect Filter transform (28 FLOP): (60+4)/(16+4+28/8)=**272%**

[1] by my calculation, different from paper, please correct me if I am wrong

[2] except for the first layer (conv1.1), where we can't neglect 24 FLOP in Inverse transform amortized over only C=3 input channels, and there are only 2 reductions per output per input channel, for the total max theoretical utilization: (60+2 \* 4/3)/(16+(2 \* 4+24)/3)=**235%**. For N=8 (60+2 \* 4/3)/(16+(2 \* 4+24)/3 + 28/8) = **208%**. On GPU (but not CPU) conv1.1 is i/o bound anyway, so no utilization improvement is actually possible.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150345130,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/150346334,150346334,MDEyOklzc3VlQ29tbWVudDE1MDM0NjMzNA==,1310570,2015-10-22T20:24:33Z,2015-10-22T20:24:33Z,OWNER,"@rsdubtso your suggested flags didn't make much difference -- IntelCaffe went from 3052 ms to 3000 ms
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150346334,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/150355221,150355221,MDEyOklzc3VlQ29tbWVudDE1MDM1NTIyMQ==,9326960,2015-10-22T21:02:07Z,2015-10-22T21:02:07Z,NONE,"@ozabluda:  Yes this is F(2x2,3x3).  This requires a batch of 16 gemms.  I'm able to fit this all in one block for K=32 and 4 overlapping coordinates of x,y each with with 8 units of minibatch.  So really it's 16 32x32 tiles.  The in block overlap is key as that's what gives you such a high L1 hit rate, otherwise the you'd be bandwidth bound on L2.  I use the standard 8 register gemm blocking so that means 64 FFMA's per outer product.  But instead of having 1 big loop of 8 outer products, I split it in two loops of 4 (or 256 FFMAs each).  1 loop (128 threads) does the image transform inline and the other (128 threads) the filter transform (256 threads total).  I can fit two blocks on an SM to cover bar.sync latencies.

Anyway, all the transform logic, pointer arithmetic, predicating and loop logic requires 138 clock consuming instructions interspersed with the FFMAs.  This drops performance about 138/512=27%.  This kernel is so dense with memory operations (even if they're mostly cache hits) that there's little opportunity for the boost clock to add much performance.  I even have a bit of instruction cache thrashing going on because the total loop size is slightly over the size of the instruction cache.

With more shared memory and/or registers I'd have a lot more headroom to increase the tile size a bit and reduce bandwidth (as well as transform overhead).  Perhaps Pascal will provide that.

Anyway, I'll have a much more detailed write up forthcoming (probably as an addition to Andrew's paper).
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150355221,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/150359639,150359639,MDEyOklzc3VlQ29tbWVudDE1MDM1OTYzOQ==,43829,2015-10-22T21:21:19Z,2015-10-22T21:21:19Z,NONE,"Awesome, looking forward to that!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150359639,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/150366837,150366837,MDEyOklzc3VlQ29tbWVudDE1MDM2NjgzNw==,14060629,2015-10-22T21:53:23Z,2015-10-22T21:53:23Z,CONTRIBUTOR,"@rsdubtso, 

> Hi, I'm one of the developers who worked on this package. [...] Looking at the logs, I see that the speedups for the convolution layers are not as high as we'd expect, but we never ran on a 6-core machine, so maybe our expectations are wrong. The CPU convolution layers often call tall and skinny SGEMMs which have limited scalability for a 2x18-core machine. But on a 6-core machine the gap between the SGEMM-based convolution and the approach we used may be much more narrow.

Thank you very much for your work. Intel's announcement was on on 2xE5-2699v3 (18 core): 

Forward pass: 1449 MFLOP \* 731images/1sec=1.059 TFLOP/s
Forw+Backw pass: 3 \* 1449 MFLOP \* 271 images/1sec=1.187 TFLOP/s

I estimate peak FLOP/s for E5-2699v3 like so:
18 cores \* 32 FLOP/cycle \* 1.9GHz (AVX base clock) =1.094 TFLOP/s

Performance numbers above look to me much more like ~100% utilization on 1 CPU with very poor scalability to 2 CPUs than 50% utilization on 1 CPU with excellent scalability to 2 CPU. It would be nice to have performance numbers for 1 CPU (especially more ""normal"" 16core E5-2698v3 and such).

FWIW, on my dual 8-core E5-2640v3 2.60 GHz, scalability of Caffe/OpenBLAS to the second CPU is almost zero. On 1 CPU, scalabity from 4 to 8 cores is so poor that my 4-core i5-4670K 3.50 GHz outperforms it by 1.5-2.2x in convolutional layers. I didn't try MKL yet.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150366837,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/150368295,150368295,MDEyOklzc3VlQ29tbWVudDE1MDM2ODI5NQ==,8460517,2015-10-22T22:00:53Z,2015-10-22T22:00:53Z,NONE,"@ozabluda F(2x2,3x3) has a maximum speedup of (2x2x3x3)/(4x4) = 2.25. In general the max speedup for F(mxn, rxs) is (m n r s) / ((m+r-1)(n+s-1)) 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150368295,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/150381461,150381461,MDEyOklzc3VlQ29tbWVudDE1MDM4MTQ2MQ==,14060629,2015-10-22T23:15:43Z,2015-10-22T23:15:43Z,CONTRIBUTOR,"@andravin 

> F(2x2,3x3) has a maximum speedup of (2x2x3x3)/(4x4) = 2.25

This is the number in the paper from below (9). But this is multiplications only. What about additions? Standard direct convolution also uses 6*4=24 additions, for the total 36+24=60 FLOP.

Both direct and Winograd convolutions also use 4 unamortized additions (1 reduction per output, asymptotically for K>>1), for the max possible utilization: (60+4)/(16+4)=3.2

It's quite possible that I got it wrong. Grateful for corrections.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150381461,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/150397244,150397244,MDEyOklzc3VlQ29tbWVudDE1MDM5NzI0NA==,8460517,2015-10-23T00:52:38Z,2015-10-23T00:52:38Z,NONE,"Because multiplication, addition, and multiply accumulate all have the same throughput, I count them all equally. That not only makes the analysis simpler, but gives you a more accurate accounting of how many arithmetic instructions you will need to execute in order to implement the algorithm.

In any case, if you wanted to count FLOPs instead of floating point instructions (FLIPs?), you would have to count the additions used in the reductions across channels, which make up for the additions that are missing from your accounting of Winograd convolution FLOPs.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150397244,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/150438670,150438670,MDEyOklzc3VlQ29tbWVudDE1MDQzODY3MA==,14060629,2015-10-23T03:06:18Z,2015-10-23T03:06:31Z,CONTRIBUTOR,"I see. It would totally make sense to introduce FLIP, AKA ""arithmetic complexity"" from your paper. Direct uses 36 FLIP and Winograd F(2x2,3,3) uses 16 FLIP. We can't use ""FLOP"", because existing terminology is too ingrained with 2 FLOP per 1 MAC/FMAC. For example, in this thread, this is how Titan-X has 6.144 TFLOP/s (3.072 TFLIP/s), Haswell can do 32 FLOP/clock (16 FLIP/clock), AlexNet(1-col) Forward pass w/direct convolutions has 1428 MFLOP (714 MFLIP) per image, etc.

FLIP (additions) may be slightly cheaper that FLOP, because they generate less heat, and CPU/GPU may clock higher, maybe for E5-2699v3 in the range 1.9GHz (AVX base clock) vs 2.6 GHz (AVX Max All Core Turbo Frequency) 2.6/1.9=1.4

@scott-gray's, sorry I couldn't quite follow, do you count this in FLOP or FLIP:

> 138 clock consuming instructions interspersed with the FFMAs. This drops performance about 138/512=27%

FLIP: 2.25 \* (1-138/512)=1.64 (i.e. you are already at the theoretical (practical) max)
FLOP: 3.2 \* (1-138/512)=2.34 (i.e ???)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150438670,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/150447583,150447583,MDEyOklzc3VlQ29tbWVudDE1MDQ0NzU4Mw==,9326960,2015-10-23T03:21:55Z,2015-10-23T03:21:55Z,NONE,"2.25(1-138/512)=1.64 was how I was calculating it.  Basically any instruction in the gemm loop that isn't dual issued dilutes the number of FFMA's that can be processed.  In this case there are a lot (138) but it turns out to not be such a bad thing as this kernel is right on the edge of being bandwidth limited.  I'm working on a scanning back and forth square wave block id remapping to see if that increases cache hits and drops power use a bit so the boost clock can kick in more.  I'll also do the fp16 version too.  I'm pretty sure that will have a lot more headroom.  Though this kernel has very few remaining instruction slots to insert the F2F.F32.F16s (dual issued) but I'm pretty sure I can squeak them in.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150447583,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/150586443,150586443,MDEyOklzc3VlQ29tbWVudDE1MDU4NjQ0Mw==,15225631,2015-10-23T14:16:46Z,2015-10-23T14:16:46Z,NONE,"@andravin,

>  Are other processors (eg i7) affected by AVX2 frequencies, if so where can
>  we find documentation of the AVX2 frequencies for those processors?

Probably the CPU support folks will have a better answer that I can find.

@soumith,

> your suggested flags didn't make much difference -- IntelCaffe went from 3052
> ms to 3000 ms

Thanks. That means that the OS already did a fine job scheduing the threads...

@ozabluda,

> Performance numbers above look to me much more like ~100% utilization on 1
> CPU with very poor scalability to 2 CPUs than 50% utilization on 1 CPU with
> excellent scalability to 2 CPU. It would be nice to have performance numbers
> for 1 CPU (especially more ""normal"" 16core E5-2698v3 and such)

Thanks, this is an interesting observation. If I interpret you correctly,
you're saying that we're running at 50% efficiency of the whole machine. My
anecdotal evidence from the times when we were tuning the benchmark, is that
the conv layers do scale pretty well with the number of threads, but I do not
have any numbers handy. I'll post new numbers from single socket runs next
week.

I'll also try to find out if we see the regression in fc layers in our setup.
This came quite as a surprise...
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150586443,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/150637326,150637326,MDEyOklzc3VlQ29tbWVudDE1MDYzNzMyNg==,14060629,2015-10-23T17:17:31Z,2015-10-23T17:27:16Z,CONTRIBUTOR,"@scott-gray, I think your 10 vTFLOP/s, **163%** utilization for F(2x2,3x3), K32xN8 is more impressive that you modestly describe. For N>>1, K>>1 max theoretical utilization is 36/16=**2.25**. But for N=8, you can't neglect Fliter transform (28 FLIP from the paper), amortized over N. For K=32, you can't neglect data transform (32 FLIP from the paper), amortized over K. 36/(16+28/8+32/32)=**1.76** [1]. Those 28+32=60 FLIP are part of your 138 ""apparent overhead"" instructions, but only 138-60=78 instructions is ""true overhead"", i.e. 78/512=15%. Or I am counting this incorrectly: I should be assuming that Filter transform could have been reused across tiles (when N>8), ditto with data transform (when K>32)?

[1] As @andravin describes in the paper, data transform over overlapped regions can be reused, and instead of 32, I counted min theoretical 24 additions (in his CPU implementation he has 80/3=26.7 additions), but it doesn't matter in this case: 36/(16+28/8+24/32)=**1.78**

> The tile size is K32xN8 so it should be pretty versatile over a wide range of dimensions. Even C=3 performance is pretty good at a 1.7 vTflops.

this must be a typo. For C=3, you can't neglect inverse, amortized over C, which is 24 additions (from the paper): 36/(16+28/8+32/32+24/3)=**1.26**. 1.26 \* 6.144 TF/s = **7.76 vTF/s**. You probably meant 6.7 vTflops.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150637326,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/150647429,150647429,MDEyOklzc3VlQ29tbWVudDE1MDY0NzQyOQ==,9326960,2015-10-23T17:57:55Z,2015-10-23T17:57:55Z,NONE,"Sharing transform code from overlapped regions is much harder in practice than it may seem.  Working around all the constraints means you need an additional pass through shared memory or perhaps warp shuffles.. which adds more overhead.  What I have right now with each thread computing one transform works pretty well.

As far as performance goes, the bigger your tile size the fewer repeated transforms you have to make, but there's only so much I can fit in the limited shared memory available.  I could do a non-fused kernel and only do the minimum number of transforms but that also adds a lot of overhead and I'm pretty sure it wont run as fast as a fused kernel that is computing extra transforms inline.  On the filter side there's also the benefit of only needing 9 loads instead of 16 if you do the transform inline.

But counting the non-ffma clock consuming instructions is all you need to figure out max performance.  And this bares out in my testing.

For C=3, that's just 1 pass through the gemm loop at 3/4 utilization.  But all the gemm setup + ouput code also has overhead that's hugely amplified by such a small time spent in gemm.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150647429,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/150837450,150837450,MDEyOklzc3VlQ29tbWVudDE1MDgzNzQ1MA==,14060629,2015-10-24T17:57:38Z,2015-10-24T17:58:45Z,CONTRIBUTOR,"@scott-gray on reddit:

> Once I have these kernels well tuned I'll move onto the much more complicated F(4x4,3x3) transform where as much as a 4x speedup may be possible (though on the GPU there's no avoiding the small transform overhead or the inefficiencies in the awkward 6x6 dimensions).

As paper says, for F(4x4,3x3), everything infinite, asymptotically, speedup would indeed be 144/36=4.

But for K32xN8 tiling block (can it be that large?), taking floating point instruction counts from the paper, theoretical max possible utilization for is:

For C>>3: 144/(36+156/32+72/8)=2.9
For C==3: 144/(36+156/32+72/8+90/3)=1.8
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150837450,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/150838794,150838794,MDEyOklzc3VlQ29tbWVudDE1MDgzODc5NA==,9326960,2015-10-24T18:12:30Z,2015-10-24T18:12:30Z,NONE,"I clarified the tiling I was using in a later post here.  It's actually 32x32, not 32x8.  The 32x8 is what is visible to the user, but 32x32 is how it actually works.  The outer product dims of the batched gemm are K and Y/4_X/4_N.  So I don't just have 8 points of N on the outer product, but 4 sets x,y coordinates of 8 points of N arranged in a 2x2 superblock.  With the 2 units of overlap in each direction, this hugely increases the utilization of the L1 cache and its what makes it possible for this kernel to have such dense global loads (16 loads in ~256 cycles is a lot).

I'm actually working on a 2x1 superblock for fp16 (2xy points of 16n) so as to eliminate the half empty 32 byte transaction size.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150838794,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/150842920,150842920,MDEyOklzc3VlQ29tbWVudDE1MDg0MjkyMA==,14060629,2015-10-24T19:36:39Z,2015-10-24T19:36:39Z,CONTRIBUTOR,"> I clarified the tiling I was using in a later post here. It's actually 32x32, not 32x8. The 32x8 is what is visible to the user, but 32x32 is how it actually works.

I think I kinda understood a little bit the main idea how you get high L1 utilization, removing L2 bottleneck, but I don't understand how this can help with max theoretical peak FLIP/s calculation I am making. You still can't amortize filter transform over ""effective"" N=32, only over real N=8. Or can you?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150842920,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/150843115,150843115,MDEyOklzc3VlQ29tbWVudDE1MDg0MzExNQ==,9326960,2015-10-24T19:40:22Z,2015-10-24T19:40:22Z,NONE,"x and y also factor into the number of image transforms you need, not just n.  So 32 is the unit you need to use when calculating redundant transforms.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150843115,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/150847883,150847883,MDEyOklzc3VlQ29tbWVudDE1MDg0Nzg4Mw==,14060629,2015-10-24T20:27:28Z,2015-10-25T20:08:58Z,CONTRIBUTOR,"Aha! I get it now. 

For F(4x4,3x3) correct formula for K32xN8, X2xY2(=4) is

For C>>3: 144/(36+156/32+72/4/8)=**3.3**
For C==3: 144/(36+156/32+72/4/8+90/4/3)=**2.8**

For the overlapped data transform, the correct number of FLIP is actually smaller than 156.

Last convolutional layer of VGG image dimention is 6x6, preventing X2xY2 superblock tiling. For that layer:

For C>>3: 144/(36+156/32+72/8)=**2.9** 

For F(2x2,3x3) correct formula for K32xN8, X2xY2(=4) is

For C>>3: 36/(16+32/32+28/4/8)=**2.01** (**1.63** actually achieved)
For C==3: 36/(16+32/32+28/4/8+24/4/3)=**1.81**

For the overlapped data transform, the correct number of FLIP is actually smaller than 32, but, since it's at least 24 (by my calculation), it doesn't matter for K=32:

For C>>3: 36/(16+24/32+28/4/8)=2.04
For C==3: 36/(16+24/32+28/4/8+24/4/3)=1.83
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150847883,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/150868383,150868383,MDEyOklzc3VlQ29tbWVudDE1MDg2ODM4Mw==,14060629,2015-10-24T23:10:19Z,2015-10-24T23:11:54Z,CONTRIBUTOR,"@scott-gray 

> The tile size is K32xN8 so it should be pretty versatile over a wide range of dimensions. Even C=3 performance is pretty good at a 1.7 vTflops.

initially, I though this was a typo (as Titan-X has 6.144 real Tflops). Now I think it may mean an awesome 1.7 utilization (C=3, theoretical max utilization is 1.8, see previous comment), although it's weird, because C=3 is i/o bound.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150868383,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/151397108,151397108,MDEyOklzc3VlQ29tbWVudDE1MTM5NzEwOA==,15225631,2015-10-27T06:58:40Z,2015-10-27T06:58:40Z,NONE,"@ozabluda,
Here's some data from a 2xE5-2697v3 machine (sorry, could did not have a desktop machine with a proper OS handy).

My colleague timed IntelCaffe on 14 and 28 cores (1 and 2 sockets). Affinity setup: `KMP_AFFINITY=granularity=fine,compact,1,0`. MKL version was 11.3.0.

There are quite a few cases when the ratio is less than 2 and even some cases where it is less than 1, but the most time-consuming layers have scaled pretty well. The total ratio is 1.83.

| layer | dir | 14 | 28 | Ratio 14/28 |
| --- | --- | --- | --- | --- |
| data | forward: | 79.75 | 69.87 | 1.14 |
| data | backward: | 0.00 | 0.00 | N/A |
| pack1 | forward: | 9.35 | 4.74 | 1.97 |
| pack1 | backward: | 0.00 | 0.00 | 2.00 |
| conv1 | forward: | 133.78 | 66.79 | 2.00 |
| conv1 | backward: | 101.10 | 56.35 | 1.79 |
| relu1 | forward: | 12.29 | 5.94 | 2.07 |
| relu1 | backward: | 17.14 | 8.42 | 2.04 |
| norm1 | forward: | 45.06 | 22.82 | 1.97 |
| norm1 | backward: | 67.51 | 32.20 | 2.10 |
| pool1 | forward: | 16.82 | 8.57 | 1.96 |
| pool1 | backward: | 27.53 | 13.77 | 2.00 |
| conv2 | forward: | 163.55 | 107.55 | 1.52 |
| conv2 | backward: | 416.93 | 208.84 | 2.00 |
| relu2 | forward: | 7.83 | 3.77 | 2.08 |
| relu2 | backward: | 10.92 | 5.34 | 2.05 |
| norm2 | forward: | 28.15 | 14.92 | 1.89 |
| norm2 | backward: | 43.41 | 20.60 | 2.11 |
| pool2 | forward: | 10.23 | 5.31 | 1.93 |
| pool2 | backward: | 17.34 | 8.76 | 1.98 |
| conv3 | forward: | 105.66 | 52.77 | 2.00 |
| conv3 | backward: | 228.13 | 114.76 | 1.99 |
| relu3 | forward: | 2.22 | 1.02 | 2.17 |
| relu3 | backward: | 3.63 | 1.82 | 1.99 |
| conv4 | forward: | 81.77 | 40.92 | 2.00 |
| conv4 | backward: | 176.46 | 88.66 | 1.99 |
| relu4 | forward: | 2.21 | 0.88 | 2.51 |
| relu4 | backward: | 3.71 | 1.81 | 2.04 |
| conv5 | forward: | 56.16 | 28.06 | 2.00 |
| conv5 | backward: | 120.98 | 60.74 | 1.99 |
| relu5 | forward: | 1.26 | 0.42 | 2.96 |
| relu5 | backward: | 2.79 | 0.82 | 3.40 |
| pool5 | forward: | 2.30 | 1.06 | 2.17 |
| pool5 | backward: | 3.82 | 0.81 | 4.73 |
| unpack6 | forward: | 0.35 | 0.17 | 2.02 |
| unpack6 | backward: | 0.24 | 0.19 | 1.30 |
| fc6 | forward: | 23.39 | 13.01 | 1.80 |
| fc6 | backward: | 41.84 | 22.64 | 1.85 |
| relu6 | forward: | 0.06 | 0.14 | 0.38 |
| relu6 | backward: | 0.12 | 0.05 | 2.58 |
| drop6 | forward: | 4.06 | 4.04 | 1.00 |
| drop6 | backward: | 0.12 | 0.07 | 1.68 |
| fc7 | forward: | 11.14 | 5.99 | 1.86 |
| fc7 | backward: | 18.95 | 10.08 | 1.88 |
| relu7 | forward: | 0.05 | 0.13 | 0.39 |
| relu7 | backward: | 0.09 | 0.05 | 1.90 |
| drop7 | forward: | 4.04 | 4.04 | 1.00 |
| drop7 | backward: | 0.16 | 0.09 | 1.69 |
| fc8 | forward: | 2.59 | 1.87 | 1.38 |
| fc8 | backward: | 5.26 | 3.10 | 1.70 |
| loss | forward: | 11.03 | 11.27 | 0.98 |
| loss | backward: | 0.18 | 0.22 | 0.79 |
| Min | Forward | 818.78 | 477.70 | 1.71 |
| Min | Backward | 1309.69 | 661.51 | 1.98 |
| Min | Forward-Backward: | 21607.00 | 11820.00 | 1.83 |
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-151397108,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/151525852,151525852,MDEyOklzc3VlQ29tbWVudDE1MTUyNTg1Mg==,1310570,2015-10-27T14:48:07Z,2015-10-27T14:48:07Z,OWNER,"@rsdubtso Taking the minimum timing of each layer rather than the average is a bit misleading and is not a standard in benchmarking. I think you should consider changing that, even though the overall difference might be minor.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-151525852,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/151590462,151590462,MDEyOklzc3VlQ29tbWVudDE1MTU5MDQ2Mg==,14060629,2015-10-27T18:06:06Z,2015-10-28T20:24:58Z,CONTRIBUTOR,"@rsdubtso, thank you, these are great. I see great scalability to 2 sockets, with ~50% utilization (either one or two sockets), exactly opposite of my earlier guesses. Next natural experiments would be to run it on 1,2,4,8  cores to see where utilization breaks down (are you using AVX2 MADD?)

E5-2697v3 has: 
14 cores \* 32FLOPs/cycle \* 2.2 GHz (AVX Core Freq) = 986 GFLOP/s
(AVX boost goes from 2.9-3.3 GHz, depending on the number of cores active)

Note that you ran 2-col AlexNet with minibatch=256, while @soumith ran 1-col AlexNet with minibatch=256

| IntelCaffe | mb=256 | E5-2697v3 |  |  |  |
| --- | --- | --- | --- | --- | --- |
|  |  | 14 core |  | 28 core |  |
|  | MFLOP | ms | Utilization | ms | Utilization |
| conv1 forward: | 211 | 133.78 | 41% | 66.79 | 41% |
| conv1 backward: | 422 | 101.1 | 108% | 56.35 | 97% |
| conv2 forward: | 448 | 163.55 | 71% | 107.55 | 54% |
| conv2 backward: | 896 | 416.93 | 56% | 208.84 | 56% |
| conv3 forward: | 299 | 105.66 | 73% | 52.77 | 74% |
| conv3 backward: | 598 | 228.13 | 68% | 114.76 | 68% |
| conv4 forward: | 224 | 81.77 | 71% | 40.92 | 71% |
| conv4 backward: | 448 | 176.46 | 66% | 88.66 | 66% |
| conv5 forward: | 150 | 56.16 | 69% | 28.06 | 69% |
| conv5 backward: | 300 | 120.98 | 64% | 60.74 | 64% |
| fc6 forward: | 75 | 23.39 | 84% | 13.01 | 75% |
| fc6 backward: | 75 | 41.84 | 47% | 22.64 | 43% |
| fc7 forward: | 34 | 11.14 | 79% | 5.99 | 74% |
| fc7 backward: | 34 | 18.95 | 47% | 10.08 | 44% |
| fc8 forward: | 8 | 2.59 | 80% | 1.87 | 56% |
| fc8 backward: | 8 | 5.26 | 39% | 3.1 | 34% |
| Conv+fc Forward | 1449 | 578.04 | 65% | 316.96 | 59% |
| Conv+fc Backward | 2781 | 1109.65 | 65% | 565.17 | 64% |
| Conv+fc Forward-Backward: | 4231 | 1687.69 | 51% | 882.13 | 46% |
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-151590462,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/151612573,151612573,MDEyOklzc3VlQ29tbWVudDE1MTYxMjU3Mw==,87486,2015-10-27T19:06:51Z,2015-10-27T19:06:51Z,NONE,"@rsdubtso 
The data transfer time seems very long and does not scale well. Could you offer  more details how is it designed? Thanks!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-151612573,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/151613386,151613386,MDEyOklzc3VlQ29tbWVudDE1MTYxMzM4Ng==,87486,2015-10-27T19:09:48Z,2015-10-27T19:09:48Z,NONE,"@rsdubtso Also, the relu forward seems much slower on two sockets. Why is that? Drop6 and Drop7 seems to still use one socket even when you have two socket? the scaling ratio is 1.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-151613386,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/151735250,151735250,MDEyOklzc3VlQ29tbWVudDE1MTczNTI1MA==,15225631,2015-10-28T06:11:08Z,2015-10-28T06:11:08Z,NONE,"@andravin 

> > Are other processors (eg i7) affected by AVX2 frequencies, if so where can  we find documentation of the AVX2 frequencies for those processors?
> 
> Probably the CPU support folks will have a better answer that I can find.

I asked around, and here's what I was told: AVX frequency is not SW visible. But even desktop processors have a fused 'AVX' frequency that they throttle down to when executing heavy instructions. I could not find the frequency fused for the i7 CPU mentioned above, but you can find it out using prime95 v27.9 or later, for example. However, current-related throttling may occur earlier than you hit TDP budget limit related to heavy instructions.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-151735250,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/151750011,151750011,MDEyOklzc3VlQ29tbWVudDE1MTc1MDAxMQ==,15356190,2015-10-28T07:18:30Z,2015-10-28T07:18:30Z,NONE,"Hi all,
I worked with @rsdubtso on the package too.

@soumith, you are right, we should've pointed we report timings for the fastest iteration. Though, if you use the same package for comparing 'intel_alexnet' and 'bvlc_alexnet', the comparison will be quite representative.

@gujunli relu1-5 scale well, relu6-7 seem to be too small for scaling across sockets. drop6 and drop7 use rng (not parallelized), which most likely takes most time. We didn't optimize drop layer, except for adding parallelization on the loop.

@ozabluda, @gujunli, I rerun the package on the same machine @rsdubtso did. The only change here is that I put database on /tmp (local hard drive). @rsdubtso reported timings when the DB was on Lustre FS (distributed cluster filesystem). That was the reason, why the timings were pure for data layer.
We didn't change data layer much, only added simple parallelization on preparation of image-minibatch.

Iterations: 10

| layer | direction | omp | omp | omp | omp | omp | cmp | cmp | cmp | cmp |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  |  | 28 | 14 | 8 | 4 | 2 | 28.vs.14 | 14.vs.8 | 8.vs.4 | 4.vs.2 |
| -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- |
| data | forward: | 18.51 | 23.93 | 24.57 | 24.94 | 29.82 | 0.64 | 0.58 | 0.50 | 0.59 |
| data | backward: | 0 | 0 | 0 | 0.00 | 0 | N/A | N/A | N/A | 0 |
| pack1 | forward: | 4.70 | 9.41 | 10.11 | 14.26 | 25.03 | 1.00 | 0.61 | 0.70 | 0.87 |
| pack1 | backward: | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.5 | 0.57 | 0.5 | 0.5 |
| conv1 | forward: | 66.96 | 133.95 | 212.41 | 345.24 | 612.40 | 1.00 | 0.90 | 0.81 | 0.88 |
| conv1 | backward: | 56.47 | 101.13 | 170.32 | 328.05 | 650.71 | 0.89 | 0.96 | 0.96 | 0.99 |
| relu1 | forward: | 5.96 | 12.46 | 12.29 | 12.86 | 19.19 | 1.04 | 0.56 | 0.52 | 0.74 |
| relu1 | backward: | 8.4 | 17.15 | 17.41 | 20.12 | 32.11 | 1.02 | 0.58 | 0.57 | 0.79 |
| norm1 | forward: | 22.99 | 44.78 | 64.66 | 126.46 | 251.12 | 0.97 | 0.82 | 0.97 | 0.99 |
| norm1 | backward: | 31.86 | 63.13 | 67.77 | 95.95 | 169.04 | 0.99 | 0.61 | 0.70 | 0.88 |
| pool1 | forward: | 8.44 | 16.41 | 27.14 | 54.10 | 106.53 | 0.97 | 0.94 | 0.99 | 0.98 |
| pool1 | backward: | 13.90 | 27.66 | 27.97 | 34.21 | 54.56 | 0.99 | 0.57 | 0.61 | 0.79 |
| conv2 | forward: | 105.79 | 164.55 | 282.32 | 561.36 | 1120.98 | 0.77 | 0.98 | 0.99 | 0.99 |
| conv2 | backward: | 208.96 | 416.54 | 712.46 | 1415.67 | 2826.54 | 0.99 | 0.97 | 0.99 | 0.99 |
| relu2 | forward: | 3.79 | 7.81 | 7.76 | 8.32 | 12.37 | 1.02 | 0.56 | 0.53 | 0.74 |
| relu2 | backward: | 5.35 | 10.90 | 11.19 | 12.94 | 20.60 | 1.01 | 0.58 | 0.57 | 0.79 |
| norm2 | forward: | 14.71 | 28.47 | 41.42 | 81.56 | 162.21 | 0.96 | 0.83 | 0.98 | 0.99 |
| norm2 | backward: | 20.93 | 40.58 | 43.59 | 60.95 | 108.32 | 0.96 | 0.61 | 0.69 | 0.88 |
| pool2 | forward: | 5.25 | 10.19 | 16.94 | 33.66 | 66.53 | 0.97 | 0.95 | 0.99 | 0.98 |
| pool2 | backward: | 8.76 | 17.97 | 17.89 | 21.02 | 33.87 | 1.02 | 0.56 | 0.58 | 0.80 |
| conv3 | forward: | 52.78 | 105.76 | 182.96 | 363.68 | 725.27 | 1.00 | 0.98 | 0.99 | 0.99 |
| conv3 | backward: | 115.63 | 228.32 | 396.63 | 784.72 | 1562.49 | 0.98 | 0.99 | 0.98 | 0.99 |
| relu3 | forward: | 1.04 | 2.27 | 2.26 | 2.73 | 4.27 | 1.09 | 0.56 | 0.60 | 0.78 |
| relu3 | backward: | 1.88 | 3.62 | 3.95 | 4.55 | 7.17 | 0.96 | 0.62 | 0.57 | 0.78 |
| conv4 | forward: | 40.89 | 81.86 | 139.66 | 275.17 | 547.34 | 1.00 | 0.97 | 0.98 | 0.99 |
| conv4 | backward: | 88.91 | 176.64 | 301.33 | 595.64 | 1183.89 | 0.99 | 0.97 | 0.98 | 0.99 |
| relu4 | forward: | 0.89 | 2.21 | 2.24 | 2.68 | 4.55 | 1.23 | 0.57 | 0.59 | 0.84 |
| relu4 | backward: | 1.82 | 3.76 | 3.91 | 4.60 | 7.18 | 1.02 | 0.59 | 0.58 | 0.77 |
| conv5 | forward: | 28.07 | 56.34 | 94.83 | 185.45 | 368.19 | 1.00 | 0.96 | 0.97 | 0.99 |
| conv5 | backward: | 60.71 | 120.89 | 204.81 | 401.62 | 797.17 | 0.99 | 0.96 | 0.98 | 0.99 |
| relu5 | forward: | 0.42 | 1.20 | 1.33 | 1.65 | 3.00 | 1.43 | 0.63 | 0.61 | 0.90 |
| relu5 | backward: | 0.81 | 2.78 | 2.80 | 3.11 | 4.79 | 1.71 | 0.57 | 0.55 | 0.76 |
| pool5 | forward: | 1.06 | 2.25 | 3.72 | 7.35 | 14.68 | 1.06 | 0.94 | 0.98 | 0.99 |
| pool5 | backward: | 0.81 | 3.85 | 3.79 | 4.57 | 7.23 | 2.37 | 0.56 | 0.60 | 0.79 |
| unpack6 | forward: | 0.16 | 0.37 | 0.48 | 0.81 | 1.59 | 1.17 | 0.73 | 0.83 | 0.98 |
| unpack6 | backward: | 0.18 | 0.24 | 0.38 | 0.74 | 1.46 | 0.67 | 0.88 | 0.97 | 0.98 |
| fc6 | forward: | 13.01 | 23.54 | 36.53 | 72.21 | 137.74 | 0.90 | 0.88 | 0.98 | 0.95 |
| fc6 | backward: | 22.70 | 41.92 | 67.19 | 133.06 | 264.57 | 0.92 | 0.91 | 0.99 | 0.99 |
| relu6 | forward: | 0.14 | 0.06 | 0.03 | 0.06 | 0.13 | 0.21 | 0.33 | 0.94 | 0.96 |
| relu6 | backward: | 0.04 | 0.11 | 0.09 | 0.19 | 0.36 | 1.25 | 0.46 | 1.06 | 0.94 |
| drop6 | forward: | 4.01 | 4.09 | 4.11 | 4.28 | 4.61 | 0.50 | 0.57 | 0.52 | 0.53 |
| drop6 | backward: | 0.07 | 0.12 | 0.18 | 0.37 | 0.72 | 0.86 | 0.85 | 1.00 | 0.95 |
| fc7 | forward: | 6.05 | 11.21 | 16.81 | 33.46 | 65.28 | 0.92 | 0.85 | 0.99 | 0.97 |
| fc7 | backward: | 10.07 | 18.99 | 30.49 | 60.42 | 118.79 | 0.94 | 0.91 | 0.99 | 0.98 |
| relu7 | forward: | 0.12 | 0.05 | 0.03 | 0.06 | 0.13 | 0.20 | 0.39 | 0.94 | 0.96 |
| relu7 | backward: | 0.05 | 0.09 | 0.12 | 0.17 | 0.32 | 0.88 | 0.72 | 0.71 | 0.94 |
| drop7 | forward: | 4.01 | 4.08 | 4.08 | 4.28 | 4.60 | 0.50 | 0.57 | 0.52 | 0.53 |
| drop7 | backward: | 0.08 | 0.17 | 0.20 | 0.38 | 0.73 | 1.00 | 0.67 | 0.91 | 0.96 |
| fc8 | forward: | 1.85 | 2.59 | 4.47 | 8.55 | 15.92 | 0.69 | 0.98 | 0.95 | 0.93 |
| fc8 | backward: | 3.09 | 5.28 | 7.53 | 14.66 | 28.83 | 0.85 | 0.81 | 0.97 | 0.98 |
| loss | forward: | 11.19 | 11.00 | 10.82 | 10.91 | 10.73 | 0.49 | 0.56 | 0.50 | 0.49 |
| loss | backward: | 0.22 | 0.17 | 0.17 | 0.17 | 0.20 | 0.39 | 0.56 | 0.50 | 0.57 |
| all | Forward | 423.95 | 761.99 | 1206.21 | 2237.89 | 4318.62 | 0.89 | 0.90 | 0.92 | 0.96 |
| all | Backward | 662.77 | 1303.28 | 2093.61 | 4000.3 | 7884.59 | 0.98 | 0.91 | 0.95 | 0.98 |
| all | Fwd-Bwd | 11080 | 20916 | 33242 | 62807 | 122290 | 0.94 | 0.91 | 0.94 | 0.97 |

small comment on cmp columns:
reported formula for X.vs.Y is: (time_Y/time_X)*(Y/X) -- i.e. parallelization efficiency
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-151750011,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/151933261,151933261,MDEyOklzc3VlQ29tbWVudDE1MTkzMzI2MQ==,14060629,2015-10-28T17:59:12Z,2015-10-28T17:59:12Z,CONTRIBUTOR,"@rsdubtso:

> I could not find the frequency fused for the i7 CPU mentioned above, but you can find it out using prime95 v27.9 or later, for example. However, current-related throttling may occur earlier than you hit TDP budget limit related to heavy instructions.

Thank you for checking. Even though Intel's documentation does lists current and power limits, I think all(?) Intel CPUs are in practice limited only by TDP. For example, overclocked Intel CPUs are known to suck 400W on Prime95 likely with long-term damage, and Intel CPUs don't prevent it, if cooled: 

From official Asus overclocking guide: 
â€œâ€â€ 
In our testing to date, the average overclocked frequency for 5960X processors is 4.5GHz. Very good processors will achieve 4.6GHz fully stable with less than 1.30Vcore. [â€¦] Users should avoid running Prime95 small FFTs on 5960X CPUs when overclocked. Over 4.4GHz, the Prime software pulls 400W of power through the CPU. 
 â€œâ€â€ 
http://rog.asus.com/365052014/overclocking/rog-overclocking-guide-core-for-5960x-5930k-5820k/
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-151933261,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/151979013,151979013,MDEyOklzc3VlQ29tbWVudDE1MTk3OTAxMw==,14060629,2015-10-28T20:23:20Z,2015-10-28T20:26:50Z,CONTRIBUTOR,"@emfomenk, thanks for the excellent table. Looking only at conv and fc layers, I see excellent scalability 2=>4=>8=>14=>28 in conv layers (except 2=>4=>8=14 in conv1 forward and 14=>28 in conv1 forward,conv2 backward), and some degradation in scalbiltiy 8=>14 and 14=>28 in fc layers. Updating my utilization table for 2 cores, we see that utilization improved conv+fc forward 65%=>73% (maybe some of it is due to AVX clock boost?), while conv+fc backward didn't improve much (65%=>68%). We can see that utilization does/doesn't improve for 2 cores. Now, the only thing missing is 1 core :-)

E5-2697v3 with 2 cores has: 
2 cores \* 32FLOPs/cycle \* 2.2 GHz (AVX Core Freq) = 141 GFLOP/s
(AVX boost goes from 2.9-3.3 GHz, depending on the number of cores active)

| IntelCaffe | mb=256 | E5-2697v3 |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- |
|  |  | 14 core |  | 28 core |  | 2 core |  |
|  | MFLOP | ms | Util | ms | Util | ms | Util |
| conv1 forward: | 211 | 133.78 | 41% | 66.79 | 41% | 612.4 | 63% |
| conv1 backward: | 422 | 101.1 | 108% | 56.35 | 97% | 650.71 | 118% |
| conv2 forward: | 448 | 163.55 | 71% | 107.55 | 54% | 1120.98 | 73% |
| conv2 backward: | 896 | 416.93 | 56% | 208.84 | 56% | 2826.54 | 58% |
| conv3 forward: | 299 | 105.66 | 73% | 52.77 | 74% | 725.27 | 75% |
| conv3 backward: | 598 | 228.13 | 68% | 114.76 | 68% | 1562.49 | 69% |
| conv4 forward: | 224 | 81.77 | 71% | 40.92 | 71% | 547.34 | 74% |
| conv4 backward: | 448 | 176.46 | 66% | 88.66 | 66% | 1183.89 | 69% |
| conv5 forward: | 150 | 56.16 | 69% | 28.06 | 69% | 368.19 | 74% |
| conv5 backward: | 300 | 120.98 | 64% | 60.74 | 64% | 797.17 | 68% |
| fc6 forward: | 75 | 23.39 | 84% | 13.01 | 75% | 137.74 | 100% |
| fc6 backward: | 75 | 41.84 | 47% | 22.64 | 43% | 264.57 | 52% |
| fc7 forward: | 34 | 11.14 | 79% | 5.99 | 74% | 65.28 | 95% |
| fc7 backward: | 34 | 18.95 | 47% | 10.08 | 44% | 118.79 | 52% |
| fc8 forward: | 8 | 2.59 | 80% | 1.87 | 56% | 15.92 | 91% |
| fc8 backward: | 8 | 5.26 | 39% | 3.1 | 34% | 28.83 | 50% |
| Conv+fc Forward | 1449 | 578.04 | 65% | 316.96 | 59% | 3593.12 | 73% |
| Conv+fc Backward | 2781 | 1109.65 | 65% | 565.17 | 64% | 7432.99 | 68% |
| Conv+fc F/B: | 4231 | 1687.69 | 51% | 882.13 | 46% | 11026.11 | 63% |
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-151979013,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/153044412,153044412,MDEyOklzc3VlQ29tbWVudDE1MzA0NDQxMg==,15356190,2015-11-02T15:04:18Z,2015-11-02T15:04:18Z,NONE,"Just quick update. Recently we released technical preview of Multinode Caffe. The link: https://software.intel.com/en-us/articles/caffe-training-on-multi-node-distributed-memory-systems-based-on-intel-xeon-processor-e5

The results are shown for Alexnet. We use data parallelism (for the first half of the net: from data till pool5) as well as model parallelism (for the second half: from fc6 till the end). The behavior of Multinode Caffe almost duplicates the behavior of Singlenode Caffe. This puts some limitations on scalability. Though we were able to achieve 12.3x, 19.2x and 29.4x speed-up on 16, 32 and 64 nodes respectively.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-153044412,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/153451903,153451903,MDEyOklzc3VlQ29tbWVudDE1MzQ1MTkwMw==,14060629,2015-11-03T18:50:16Z,2015-11-03T18:50:16Z,CONTRIBUTOR,"@emfomenk, thank you for the summary. Sorry, I don't understand what you mean by

> The behavior of Multinode Caffe almost duplicates the behavior of Singlenode Caffe. This puts some limitations on scalability. 

I also don't understand from the article what the effective minibatch is for, say, 64 nodes. Is is still 256 i.e. 4 per node? For multinode syncronous SGD, it's probably best to switch to the 1-col AlexNet from the ""One weird Trick..."" paper and follow the paper. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-153451903,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/153759102,153759102,MDEyOklzc3VlQ29tbWVudDE1Mzc1OTEwMg==,15356190,2015-11-04T15:17:53Z,2015-11-04T15:17:53Z,NONE,"@ozabluda,
I mean the algorithm in Multinode Caffe (underlying math) is the same as in  Singlenode Caffe: Forward, Backward, SGD, the same parameters and so on. In particular it means that there is no much possibilities to parallelize the work.

The only difference in Multinode version (from math point of view) is slightly modified SGD solver, which allows to apply diff right after backward step for current layer (this was made to be able to benefit from MPI parallelization in current approach). It looks like this modification doesn't affect convergence -- at least we were able to train Alexnet in the same amount of iterations as in Singlenode case.

Regarding minibatch: for 16 nodes minibatch=256 was used, for 32 nodes minibatch=512, and for 64 nodes minibatch=1024. It means that each node (in 16 nodes case) took 256/16=16 images in its ""local"" minibatch.

Yes, you are right that there are much better ways to implement multinode training (though, the math would be slightly different...), but the original idea was just to show that it possible to implement good parallelization even for this particular model.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-153759102,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/153854552,153854552,MDEyOklzc3VlQ29tbWVudDE1Mzg1NDU1Mg==,14060629,2015-11-04T20:34:39Z,2015-11-04T20:34:39Z,CONTRIBUTOR,"> I mean the algorithm in Multinode Caffe (underlying math) is the same as in Singlenode Caffe: Forward, Backward, SGD, the same parameters and so on. In particular it means that there is no much possibilities to parallelize the work.

I see. Does it mean it is approximately the same as single-node multi-GPU Caffe? What about parameter update step? Is it centralized, or also distributed, just single-node multi-GPU Caffe?

Article says """"""reached 80% top-5 accuracy in just over 5 hours on a 64-node"""""". Is that 90 epochs with minibatch=1024? AlexNet from the original paper reached 81.8% after 90 epochs with minbatch=128.

P.S. Graph incorrectly says ""E5-2697 v3 18 cores""
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-153854552,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/154125350,154125350,MDEyOklzc3VlQ29tbWVudDE1NDEyNTM1MA==,14060629,2015-11-05T17:11:03Z,2015-11-05T17:11:03Z,CONTRIBUTOR,"> Article says """"""reached 80% top-5 accuracy in just over 5 hours on a 64-node"""""". Is that 90 epochs with minibatch=1024? AlexNet from the original paper reached 81.8% after 90 epochs with minbatch=128.

Correction: 81.8% top-5 from the paper was with averaging predictions of 5 crops plus their horizontal reflections. Standard Caffe ""test"" does 1 random crop with no reflections, for which 80.2-80.4% top-5 is reached in 60-70 epochs, depending. How many epochs was it with minibatch=1024?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-154125350,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/154136120,154136120,MDEyOklzc3VlQ29tbWVudDE1NDEzNjEyMA==,15356190,2015-11-05T17:50:12Z,2015-11-05T17:50:12Z,NONE,"@ozabluda,
sorry, but i am not familiar with multi-GPU Caffe. I need to look at the codes.

In Multinode Caffe for the first half of the net the parameter updates are centralized (since parallelization happens on minibatch, all convolutions parameters are the same for all nodes). For the second half updates are distributed, since fully-connected layers' weights are distributed across the nodes.

Just to be aligned: one epoch == one full database turn around.
We always ran Caffe (singlenode and multinode versions) for 90 epochs (this number was just fixed).  We saw that accuracy didn't improved much since ~40-50 epoch, but I didn't save intermediate snapshots and can't say for sure the accuracy after 60 or 70 epochs right now. If you want I can rerun the training and report the top-5 accuracy for these epoch numbers.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-154136120,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/154194068,154194068,MDEyOklzc3VlQ29tbWVudDE1NDE5NDA2OA==,14060629,2015-11-05T21:11:15Z,2015-11-05T21:11:42Z,CONTRIBUTOR,"> We always ran Caffe (singlenode and multinode versions) for 90 epochs

Great. I think the web article should say that explicitly, especially since it is actually faster than what could be guessed from  """"""reached 80% top-5 accuracy"""""", which can mean as little as 40, as you noticed:

> We saw that accuracy didn't improved much since ~40-50 epoch [...] If you want I can rerun the training and report the top-5 accuracy for these epoch numbers.

Thank you for the offer, knowing that it's 90 epochs is good enough for me. 
 ===========
Off-topic part:

I am actually more interested in the number more precise than 80% (precision like 80.xx% would be better) for minibatch=1024 [1], single model, single crop, top-5 and top-1 (Caffe can do both simultaneously). I am also interested your ultimate accuracy for minibatch=256,512 as well. As you noticed, with the growing number of nodes you have to increase minibatch size, which negatively affects accuracy.

[1] BTW, did you increase learning rate 4x, compared to minibatch=256? If yes, how did that affect accuracy? How about increasing learning rate sqtr(4)=2x?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-154194068,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/155250891,155250891,MDEyOklzc3VlQ29tbWVudDE1NTI1MDg5MQ==,14060629,2015-11-10T01:09:51Z,2015-11-10T01:09:51Z,CONTRIBUTOR,"This somewhat explains how Intel's Multi-node Caffe works
https://github.com/BVLC/caffe/pull/3252
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-155250891,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/157453249,157453249,MDEyOklzc3VlQ29tbWVudDE1NzQ1MzI0OQ==,15356190,2015-11-17T17:58:04Z,2015-11-17T17:58:04Z,NONE,"Please take a look at https://communities.intel.com/community/itpeernetwork/datastack/blog/2015/11/12/myth-busted-general-purpose-cpus-can-t-tackle-deep-neural-network-training-part-2 for more information on technical details of Intel Multinode Caffe tech-preview, which actually uses one weird trick... :)
There is more on technical side. Unfortunately we didn't play with learning rate and it was always the same (the default one from bvlc_alextnet/sover.prototxt).
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-157453249,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/158099758,158099758,MDEyOklzc3VlQ29tbWVudDE1ODA5OTc1OA==,4803565,2015-11-19T15:58:31Z,2015-11-19T15:58:31Z,NONE,"Speaking about accuracy, this could be used as baseline:
https://github.com/BVLC/caffe/wiki/Models-accuracy-on-ImageNet-2012-val 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-158099758,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/158620215,158620215,MDEyOklzc3VlQ29tbWVudDE1ODYyMDIxNQ==,1710528,2015-11-21T10:06:13Z,2015-11-21T10:11:27Z,NONE,"There is now an official Intel Opencl PR at https://github.com/BVLC/caffe/pull/3355. /cc @gongzg
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-158620215,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/231192129,231192129,MDEyOklzc3VlQ29tbWVudDIzMTE5MjEyOQ==,14060629,2016-07-07T20:08:20Z,2016-07-07T20:08:20Z,CONTRIBUTOR,"> @scott-gray>Yes this is F(2x2,3x3). [...] I'm able to fit this all in one block for K=32 and 4 overlapping coordinates of x,y each with with 8 units of minibatch. [...] The in block overlap is key as that's what gives you such a high L1 hit rate, otherwise the you'd be bandwidth bound on L2. 

With F(2x2,3x3), (super)block 2x2 we have tile size of 6x6. In two other dimensions the tile size is K32xN8. Outer loop is over input channels (C). With 4-byte fp32 each 6x6 (super)block (=tile) we have: 

Filters: 32_3_3_4=1152 bytes
Input: 6_6_8_4=1152 bytes
Output: 4_4_32_8_4=16384 bytes

> 1 loop (128 threads) does the image transform inline and the other (128 threads) the filter transform (256 threads total). I can fit two blocks on an SM to cover bar.sync latencies.

Do I understand correctly that Filters and Input go to L1 (24 KB per SM) and output is accumulated in the registers (64k 32-bit registers per SM)? Do you use Shared Memory (96 KB per SM) at all? What limits it to two blocks on an SM?

> I clarified the tiling I was using in a later post here. It's actually 32x32, not 32x8. The 32x8 is what is visible to the user, but 32x32 is how it actually works. The outer product dims of the batched gemm are K and Y/4X/4N. So I don't just have 8 points of N on the outer product, but 4 sets x,y coordinates of 8 points of N arranged in a 2x2 superblock. With the 2 units of overlap in each direction, this hugely increases the utilization of the L1 cache and its what makes it possible for this kernel to have such dense global loads (16 loads in ~256 cycles is a lot).

Do I understand correctly that the 4 thread blocks (256 threads each) that work on the same 2x2 superblock, really know nothing about each other, solely relying on L1 for transparent data reuse?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-231192129,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,111522611,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/231200374,231200374,MDEyOklzc3VlQ29tbWVudDIzMTIwMDM3NA==,9326960,2016-07-07T20:39:44Z,2016-07-07T20:39:44Z,NONE,"You can find the latest code for F(2x2,3x3) here:

https://github.com/NervanaSystems/neon/blob/master/neon/backends/kernels/sass/xconv_winograd_2x2_3x3_32x32.sass

This kernel uses 256 threads, 128 registers and 32kb shared memory.  This means the threads and registers are limiting the occupancy to 2 blocks per SM and 4 warps per scheduler.

The shared memory is mainly used for storing the computed transforms and facilitating the batched gemm.  The gemm tile is 32x32 and we have 16 of them in the same block.  This means we only have enough shared memory to store 4 outer product lines at a time, double buffered.  So the gemm loops are unrolled 4 times.  We use 2 separate loops to compute the image and filter transforms inline. 

When super blocking is in effect, you can get a lot of L1 cache hits, reducing the bandwidth from L2.

This implementation is currently significantly more efficient than the one found in cuDNN 5.0 and up.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59/comments,https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-231200374,https://api.github.com/repos/soumith/convnet-benchmarks/issues/59
soumith,convnet-benchmarks,108020480,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/142750803,142750803,MDEyOklzc3VlQ29tbWVudDE0Mjc1MDgwMw==,1310570,2015-09-23T22:40:21Z,2015-09-23T22:40:21Z,OWNER,"@ozabluda those timings look like they are of cudnn R2, or cudnn R3 with R2 bindings.

To make sure everything is kosher, can you do the following and report the output here:

```
th -lcudnn -e ""print(cudnn.version)""
```

it should be greater than 3000 (which is the R3 version of cudnn).

Also, make sure you are using the latest master branch of cudnn.torch: 

```
git clone https://github.com/soumith/cudnn.torch
cd cudnn.torch
luarocks make
```

I checked that the numbers are in the same range of things on:
- NVIDIA Digits box (4x Titan X)
- NVIDIA M6000
- Another box of mine which has 1x Titan-X

If you still cant reproduce, I can give ssh access to one of my boxes which I reproduced the numbers.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/57/comments,https://github.com/soumith/convnet-benchmarks/issues/57#issuecomment-142750803,https://api.github.com/repos/soumith/convnet-benchmarks/issues/57
soumith,convnet-benchmarks,108020480,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/142760046,142760046,MDEyOklzc3VlQ29tbWVudDE0Mjc2MDA0Ng==,14060629,2015-09-23T23:35:19Z,2015-09-23T23:35:19Z,CONTRIBUTOR,"user@dnn1:~/convnet-benchmarks/torch7/imagenet_winners$ th -lcudnn -e ""print(cudnn.version)""
3007

after updating to the latest master branch of cudnn.torch, as described above, my timings are actually 10-25% faster than yours (all that overclocking paid off :-). Thank you. I will report on Caffe/cudnn-v3 in another post.

user@dnn1:~/convnet-benchmarks/torch7/imagenet_winners$ th benchmark.lua 
Running on device: GeForce GTX TITAN X  
ModelType: AlexNet      Kernels: cudnn  Input shape: 128x3x224x224
cudnn                                   :updateOutput():      29.65
cudnn                                :updateGradInput():      28.08
cudnn                              :accGradParameters():      29.85
cudnn                                          :Forward:      29.65
cudnn                                         :Backward:      57.93
cudnn                                            :TOTAL:      87.58
ModelType: OverFeat[fast]       Kernels: cudnn  Input shape: 128x3x231x231
cudnn                                   :updateOutput():     103.65
cudnn                                :updateGradInput():      91.89
cudnn                              :accGradParameters():     101.63
cudnn                                          :Forward:     103.65
cudnn                                         :Backward:     193.52
cudnn                                            :TOTAL:     297.17
ModelType: VGG Model-A  Kernels: cudnn  Input shape: 64x3x224x224
cudnn                                   :updateOutput():     180.06
cudnn                                :updateGradInput():     192.06
cudnn                              :accGradParameters():     184.81
cudnn                                          :Forward:     180.06
cudnn                                         :Backward:     376.87
cudnn                                            :TOTAL:     556.94
ModelType: GoogleNet    Kernels: cudnn  Input shape: 128x3x224x224
cudnn                                   :updateOutput():      96.69
cudnn                                :updateGradInput():     156.27
cudnn                              :accGradParameters():      85.10
cudnn                                          :Forward:      96.69
cudnn                                         :Backward:     241.37
cudnn                                            :TOTAL:     338.06
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/57/comments,https://github.com/soumith/convnet-benchmarks/issues/57#issuecomment-142760046,https://api.github.com/repos/soumith/convnet-benchmarks/issues/57
soumith,convnet-benchmarks,103818583,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135924565,135924565,MDEyOklzc3VlQ29tbWVudDEzNTkyNDU2NQ==,9326960,2015-08-29T01:15:10Z,2015-08-29T01:15:10Z,NONE,"Reposting this here:

Looks like cuDNN is catching up. Those VGG numbers are really good for an FFT implementation. Perhaps with a bit more optimization they can overtake spatial domain on 3x3 filters? I wouldn't be surprised if we see much better fp16 numbers from them soon.

My GoogLeNet numbers may look good but I still have a lot of optimizations to make for the smaller feature map values in there. Right now I'm optimized for multiples of 64. I'll get that down to 32 this weekend. My CHWN tensor layout is also really helpful on those inception groupings.

A brand new version of neon is about to be released. You'll be able to run all these networks out of the box (plus lots more). The new syntax is much improved and more torch or keras like (perhaps better even).

Anyway, here's a changelog of updates since the last version:

No more multiplying by zero to implement padding in fprop and bprop (I now slice both the input and the filter)

Figured out a different way to do integer division for the now dynamically sized slice lookup table.

No more atomic adds in bprop. I've cast bprop as fprop upside down and the kernels are nearly identical. It requires a dimshuffle on the filter but this just takes microseconds and a small amount of additional memory that can be shared with all conv ops. Bprop used to be bandwidth bound on those atomic adds.

Tweaked the p,q block ordering to improve L2 cache performance. I'm using a zigzag pattern now for all operations.

Update already had a mode where you could stack all the gemm ops to eliminate atomic adds, but I've streamlined that stacking operation. Update also now fetches 32 rows deep. This comes at the cost of an instruction cache miss inside the main gemm loop, but is easily covered by the occupancy. The reason for doing this is the same for using a 32x33 shared memory block to implement transpose. With N contiguous the update op has the expensive strided access patterns on both the input and delta.

I also eliminate all shared memory bank conflicts when storing the global loads to shared with some clever shifting.

Added a beta param to bprop to allow delta accumulation for inception groupings.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56/comments,https://github.com/soumith/convnet-benchmarks/issues/56#issuecomment-135924565,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56
soumith,convnet-benchmarks,103818583,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135925187,135925187,MDEyOklzc3VlQ29tbWVudDEzNTkyNTE4Nw==,1310570,2015-08-29T01:29:08Z,2015-08-29T01:29:08Z,OWNER,"The CuDNN guys said that they have more slated optimizations as well, that will help GoogleNet, will updated the numbers after the final release happens.

I'm excited to see the new Neon.

In the OpenCL land, there's lots of catching up to do in terms of perf, but I am really happy that the libraries are getting feature-complete. Thanks to Hugh Perkins, Fabian Tschopp and other OpenCL torch-bearers (no pun intended). 

Another interesting data point comes this week from Intel, who claim to use 64 Xeon Phi nodes to train Overfeat in 3-4 hours. That comes into the distributed territory, and it does not make me super-duper impressed, seeing internal distributed systems based on GPUs that are equally good or better. Distributed training is a separate optimization from optimizing single nodes.
Intel slides at the HotChips Conference here [behind conference paywall](http://www.hotchips.org/wp-content/uploads/hc_archives/hc27/HC27.25-Tuesday-Epub/HC27.25.40-FPGAs-Epub/HC27.25.431-DeepLearning-Ovicharov-MSResearch-V7.pdf)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56/comments,https://github.com/soumith/convnet-benchmarks/issues/56#issuecomment-135925187,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56
soumith,convnet-benchmarks,103818583,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135944263,135944263,MDEyOklzc3VlQ29tbWVudDEzNTk0NDI2Mw==,9326960,2015-08-29T05:48:22Z,2015-08-29T05:48:22Z,NONE,"Optimizing for Overfeat is like shooting fish in a barrel.  And the kind of fish that no one particularly wants to eat.  I'd be more impressed with Intel's results on SOTA networks of this year.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56/comments,https://github.com/soumith/convnet-benchmarks/issues/56#issuecomment-135944263,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56
soumith,convnet-benchmarks,103818583,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/136130292,136130292,MDEyOklzc3VlQ29tbWVudDEzNjEzMDI5Mg==,1850503,2015-08-30T11:37:51Z,2015-08-30T11:37:51Z,NONE,"Has anyone been able to, or interested in, compiling opencl convolutions to fpgas ( using the alters Sdk). There was the somewhat similar neuflow project but I haven't heard from them in a while.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56/comments,https://github.com/soumith/convnet-benchmarks/issues/56#issuecomment-136130292,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56
soumith,convnet-benchmarks,103818583,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/136161586,136161586,MDEyOklzc3VlQ29tbWVudDEzNjE2MTU4Ng==,1310570,2015-08-30T17:21:55Z,2015-08-30T17:21:55Z,OWNER,"@BlGene neuflow used to be open-source, but they closed it up and are building a startup around it called TeraDeep. Neuflow was also built for inference, not training, and it was based on fixed-point FPGAs. If there are good floating point FPGAs that are affordable, it might be a good idea to start a new community project in that direction.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56/comments,https://github.com/soumith/convnet-benchmarks/issues/56#issuecomment-136161586,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56
soumith,convnet-benchmarks,103818583,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/136213248,136213248,MDEyOklzc3VlQ29tbWVudDEzNjIxMzI0OA==,123560,2015-08-30T23:06:08Z,2015-08-30T23:06:08Z,CONTRIBUTOR,"@BlGene Do you have some kind of indicative figures on a suitable FPGA in 100-400usd price range, and its relative performance, on convolution, with GPUs in same price range?  Could be theoretical analysis, and could be approximate, as long as such caveats are clearly stated.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56/comments,https://github.com/soumith/convnet-benchmarks/issues/56#issuecomment-136213248,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56
soumith,convnet-benchmarks,103818583,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/136318515,136318515,MDEyOklzc3VlQ29tbWVudDEzNjMxODUxNQ==,1850503,2015-08-31T09:52:36Z,2015-08-31T09:52:36Z,NONE,"@soumith @hughperkins 

I don't have anything concrete. I did a  bit of research and came up with the following. I haven't read the papers in detail but from what I understand the sentiment seems to be a lot of work and not much faster yet ( is this correct? ). It might be possible to ask people if they are interested in comparable benchmarking... if they feel confident ;)

Quora: http://www.quora.com/Is-implementing-deep-learning-on-FPGAs-a-natural-next-step-after-the-success-with-GPUs

Research Papers:
1. http://dl.acm.org/citation.cfm?id=2689060
2. http://ieeexplore.ieee.org/xpls/icp.jsp?arnumber=7160081

Companies: 
1. At Altera: http://www.slideshare.net/embeddedvision/a04-altera-singh
2. At Altera-Baidu: http://newsroom.altera.com/press-releases/altera-baidu-fpga-cloud-data-centers.htm
3. At Nervana Systems: https://gigaom.com/2014/08/21/nervana-systems-raises-3-3m-to-build-hardware-designed-for-deep-learning/
4. At Auviz Systems: http://auvizsystems.com/products/auvizdnn/

What are your thoughts? Maybe @scott-gray can say something for Nervana?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56/comments,https://github.com/soumith/convnet-benchmarks/issues/56#issuecomment-136318515,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56
soumith,convnet-benchmarks,103818583,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/136326498,136326498,MDEyOklzc3VlQ29tbWVudDEzNjMyNjQ5OA==,9326960,2015-08-31T10:27:01Z,2015-08-31T10:27:01Z,NONE,"I cant really speak much of our own hardware efforts except to say it should be extremely competitive to GPUs.  Generally speaking, any ASIC that's custom designed for a particular task is going to be faster than one with a more general purpose.

Though there is this recent bit on Microsoft's efforts with FPGAs:
http://www.theplatform.net/2015/08/27/microsoft-extends-fpga-reach-from-bing-to-deep-learning/

Seems for them the real advantage is in power savings.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56/comments,https://github.com/soumith/convnet-benchmarks/issues/56#issuecomment-136326498,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56
soumith,convnet-benchmarks,103818583,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/220806716,220806716,MDEyOklzc3VlQ29tbWVudDIyMDgwNjcxNg==,123560,2016-05-21T23:56:10Z,2016-05-21T23:56:10Z,CONTRIBUTOR,"> Looks like cuDNN is catching up. Those VGG numbers are really good for an FFT implementation.

Is cuDNN using FFT then?  How/what do we know about how cuDNN works?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56/comments,https://github.com/soumith/convnet-benchmarks/issues/56#issuecomment-220806716,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56
soumith,convnet-benchmarks,103818583,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/220808909,220808909,MDEyOklzc3VlQ29tbWVudDIyMDgwODkwOQ==,1310570,2016-05-22T01:17:42Z,2016-05-22T01:17:42Z,OWNER,"@hughperkins the CuDNN manual details the available algorithms they use. Also, the headers give hints as well. https://github.com/soumith/cudnn.torch/blob/R5/ffi.lua#L394-L402
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56/comments,https://github.com/soumith/convnet-benchmarks/issues/56#issuecomment-220808909,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56
soumith,convnet-benchmarks,103818583,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/220823457,220823457,MDEyOklzc3VlQ29tbWVudDIyMDgyMzQ1Nw==,123560,2016-05-22T09:47:20Z,2016-05-22T09:47:20Z,CONTRIBUTOR,"> the CuDNN manual details the available algorithms they use.

Ok.   To what extent is the CuDNN manual publicly, and to what extent does one have to click through some agreement where one agrees not to reveal its contents?  I guess NVIDIA has larger pockets than I do :-D

(I can find plenty of stuff about v1, eg http://on-demand.gputechconf.com/gtc/2014/webinar/gtc-express-sharan-chetlur-cudnn-webinar.pdf and http://arxiv.org/pdf/1410.0759v2.pdf , but dont seem to be able to find any sources for v2++?)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56/comments,https://github.com/soumith/convnet-benchmarks/issues/56#issuecomment-220823457,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56
soumith,convnet-benchmarks,103818583,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/220824143,220824143,MDEyOklzc3VlQ29tbWVudDIyMDgyNDE0Mw==,123560,2016-05-22T10:03:34Z,2016-05-22T11:28:33Z,CONTRIBUTOR,"(well... seems there is a paragaph in the Lavin paper https://arxiv.org/pdf/1509.09308.pdf which asserts it is using FFT:

""The FFT and convolution theorem have been used to reduce the arithmetic complexity of convnet layers,  first by Mathieu et al. [10],  then refined by Visalache et al. [12], and then implemented in the NVIDIA cuDNN library[1]""

... so maybe I can just cite that???  (Although figure 1 seems to imply that:
- there are two CUDNN implementations: one is FFT, one is not FFT, and CUDNN switches between them using probably a heuristic, eg from the text ""cuDNN appears to erroneously select its FFT algorithm for intermediate values of N despite the fact that it performs very poorly, under 2 TFLOPS.""
- for the lowest-level layers, which as far as I know basically dominate the time? non-FFT is being used, for all batch sizes

... so seems that stating ""CUDNN is proprietary, so we cannot reason well on how it works"" is not an entirely unreasonable position?)

Edit: seems I should cite http://arxiv.org/abs/1412.7580 , but I remember these used to be in convnet-benchmarks, and maybe was even the reason convnet-benchmarks were originally created :-P  , but have vanished since around the time of CUDNNv2-v3, presumably because fbfft is no longer competitive with CUDNNv2-v4?

Edit2: oh wait, fbfft is still there :-)

~~Edit3: hmmm, I guess fbfft is not dependent on a blas implementation or similar?  just uses its own native code?  therefore easily portable to OpenCL?  And very excellent performance~~  Edit4: noticed fbfft does depend on blas, so removed edit3 :-)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56/comments,https://github.com/soumith/convnet-benchmarks/issues/56#issuecomment-220824143,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56
soumith,convnet-benchmarks,103818583,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/220837478,220837478,MDEyOklzc3VlQ29tbWVudDIyMDgzNzQ3OA==,9682398,2016-05-22T15:11:48Z,2016-05-22T15:11:48Z,NONE,"You can easily query which algorithm cuDNN's heuristic has selected for
your problem size and memory availability.  You can also just pick an
algorithm and force cuDNN to use it.  Or you can ask cuDNN to try every
algorithm it can and report how long they took, so that you can for example
pick the definitely fastest available one even if the heuristically chosen
one wouldn't have been the optimal choice.

cuDNN's EULA (while yes you do have to agree to it to use the library) does
not have any non-disclosure clause.  Plenty of people have published papers
with the results of experimenting with cuDNN already...

Hope this helps,
-Cliff
On May 22, 2016 3:03 AM, ""Hugh Perkins"" notifications@github.com wrote:

> (well... seems there is a paragaph in the Lavin paper which asserts it is
> using FFT:
> 
> ""The FFT and convolution theorem have been used to reduce the arithmetic
> complexity of convnet layers, first by Mathieu et al. [10], then refined by
> Visalache et al. [12], and then implemented in the NVIDIA cuDNN library[1]""
> 
> ... so maybe I can just cite that??? (Although figure 1 seems to imply
> that:
> - there are two CUDNN implementations: one is FFT, one is not FFT, and
>   CUDNN switches between them using probably a heuristic, eg from the text
>   ""cuDNN appears to erroneously select its FFT algorithm for intermediate
>   values of N despite the fact that it performs very poorly, under 2 TFLOPS.""
> - for the lowest-level layers, which as far as I know basically
>   dominate the time? non-FFT is being used, for all batch sizes
> 
> ... so seems that stating ""CUDNN is proprietary, so we cannot reason well
> on how it works"" is not an entirely unreasonable position?)
> 
> â€”
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/issues/56#issuecomment-220824143
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56/comments,https://github.com/soumith/convnet-benchmarks/issues/56#issuecomment-220837478,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56
soumith,convnet-benchmarks,103818583,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/220837959,220837959,MDEyOklzc3VlQ29tbWVudDIyMDgzNzk1OQ==,123560,2016-05-22T15:20:51Z,2016-05-22T16:00:07Z,CONTRIBUTOR,"@cliffwoolley   Thanks!  Bang goes my excuse for not learning about CUDNN :-D

Edit: ok, seems your reading seems plausible.  The API itself might be protected by copyright plausibly, but presumably 'fair use' applies, as far as stating what is in the api, describing it and so on?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56/comments,https://github.com/soumith/convnet-benchmarks/issues/56#issuecomment-220837959,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56
soumith,convnet-benchmarks,103818583,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/220842002,220842002,MDEyOklzc3VlQ29tbWVudDIyMDg0MjAwMg==,8460517,2016-05-22T16:35:25Z,2016-05-22T16:41:05Z,NONE,"@hughperkins as @cliffwoolley suggested, call  `cudnnFindConvolutionForwardAlgorithm()` to find the fastest cuDNN convolution algorithm for a given layer configuration.

In my paper I used `cudnnGetConvolutionForwardAlgorithm()` which cannot be relied on to select the fastest algorithm. As of cuDNN v.3. at least, it would select FFT for moderate batch sizes where direct convolution would have been faster. I wanted to compare FFT and Winograd directly at those sizes, so I left it that way, but perhaps it is a bit confusing.

Note that you explicitly select the algorithm to use in any cuDNN operation. For fprop convolution, the algorithms are enumerated in `cudnnConvolutionFwdAlgo_t`. As of cuDNN v.5., one of the choices is `CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD`. You could select algorithms manually and see what happens if you are curious about how they compare.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56/comments,https://github.com/soumith/convnet-benchmarks/issues/56#issuecomment-220842002,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56
soumith,convnet-benchmarks,103818583,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/220843824,220843824,MDEyOklzc3VlQ29tbWVudDIyMDg0MzgyNA==,123560,2016-05-22T17:10:50Z,2016-05-22T17:10:50Z,CONTRIBUTOR,"> As of cuDNN v.5., one of the choices is CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD.

Heh! Nice :-)  By the way, my apologies, I didnt realize until about 17 hours ago who you are.  But I know now :-)  Or, at least, I am becoming aware of your contribution to http://arxiv.org/abs/1509.09308 , which obviously blows the previous approaches to GPU convolution out of the water :-)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56/comments,https://github.com/soumith/convnet-benchmarks/issues/56#issuecomment-220843824,https://api.github.com/repos/soumith/convnet-benchmarks/issues/56
soumith,convnet-benchmarks,102363969,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/133429568,133429568,MDEyOklzc3VlQ29tbWVudDEzMzQyOTU2OA==,123560,2015-08-21T13:49:48Z,2015-08-21T13:49:48Z,CONTRIBUTOR,"Thanks!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/55/comments,https://github.com/soumith/convnet-benchmarks/pull/55#issuecomment-133429568,https://api.github.com/repos/soumith/convnet-benchmarks/issues/55
soumith,convnet-benchmarks,102201556,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/133109399,133109399,MDEyOklzc3VlQ29tbWVudDEzMzEwOTM5OQ==,1310570,2015-08-20T18:27:03Z,2015-08-20T18:27:03Z,OWNER,"thanks brandon. the link checker looks cool :)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/54/comments,https://github.com/soumith/convnet-benchmarks/pull/54#issuecomment-133109399,https://api.github.com/repos/soumith/convnet-benchmarks/issues/54
soumith,convnet-benchmarks,100714601,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/131649228,131649228,MDEyOklzc3VlQ29tbWVudDEzMTY0OTIyOA==,1310570,2015-08-17T01:33:45Z,2015-08-17T01:33:45Z,OWNER,"The time is for the full batch.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/53/comments,https://github.com/soumith/convnet-benchmarks/issues/53#issuecomment-131649228,https://api.github.com/repos/soumith/convnet-benchmarks/issues/53
soumith,convnet-benchmarks,99904948,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/129240231,129240231,MDEyOklzc3VlQ29tbWVudDEyOTI0MDIzMQ==,1310570,2015-08-09T21:02:15Z,2015-08-09T21:02:15Z,OWNER,"thanks hugh! I think the vgg model will run fine on a Titan-X
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/50/comments,https://github.com/soumith/convnet-benchmarks/pull/50#issuecomment-129240231,https://api.github.com/repos/soumith/convnet-benchmarks/issues/50
soumith,convnet-benchmarks,99850257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/129097678,129097678,MDEyOklzc3VlQ29tbWVudDEyOTA5NzY3OA==,123560,2015-08-09T03:02:37Z,2015-08-09T03:02:37Z,CONTRIBUTOR,"I am not benchmarks maintainer, just an onlooker, but I reckon you might want to do everything you can so you longer need to write:

```
Tested & found working on Fedora 22. The installation script is written 
so that it should theoretically work on Ubuntu 13.04 to 15.04, but is not tested yet.
The biggest pitfall here is a faulty OpenCL installation and/or incompatible libraries.
```

The benchmarks run on Ubuntu 14.04, against an NVIDIA GPU.  You can run up a GPU instance on Amazon for 65 cents an hour http://aws.amazon.com/ec2/pricing/  I'm happy to pay for one for a day or two, and grant you access.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/49/comments,https://github.com/soumith/convnet-benchmarks/pull/49#issuecomment-129097678,https://api.github.com/repos/soumith/convnet-benchmarks/issues/49
soumith,convnet-benchmarks,99850257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/129168538,129168538,MDEyOklzc3VlQ29tbWVudDEyOTE2ODUzOA==,5577650,2015-08-09T11:41:23Z,2015-08-09T11:41:23Z,NONE,"@hughperkins 
Thanks, I'll try that out.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/49/comments,https://github.com/soumith/convnet-benchmarks/pull/49#issuecomment-129168538,https://api.github.com/repos/soumith/convnet-benchmarks/issues/49
soumith,convnet-benchmarks,99850257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/129186891,129186891,MDEyOklzc3VlQ29tbWVudDEyOTE4Njg5MQ==,123560,2015-08-09T13:11:49Z,2015-08-09T13:11:49Z,CONTRIBUTOR,"Ok.  Hmmm, you have benchmarks for all the sections, including overfeat, and vgg.  That's cool :-)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/49/comments,https://github.com/soumith/convnet-benchmarks/pull/49#issuecomment-129186891,https://api.github.com/repos/soumith/convnet-benchmarks/issues/49
soumith,convnet-benchmarks,99850257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/129186961,129186961,MDEyOklzc3VlQ29tbWVudDEyOTE4Njk2MQ==,5577650,2015-08-09T13:13:49Z,2015-08-09T13:13:49Z,NONE,"@hughperkins 
Yes, but I saw my OpenCL kernels are not really optimized for that types of networks, as I mainly focussed on getting this fast:
https://github.com/naibaf7/caffe_neural_models

But we'll see, there's definitely room for kernel optimization. At least it runs all networks without problems.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/49/comments,https://github.com/soumith/convnet-benchmarks/pull/49#issuecomment-129186961,https://api.github.com/repos/soumith/convnet-benchmarks/issues/49
soumith,convnet-benchmarks,99850257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/129189275,129189275,MDEyOklzc3VlQ29tbWVudDEyOTE4OTI3NQ==,5577650,2015-08-09T13:54:02Z,2015-08-09T13:54:02Z,NONE,"@hughperkins 
OK, tested on Ubuntu 14.04 on Amazon AWS EC2 g2.2 GPU.
Once the CUDA driver and ICD loader are installed (should be fine when CUDA 7.0 is set up) then it works.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/49/comments,https://github.com/soumith/convnet-benchmarks/pull/49#issuecomment-129189275,https://api.github.com/repos/soumith/convnet-benchmarks/issues/49
soumith,convnet-benchmarks,99850257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/129192857,129192857,MDEyOklzc3VlQ29tbWVudDEyOTE5Mjg1Nw==,123560,2015-08-09T14:46:46Z,2015-08-09T14:46:46Z,CONTRIBUTOR,"Cool :-)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/49/comments,https://github.com/soumith/convnet-benchmarks/pull/49#issuecomment-129192857,https://api.github.com/repos/soumith/convnet-benchmarks/issues/49
soumith,convnet-benchmarks,99850257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/129238808,129238808,MDEyOklzc3VlQ29tbWVudDEyOTIzODgwOA==,1310570,2015-08-09T20:51:20Z,2015-08-09T20:51:20Z,OWNER,"thanks a lot for this Fabian. You made my life much easier with this. Looking forward to benchmarking this.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/49/comments,https://github.com/soumith/convnet-benchmarks/pull/49#issuecomment-129238808,https://api.github.com/repos/soumith/convnet-benchmarks/issues/49
soumith,convnet-benchmarks,99850257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/129239833,129239833,MDEyOklzc3VlQ29tbWVudDEyOTIzOTgzMw==,5577650,2015-08-09T20:59:32Z,2015-08-09T20:59:32Z,NONE,"@soumith 
Cool :)
Yeah, but it won't be very fast... it's a start though. At least it's very versatile :)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/49/comments,https://github.com/soumith/convnet-benchmarks/pull/49#issuecomment-129239833,https://api.github.com/repos/soumith/convnet-benchmarks/issues/49
soumith,convnet-benchmarks,99456271,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/128406084,128406084,MDEyOklzc3VlQ29tbWVudDEyODQwNjA4NA==,1310570,2015-08-06T15:08:53Z,2015-08-06T15:08:53Z,OWNER,"fwiw, the weights that the overfeat guys released are a binary file, and the format is pretty well known:
https://github.com/jhjin/overfeat-torch/blob/master/run.lua#L61-L77

If you are feeling adventurous, you can do some C++ to do the same in Caffe.
I guess this issue should also be opened in https://github.com/BVLC/caffe where all the caffe users will see it.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/48/comments,https://github.com/soumith/convnet-benchmarks/issues/48#issuecomment-128406084,https://api.github.com/repos/soumith/convnet-benchmarks/issues/48
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/127083537,127083537,MDEyOklzc3VlQ29tbWVudDEyNzA4MzUzNw==,9326960,2015-08-02T23:28:26Z,2015-08-02T23:28:26Z,NONE,"Any plans to incorporate batchnorm in this run?  What about other potential bottlenecks in the network like the data loading and augmentation?  Will fp16 testing now become standard with it being available in cuDNN (as far as I know)?

I think we're working on some standard rnn/lstm network benchmarks so that might be a place to start for other network types.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-127083537,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/129042103,129042103,MDEyOklzc3VlQ29tbWVudDEyOTA0MjEwMw==,5577650,2015-08-08T20:10:55Z,2015-08-08T20:11:10Z,NONE,"@soumith 
How could we include Caffe #2610 (https://github.com/BVLC/caffe/pull/2610) for testing?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-129042103,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/129078537,129078537,MDEyOklzc3VlQ29tbWVudDEyOTA3ODUzNw==,123560,2015-08-09T00:31:42Z,2015-08-09T00:31:42Z,CONTRIBUTOR,"@naibaf7 

Technically, objectively:
- fork this repo
- add a subdirectory, with:
  - script that installs your software, runs it, prints benchmarking results
  - a short README explaining how to run the script
- submit a pull request

Subjectively, my opinion:
- you need a unique name
- I really like your Greentea name
- Suggest using either 'Caffe-Greentea', or simply 'Greentea'
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-129078537,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/133776757,133776757,MDEyOklzc3VlQ29tbWVudDEzMzc3Njc1Nw==,123560,2015-08-23T03:01:25Z,2015-08-23T03:01:59Z,CONTRIBUTOR,"For cltorch, just noticed the test/test-perf.lua script, in clnn repo, is missing the layer 2 single-layer timings, so I've pushed an update to clnn repo just now:
- uncommented single-layer layer 2 test
- removed colors
- prints the name of each layer now

If you reinstall clnn, it should bring down these changes, if needed.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-133776757,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/133776776,133776776,MDEyOklzc3VlQ29tbWVudDEzMzc3Njc3Ng==,1310570,2015-08-23T03:02:03Z,2015-08-23T03:02:03Z,OWNER,"@hughperkins thanks, will do.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-133776776,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/133776793,133776793,MDEyOklzc3VlQ29tbWVudDEzMzc3Njc5Mw==,5577650,2015-08-23T03:02:38Z,2015-08-23T03:02:38Z,NONE,"@soumith 
When can we expect results of the benchmarks? :)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-133776793,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/133776841,133776841,MDEyOklzc3VlQ29tbWVudDEzMzc3Njg0MQ==,1310570,2015-08-23T03:04:10Z,2015-08-23T03:04:10Z,OWNER,"@naibaf7 i am concluding them. i finished benchmarking everything except fb-cunn, cudnn in FP16 mode and chainer. Hopefully by Monday I will write a detailed comment with my findings.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-133776841,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/133776863,133776863,MDEyOklzc3VlQ29tbWVudDEzMzc3Njg2Mw==,5577650,2015-08-23T03:04:55Z,2015-08-23T03:04:55Z,NONE,"@soumith 
Do you also test on the CPU or is only the Titan X evaluated?
Because Caffe could also be run in CPU mode, and Greentea supports CPUs very well in the OpenCL mode.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-133776863,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/133776991,133776991,MDEyOklzc3VlQ29tbWVudDEzMzc3Njk5MQ==,1310570,2015-08-23T03:06:30Z,2015-08-23T03:06:30Z,OWNER,"@naibaf7 at the moment I am only doing GPU side of things. 
For Caffe, Torch, GreenTea I suppose I can also run CPU without too much effort from my side. I did not think people were interested in those.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-133776991,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/133777464,133777464,MDEyOklzc3VlQ29tbWVudDEzMzc3NzQ2NA==,123560,2015-08-23T03:17:03Z,2015-08-23T03:17:03Z,CONTRIBUTOR,"I guess mostly CPU would only be used during development, whereas actual training will tend to be GPU-based?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-133777464,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/133777591,133777591,MDEyOklzc3VlQ29tbWVudDEzMzc3NzU5MQ==,5577650,2015-08-23T03:19:43Z,2015-08-23T03:34:45Z,NONE,"@hughperkins 
For the moment, it seems like this. But with upcoming asynchronous solvers and MPI support, as well as good parallelized backends (the Caffe CPU backend is single threaded except for BLAS calls, while the GreenTea OpenCL backend uses parallelized kernels and a parallel CPU BLAS) it might become reasonably interesting again to use existing CPU clusters for training.

A second perspective will be with APU/HSA devices. Using an i7-4790K for example on the Caffe CPU backend gives a speedup of 1x on Alexnet. Using it on the Greentea is almost 2x. The integrated graphics would also evaluate from 1.5x to 2x.
When splitting up Alexnet over the integrated graphics and CPU, 4x the speed of the old CPU backend in Caffe can be reached, using the same device. This already approaches the training speed of mid-end GPUs.

Just something to keep in mind when seeing that the future exascale device we have to work with look much like this:
http://www.hpcwire.com/2015/07/29/amds-exascale-strategy-hinges-on-heterogeneity/
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-133777591,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/133778148,133778148,MDEyOklzc3VlQ29tbWVudDEzMzc3ODE0OA==,123560,2015-08-23T03:35:59Z,2015-08-23T03:35:59Z,CONTRIBUTOR,"@Fabian Re: APU/HSA devices, interesting, will reply in your PR, to keep this thread clean(er).
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-133778148,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/133789813,133789813,MDEyOklzc3VlQ29tbWVudDEzMzc4OTgxMw==,123560,2015-08-23T05:45:25Z,2015-08-23T05:45:36Z,CONTRIBUTOR,"@Soumith by the way, quick heads-up, before you inadvertently step into a mine-field :-P  There are actually multiple Caffe OpenCL forks, none of which have been officially recognized/endorsed, and I am not aware of any plans to merge any of the forks into main Caffe in the near-term.

Therefore, strongly recommend choosing a name for Fabian's fork which does not imply that it is the one and only Caffe OpenCL fork.  I think that using 'greentea' or 'caffe greentea' meets this requirement, and will plausibly provide you a mine-free life :-)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-133789813,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/134479066,134479066,MDEyOklzc3VlQ29tbWVudDEzNDQ3OTA2Ng==,1310570,2015-08-25T05:18:40Z,2015-08-25T05:18:40Z,OWNER,"@hughperkins thanks for the heads up :) . Everything done except chainer. That should be done tomorrow as well, along with the write-up.

Will do CPU candidates later, but there seem to be a few. I have to collect them, read a bit on each, and get to proper benchmarking each of them. CPU benchmarking gets much more complicated in general.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-134479066,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/134514693,134514693,MDEyOklzc3VlQ29tbWVudDEzNDUxNDY5Mw==,1710528,2015-08-25T07:56:44Z,2015-08-25T07:56:44Z,NONE,"/cc @gujunli
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-134514693,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/134525121,134525121,MDEyOklzc3VlQ29tbWVudDEzNDUyNTEyMQ==,5577650,2015-08-25T08:44:11Z,2015-08-25T08:44:48Z,NONE,"@soumith 
Oh you plan on doing CPU? Cool :)
Remember to use a good BLAS (OpenBLAS compiled from source or MKL) on your CPU with Greentea and Caffe and remember to configure it in Makefile.config.
Additionally, on Greentea, the CPU must be found with device_query and using the -gpu=x flag, where x is the ID of the CPU.

Thanks :)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-134525121,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/134554024,134554024,MDEyOklzc3VlQ29tbWVudDEzNDU1NDAyNA==,123560,2015-08-25T10:52:21Z,2015-08-25T10:52:21Z,CONTRIBUTOR,"@soumith Note that the 3rd Caffe OpenCL fork is public now :-P  https://github.com/gujunli/OpenCL-caffe-upstream-test.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-134554024,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/134562682,134562682,MDEyOklzc3VlQ29tbWVudDEzNDU2MjY4Mg==,1710528,2015-08-25T11:53:17Z,2015-08-25T11:53:17Z,NONE,"@soumith I think that @michaellarabel of phoronix.com has many interesting hardware to run your benchmark. I think that you can talk with him.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-134562682,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/134589857,134589857,MDEyOklzc3VlQ29tbWVudDEzNDU4OTg1Nw==,5577650,2015-08-25T13:41:01Z,2015-08-25T13:41:01Z,NONE,"@bhack @hughperkins @soumith 
We compared the OpenCL performance.
Using nVidia hardware, Alexnet will run approximately with the same speed using https://github.com/BVLC/caffe/pull/2610 and https://github.com/BVLC/caffe/pull/2195 PRs of Caffe.

So what you will see in your benchmarks is equally applicable at the moment. For AMD hardware, it would be a bit different right now.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-134589857,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/134774752,134774752,MDEyOklzc3VlQ29tbWVudDEzNDc3NDc1Mg==,123560,2015-08-26T00:06:01Z,2015-08-26T00:06:01Z,CONTRIBUTOR,"Note: apparently the link to the AMD repo above is just a test link, gone now.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-134774752,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/134776919,134776919,MDEyOklzc3VlQ29tbWVudDEzNDc3NjkxOQ==,1710528,2015-08-26T00:18:29Z,2015-08-26T00:18:29Z,NONE,"@hughperkins A really strange timing considering its commit history was older then one month and license in source file was already change with AMD copyright.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-134776919,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/134778030,134778030,MDEyOklzc3VlQ29tbWVudDEzNDc3ODAzMA==,5577650,2015-08-26T00:27:57Z,2015-08-26T00:34:16Z,NONE,"@bhack @hughperkins 
Please no speculation for now, just adds to the confusion I think :)
We're planning on discussing how to go forward with OpenCL soon, as it's obviously not very good for the cause to have so many branches.

@bhack
You kind of advertised my branch/PR everywhere.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-134778030,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/134778759,134778759,MDEyOklzc3VlQ29tbWVudDEzNDc3ODc1OQ==,1710528,2015-08-26T00:34:12Z,2015-08-26T00:34:12Z,NONE,"@naibaf7 Yes AMD people under coverage on github doesn't really help to clarify the situation. I'm confident that you can put back the discussion in a public space as soon you can.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-134778759,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/134778981,134778981,MDEyOklzc3VlQ29tbWVudDEzNDc3ODk4MQ==,123560,2015-08-26T00:36:18Z,2015-08-26T00:36:18Z,CONTRIBUTOR,"Concur with bhack's view.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-134778981,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/134779103,134779103,MDEyOklzc3VlQ29tbWVudDEzNDc3OTEwMw==,5577650,2015-08-26T00:37:18Z,2015-08-26T00:42:48Z,NONE,"@bhack @hughperkins 

Yes I know but as a clarification, I think AMD is okay with me sharing this:
One of the branches from junli gu was actually a research branch for AMD internal. Robert started his OpenCL branch inofficially in spare time. I shortly after that started my OpenCL branch as part of my thesis with AMD sponsoring.

 As of now, there is no OpenCL branch that has the full official support by AMD. Please note that all branches have their pros and cons, and this needs to be resolved, as well as a plan on how to proceed (who does what, plans on merging or feature branch, device abstraction, speeding up the convolutions...)

#2195 and #2610 have also been concurrent projects until now, which did power some advancements but also lead to heated discussions. This will be resolved now and we will plan collaboration.

However it is a bad idea to lead all discussions public first (confusion), especially because only a handful of developers are involved.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-134779103,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/134950020,134950020,MDEyOklzc3VlQ29tbWVudDEzNDk1MDAyMA==,123560,2015-08-26T11:10:32Z,2015-08-26T11:10:32Z,CONTRIBUTOR,"> One of the branches from junli gu was actually a research branch for AMD internal. Robert started his OpenCL branch inofficially in spare time. I shortly after that started my OpenCL branch as part of my thesis with AMD sponsoring.

It seems a bit sub-optimal to me for a company which gives the impression to me of not being overly endowed with cash, to use three resources to work independently on the exact same problem, whilst the AMD compiler still is buggy, and optimization plausibly patchy.  Even if you argue 'well, two of them were working for free', yes but the opportunity cost is still massive.  Theano is still cuda-only, Chainer too, and so on.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-134950020,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/134952858,134952858,MDEyOklzc3VlQ29tbWVudDEzNDk1Mjg1OA==,5577650,2015-08-26T11:23:28Z,2015-08-26T11:27:00Z,NONE,"@hughperkins 
That's why I would prefer to not see speculation on here. I just told you what I know the clear the situation, now I'll discuss with AMD how to proceed, so let's see about that first before jumping to conclusions.

Besides, the branches also have some specific advantages and domain specific optimizations, so there's a lot to profit from now when going forward on OpenCL.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-134952858,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135040040,135040040,MDEyOklzc3VlQ29tbWVudDEzNTA0MDA0MA==,1310570,2015-08-26T14:27:12Z,2015-08-26T14:27:54Z,OWNER,"@hughperkins when a company is large and distributed, parallel efforts might also happen out of interest. let's not waste a discussion on this speculation. FYI there's also a C++ AMP implementation of the Torch backends for AMD (funded by AMD) here: https://github.com/NEELMCW/MCW_CPPAMP_TORCH but not as clean and nice as your stuff.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-135040040,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135049329,135049329,MDEyOklzc3VlQ29tbWVudDEzNTA0OTMyOQ==,123560,2015-08-26T14:55:41Z,2015-08-26T14:55:41Z,CONTRIBUTOR,"@soumith Basically, I dont want to see AMD go the way of Sun, since AMD are pretty much the main competitor to NVIDIA right now, although Intel do have some offerings, but mostly integrated GPUs right now, AFAIK?  Sun in my opinion seemed to invest tons of money in opensource, which didnt seem to generate any return.  I reckon that AMD in my opinion should either focus on AMD-specific stuff, like the compilers and so on, or else, I dont see any reason why they cant make proprietary, non-free, libraries, that generate revenue.  Intel do this with MKL, and MKL seems to be doing ok for itself.  I know this might seem odd coming from someone who writes lots of opensource, but I'd rather have an AMD producing non-free stuff and surviving, than producing lots of free stuff, and get eaten :-(
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-135049329,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135064412,135064412,MDEyOklzc3VlQ29tbWVudDEzNTA2NDQxMg==,5577650,2015-08-26T15:30:44Z,2015-08-26T15:30:44Z,NONE,"@soumith 
I've seen you changed the ViennaCL installation script on Greentea. Are you on a distribution which does not ship ViennaCL via APT/DNF/YUM?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-135064412,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135120794,135120794,MDEyOklzc3VlQ29tbWVudDEzNTEyMDc5NA==,1310570,2015-08-26T17:45:13Z,2015-08-26T17:45:13Z,OWNER,"@naibaf7 i am on ubuntu 14.04, but the makefile wasn't picking it up, so I just did things manually, did not look too much into it.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-135120794,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135121324,135121324,MDEyOklzc3VlQ29tbWVudDEzNTEyMTMyNA==,5577650,2015-08-26T17:47:38Z,2015-08-26T17:47:38Z,NONE,"Ok that's interesting. Here and on Amazon servers it did with Ubuntu 14.04... well anyways, cool that you could make it work for you.
The more advanced method is to switch to clBLAS. But I think it's a pretty cool feature to be able to use both BLAS libraries to provide easy installation & flexibility.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-135121324,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135342879,135342879,MDEyOklzc3VlQ29tbWVudDEzNTM0Mjg3OQ==,1710528,2015-08-27T08:45:46Z,2015-08-27T08:46:53Z,NONE,"@michaellarabel has an AMD R9 Fury close at hand. @naibaf7 It would be great to benchmark on that.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-135342879,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135647115,135647115,MDEyOklzc3VlQ29tbWVudDEzNTY0NzExNQ==,87486,2015-08-28T06:22:09Z,2015-08-28T06:22:09Z,NONE,"I have a R9 Fury, if anyone wants to test performance on it. I am not sure
how easy the access is. but if I can access your code, that will be easy.
@naibaf7 https://github.com/naibaf7 @hughperkins
https://github.com/hughperkins

On Thu, Aug 27, 2015 at 1:45 AM, bhack notifications@github.com wrote:

> @michaellarabel https://github.com/michaellarabel has an AMD R9 Fury
> under the hand. @naibaf7 https://github.com/naibaf7 It would be great
> to benchmark on that.
> 
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-135342879
> .

## 

---

Junli Gu--è°·ä¿Šä¸½
Coordinated Science Lab
University of Illinois at Urbana-Champaign

---
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-135647115,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135647963,135647963,MDEyOklzc3VlQ29tbWVudDEzNTY0Nzk2Mw==,87486,2015-08-28T06:29:17Z,2015-08-28T06:29:17Z,NONE,"@naibaf7 I like your early comment of you working on HSA/APU. We have done some work of CAFFE on APU. I would like to discuss with you.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-135647963,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135648425,135648425,MDEyOklzc3VlQ29tbWVudDEzNTY0ODQyNQ==,87486,2015-08-28T06:33:38Z,2015-08-28T06:33:38Z,NONE,"@Soumith We have some evaluation results of OpenCL CAFFE (reserach lab's internal) on Fury and W9100. I wonder whether it helps for me to share the results with you but now the code for now. We evaluated against TitanX and GTX980. But it might be nice to know where we are now on your performance list.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-135648425,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135664452,135664452,MDEyOklzc3VlQ29tbWVudDEzNTY2NDQ1Mg==,1710528,2015-08-28T07:48:21Z,2015-08-28T07:48:30Z,NONE,"@gujunli is ""but now the code for now"" interpreted as ""but not the code for now""? 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-135664452,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135708416,135708416,MDEyOklzc3VlQ29tbWVudDEzNTcwODQxNg==,1710528,2015-08-28T09:22:23Z,2015-08-28T11:39:21Z,NONE,"@soumith Is the Intel framework at https://github.com/01org/idlf testable?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-135708416,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135745240,135745240,MDEyOklzc3VlQ29tbWVudDEzNTc0NTI0MA==,123560,2015-08-28T11:31:25Z,2015-08-28T11:31:25Z,CONTRIBUTOR,"re: ""can I submit results without sourcecode, using a non-publically available library?""  What benefit do you see in doing this?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-135745240,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135750453,135750453,MDEyOklzc3VlQ29tbWVudDEzNTc1MDQ1Mw==,1710528,2015-08-28T12:01:02Z,2015-08-28T12:01:02Z,NONE,"@hughperkins It was a my fault to have pointed all to an internal repository of AMD Chinese research center that was maintained on a public github repository. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-135750453,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135751483,135751483,MDEyOklzc3VlQ29tbWVudDEzNTc1MTQ4Mw==,123560,2015-08-28T12:03:24Z,2015-08-28T12:03:24Z,CONTRIBUTOR,"@bhack No, I put Junli on my 'follow' list long ago, so I saw anyway :-)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-135751483,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135754888,135754888,MDEyOklzc3VlQ29tbWVudDEzNTc1NDg4OA==,1710528,2015-08-28T12:11:58Z,2015-08-28T12:11:58Z,NONE,"@hughperkins It was not an AMD Chinese center but the USA research center in Illinois.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-135754888,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135755094,135755094,MDEyOklzc3VlQ29tbWVudDEzNTc1NTA5NA==,123560,2015-08-28T12:12:29Z,2015-08-28T12:12:29Z,CONTRIBUTOR,"@bhack You know that AMD might have more than one office right? :-P
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-135755094,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135755664,135755664,MDEyOklzc3VlQ29tbWVudDEzNTc1NTY2NA==,1710528,2015-08-28T12:14:22Z,2015-08-28T12:14:22Z,NONE,"@hughperkins Yes my bad. Sunnyvale, California
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-135755664,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135756248,135756248,MDEyOklzc3VlQ29tbWVudDEzNTc1NjI0OA==,123560,2015-08-28T12:15:43Z,2015-08-28T12:15:43Z,CONTRIBUTOR,"@bhack hmmm, what I thought I was communicating is not exactly what I communicated.  Anyway.... I wouldnt read too much into geographic locations, it is a global corporation.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-135756248,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135758833,135758833,MDEyOklzc3VlQ29tbWVudDEzNTc1ODgzMw==,1710528,2015-08-28T12:21:59Z,2015-08-28T12:23:17Z,NONE,"@hughperkins Yes but my ""location"" was referenced only by declaration of ""I am leading the AMD research's DNN project"". So probably she is directing this effort with distributed resources around the globe and there is really no physical DNN group co-located in Sunnyvale facilities.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-135758833,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135766281,135766281,MDEyOklzc3VlQ29tbWVudDEzNTc2NjI4MQ==,5577650,2015-08-28T12:48:51Z,2015-08-28T12:52:35Z,NONE,"@bhack @hughperkins 
Okay... you are very off topic now, just saying :D
Not sure if @soumith is very happy with that.

To stop speculation once again:
Currently the DNN people are physically in the same location, so don't worry about coordination, it will be great. AMD is pulling together all the important people to get this done the right way.

I also found AMD more reachable and easier to contact than nVidia and Intel, it is fun to work with them.
I once won an Intel ISEF award and even then it was crazy difficult to get into contact with anyone other than public relations people at Intel.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-135766281,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135768068,135768068,MDEyOklzc3VlQ29tbWVudDEzNTc2ODA2OA==,1710528,2015-08-28T12:59:37Z,2015-08-28T13:00:33Z,NONE,"@naibaf7 Yes we are surely off topic, it is true. I'm really happy that AMD is starting to have some kind of direction and coordination now but you can agree with me that the approach was quite confusing and sparse. Removing repositories just after a little bit of posting and without a comment is generally not a good marketing for a big company like AMD.
But nevermind it was only a start with some little false steps.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-135768068,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135768685,135768685,MDEyOklzc3VlQ29tbWVudDEzNTc2ODY4NQ==,123560,2015-08-28T13:03:22Z,2015-08-28T13:03:22Z,CONTRIBUTOR,"> Currently the DNN people are physically in the same location, so don't worry about coordination

I'm not sure that's exactly quite true, unless your definition of 'DNN people' is different than my own interpretation of this.

> I also found AMD more reachable and easier to contact than nVidia and Intel

I never tried to contact nvidia.  Or Intel actually....  I reckon for cuda projects, they have a fair amount of contact with nvidia though.  Caffe has sponsorship by nvidia, or at lesat provision of one or more nvidia gpus by nvidia, right on their front page.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-135768685,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135776741,135776741,MDEyOklzc3VlQ29tbWVudDEzNTc3Njc0MQ==,1710528,2015-08-28T13:34:36Z,2015-08-28T13:35:02Z,NONE,"@hughperkins Really it is off topic. The only important thing here it is to understand what kind version of opencl caffe will be benchmarked. I don't know if it is useful to have benchmark results of private versions but only @soumith could tell us.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-135776741,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135913241,135913241,MDEyOklzc3VlQ29tbWVudDEzNTkxMzI0MQ==,123560,2015-08-28T23:33:33Z,2015-08-28T23:33:33Z,CONTRIBUTOR,"@soumith thank-you very much for providing these benchmark results.  Very useful :-)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-135913241,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135916326,135916326,MDEyOklzc3VlQ29tbWVudDEzNTkxNjMyNg==,5577650,2015-08-28T23:49:38Z,2015-08-28T23:51:18Z,NONE,"@soumith 
Thanks - clearly there is work to do on the GreenTea convolution code.
Also there is some bottleneck in backward processing that needs to be solved.
Performance is expected to get much faster within the next 2 months (Batched-GEMM/GEMV).

Interesting to see how much slower an OpenCL implementation is with minibatches using identical code to CUDA Caffe - with optimized OpenCL code this will change.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-135916326,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135917231,135917231,MDEyOklzc3VlQ29tbWVudDEzNTkxNzIzMQ==,1710528,2015-08-28T23:57:29Z,2015-08-28T23:57:29Z,NONE,"Kudos to @scott-gray. Actually he has the fastest open source implementation. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-135917231,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135918963,135918963,MDEyOklzc3VlQ29tbWVudDEzNTkxODk2Mw==,5577650,2015-08-29T00:08:17Z,2015-08-29T00:51:49Z,NONE,"@bhack 
Probably it's a really good idea to replicate his kernels in GCN-assembly then ;)

Oh on another note, this was ViennaCL-BLAS. Probably higher performance with clBLAS for the next time.

@lunochod
Have you seen this? Still a lot of work on the OpenCL (compute kernel) side. Seems like just duplicating CUDA kernels does not really give good performance (as we know already...)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-135918963,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135920973,135920973,MDEyOklzc3VlQ29tbWVudDEzNTkyMDk3Mw==,9326960,2015-08-29T00:27:40Z,2015-08-29T00:27:40Z,NONE,"Looks like cuDNN is catching up.  Those VGG numbers are really good for an FFT implementation.  Perhaps with a bit more optimization they can overtake spatial domain on 3x3 filters?  I wouldn't be surprised if we see much better fp16 numbers from them soon.

My GoogLeNet numbers may look good but I still have a lot of optimizations to make for the smaller feature map values in there.  Right now I'm optimized for multiples of 64.  I'll get that down to 32 this weekend.  My CHWN tensor layout is also really helpful on those inception groupings.

A brand new version of neon is about to be released.  You'll be able to run all these networks out of the box (plus lots more).  The new syntax is much improved and more torch or keras like (perhaps better even).

Anyway, here's a changelog of updates since the last version:

No more multiplying by zero to implement padding in fprop and bprop (I now slice both the input and the filter)  

Figured out a different way to do integer division for the now dynamically sized slice lookup table.

No more atomic adds in bprop.  I've cast bprop as fprop upside down and the kernels are nearly identical.  It requires a dimshuffle on the filter but this just takes microseconds and a small amount of additional memory that can be shared with all conv ops.  Bprop used to be bandwidth bound on those atomic adds.

Tweaked the p,q block ordering to improve L2 cache performance.  I'm using a zigzag pattern now for all operations.

Update already had a mode where you could stack all the gemm ops to eliminate atomic adds, but I've streamlined that stacking operation.  Update also now fetches 32 rows deep.  This comes at the cost of an instruction cache miss inside the main gemm loop, but is easily covered by the occupancy.  The reason for doing this is the same for using a 32x33 shared memory block to implement transpose.  With N contiguous the update op has the expensive strided access patterns on both the input and delta.

I also eliminate all shared memory bank conflicts when storing the global loads to shared with some clever shifting.

Added a beta param to bprop to allow delta accumulation for inception groupings.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-135920973,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,98644257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/135924027,135924027,MDEyOklzc3VlQ29tbWVudDEzNTkyNDAyNw==,1310570,2015-08-29T01:04:14Z,2015-08-29T01:04:14Z,OWNER,"sorry, closed the issue as it got side-tracked by lots of other discussions. lets discuss more in 
https://github.com/soumith/convnet-benchmarks/issues/56

Scott, I'd appreciate if you re-paste your comment there to discuss further.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46/comments,https://github.com/soumith/convnet-benchmarks/issues/46#issuecomment-135924027,https://api.github.com/repos/soumith/convnet-benchmarks/issues/46
soumith,convnet-benchmarks,90078769,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/127082865,127082865,MDEyOklzc3VlQ29tbWVudDEyNzA4Mjg2NQ==,1310570,2015-08-02T23:12:31Z,2015-08-02T23:12:31Z,OWNER,"@pgera a new push to fbcunn should come before August 15th, and they have proper fallbacks (to cudnn) on out-of-memory etc. will keep you posted.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/45/comments,https://github.com/soumith/convnet-benchmarks/issues/45#issuecomment-127082865,https://api.github.com/repos/soumith/convnet-benchmarks/issues/45
soumith,convnet-benchmarks,83344869,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/107343129,107343129,MDEyOklzc3VlQ29tbWVudDEwNzM0MzEyOQ==,2884845,2015-06-01T07:49:19Z,2015-06-01T07:49:19Z,NONE,"solved. Reinstall torch first. http://torch.ch/docs/getting-started.html#_
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/44/comments,https://github.com/soumith/convnet-benchmarks/issues/44#issuecomment-107343129,https://api.github.com/repos/soumith/convnet-benchmarks/issues/44
soumith,convnet-benchmarks,72168231,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/98146744,98146744,MDEyOklzc3VlQ29tbWVudDk4MTQ2NzQ0,1310570,2015-05-01T14:43:45Z,2015-05-01T14:43:45Z,OWNER,"fbcunn's FFT modules need 12GB for Overfeat/VGG. We have some upcoming work that gives a flexibility of choosing mem usage vs performance (using Tiling FFT). Stay tuned, we will hopefully release it before our talk at ICLR on May 8th.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/42/comments,https://github.com/soumith/convnet-benchmarks/issues/42#issuecomment-98146744,https://api.github.com/repos/soumith/convnet-benchmarks/issues/42
soumith,convnet-benchmarks,69913944,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/94892649,94892649,MDEyOklzc3VlQ29tbWVudDk0ODkyNjQ5,1310570,2015-04-21T18:11:47Z,2015-04-21T18:11:47Z,OWNER,"Thanks. layers did not surprisingly work for me for ""InnerProduct"", maybe the trick is INNER_PRODUCT. I could not figure it out because the error message was pretty uninformative.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/41/comments,https://github.com/soumith/convnet-benchmarks/pull/41#issuecomment-94892649,https://api.github.com/repos/soumith/convnet-benchmarks/issues/41
soumith,convnet-benchmarks,67003422,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/91051565,91051565,MDEyOklzc3VlQ29tbWVudDkxMDUxNTY1,1310570,2015-04-08T22:12:04Z,2015-04-08T22:12:04Z,OWNER,"Thanks @dadaism I just got back access to my machine. I'll run these benchmarks soon.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/40/comments,https://github.com/soumith/convnet-benchmarks/pull/40#issuecomment-91051565,https://api.github.com/repos/soumith/convnet-benchmarks/issues/40
soumith,convnet-benchmarks,66871908,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/91051625,91051625,MDEyOklzc3VlQ29tbWVudDkxMDUxNjI1,1310570,2015-04-08T22:12:28Z,2015-04-08T22:12:28Z,OWNER,"Thanks @hughperkins, benchmarks on the way.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/39/comments,https://github.com/soumith/convnet-benchmarks/pull/39#issuecomment-91051625,https://api.github.com/repos/soumith/convnet-benchmarks/issues/39
soumith,convnet-benchmarks,66667789,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/90227144,90227144,MDEyOklzc3VlQ29tbWVudDkwMjI3MTQ0,43829,2015-04-06T20:14:46Z,2015-04-06T20:14:46Z,NONE,"Sweet, looking forward to this! Also slightly jealous ;)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38/comments,https://github.com/soumith/convnet-benchmarks/issues/38#issuecomment-90227144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38
soumith,convnet-benchmarks,66667789,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/90233338,90233338,MDEyOklzc3VlQ29tbWVudDkwMjMzMzM4,127987,2015-04-06T20:42:00Z,2015-04-06T20:42:00Z,CONTRIBUTOR,"Second the Jealousy!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38/comments,https://github.com/soumith/convnet-benchmarks/issues/38#issuecomment-90233338,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38
soumith,convnet-benchmarks,66667789,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/90299032,90299032,MDEyOklzc3VlQ29tbWVudDkwMjk5MDMy,1538540,2015-04-07T00:50:21Z,2015-04-07T00:50:21Z,CONTRIBUTOR,"@soumith , thanks for the reminder.  alexnet, vgg_a, overfeat are done. I will get googlenet done tonight and test a little bit. I will create PR on Tuesday (April 7) night. looking forward to seeing the results. : ) 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38/comments,https://github.com/soumith/convnet-benchmarks/issues/38#issuecomment-90299032,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38
soumith,convnet-benchmarks,66667789,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/93899812,93899812,MDEyOklzc3VlQ29tbWVudDkzODk5ODEy,1310570,2015-04-17T05:58:21Z,2015-04-17T05:58:21Z,OWNER,"I just finished up benchmarking on Titan-X.

What's extremely exciting is how fast GPUs are going, and even more exciting is the fact that people are pushing the limits of GPUs everyday.

Starting with imagenet-winners, AlexNet, Overfeat, VGG-A

The BIG winner of this round is NervanaSys's Maxwell-only convolution kernels, written by @scott-gray et. al. A big shout-out to them. They gave me an early-release of their kernels because they're a small company and dont have the bandwidth to support a public release. If you're pretty serious about your convnets, drop them an email. They have both 16-bit and 32-bit kernels, both of them with impressive performance.

The FB-FFT kernels by Facebook come a close second. They've improved quite a bit since their release into open-source in December 2014; Nicolas Vasilache and Jeff Johnson ( @wickedfoo ) have been improving them constantly.

cuda-convnet2 is still leader of the old bunch even on Maxwell.
NVIDIA's CuDNN comes 4th, but is much more flexible than cuda-convnet2 (no batch-size or feature-map restrictions) and FBFFT (free zero-padding).

On the Layer-wise benchmarks, FB-FFT is ahead of the pack by almost 4x. These are still somewhat useful because not all libraries have imagenet benchmarks.
I could not benchmark Theano-FFT, because of an error that's showing up, but with @benanne 's help I should get the numbers out for them pretty soon as well.

Conclusion: 
- If you run imagenet-style networks, go bug the NervanaSys guys, their kernels are pulling numbers that make me do backflips!
- If you have convnets with large kernel sizes (anything above 7x7), just go with FB-FFT. It's constantly being improved as well.

Cheers,
S
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38/comments,https://github.com/soumith/convnet-benchmarks/issues/38#issuecomment-93899812,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38
soumith,convnet-benchmarks,66667789,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/93903073,93903073,MDEyOklzc3VlQ29tbWVudDkzOTAzMDcz,127987,2015-04-17T06:18:21Z,2015-04-17T06:18:21Z,CONTRIBUTOR,"Wow, nice work!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38/comments,https://github.com/soumith/convnet-benchmarks/issues/38#issuecomment-93903073,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38
soumith,convnet-benchmarks,66667789,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/93904576,93904576,MDEyOklzc3VlQ29tbWVudDkzOTA0NTc2,814866,2015-04-17T06:23:04Z,2015-04-17T06:23:04Z,NONE,"So where are the benchmarks?  :-)

I'm trying to decide if I should upgrade from my 6GB GTX 780s.

Thanks,
Sean

On Thu, Apr 16, 2015 at 10:58 PM, Soumith Chintala <notifications@github.com

> wrote:
> 
> I just finished up benchmarking on Titan-X.
> 
> What's extremely exciting is how fast GPUs are going, and even more
> exciting is the fact that people are pushing the limits of GPUs everyday.
> 
> Starting with imagenet-winners, AlexNet, Overfeat, VGG-A
> 
> The BIG winner of this round is NervanaSys's Maxwell-only convolution
> kernels, written by @scott-gray https://github.com/scott-gray et. al. A
> big shout-out to them. They gave me an early-release of their kernels
> because they're a small company and dont have the bandwidth to support a
> public release. If you're pretty serious about your convnets, drop them an
> email. They have both 16-bit and 32-bit kernels, both of them with
> impressive performance.
> 
> The FB-FFT kernels by Facebook come a close second. They've improved quite
> a bit since their release into open-source in December 2014; Nicolas
> Vasilache and Jeff Johnson ( @wickedfoo https://github.com/wickedfoo )
> have been improving them constantly.
> 
> cuda-convnet2 is still leader of the old bunch even on Maxwell.
> NVIDIA's CuDNN comes 4th, but is much more flexible than cuda-convnet2 (no
> batch-size or feature-map restrictions) and FBFFT (free zero-padding).
> 
> On the Layer-wise benchmarks, FB-FFT is ahead of the pack by almost 4x.
> These are still somewhat useful because not all libraries have imagenet
> benchmarks.
> I could not benchmark Theano-FFT, because of an error that's showing up,
> but with @benanne https://github.com/benanne 's help I should get the
> numbers out for them pretty soon as well.
> 
> Conclusion:
> - If you run imagenet-style networks, go bug the NervanaSys guys,
>   their kernels are pulling numbers that make me do backflips!
> - If you have convnets with large kernel sizes (anything above 7x7),
>   just go with FB-FFT. It's constantly being improved as well.
> 
> Cheers,
> S
> 
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/issues/38#issuecomment-93899812
> .
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38/comments,https://github.com/soumith/convnet-benchmarks/issues/38#issuecomment-93904576,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38
soumith,convnet-benchmarks,66667789,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/93908662,93908662,MDEyOklzc3VlQ29tbWVudDkzOTA4NjYy,1310570,2015-04-17T06:30:23Z,2015-04-17T06:34:29Z,OWNER,"@skelleher right on the front-page readme:
https://github.com/soumith/convnet-benchmarks/blob/master/README.md

It's interesting to compare just the GPUs themselves.
For example: Titan-X vs Titan Black (which is slightly faster than 780, as fast as 780Ti):
Titan-Black:
https://github.com/soumith/convnet-benchmarks/tree/de7cfc32a93f5e14863666e3d3636aca662824fa#imagenet-winners-benchmarking

Titan-X:
https://github.com/soumith/convnet-benchmarks#imagenet-winners-benchmarking

Titan-X is quite a bit faster. Just comparing CuDNN R2 on both cards, Alexnet is **1.5x** faster on Titan-X. And if you take into account Nervana's kernels, Titan-X becomes **3.7x faster** than Titan-Black.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38/comments,https://github.com/soumith/convnet-benchmarks/issues/38#issuecomment-93908662,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38
soumith,convnet-benchmarks,66667789,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/93933673,93933673,MDEyOklzc3VlQ29tbWVudDkzOTMzNjcz,43829,2015-04-17T07:26:37Z,2015-04-17T07:26:37Z,NONE,"Awesome work @soumith, thanks for doing this :)

Those are some incredible numbers! I knew we had some leeway on Maxwell since almost all the code out there right now seems to be Kepler-optimized, but I was expecting the gains to be quite a bit smaller. I didn't know fp16 was possible yet, either! Really impressive stuff. Kudos to @scott-gray!

How 'flexible' are the Nervana kernels in terms of sizes/strides etc.? Since that seems to be the main benefit of cuDNN right now. I'm really looking forward to cuDNN v3 because I believe that's supposed to come Maxwell-optimized as well (and won't require any extensive wrapping).

Regarding Theano's FFT convolution - I don't think it's worth spending time on that anymore to be quite honest. It was a ""lazy"" wrapper in pure Python (using scikits.cuda) so it was never intended to be competitive and supposed to be replaced by a C-based wrapper anyway (there were plans for this at some point but I don't know what the current situation is). I only wrote it because there was nothing else at the time.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38/comments,https://github.com/soumith/convnet-benchmarks/issues/38#issuecomment-93933673,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38
soumith,convnet-benchmarks,66667789,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/93937759,93937759,MDEyOklzc3VlQ29tbWVudDkzOTM3NzU5,9326960,2015-04-17T07:55:25Z,2015-04-17T07:55:25Z,NONE,"My kernels are at least as flexible as cuDNN (though I left out upscaling and cross-correlation mode as I don't think they're used much).  The kernels are wrapped in a fairly easy to use python wrapper complete with automatically compounding elementwise operations.  Here is the interface to the conv and pooling layers:

``` python
    def conv_layer(self, dtype,
            N, C, K, 
            D=1, H=1, W=1,
            T=1, R=1, S=1,
            pad_d=0, pad_h=0, pad_w=0,
            str_d=1, str_h=1, str_w=1):
        """"""
        Create a new ConvLayer parameter object.

        N: Number of images in mini-batch
        C: Number of input feature maps
        K: Number of output feature maps

        D: Depth  of input image
        H: Height of input image
        W: Width  of input image

        T: Depth  of filter kernel
        R: Height of filter kernel
        S: Width  of filter kernel

        padding: amount of zero-padding around the given edge
        strides: factor to step the filters by in a given direction
        """"""

    def pool_layer(self, dtype,
            op, N, C, 
            D=1, H=1, W=1,
            J=1, T=1, R=1, S=1,
            pad_j=0, pad_d=0, pad_h=0, pad_w=0,
            str_j=None, str_d=None, str_h=None, str_w=None):
        """"""
        Create a new PoolLayer parameter object.
        This then is passed as an argument to all pooling kernels.

        op: max, avg, l2 pooling
        N: Number of images in mini-batch

        C: Number of input feature maps
        D: Depth  of input image
        H: Height of input image
        W: Width  of input image

        J: Size of feature map pooling window (maxout n_pieces)
        T: Depth  of pooling window
        R: Height of pooling window
        S: Width  of pooling window

        padding: amount of zero-padding around the given image or feature map edge
        strides: factor to step the window by in a given direction (overlap allowed)

        Leave spatial dimensions at 1 to allow feature map pooling in the fc layers.
        """"""
```

Also included in the lib is a full set of gemm kernels for fp16 and fp32 that generally far outperform cublas, particularly at small minibatch sizes.  

When using fp16, stochastic rounding is also available for all kernels.  Additionally all gemm and conv kernels have the option to apply a relu prior to output, saving you some elementwise overhead.

For fp16 we're currently developing techniques to train large networks with generally the same performance as fp32.  We should be publishing some of those results in the near future.

Oh and all of this (except for the element-wise stuff and python wrapper) was written in shader assembly using maxas (https://github.com/NervanaSystems/maxas).  
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38/comments,https://github.com/soumith/convnet-benchmarks/issues/38#issuecomment-93937759,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38
soumith,convnet-benchmarks,66667789,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/93938913,93938913,MDEyOklzc3VlQ29tbWVudDkzOTM4OTEz,43829,2015-04-17T08:02:25Z,2015-04-17T08:02:25Z,NONE,"That is awesome news! That Python wrapper looks really sweet as well, it should make Theano integration a breeze! I'd be very interested to play around with it :)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38/comments,https://github.com/soumith/convnet-benchmarks/issues/38#issuecomment-93938913,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38
soumith,convnet-benchmarks,66667789,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/94500758,94500758,MDEyOklzc3VlQ29tbWVudDk0NTAwNzU4,629706,2015-04-20T16:29:13Z,2015-04-20T16:29:13Z,CONTRIBUTOR,"Very cool indeed!

> it should make Theano integration a breeze!

The license prevents incorporating it directly in Theano, but it should be possible to do a separate module providing Theano Ops and registering the necessary optimizers. However, having a closer look at the code, the provided wrappers seem to assume the data is provided in fp16 format on CPU. For Theano, we'd instead need Ops for converting float32 to fp16 and back on the device (using code from [flexpt_ew.py](https://github.com/NervanaSystems/nervana-lib-gpu-performance-preview/blob/master/flexpt_ew.py), I guess), and Ops performing the different convolution (and possibly pooling) passes, as well as gemm. So there's still a bit of work to do.
I'm not really sure where to discuss this further; the license ties everything close to Nervana Systems (requiring prior approval of resulting publications), so maybe this should be discussed on their repository rather than Theano's. I'll keep that on my mind until some upcoming paper deadlines have passed and some Maxwell GPUs have arrived :)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38/comments,https://github.com/soumith/convnet-benchmarks/issues/38#issuecomment-94500758,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38
soumith,convnet-benchmarks,66667789,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/94518012,94518012,MDEyOklzc3VlQ29tbWVudDk0NTE4MDEy,1310570,2015-04-20T17:35:14Z,2015-04-20T17:35:14Z,OWNER,"@f0k The repo I benchmarked is: https://github.com/NervanaSystems/nervanagpu . It is private and you need to request access from the Nervana guys. In this new repo they have ops that take in float32 as well.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38/comments,https://github.com/soumith/convnet-benchmarks/issues/38#issuecomment-94518012,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38
soumith,convnet-benchmarks,66667789,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/94518363,94518363,MDEyOklzc3VlQ29tbWVudDk0NTE4MzYz,52591,2015-04-20T17:37:05Z,2015-04-20T17:37:05Z,NONE,"@f0k, the current license for the nervanagpu library is a place-holder. When we release, it will be an open source variant allowing you to integrate and publish freely. We have a big unrelated project in-house we need to complete in the next few weeks. We should have time afterwards to clean up and release this library. In the interim, I'd appreciate feedback under Issues in the nervanagpu repo.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38/comments,https://github.com/soumith/convnet-benchmarks/issues/38#issuecomment-94518363,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38
soumith,convnet-benchmarks,66667789,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/94522443,94522443,MDEyOklzc3VlQ29tbWVudDk0NTIyNDQz,180987,2015-04-20T17:48:17Z,2015-04-20T17:48:17Z,CONTRIBUTOR,"Just to note, we also have access to it and have part of it already wrapped
in Theano. If you have access to the Nervana code, I can give access to the
wrapper.

On Mon, Apr 20, 2015 at 1:37 PM, Amir Khosrowshahi <notifications@github.com

> wrote:
> 
> @f0k https://github.com/f0k, the current license for the nervanagpu
> library is a place-holder. When we release, it will be an open source
> variant allowing you to integrate and publish freely. We have a big
> unrelated project in-house we need to complete in the next few weeks. We
> should have time afterwards to clean up and release this library. In the
> interim, I'd appreciate feedback under Issues in the nervanagpu repo.
> 
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/issues/38#issuecomment-94518363
> .
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38/comments,https://github.com/soumith/convnet-benchmarks/issues/38#issuecomment-94522443,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38
soumith,convnet-benchmarks,66667789,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/94537730,94537730,MDEyOklzc3VlQ29tbWVudDk0NTM3NzMw,52591,2015-04-20T18:48:15Z,2015-04-20T18:48:15Z,NONE,"@soumith, hope I don't hijack your thread, but if you want access to the nervanagpu repo, please post here and I will add your github id. (Also please consider joining Nervana -- we are looking for engineers to help @scott-gray with these kernels.) 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38/comments,https://github.com/soumith/convnet-benchmarks/issues/38#issuecomment-94537730,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38
soumith,convnet-benchmarks,66667789,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/94564909,94564909,MDEyOklzc3VlQ29tbWVudDk0NTY0OTA5,629706,2015-04-20T20:50:09Z,2015-04-20T20:50:09Z,CONTRIBUTOR,"Now that's a wealth of great news! :) I don't have a Maxwell GPU to work with yet, but it can't hurt to have a look and follow along with the Theano integration. @khosra, I'd be glad to get access to the nervanagpu repository, thanks!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38/comments,https://github.com/soumith/convnet-benchmarks/issues/38#issuecomment-94564909,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38
soumith,convnet-benchmarks,66667789,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/94623543,94623543,MDEyOklzc3VlQ29tbWVudDk0NjIzNTQz,4626592,2015-04-21T03:03:28Z,2015-04-21T03:03:28Z,NONE,"sorry @soumith for being off-topic, but @khosra - is it possible for me to get access to nervanagpu repository as well? I would love to play around with them. I was a post doc at NYU, and have helped with porting caffe convolutions to theano at some pt (along with @f0k and others) and 3D convolution for torch. Thanks!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38/comments,https://github.com/soumith/convnet-benchmarks/issues/38#issuecomment-94623543,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38
soumith,convnet-benchmarks,66667789,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/94628196,94628196,MDEyOklzc3VlQ29tbWVudDk0NjI4MTk2,1310570,2015-04-21T03:34:59Z,2015-04-21T03:34:59Z,OWNER,"Completely forgot, @hughperkins 's DeepCL numbers added. DeepCL is unique in the sense that it's the only real contender at this point for OpenCL based deep learning, Huge props to Hugh for taking it upon himself to make all the AMD card owners happy. The numbers dont look very great compared to the CUDA based stuff, but the CUDA stuff started pretty slow as well. So in due time, I'm sure OpenCL will catch up to reasonable numbers.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38/comments,https://github.com/soumith/convnet-benchmarks/issues/38#issuecomment-94628196,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38
soumith,convnet-benchmarks,66667789,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/94702984,94702984,MDEyOklzc3VlQ29tbWVudDk0NzAyOTg0,8817449,2015-04-21T08:37:50Z,2015-04-21T08:37:50Z,NONE,"@khosra I would be interested as well! I have a Maxwell GPU and some free time in the next few weeks.

@soumith Interesting to see some OpenCL numbers as well. Thanks!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38/comments,https://github.com/soumith/convnet-benchmarks/issues/38#issuecomment-94702984,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38
soumith,convnet-benchmarks,66667789,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/94727011,94727011,MDEyOklzc3VlQ29tbWVudDk0NzI3MDEx,123560,2015-04-21T10:02:06Z,2015-04-21T10:02:06Z,CONTRIBUTOR,"Great!  Thank-you Soumith.  Really great to have a place on your benchmarking board :-)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38/comments,https://github.com/soumith/convnet-benchmarks/issues/38#issuecomment-94727011,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38
soumith,convnet-benchmarks,66667789,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/95086741,95086741,MDEyOklzc3VlQ29tbWVudDk1MDg2NzQx,9136322,2015-04-22T09:14:41Z,2015-04-22T09:14:41Z,NONE,"Good job! Thanks
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38/comments,https://github.com/soumith/convnet-benchmarks/issues/38#issuecomment-95086741,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38
soumith,convnet-benchmarks,66667789,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/98592338,98592338,MDEyOklzc3VlQ29tbWVudDk4NTkyMzM4,52591,2015-05-04T05:40:19Z,2015-05-04T05:40:19Z,NONE,"@soumith, we made our library public with Apache license. Again, it's here if anyone wants to try it: https://github.com/NervanaSystems/nervanagpu.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38/comments,https://github.com/soumith/convnet-benchmarks/issues/38#issuecomment-98592338,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38
soumith,convnet-benchmarks,66667789,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/111952823,111952823,MDEyOklzc3VlQ29tbWVudDExMTk1MjgyMw==,6687176,2015-06-15T06:42:33Z,2015-06-15T06:42:33Z,NONE,"Gooood job!  I wanna to know how about the cxxnet rank in these benchmark. And how about train a same vgg-like model on cifar dataset?  the total train time is worth comparable. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38/comments,https://github.com/soumith/convnet-benchmarks/issues/38#issuecomment-111952823,https://api.github.com/repos/soumith/convnet-benchmarks/issues/38
soumith,convnet-benchmarks,66555800,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/90101094,90101094,MDEyOklzc3VlQ29tbWVudDkwMTAxMDk0,1310570,2015-04-06T15:19:46Z,2015-04-06T15:19:46Z,OWNER,"thanks a lot Hugh, I spent some time over the weekend looking at the code. It's great that you prepared the benchmark for me. I'll run it now.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/37/comments,https://github.com/soumith/convnet-benchmarks/pull/37#issuecomment-90101094,https://api.github.com/repos/soumith/convnet-benchmarks/issues/37
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/78167815,78167815,MDEyOklzc3VlQ29tbWVudDc4MTY3ODE1,10279780,2015-03-10T23:04:57Z,2015-03-11T00:47:11Z,NONE,"So to make some concrete progress on this, I think we need a really simple textbook problem and test data set, suitable for LSTM.

At the moment the simplest problem I can think of is [predicting the next bit](http://arxiv.org/pdf/1410.5401v2.pdf) in a dynamic binary n-gram, based on a betabinomial distribution. The advantage of this is we're using a simple generative model, and so we can calculate the optimal estimator, hence the cost _exactly_.  Also this simple Markov chain model is nice because its a first step towards latent variable generative models. 

All the details for this are given in Kevin Murphy's textbook, [Machine Learning
A Probabilistic Perspective](http://www.cs.ubc.ca/~murphyk/MLbook/), 2012 edition, sections 3.3.4 p77 and 17.2.2 p591, so it should be very easy to code up and test this sequence generator in octave/matlab/R, and then to port it over to lua, python or C++.

The Torch LSTM code is given in the slides for this [youtube video](https://www.youtube.com/watch?v=56TYLaQN4N8), which explains it a bit.

The point of this exercise is - I don't fully understand the above Torch LSTM code, (and _no one has given me a convincing explanation yet_), and so I want one very simple 'clean' example where I understand _every_ line of the code. 

Once I've done this in Torch, I can try to solve this problem using Theano, hence get clean benchmark implementations using both calculators, using the same optimizers. In particular I want to do this without using any extra 'junk', so the comparisons between the two calculators are very clear, (so I can spot if I've made a mistake)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-78167815,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/78204773,78204773,MDEyOklzc3VlQ29tbWVudDc4MjA0Nzcz,1310570,2015-03-11T04:50:28Z,2015-03-11T04:50:28Z,OWNER,"I think you're powering through this faster than I am. The only thing that I would suggest is that make the benchmark on what people think matters most, i.e. benchmark RNN-LSTM configurations from published papers. I am happy to help in any way.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-78204773,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/78204804,78204804,MDEyOklzc3VlQ29tbWVudDc4MjA0ODA0,1310570,2015-03-11T04:50:57Z,2015-03-11T04:50:57Z,OWNER,"*published and hotly discussed papers, papers that matter to the audience now and today.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-78204804,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/78279465,78279465,MDEyOklzc3VlQ29tbWVudDc4Mjc5NDY1,10279780,2015-03-11T14:57:30Z,2015-03-11T22:44:57Z,NONE,"Some new open source code for LSTM in Torch, from the Oxford University Computer Science Masters Course.

https://github.com/oxford-cs-ml-2015/practical6

It looks pretty good, seems to be based a lot on [learning to execute](https://github.com/wojciechz/learning_to_execute)? I don't understand it yet, so all insights are very warmly welcomed :+1: 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-78279465,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/78289182,78289182,MDEyOklzc3VlQ29tbWVudDc4Mjg5MTgy,1310570,2015-03-11T15:42:16Z,2015-03-11T15:42:16Z,OWNER,"It uses [nngraph](https://github.com/torch/nngraph), which I guess you are already familiar with, looking at your comments from the NTM repository. 

It is simply constructing the LSTM cell for a given time-step. 

I am not sure of that particular repository (oxford-cs-ml-2015), but learning to execute and [lstm](https://github.com/wojzaremba/lstm) work out of the box afaik.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-78289182,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/78372388,78372388,MDEyOklzc3VlQ29tbWVudDc4MzcyMzg4,180987,2015-03-11T21:10:05Z,2015-03-11T21:10:05Z,CONTRIBUTOR,"Just to tell that for Theano, there is a tutorial with a working lstm
example there:

http://deeplearning.net/tutorial/lstm.html

I won't have time to work on it, but I can answer questions on the code.

On Wed, Mar 11, 2015 at 12:50 AM, Soumith Chintala <notifications@github.com

> wrote:
> 
> *published and hotly discussed papers, papers that matter to the audience
> now and today.
> 
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-78204804
> .
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-78372388,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/78393578,78393578,MDEyOklzc3VlQ29tbWVudDc4MzkzNTc4,10279780,2015-03-11T23:15:30Z,2015-03-12T05:38:46Z,NONE,"Hey Soumith,

> I am happy to help in any way.

Thanks man for the show of interest and support - hopefully I'll get some results in the next few days!

> It is simply constructing the LSTM cell for a given time-step. 

Yes I think so too. They've chosen a very ambitious problem, and made it very complicated for a tutorial? Not sure a student's going to get much out of just running the code? 

Their hearts in the right place though, seem's they've tried to make the code more like the mathematical/verbal description of a LSTM. I'm sympathetic to these guy's because they tried to make some difficult code, easier to understand, and more flexible too. 

I'm not sure it's a good idea to start with a multilayer LSTM. I'm going to start with a one layer LSTM - that's the simplest config to start with - best to start simple and get results. I know I'm _shallow_ 

This is the Oxford code for a single layer, with 256 cells, given in [LSTM.lua](https://github.com/oxford-cs-ml-2015/practical6/blob/master/LSTM.lua) 

![lstm_master_cell](https://cloud.githubusercontent.com/assets/10279780/6609199/91eafd22-c846-11e4-84ef-0c45f8c67f60.jpg)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-78393578,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/78397875,78397875,MDEyOklzc3VlQ29tbWVudDc4Mzk3ODc1,10279780,2015-03-11T23:48:40Z,2015-03-12T10:47:42Z,NONE,"Hi FrÃ©dÃ©ric,

> I won't have time to work on it, but I can answer questions on the code.

_that's great that you're offering to support Theano!_

At the moment I'm not very productive because everything I do with one calculator (Torch or Theano), I have to do with the other, and I'm quite impatient to get results! 

So at the moment, the two _single layer_ LSTM pieces of code I'm working on are,
- Shawn Tan's single layer [LSTM](https://github.com/shawntan/neural-qa/blob/master/lstm.py), which he's applying to Soumiths paper [Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks](http://arxiv.org/pdf/1502.05698v3.pdf), very interesting and cool work, but a bit too ambitious for me at the moment :)
- The Oxford [LSTM Torch code](https://github.com/oxford-cs-ml-2015/practical6/blob/master/LSTM.lua) which was just released this morning. 

So to make some quick progress here, may I ask do you know of any simple tests, using RNG based sequence generators, I could apply to check the code of a single layer LSTM?
The only thing I can think of is binary, n-grams with n=3 or 4, as I don't think a single layer LSTM would work for more complex n-grams. I'd prefer not to use any datasets, and simply to use a probabilistic sequence, which I can calculate the optimal (Bayesian) estimator for - it just seems to be the simplest LSTM configuration and purest test to start with?

Best regards 

Aj. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-78397875,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/78406821,78406821,MDEyOklzc3VlQ29tbWVudDc4NDA2ODIx,4028979,2015-03-12T01:15:28Z,2015-03-12T01:15:28Z,NONE,"@AjayTalati 

I don't fully understand what you mean by ""no internal state tracking"". If there's no internal state, by definition is not a recurrent neural network (hence, no LSTM.) What did you mean by this?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-78406821,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/78422281,78422281,MDEyOklzc3VlQ29tbWVudDc4NDIyMjgx,10279780,2015-03-12T04:30:17Z,2015-03-12T06:26:38Z,NONE,"Hi, kyunghyuncho 

yes you're absolutely right, my wording doesnâ€™t make any sense? 

I'm interested in using LSTM for encoder and decoder networks. For this particular application any RNN will do, but LSTM are more interesting.

So let RNN^enc be a function enacted by the encoder network at a single time-step t. It outputs a hidden vector h^enc_t, and receives some input vector x, and the previous encoder hidden vector
h^enc_t-1. For my problem that's all I need, for the other calculations in the system. 

So I guess to clarity things, if I want to uses LSTM networks for the encoder (and decoder), I need to add that LSTM^enc in addition, outputs c^enc_t and receives c^enc_t-1, but these are not used for any other calculations in the system, (except for propagating the LSTM).   

Does that help/make sense?

I'm very happy to work with people in both the Torch and Theano communities on this. 

Thanks a lot for your helpful comment,

Cheers,

Aj
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-78422281,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/78466957,78466957,MDEyOklzc3VlQ29tbWVudDc4NDY2OTU3,4028979,2015-03-12T11:59:13Z,2015-03-12T11:59:13Z,NONE,"I think I understand what you meant by 'internal state tracking'. So, essentially, you want to do language modelling (next symbol prediction) with RNN (but not necessarily with LSTM). 

Now, let me restate what you want: a small task (dataset) and two different implementations of RNN (implemented using Theano and Torch). Is this right?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-78466957,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/78471203,78471203,MDEyOklzc3VlQ29tbWVudDc4NDcxMjAz,10279780,2015-03-12T12:33:32Z,2015-03-12T15:40:25Z,NONE,"Hi, think you've made the mistake of trying to fit applications to the test problems I suggested? 

> I think I understand what you meant by 'internal state tracking'. So, essentially, you want to do  language modelling (next symbol prediction) with RNN (but not necessarily with LSTM). 

I have no real applications in mind for my work. At the moment I'm actually working on just generally building recurrent variational auto-encoders. So variational autoencoders were introduced by Durk Kingma & Max Welling recently. They originally used MLP for approximating the posterior, but this was for simple illustration purposes, and their theory is much more general. It's possible to use a RNN, (or a LSTM), as the approximator for the posterior, and that gives a new class of variational auto encoders, suitable for generative time series modelling, (which as a special case could be based on Gaussian latent variable models). If you are interested, I suggest Justin Bayers recently updated paper,

[LEARNING STOCHASTIC RECURRENT NETWORKS](http://arxiv.org/pdf/1411.7610v3.pdf)

I think he used python/theano for his examples, as well as Alex Graves, now classic report,

[GENERATING SEQUENCES WITH RECURRENT NEURAL NETWORKS](http://arxiv.org/pdf/1308.0850v5.pdf)  

Which I'm sure you would have read.

> Now, let me restate what you want: a small task (dataset) and two different implementations of RNN (implemented using Theano and Torch). Is this right?

Yes, that's close to what I would like to do. In order to test my code I'd prefer a Markov Chain model, rather than using a real dataset. So flicking through my textbooks, the one simple sequence generation example that I remembered which had a simple form for the optimal estimator was the beta-binomial. All the details for this are given in Kevin Murphy's textbook, [Machine Learning A Probabilistic Perspective](http://www.cs.ubc.ca/~murphyk/MLbook/), 2012 edition, sections 3.3.4 p77 and 17.2.2 p591. 

This is also task 4.4 in the [Neural Turing Machine paper](http://arxiv.org/pdf/1410.5401v2.pdf).
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-78471203,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/78497681,78497681,MDEyOklzc3VlQ29tbWVudDc4NDk3Njgx,3520613,2015-03-12T15:04:20Z,2015-03-12T15:04:20Z,CONTRIBUTOR,"@nouiz @kyunghyuncho good job on that LSTM tutorial guys. Looks great.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-78497681,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/78511922,78511922,MDEyOklzc3VlQ29tbWVudDc4NTExOTIy,4028979,2015-03-12T16:07:56Z,2015-03-12T16:07:56Z,NONE,"@nicholas-leonard  Thanks! And, for proper credit assignment, thanks to @carriepl 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-78511922,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/78521761,78521761,MDEyOklzc3VlQ29tbWVudDc4NTIxNzYx,10279780,2015-03-12T16:42:34Z,2015-03-12T16:45:04Z,NONE,"@nouiz @kyunghyuncho may I ask if you guys are interested in coding a LSTM variational autoencoder using Theano? Is this a project which would be fun for you both, and other Theano people?

I definitely am committed to doing it, and am actively working on it in Torch. I know @nicholas-leonard wants something like this for DP, and I think @soumith also likes this.

If everyone is happy then maybe developing a LSTM variational autoencoder would be a good benchmark problem to work on?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-78521761,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/79012726,79012726,MDEyOklzc3VlQ29tbWVudDc5MDEyNzI2,10279780,2015-03-13T14:54:51Z,2015-03-13T14:54:51Z,NONE,"So coding the LSTM variational autoencoder is straightforward using `nngraph` and the tricks we learnt from Oxford University on Wednesday. 

I'm guessing that it's going to be fairly straightforward in Theano too? So if @kyunghyuncho and @nouiz don't want to do it, I might give it ago when I have my full system working and tested in Torch. 

At the moment this seems to be the most interesting benchmark problem - one I could do on my own anyway. 

Here's the computational graph. Inputs from t-1 are in `lime`, outputs for this time step t are in `cyan`, and the `gold` bit is where the _Monte Carlo magic happens_ 

The `LSTM encoder` is the top group of nodes, the middle bit is where the `sampling of Q(z|x)` takes place, and the bottom set of node is the `LSTM decoder` 

![system_forward_graph](https://cloud.githubusercontent.com/assets/10279780/6640625/9c625d9a-c990-11e4-9384-3e3c16b1757f.jpeg)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-79012726,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/79017635,79017635,MDEyOklzc3VlQ29tbWVudDc5MDE3NjM1,4028979,2015-03-13T14:59:47Z,2015-03-13T14:59:47Z,NONE,"Unfortunately, I don't have time to implement STORN at the moment.

On Fri, Mar 13, 2015 at 10:54 AM, Ajay Talati notifications@github.com
wrote:

> So coding the LSTM variational autoencoder is straightforward using
> nngraph and the tricks we learnt from Oxford University on Wednesday.
> 
> I'm guessing that it's going to be fairly straightforward in Theano too?
> So if @kyunghyuncho https://github.com/kyunghyuncho and @nouiz
> https://github.com/nouiz don't want to do it, I might give it ago when
> I have my full system working and tested in Torch.
> 
> At the moment this seems to be the most interesting benchmark problem -
> one I could do on my own anyway.
> 
> Here's the computational graph. Inputs from t-1 are in lime, outputs for
> this time step t are in cyan, and the gold bit is where the _Monte Carlo
> magic happens_
> 
> The LSTM encoder is the top group of nodes, the middle bit is where the sampling
> of Q(z|x) takes place, and the bottom set of node is the LSTM decoder
> 
> [image: system_forward_graph]
> https://cloud.githubusercontent.com/assets/10279780/6640625/9c625d9a-c990-11e4-9384-3e3c16b1757f.jpeg
> 
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-79012726
> .
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-79017635,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/79474731,79474731,MDEyOklzc3VlQ29tbWVudDc5NDc0NzMx,180987,2015-03-13T22:18:08Z,2015-03-13T22:18:08Z,CONTRIBUTOR,"Same for me. I'll be busy for a few weeks at least. But I can answer
question on the code.
Le 13 mars 2015 10:59, ""Kyunghyun Cho"" notifications@github.com a Ã©crit :

> Unfortunately, I don't have time to implement STORN at the moment.
> 
> On Fri, Mar 13, 2015 at 10:54 AM, Ajay Talati notifications@github.com
> wrote:
> 
> > So coding the LSTM variational autoencoder is straightforward using
> > nngraph and the tricks we learnt from Oxford University on Wednesday.
> > 
> > I'm guessing that it's going to be fairly straightforward in Theano too?
> > So if @kyunghyuncho https://github.com/kyunghyuncho and @nouiz
> > https://github.com/nouiz don't want to do it, I might give it ago when
> > I have my full system working and tested in Torch.
> > 
> > At the moment this seems to be the most interesting benchmark problem -
> > one I could do on my own anyway.
> > 
> > Here's the computational graph. Inputs from t-1 are in lime, outputs for
> > this time step t are in cyan, and the gold bit is where the _Monte Carlo
> > magic happens_
> > 
> > The LSTM encoder is the top group of nodes, the middle bit is where the
> > sampling
> > of Q(z|x) takes place, and the bottom set of node is the LSTM decoder
> > 
> > [image: system_forward_graph]
> > <
> > https://cloud.githubusercontent.com/assets/10279780/6640625/9c625d9a-c990-11e4-9384-3e3c16b1757f.jpeg
> > 
> > â€”
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-79012726
> > 
> > .
> 
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-79017635
> .
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-79474731,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/79503987,79503987,MDEyOklzc3VlQ29tbWVudDc5NTAzOTg3,10279780,2015-03-13T22:42:36Z,2015-03-13T22:42:36Z,NONE,"Hi @nouiz and @kyunghyuncho 

No problem guy's I can understand that this is not a high priority for anyone. 

It's really nice that you both took the time to show your interest - I appreciate that a lot. 

The general approach of checking things using a different calculation method, is important to me so if I run into problems with my Torch system - which is likely - I'll definitely fall back to a Theano implementation. 

A working bare bones system is less than 250 lines of Torch code, I expect it would be about the same for Theano. I'm happy to explain it to you, as well as help to implement it. Hopefully we can catch up in a couple of months.

Best regards,

Aj   
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-79503987,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/79511224,79511224,MDEyOklzc3VlQ29tbWVudDc5NTExMjI0,4028979,2015-03-13T22:48:59Z,2015-03-13T22:48:59Z,NONE,"Ajay, feel free to contact me or @nouis, if you run into any issue
implementing LSTM with Theano.

On Fri, Mar 13, 2015 at 6:42 PM, Ajay Talati notifications@github.com
wrote:

> Hi @nouiz https://github.com/nouiz and @kyunghyuncho
> https://github.com/kyunghyuncho
> 
> No problem guy's I can understand that this is not a high priority for
> anyone.
> 
> It's really nice that you both took the time to show your interest - I
> appreciate that a lot.
> 
> The general approach of checking things using a different calculation
> method, is important to me so if I run into problems with my Torch system -
> which is likely - I'll definitely fall back to a Theano implementation.
> 
> A working bare bones system is less than 250 lines of Torch code, I expect
> it would be about the same for Theano. I'm happy to explain it to you, as
> well as help to implement it. Hopefully we can catch up in a couple of
> months.
> 
> Best regards,
> 
> Aj
> 
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-79503987
> .
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-79511224,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/84559085,84559085,MDEyOklzc3VlQ29tbWVudDg0NTU5MDg1,10279780,2015-03-22T08:39:06Z,2015-03-22T09:48:20Z,NONE,"Hey @nicholas-leonard and  @soumith how's it going? I've got a torch version of the [DRAW paper](http://arxiv.org/pdf/1502.04623v1.pdf) implemented as development code. I'm still testing the attention mechanisms for the full version of the paper, but training this thing takes _forever_!  

I was just wondering if you guys, (or anyone else), wants to help out with optimizing the code and testing it? I'm a problem solver, (research physicist/mathematician), not a pro software engineer or a data scientist. So if you guys can take it to the consumer/pro level, we could get a test going of it verses this [Theano/Blocks implementation of the DRAW paper](https://github.com/jbornschein/draw). I've got no idea of how `Blocks` works, don't have the time to read it's code, and clueless how to adapt it to different problems? 

Fair play to them though, annealing the Adam learning rate, setting the number of glimpses/iterations/cloning of the LSTM sequence to 10, (I was using 64), and their KLD plot are all really cool, and I'm looking forward to trying this stuff :+1:  

To be honest I like my development code with all its readability, error catching, and print outs :+1: - at least I wrote it all, understand it, and can quickly diagnose/change it & use it for other projects :100: I've got some interesting modifications/applications in the pipeline :+1:  It's bare bones, all the modules are written as required, (there's no extra junk), the only required torch packages are `nn` and `nngraph`. So it should be possible to easily fit it into [DP](https://github.com/nicholas-leonard/dp), and the new [rnn](https://github.com/nicholas-leonard/rnn) library, which would be a perfect collaboration and environment for testing it and some other cool stuff  :+1: 

Best,

Aj    

![sampler_module_backward_graph](https://cloud.githubusercontent.com/assets/10279780/6768556/7e50f73a-d06b-11e4-93b2-2536d568740f.jpg)

![encoder_module_backward_graph](https://cloud.githubusercontent.com/assets/10279780/6768591/0a62a1f0-d06d-11e4-9574-3b28e8ef0970.jpg)

![decoder_module_backward_graph](https://cloud.githubusercontent.com/assets/10279780/6768606/e955471e-d06d-11e4-89ad-aa58ba4a2e1b.jpg)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-84559085,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/84635853,84635853,MDEyOklzc3VlQ29tbWVudDg0NjM1ODUz,1310570,2015-03-22T15:41:30Z,2015-03-22T15:41:30Z,OWNER,"@AjayTalati if you give me access today, i can help you optimize it. I have some free time today. This is awesome, that you've gotten everything up and working. Let's chat on gitter.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-84635853,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/84649508,84649508,MDEyOklzc3VlQ29tbWVudDg0NjQ5NTA4,10279780,2015-03-22T16:33:40Z,2015-03-22T20:25:41Z,NONE,"Sorry guys, I'm getting bad results, AGAIN? 

Seems I'm running into instabilities, of my encoder cell values, after the middle of training. That's `c_enc_t` in the above. For a few small toy data sets, I tried it worked OK - but for the reasonable sized training sets it's screwed. 

Durk Kingma, just posted some great great advice - [here](https://github.com/y0ast/VAE-Torch/issues/3). _That guy's really smart!_ So I'm going to follow his advice, and try using [batch normalization](http://arxiv.org/pdf/1502.03167.pdf) - _this might take a while_?

To be honest batch norm looks a bit complicated/involved, I might just try using a simple L2 cost penalty on the terminal cell values of the of the encoder, `||c_enc_T||_2` ? _I like simple things_ 

Bottom line - it ain't working, and ready for you guys to optimize yet - maybe next weekend - apologies. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-84649508,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/97798047,97798047,MDEyOklzc3VlQ29tbWVudDk3Nzk4MDQ3,2623134,2015-04-30T13:46:52Z,2015-04-30T13:47:02Z,NONE,"I didn't read the entire thread, but here is a few suggestions.
I did a DRAW implementation in theano it is available here: https://github.com/skaae/lasagne-draw
Feel free to ask if you have any questions.

I modified the oxford LSTM code a little and got a ~40% speedup by concatenating the dot products and slicing. 
https://gist.github.com/skaae/ea4320e17379d408e693
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-97798047,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/97807133,97807133,MDEyOklzc3VlQ29tbWVudDk3ODA3MTMz,10279780,2015-04-30T14:04:29Z,2015-04-30T17:35:16Z,NONE,"Hi Soren,

thanks a lot for the suggestions - I've got a Torch implementation of DRAW, with attention working now. 

I'm just trying to train it for [two & three digit MNIST](https://github.com/deepmind/mnist-cluttered) at the moment. To be honest I've found training DRAW with attention for two digits to be much more time consuming than for one digit MNIST - it's taken me two weeks of trial & error, to get it looking right? In the end I've got fed up and added dropout and got better results - still training but will post a video latter.

![training_draw_with_attention_on_2_digit_mnist](https://cloud.githubusercontent.com/assets/10279780/7415703/8ed7cbfa-ef52-11e4-877d-ced64a254c6f.png)

Thank you very much for the optimized LSTM code, I'll try it later, and will give you feedback. Hopefully I'll be able to tidy up my code and post it on Github soon!

Best regards,

Aj

 Edit - I just saw that you've managed to generate digits conditioned on their class - your [second plot](https://github.com/skaae/lasagne-draw) - that's REALLY COOL :+1: I have'nt done that yet? How did you do it?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-97807133,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/97835257,97835257,MDEyOklzc3VlQ29tbWVudDk3ODM1MjU3,2623134,2015-04-30T15:17:17Z,2015-04-30T15:17:17Z,NONE,"I  concatenated the latent z with y during training. Then you can conditionally generate digits by clamping y during the generation. On what hard ware are you training the network? I did it on a K40 which took around 2 days. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-97835257,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/97886046,97886046,MDEyOklzc3VlQ29tbWVudDk3ODg2MDQ2,10279780,2015-04-30T17:21:06Z,2015-05-03T22:09:30Z,NONE,"> I concatenated the latent z with y during training. Then you can conditionally generate digits by clamping y during the generation. 

That's really cool thanks :+1: I'll have to have a think about how to implement it?

Unfortunately it takes forever to train 2 digit MNIST - at least with my implementation. IIRC one digit MNIST with attention, and 64 timesteps, and 256 LSTM cells, took 2 days as well! Sorry my wording above was poor, it's taken me 2 weeks of trial and error in hyper parameter tuning and messing around to get a stable system. In terms of actual training time, it takes about 48 hours in wall clock time, and about 25K minibatches, (2.5 million training images) to get a decent system for 2 digit MNIST. 

I'm not too clued up on hardware, this is what I think is in the box?
- an old (1.5 compute capability), NVIDIA GTX 570. 
- 8 core's
- 16 GB

Cheer's 

Aj
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-97886046,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/98253180,98253180,MDEyOklzc3VlQ29tbWVudDk4MjUzMTgw,10279780,2015-05-01T22:15:51Z,2015-05-04T00:08:18Z,NONE,"Hey folks :)

here's a [video of the best 2 digit MNIST model](https://youtu.be/EgeLflrN8X0) I've managed to train so far with my Torch implementation of DRAW. Not great? 

I was just wondering if anyone with a Theano implementation of DRAW has managed to train anything better on this data set?  

It took 3 days to train and got a negLL lower bound of 223 in the end. The hyperparamters are about the same as in [section 4.3 of the DRAW paper](http://arxiv.org/pdf/1502.04623.pdf), except I added dropout -

imagesize - 60x60
dataset - MNIST 28 binarized
timesteps/glimpses - 32 
dropout mask prob - 0.15
ADAM learning rate - 3e-4
minibatchsize - 100

To date I still hav'nt managed to train a good 64 timestep model on the 2 digit MNIST dataset, and it's REALLY BUGGING me :( !!! I'm guessing that a 64 timestep model would get a negLL bound of sub 200 on the 2 digit dataset, but that's conjecture at the moment.

For any implementation (Torch or Theano), I think you need to get a sub 200 negLL on this dataset to get any credibility for your implementation. If you can do that, it's fairly convincing that your system actually works :+1: At the moment I'm guessing this would take around 30K minibatches, (or more), so at a rate of 10K minibatches per day, it's going to be a long 3 or 4 days :-1:   
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-98253180,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/98253761,98253761,MDEyOklzc3VlQ29tbWVudDk4MjUzNzYx,1310570,2015-05-01T22:20:56Z,2015-05-01T22:20:56Z,OWNER,"whats the one on the left vs the one on the right?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-98253761,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/98256123,98256123,MDEyOklzc3VlQ29tbWVudDk4MjU2MTIz,10279780,2015-05-01T22:35:10Z,2015-05-02T10:24:59Z,NONE,"The one on the left is the input image, (the data), that's presented to the DRAW reader network. The one on the right is the reconstructed image, (probabilistic model of the data), generated by the DRAW writer network. 

So the reader with attention focuses where the green box is on the input image, the one on the left. The output of the reader goes into the encoder, which sends a code, via a noisy channel, to the decoder, which controls the writer. The writer focuses it's attention where the red box is, adding/subtracting probability mass to the reconstructed image at each timestep. The decoder also controls where the reader focuses it's attention on the next timestep. 

Apologies mate - it's not that easy to explain ???

Edit - I think the paper has a different way of animating/defining the green and red bounding boxes - this is just something I hacked together on the fly?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-98256123,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/98798988,98798988,MDEyOklzc3VlQ29tbWVudDk4Nzk4OTg4,3520613,2015-05-04T18:05:11Z,2015-05-04T18:05:11Z,CONTRIBUTOR,"Can it sample double digits from random latent variables instead of an
input?

Nicholas LÃ©onard
450-210-1214

On Fri, May 1, 2015 at 6:35 PM, Ajay Talati notifications@github.com
wrote:

> The one on the left is the input image, (the data), that's presented to
> the DRAW reader network. The one on the right is the reconstructed image,
> (probabilistic model of the data), generated by the DRAW writer network.
> 
> So the reader with attention focuses where the green box is on the input
> image, the one on the left. The output of the reader goes into the encoder,
> which sends a code to the decoder, which controls the writer. The writer
> focuses it's attention where the red box is, adding/subtracting probability
> mass to the reconstructed image at each timestep.
> 
> Apologies mate - it's not that easy to explain ???
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-98256123
> .
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-98798988,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/98831931,98831931,MDEyOklzc3VlQ29tbWVudDk4ODMxOTMx,10279780,2015-05-04T19:50:38Z,2015-05-06T01:44:47Z,NONE,"Hi Nick, 

yes it can do that now. For two digits, picked out of the set `(0,1,2,,,,9)`, there are 55 possible unordered combinations with repetition, i.e. (11!)/(2!9!) = 55. 

If I train the network on input images, and this classification vector of length 55 - as @skaae described above - for stochastic data generation, I can pick any of the 55 classes, and generate a probabilistic model for it. So I could generate a fantasy/probabilistic model for say the unordered combination `0` and `7`, if requested.

I'm just trying to train a 64 timestep system at the moment, because the quality of the 55 fantasy pairs of digits, is not so good for the 32 timestep systems I've trained? Going from 32 to 64 timesteps is turning out to be not so easy? LOTS OF NUMERICAL PROBLEMS, especially towards the end of training, when it starts to overfit ????
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-98831931,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/106280964,106280964,MDEyOklzc3VlQ29tbWVudDEwNjI4MDk2NA==,4983224,2015-05-28T11:15:09Z,2015-05-28T11:15:09Z,NONE,"Hi!

I hope you @AjayTalati are still having interests in implementing DRAW. 

I'm having trouble with understanding the selective attention model in DRAW. I'm not sure here is the write place to ask, but it seems the only place where people showed their interests in DRAW implementation. 

Specifically, I want to understand 1) how Fig. 3 in the DRAW paper is drawn and 2) why the authors did not normalize w_t in eq. 28. 

As far as I understood, the differentiable attention mechanisms (esp. the ones in the two ref papers mentioned in DRAW paper) are basically Gaussian mixture. In that sense, I have no idea how to draw Fig. 3 (clean squared rectangle). Is the rectangle just drawn with the size of (N-1)_delta x (N-1)_delta and the thickness of sigma?

In the same sense, considering the soft attention as Gaussian mixture, it is natural to normalize w_t in eq. 28; however, it didn't. I think there should be a reason, but I couldn't find it. 

Can anyone help me to understand the paper properly?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-106280964,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/106300774,106300774,MDEyOklzc3VlQ29tbWVudDEwNjMwMDc3NA==,10279780,2015-05-28T12:55:04Z,2015-05-28T13:07:20Z,NONE,"Hi @lim0606 

The size of the zoomable attention boxes in the video and fig 3, are controlled by the stride eqns 24 and 21. These are trainable functions of the previous decoders output `h^dec_t-1` for the reader, and the current decoders output `h^dec_t` for the writer.

`w_t` in eqn 28 is a NxN patch of probability mass that the decoder wants to add to the canvas IIRC, not a weight. The writers filterbank matrices in eqn 29 control where on the canvas this probability mass is added/substracted. 

Have a look at Jorg's implementation [here](https://github.com/jbornschein/draw/blob/master/draw/attention.py), it will be much easier for you to understand than my code!

Hope that helps,

Cheers,

Aj
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-106300774,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,60287270,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/107128042,107128042,MDEyOklzc3VlQ29tbWVudDEwNzEyODA0Mg==,4983224,2015-05-31T04:53:48Z,2015-05-31T04:53:48Z,NONE,"Thanks a lot @AjayTalati 

I think I get it :)

Jaehyun Lim 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36/comments,https://github.com/soumith/convnet-benchmarks/issues/36#issuecomment-107128042,https://api.github.com/repos/soumith/convnet-benchmarks/issues/36
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/65975736,65975736,MDEyOklzc3VlQ29tbWVudDY1OTc1NzM2,1310570,2014-12-08T04:47:37Z,2014-12-08T04:47:37Z,OWNER,"Here are the CuDNN, Torch7-CuNN numbers:
https://github.com/soumith/convnet-benchmarks/blob/master/torch7/imagenet_winners/output.log

The same directory contains the benchmark script, and the model configurations for all the networks.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-65975736,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/65992667,65992667,MDEyOklzc3VlQ29tbWVudDY1OTkyNjY3,127987,2014-12-08T06:02:03Z,2014-12-08T06:02:03Z,CONTRIBUTOR,"(y)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-65992667,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/66041908,66041908,MDEyOklzc3VlQ29tbWVudDY2MDQxOTA4,43829,2014-12-08T09:21:26Z,2014-12-08T17:47:06Z,NONE,"I can try coding up those first 3 probably, but GoogLeNet in Theano is still a challenge at the moment, and it would require some tricks that would slow it down unnecessarily (like the 3x3s1 pooling that retains the size of the input, for example). So it's probably not worth spending time on that just yet. I have a lot on my plate at the moment but hopefully I'll find some time for it this week.

EDIT: actually, looking into this, Theano's support for 'same' convolutions / arbitary zero padding is not good enough yet for this to be worth the effort. Some of the individual implementations support it now, but the overarching `T.nnet.conv.conv2d` only supports 'valid' and 'full' modes, so that rules out the metaoptimizer for example. There was discussion about implementing this recently, but as far as I can tell it hasn't happened: https://github.com/Theano/Theano/issues/2118
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-66041908,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/67434122,67434122,MDEyOklzc3VlQ29tbWVudDY3NDM0MTIy,1310570,2014-12-18T02:48:59Z,2014-12-18T02:48:59Z,OWNER,"a small preview on alexnet and overfeat:
fbcufft is the winner, and cuda-convnet2 is a close second. cudnn does not even come close, and I am expecting caffe to be in the cudnn territory. Anyways, I will be doing the caffe numbers as well, working on it.
https://gist.github.com/soumith/e6297e93dd2fe3751562

I made a mistake in the first round of cuda-convnet2 numbers. I did not play around with the partialSum setting, it makes a real difference in the gradParameters computation. I use the ideal settings proposed by alex in this file:
https://code.google.com/p/cuda-convnet2/source/browse/layers/layers-imagenet-1gpu.cfg
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-67434122,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/67434166,67434166,MDEyOklzc3VlQ29tbWVudDY3NDM0MTY2,1310570,2014-12-18T02:49:35Z,2014-12-18T02:49:35Z,OWNER,"fbcufft's source is here btw: https://github.com/facebook/fbcunn/tree/master/layers/cuda/fft
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-67434166,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/67434297,67434297,MDEyOklzc3VlQ29tbWVudDY3NDM0Mjk3,4626592,2014-12-18T02:51:45Z,2014-12-18T02:51:45Z,NONE,"Wow! Looks like we should port this to Theano!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-67434297,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/67458383,67458383,MDEyOklzc3VlQ29tbWVudDY3NDU4Mzgz,43829,2014-12-18T09:02:02Z,2014-12-18T09:02:37Z,NONE,"Awesome! Very happy the FFT approach is still being pursued and it's starting to bear fruit :) It's also cool that it's using CuFFT and doesn't require a custom implementation, which should make it resilient to changes in GPU architectures, just like the Caffe/cudnn approach.

Compared to Theano's Python implementation Is there anything special that was done to make it run fast and efficiently? Or is it just the change from Python to C++ that makes it both faster and more memory efficient? I'm curious :) Is there anywhere I can read about this? (besides the source code that is...)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-67458383,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/67507310,67507310,MDEyOklzc3VlQ29tbWVudDY3NTA3MzEw,1310570,2014-12-18T16:01:19Z,2014-12-18T16:01:19Z,OWNER,"We are releasing a paper on this, stay tuned. It was quite a piece of art to get it right, mostly work done by my colleague Nicholas vasilache
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-67507310,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/67508134,67508134,MDEyOklzc3VlQ29tbWVudDY3NTA4MTM0,43829,2014-12-18T16:05:19Z,2014-12-18T16:05:19Z,NONE,"Cool, would love to see a preprint of that at some point, if you and your colleagues are willing to share :)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-67508134,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/67508285,67508285,MDEyOklzc3VlQ29tbWVudDY3NTA4Mjg1,1310570,2014-12-18T16:06:04Z,2014-12-18T16:06:04Z,OWNER,"Yes releasing it on arxiv tomorrow
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-67508285,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/67508458,67508458,MDEyOklzc3VlQ29tbWVudDY3NTA4NDU4,43829,2014-12-18T16:07:06Z,2014-12-18T16:07:06Z,NONE,"Awesome!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-67508458,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/67509569,67509569,MDEyOklzc3VlQ29tbWVudDY3NTA5NTY5,1310570,2014-12-18T16:12:36Z,2014-12-18T16:12:36Z,OWNER,"@benanne not sure if you've noticed, but I changed alexnet to use the One weird trick paper's implementation, rather than the original paper's implementation.
https://github.com/soumith/convnet-benchmarks/blob/master/torch7/imagenet_winners/alexnet.lua

very curious to see theano FFT numbers as well. we wanted to put it in the paper (for alexnet/overfeat), but we dont know how to build nets in theano tsk!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-67509569,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/67510352,67510352,MDEyOklzc3VlQ29tbWVudDY3NTEwMzUy,43829,2014-12-18T16:17:03Z,2014-12-18T16:17:03Z,NONE,"I hadn't noticed, thanks for pointing that out!

I don't know how to build AlexNet in Theano either, it's not easy to do 'same' convolutions across the different convolution implementations at the moment. The way we do it in Lasagne right now is by performing a full convolution and then slicing the result, but that seems wasteful because many implementations actually support implicit zero padding already (see https://github.com/benanne/Lasagne/blob/master/lasagne/layers/base.py#L635).
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-67510352,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/67510824,67510824,MDEyOklzc3VlQ29tbWVudDY3NTEwODI0,1310570,2014-12-18T16:18:55Z,2014-12-18T16:18:55Z,OWNER,"ok, so our FFT module does not support implicit zero padding yet either. So we add zero padding before hand as a separate op, and then do valid convolutions. the timing difference is negligible between both.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-67510824,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/67511736,67511736,MDEyOklzc3VlQ29tbWVudDY3NTExNzM2,43829,2014-12-18T16:24:01Z,2014-12-18T16:24:01Z,NONE,"Ok, then the problem is the 3x3 pooling with stride 2. Someone is working on implementing strided pooling, but it hasn't been merged: https://github.com/Theano/Theano/issues/2196

It's possible to do a custom strided pooling implementation in pure Theano, but that's pretty slow in my experience.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-67511736,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/67512110,67512110,MDEyOklzc3VlQ29tbWVudDY3NTEyMTEw,1310570,2014-12-18T16:26:15Z,2014-12-18T16:26:15Z,OWNER,"aren't you guys a little behind the times ;) 
cudnn has the pooling implementation, i thought someone wrote the bindings for them to theano.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-67512110,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/67513963,67513963,MDEyOklzc3VlQ29tbWVudDY3NTEzOTYz,43829,2014-12-18T16:37:01Z,2014-12-18T16:37:01Z,NONE,"Hey, don't look at me, I've brought this up in the past :) (See https://github.com/Theano/Theano/issues/2118 and https://github.com/Theano/Theano/issues/2196 )

Yeah, the cudnn pooling has been wrapped, but ideally you don't want to use it directly. Rather, you want to use the default pooling functions/classes and rely on Theano's optimizer to swap in alternate implementations, depending on preference.

The problem is that Theano's default pooling operator is not expressive enough (no implicit padding, no strides). Same with the default convolution operator, which only allows full and valid convolutions, no custom padding or same convolutions.

Some implementations support these features, but there is no way to use them if you're things the 'correct' way - only if you directly use the custom implementations instead of relying on the optimizer.

I really like the concept of an optimizing compiler for this kind of code, but it's definitely true that Theano's default interfaces are not keeping up with the new features of some alternative implementations. They're getting stale and we're at a point where it's starting to limit the possibilities.

I can't do without the automatic differentiation though, so you won't be able to convert me just yet :p 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-67513963,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/67514727,67514727,MDEyOklzc3VlQ29tbWVudDY3NTE0NzI3,1310570,2014-12-18T16:41:16Z,2014-12-18T16:41:16Z,OWNER,"for what it's worth, all of our 100+ nn ops, (along with another 100+ from the community) have a backprop wrt weights and backprop wrt input. and, we are at a faster pace in terms of development (for example googlenet can be implemented, rnns of all kinds, lstm  etc.).
In that aspect neither theano or caffe can touch us.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-67514727,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/67515723,67515723,MDEyOklzc3VlQ29tbWVudDY3NTE1NzIz,43829,2014-12-18T16:47:13Z,2014-12-18T16:47:13Z,NONE,"Sure, but I'm thinking of stuff like the custom output layer I used for the Galaxy Challenge, to incorporate the constraints on the predicted probabilities (this monster: https://github.com/benanne/kaggle-galaxies/blob/master/custom.py#L576 ). There is no way in hell I'd want to implement that gradient myself. I don't think it's possible to avoid that in any other library for now. Theano's gradient implementations are at the level of individual operations, which is what makes the automatic differentiation so powerful. I guess the gradients implemented in Torch are too coarse-grained for that.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-67515723,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/67516268,67516268,MDEyOklzc3VlQ29tbWVudDY3NTE2MjY4,1310570,2014-12-18T16:50:35Z,2014-12-18T16:50:35Z,OWNER,"agreed. we dont have a graph optimizer. i implemented the same (or similar) here: https://github.com/soumith/galaxyzoo/blob/master/2_model.lua#L35
but theano probably optimizes out certain common expressions and makes them efficient.

anyways, the thread is going off track :) i will update when I have the end-to-end benchmarks on caffe, and start a table with nn + cudnn + caffe + ccn2 for ovefeat,alexnet,oxfordnet.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-67516268,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/67568456,67568456,MDEyOklzc3VlQ29tbWVudDY3NTY4NDU2,127987,2014-12-18T22:28:46Z,2014-12-18T22:28:46Z,CONTRIBUTOR,"Have you used my ""two buffer for backprop"" trick on GoogLeNet thing :P
Interested to hear if you have any success on reprod GoogLeNet, I probably will start that pretty soon.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-67568456,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/67702956,67702956,MDEyOklzc3VlQ29tbWVudDY3NzAyOTU2,1310570,2014-12-19T21:52:25Z,2014-12-19T21:52:25Z,OWNER,"@liuliu did not use your two-buffer trick yet. so many things to try :) 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-67702956,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/67703097,67703097,MDEyOklzc3VlQ29tbWVudDY3NzAzMDk3,1310570,2014-12-19T21:53:38Z,2014-12-19T21:53:38Z,OWNER,"okay so I added the titan black numbers for alexnet,overfeat,vggmodel-A and in the README.md for ccn2, cudnn Release 2, cudnn Release1, torch-cunn.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-67703097,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/67721418,67721418,MDEyOklzc3VlQ29tbWVudDY3NzIxNDE4,180987,2014-12-20T02:15:56Z,2014-12-20T02:15:56Z,CONTRIBUTOR,"Merry Christmas and Happy New Year! 

(I won't be checking my emails/github frequently until next year, so do not expect fast response)

Thanks for the release of fbcufft that will be useful to many other project I think.

For the cudnn r2 benchmark, I think you should make 2 cases. One without extra memory and one with. As fft and cuda-convnet[2] use/can use extra memory, I think knowing both timing from cuddn r2 would make a better comparison. Which one did you timed?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-67721418,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/67723210,67723210,MDEyOklzc3VlQ29tbWVudDY3NzIzMjEw,180987,2014-12-20T03:24:29Z,2014-12-20T03:24:29Z,CONTRIBUTOR,"@benanne, I agree having the CPU code support all the version is great. But sometimes we need to go faster for research and implementing just the case we use (CUDA) allow this. But we also continue to make available in the ""normal""/CPU interface the new features later.

I must disagree with @soumith. Most Theano op have its grad implement for a long time. I do not think torch have more. In fact, in Theano we can in many case take the second derivate and the Lop/Rop needed for hessian free. I didn't see that in other software that automatize this (but I didn't looked). So I do not think there is more grad in Torch :)

Also, for the speed of development, we had LSTM and RNN before torch I think. For RNN, @nicholas-leonard asked on our lab mailing list question on now to implement it. We had it for a long time. With the same functionality we can implement LSTM. There wasn't new functionality needed. We are in the process to release an example for LSTM base on some old code:  https://github.com/lisa-lab/DeepLearningTutorials/pull/55. There is also Alex conv net code that was linked to by a Theano user, I forget where.

For multi-GPU, it was possible with multiple process for a long time with an example. For the case of Multi-GPU in the same process as current Torch, there is a PR that is just waiting for my review :)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-67723210,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/67731341,67731341,MDEyOklzc3VlQ29tbWVudDY3NzMxMzQx,43829,2014-12-20T10:18:19Z,2014-12-20T10:18:19Z,NONE,"Merry Christmas and a happy new year to everyone as well :)

@nouiz: To be honest, I don't really care about CPU support for every feature either. All the machines I use have GPUs, including my laptop. But the way Theano is built requires using 'more general' ops in your graph that get replaced by optimizations to profit maximally. Having to use specialized ops in a graph directly undoes some of the advantages that 'the Theano way' offers.

And let's face it, for many people the CPU convolution is just a placeholder anyway, they never execute it directly. Whether it has all these features implemented or not wouldn't matter, there just needs to be a way to specify all the parameters so the more specialized implementations can pick them up when inserted during the optimization phase.

I would almost advocate for making the 'default' Theano convolution non-executable: just a placeholder to hold all the parameters, and then the various executable versions would be inserted only at optimization time. Because with the current set-up, where the CPU version is also the placeholder, rapid progress in the GPU implementations seems to be held back by the fact that the new features can't be used in the ""normal"" Theano way: you have to use the specialized implementations directly if you want those. If you just want to use T.nnet.conv2d, that would require for there to be a matching CPU implementation first.

With the advent of @f0k's meta-optimizers, this has become even more apparent: if you want to take advantage of those, you have to use conv2d in your graph and you are unable to use features like arbitrary zero padding that GpuCorrMM and dnn_conv now offer, for example.

In practice, what we're doing for Lasagne now is implementing several `Conv2DLayer` classes that each wrap a different implementation, to allow them to be easily swappable. This works, but it feels kind of wrong because the whole idea of Theano is to represent your computation as a graph so you can replace parts easily. So this problem would be much better solved at the Theano level.

Everything I've said also goes for pooling of course (and probably a bunch of other operations). I don't know if making the default ops 'abstract' (i.e. without implementation) is the best solution, but the current set-up does seem to slow down the integration of new features.

That said, despite all my whining and complaining I still think Theano is an incredible piece of software that I've been able to rely on for almost 4 years now. It's certainly made my life a lot easier. So please don't take all of this the wrong way :)

... I feel like I've hijacked this issue twice over now :p Sorry @soumith ! If this point is open to discussion I could create an issue for it at the Theano repo.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-67731341,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/67740126,67740126,MDEyOklzc3VlQ29tbWVudDY3NzQwMTI2,1310570,2014-12-20T16:02:58Z,2014-12-20T16:02:58Z,OWNER,"i agree, we need to move the discussion of theano to theano, mainly because the repo is watched by a diverse set of people who might not be interested in a lot of things. 
ps: thanks to the CuDNN guys for giving us 3D convolutions in the new release (which dont need extra memory of course), i've interfaced them, and they work great.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-67740126,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/67869933,67869933,MDEyOklzc3VlQ29tbWVudDY3ODY5OTMz,1310570,2014-12-22T18:12:36Z,2014-12-22T18:12:36Z,OWNER,"@nouiz For CUDNN R2, I implemented the ""FASTEST"" mode. Sometimes it uses some extra memory, but the extra memory is in the order of a few kilobytes, not at all significant, so I do not think it is significant.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-67869933,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/68109030,68109030,MDEyOklzc3VlQ29tbWVudDY4MTA5MDMw,1310570,2014-12-25T17:42:28Z,2014-12-25T17:42:28Z,OWNER,"@benanne http://arxiv.org/abs/1412.7580
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-68109030,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/68110325,68110325,MDEyOklzc3VlQ29tbWVudDY4MTEwMzI1,43829,2014-12-25T18:44:00Z,2014-12-25T18:44:00Z,NONE,"Excellent, thanks for the heads up!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-68110325,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/68162721,68162721,MDEyOklzc3VlQ29tbWVudDY4MTYyNzIx,3529132,2014-12-26T23:30:54Z,2014-12-26T23:30:54Z,NONE,"For L1 the IFM->OFM is 3->96, but in L2, it goes from 64->128, were these numbers selected to intentionally be mismatched for benchmarking purposes?  Or should L2 be 96->128?  
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-68162721,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/68162902,68162902,MDEyOklzc3VlQ29tbWVudDY4MTYyOTAy,1310570,2014-12-26T23:39:18Z,2014-12-26T23:39:18Z,OWNER,"@apark263 the numbers were very arbitrarily chosen to what I thought might be a ""typical"" L2. I've moved to more realistic and real-world ""imagenet-winners"" benchmarks for exactly this reason, that the layer sizes were arbitrarily chosen.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-68162902,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/74956600,74956600,MDEyOklzc3VlQ29tbWVudDc0OTU2NjAw,1538540,2015-02-18T21:56:20Z,2015-02-18T21:56:20Z,CONTRIBUTOR,"Hi @soumith , it seems that you've fixed the ""numFilters assertions"" problem of cuda-convnet2. I also have the same problem. Can you give me a hint on how to do that? Also, have you finished Caffe version of these four imagenets? If you haven't and are busy, I think I can help.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-74956600,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/74987535,74987535,MDEyOklzc3VlQ29tbWVudDc0OTg3NTM1,1310570,2015-02-19T02:03:36Z,2015-02-19T02:03:36Z,OWNER,"Hey @dadaism. I fixed it in my cuda-convnet2.torch fork by rewriting some of the kernels without texture loading and adding them to the dispatcher for the case when the tensor is bigger than 2^27. You can find some commits doing so (on phone, so can't pull them up now).

I have not gotten time to tackle the imagenet-winners on Caffe side. Any help from you would be very nice. I am working on benchmarking on maxwell architecture.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-74987535,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/74989098,74989098,MDEyOklzc3VlQ29tbWVudDc0OTg5MDk4,8460517,2015-02-19T02:23:19Z,2015-02-19T02:23:19Z,NONE,"Why not use 128-bit texture loads instead? That will extend the reach of the texture indexes, reduce the number of load instructions, and reduce the indexing arithmetic.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-74989098,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/74999768,74999768,MDEyOklzc3VlQ29tbWVudDc0OTk5NzY4,1310570,2015-02-19T05:04:07Z,2015-02-19T05:04:07Z,OWNER,"It could be done. And the corner cases can be loaded via a regular load. It would also remove this 512mb limit in ccn2. No one has done it, that's all.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-74999768,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/75407046,75407046,MDEyOklzc3VlQ29tbWVudDc1NDA3MDQ2,8460517,2015-02-22T00:35:49Z,2015-02-22T00:35:49Z,NONE,"Why is the VGG benchmark using a mini-batch size of 32? The paper seems to say the mini-batch size was 256/4GPUs = 64/GPU. http://arxiv.org/abs/1409.1556
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-75407046,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/75419299,75419299,MDEyOklzc3VlQ29tbWVudDc1NDE5Mjk5,1310570,2015-02-22T04:34:35Z,2015-02-22T04:34:35Z,OWNER,"Some of the libs don't support in-place relu (without which you won't fit minibatch 64), and I was going to try moving the benchmarks to gtx980, so I made this small trade-ofg
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-75419299,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/75441531,75441531,MDEyOklzc3VlQ29tbWVudDc1NDQxNTMx,8460517,2015-02-22T15:51:15Z,2015-02-22T15:51:15Z,NONE,"Mini-batch size can have a drastic affect on performance, so changing it for a benchmark is significant. Would also be useful to know which libraries failed to run full VGG.

Maybe at least document clearly where you deviate from the original experiment and why.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-75441531,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,51248144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/75447747,75447747,MDEyOklzc3VlQ29tbWVudDc1NDQ3NzQ3,1310570,2015-02-22T17:52:34Z,2015-02-22T17:52:34Z,OWNER,"@andravin I think you make a good point. I will just make it mini-batch size 64. Waiting for a new card for that to happen. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35/comments,https://github.com/soumith/convnet-benchmarks/issues/35#issuecomment-75447747,https://api.github.com/repos/soumith/convnet-benchmarks/issues/35
soumith,convnet-benchmarks,50509631,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/65079445,65079445,MDEyOklzc3VlQ29tbWVudDY1MDc5NDQ1,180987,2014-12-01T15:22:06Z,2014-12-01T15:22:06Z,CONTRIBUTOR,"This still isn't the best. This create the shared variable on the CPU when device=cpu. using device=gpu do two things:

1) make theano.shared() default to be created on the GPU
2) enable gpu optimization by default.

Run the script with the flag profile=True and you will see transfer to/from the GPU there (I just checked to confirm).

device=gpu, cause theano.sandbox.cuda.use('gpu') to be used. You can call it in the script, but this remove the user choice of the GPU. Or you can call it only if theano.config.device do not start with gpu. That way, the user can still select a different GPU, but it will default to the GPU and not CPU. Then you can remove the including('gpu') from the mode too.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/34/comments,https://github.com/soumith/convnet-benchmarks/pull/34#issuecomment-65079445,https://api.github.com/repos/soumith/convnet-benchmarks/issues/34
soumith,convnet-benchmarks,50509631,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/65086155,65086155,MDEyOklzc3VlQ29tbWVudDY1MDg2MTU1,629706,2014-12-01T16:02:26Z,2014-12-01T16:02:26Z,CONTRIBUTOR,"Oh, good hint. So we could add:

``` python
if not theano.config.device.startswith('gpu'):
    theano.sandbox.cuda.use('gpu')
```

But does that interfere with `theano.misc.pycuda_init` in some way? Should it be placed before or after [lines 6-12](https://github.com/soumith/convnet-benchmarks/blob/master/theano/pylearn2_benchmark.py#L6-12)?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/34/comments,https://github.com/soumith/convnet-benchmarks/pull/34#issuecomment-65086155,https://api.github.com/repos/soumith/convnet-benchmarks/issues/34
soumith,convnet-benchmarks,50509631,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/65130406,65130406,MDEyOklzc3VlQ29tbWVudDY1MTMwNDA2,180987,2014-12-01T20:37:10Z,2014-12-01T20:37:10Z,CONTRIBUTOR,"Before pycuda init.

On Mon, Dec 1, 2014 at 11:02 AM, Jan SchlÃ¼ter notifications@github.com
wrote:

> Oh, good hint. So we could add:
> 
> if not theano.config.device.startswith('gpu'):
>     theano.sandbox.cuda.use('gpu')
> 
> But does that interfere with theano.misc.pycuda_init in some way? Should
> it be placed before or after lines 6-12
> https://github.com/soumith/convnet-benchmarks/blob/master/theano/pylearn2_benchmark.py#L6-12
> ?
> 
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/pull/34#issuecomment-65086155
> .
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/34/comments,https://github.com/soumith/convnet-benchmarks/pull/34#issuecomment-65130406,https://api.github.com/repos/soumith/convnet-benchmarks/issues/34
soumith,convnet-benchmarks,50509631,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/65267408,65267408,MDEyOklzc3VlQ29tbWVudDY1MjY3NDA4,629706,2014-12-02T17:16:46Z,2014-12-02T17:16:46Z,CONTRIBUTOR,"Should be okay now. Works without any `~/.theanorc` file or `THEANO_FLAGS`, but the GPU to use can still be overridden with `THEANO_FLAGS=device=gpu1`, for example.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/34/comments,https://github.com/soumith/convnet-benchmarks/pull/34#issuecomment-65267408,https://api.github.com/repos/soumith/convnet-benchmarks/issues/34
soumith,convnet-benchmarks,50509631,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/66552688,66552688,MDEyOklzc3VlQ29tbWVudDY2NTUyNjg4,1310570,2014-12-11T00:54:17Z,2014-12-11T00:54:17Z,OWNER,"i had a dream that someone asked me to rerun the metaoptimizer numbers? 
@nouiz pointed out that the metaoptimizer log still looks wrong. what should I do? change some flags and rerun?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/34/comments,https://github.com/soumith/convnet-benchmarks/pull/34#issuecomment-66552688,https://api.github.com/repos/soumith/convnet-benchmarks/issues/34
soumith,convnet-benchmarks,50509631,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/66612385,66612385,MDEyOklzc3VlQ29tbWVudDY2NjEyMzg1,629706,2014-12-11T12:30:38Z,2014-12-11T12:31:27Z,CONTRIBUTOR,"Hmm, I noticed the same and sent you an email a few days ago, did you get it? What strikes me in the log is the memory error in the beginning, which might explain why the meta-optimized L1 bprop wrt. weights is slower than cuDNN. I wonder where the memory error comes from, though, it seems it tried to allocate almost 8 GiB?
Anyway, for details on what's happening, I'd suggest to run:

``` python
THEANO_FLAGS=metaopt.verbose=1 SKIP=legacy,gemm,dnn,convnet PRINT_GRAPH=1 python pylearn2_benchmark.py 1
```

This will give a long output, you might want to send it to me by email or post it on pastebin or something. That would be helpful, thank you!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/34/comments,https://github.com/soumith/convnet-benchmarks/pull/34#issuecomment-66612385,https://api.github.com/repos/soumith/convnet-benchmarks/issues/34
soumith,convnet-benchmarks,50509631,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/66623040,66623040,MDEyOklzc3VlQ29tbWVudDY2NjIzMDQw,180987,2014-12-11T14:07:19Z,2014-12-11T14:07:19Z,CONTRIBUTOR,"Update Theano. There where changes and I'm not sure of the timeline. So
maybe they would change that.

On Thu, Dec 11, 2014 at 7:30 AM, Jan SchlÃ¼ter notifications@github.com
wrote:

> Hmm, I noticed the same and sent you an email a few days ago, did you get
> it? What strikes me in the log is the memory error in the beginning, which
> might explain why the meta-optimized L1 bprop wrt. weights is slower than
> cuDNN. I wonder where the memory error comes from, though, it seems it
> tried to allocate some 8 GiB?
> Anyway, for details on what's happening, I'd suggest to run:
> 
> THEANO_FLAGS=metaopt.verbose=1 SKIP=legacy,gemm,dnn,convnet PRINT_GRAPH=1 python pylearn2_benchmark.py 1
> 
> This will give a long output, you might want to send it to me by email or
> post it on pastebin or something. That would be helpful, thank you!
> 
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/pull/34#issuecomment-66612385
> .
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/34/comments,https://github.com/soumith/convnet-benchmarks/pull/34#issuecomment-66623040,https://api.github.com/repos/soumith/convnet-benchmarks/issues/34
soumith,convnet-benchmarks,50509631,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/66635136,66635136,MDEyOklzc3VlQ29tbWVudDY2NjM1MTM2,629706,2014-12-11T15:29:03Z,2014-12-11T15:29:03Z,CONTRIBUTOR,"> Update Theano. There where changes and I'm not sure of the timeline. So
> maybe they would change that.

But then please still just run what I suggested. This will be a lot faster than rerunning the full benchmark and it will also give a lot more information on what's happening internally, so it's a win/win :)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/34/comments,https://github.com/soumith/convnet-benchmarks/pull/34#issuecomment-66635136,https://api.github.com/repos/soumith/convnet-benchmarks/issues/34
soumith,convnet-benchmarks,50509631,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/67001432,67001432,MDEyOklzc3VlQ29tbWVudDY3MDAxNDMy,43829,2014-12-15T14:30:39Z,2014-12-15T14:30:39Z,NONE,"Looks like something else is still going on, the metaoptimizer is slower than cudnn for L1 bprop weights:

```
sedielem@koe:~/git/convnet-benchmarks/theano$ THEANO_FLAGS=metaopt.verbose=1 SKIP=legacy,fft,gemm,convnet python pylearn2_benchmark.py 1
Using gpu device 0: Tesla K40c

CONFIG: input = 3 x 128 x 128 * ker = 3 x 96 x 11 x 11 ( bs = 128 , stride = 1 )
ConvMetaOptimizer meta-optimizing GpuConv{valid, (1, 1), None, (11, 11), True, (3, 128, 128), (11, 11)}(GpuFromHost.0, GpuFromHost.0) (5 choices):
* local_conv_fft_full: not applicable
* local_conv_fft_valid: 0.16624 sec
* local_conv_dnn: 0.10293 sec
* local_conv_gemm: 0.11213 sec
* local_conv_dnn_alternative: 0.12774 sec
= local_conv_dnn
(experimental) meta-optimizer                      ==> fprop         ==>     101
ConvMetaOptimizer meta-optimizing GpuConv{full, (1, 1), None, (11, 11), True, (96, 118, 118), (11, 11)}(GpuFromHost.0, GpuFromHost.0) (5 choices):
* local_conv_fft_full: 0.27754 sec
* local_conv_fft_valid: not applicable
* local_conv_dnn: 0.13291 sec
* local_conv_gemm: 0.10229 sec
* local_conv_dnn_alternative: 1.472 sec
= local_conv_gemm
(experimental) meta-optimizer                      ==> bprop inputs  ==>     102
ConvMetaOptimizer meta-optimizing GpuConv{valid, (1, 1), None, (118, 118), False, (128, 128, 128), (118, 118)}(GpuFromHost.0, GpuFromHost.0) (5 choices):
* local_conv_fft_full: not applicable
* local_conv_fft_valid: 0.22289 sec
* local_conv_dnn: 0.23396 sec
* local_conv_gemm: 0.33731 sec
* local_conv_dnn_alternative: 0.54578 sec
= local_conv_fft_valid
(experimental) meta-optimizer                      ==> bprop weights ==>     224

(auto) theano.sandbox.cuda.dnn.GpuDnnConv          ==> fprop         ==>     101
(auto) theano.sandbox.cuda.dnn.GpuDnnConv          ==> bprop inputs  ==>     132
(auto) theano.sandbox.cuda.dnn.GpuDnnConv          ==> bprop weights ==>     126

(manual conv) theano.sandbox.cuda.dnn.GpuDnnConv   ==> fprop         ==>     101
(manual conv) theano.sandbox.cuda.dnn.GpuDnnConv   ==> bprop inputs  ==>     131
(manual conv) theano.sandbox.cuda.dnn.GpuDnnConv   ==> bprop weights ==>     125
```

Note that local_conv_dnn executed as part of the metaoptimizer is measured to take about 234 ms, but during the benchmark it only takes 126 ms. @f0k says it's probably got something to do with differing strides, but I'll let him explain it in detail.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/34/comments,https://github.com/soumith/convnet-benchmarks/pull/34#issuecomment-67001432,https://api.github.com/repos/soumith/convnet-benchmarks/issues/34
soumith,convnet-benchmarks,50509631,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/67020673,67020673,MDEyOklzc3VlQ29tbWVudDY3MDIwNjcz,629706,2014-12-15T16:27:00Z,2014-12-15T16:27:00Z,CONTRIBUTOR,"Thanks for running the test, @benanne!

There is a minor difference between how the meta-optimizer does the benchmark and how `pylearn2_benchmark.py` does the benchmark: The former takes the minimum of 3 trials, while the latter takes the average of 10 trials (for compatibility with the other benchmarks). But this will not be the problem.

There is another difference between the convolution subgraph timed by the meta-optimizer and the convolution subgraph in the original graph that is being optimized. The meta-optimizer needs to time the convolution in isolation, so it creates a small graph consisting of the convolution operation and brand new `SharedVariable` instances of random data matching the shape of the convolution inputs in the original graph. But these instances only match the shape, not the strides -- hence, the `gpu_contiguous` ops that are part of `dnn_conv` (and `GpuCorrMM`) may incur some copying operations in the timed graph that wouldn't have occurred in the original graph, skewing the timings.

To fix this we need to be able to infer the strides of a variable in a Theano graph, not just the shape. (This would also create opportunities for further graph optimizations.) I don't have an idea for a short-term solution. @nouiz, do you?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/34/comments,https://github.com/soumith/convnet-benchmarks/pull/34#issuecomment-67020673,https://api.github.com/repos/soumith/convnet-benchmarks/issues/34
soumith,convnet-benchmarks,50509631,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/67720998,67720998,MDEyOklzc3VlQ29tbWVudDY3NzIwOTk4,180987,2014-12-20T02:02:56Z,2014-12-20T02:02:56Z,CONTRIBUTOR,"Hi,

I agree we need to infer the strides. But this shouldn't be too long. We do
not need to implement it for all ops, just the few one needed here. I think
1 or 2 days would be enough for someone not new in Theano contribution. I
made an issue that describe how to do it:

https://github.com/Theano/Theano/issues/2365

The gpu_contiguous are too strict. The fastest would be to remove them from
the graph and just let cudnn r2 handle it:) They support more strides and I
think they cover our case. At worst we would need to remove the subtensor
[:,:,::-1,::-1] that flip the kernel (this introduce negative strides and
cudnn r1 didn't implemented that) and swap the convolution mode (conv vs
cross).

On Mon, Dec 15, 2014 at 11:27 AM, Jan SchlÃ¼ter notifications@github.com
wrote:

> Thanks for running the test, @benanne https://github.com/benanne!
> 
> There is a minor difference between how the meta-optimizer does the
> benchmark and how pylearn2_benchmark.py does the benchmark: The former
> takes the minimum of 3 trials, while the latter takes the average of 10
> trials (for compatibility with the other benchmarks). But this will not be
> the problem.
> 
> There is another difference between the convolution subgraph timed by the
> meta-optimizer and the convolution subgraph in the original graph that is
> being optimized. The meta-optimizer needs to time the convolution in
> isolation, so it creates a small graph consisting of the convolution
> operation and brand new SharedVariable instances of random data matching
> the shape of the convolution inputs in the original graph. But these
> instances only match the shape, not the strides -- hence, the
> gpu_contiguous ops that are part of dnn_conv (and GpuCorrMM) may incur
> some copying operations in the timed graph that wouldn't have occurred in
> the original graph, skewing the timings.
> 
> To fix this we need to be able to infer the strides of a variable in a
> Theano graph, not just the shape. (This would also create opportunities for
> further graph optimizations.) I don't have an idea for a short-term
> solution. @nouiz https://github.com/nouiz, do you?
> 
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/pull/34#issuecomment-67020673
> .
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/34/comments,https://github.com/soumith/convnet-benchmarks/pull/34#issuecomment-67720998,https://api.github.com/repos/soumith/convnet-benchmarks/issues/34
soumith,convnet-benchmarks,49949739,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/64511034,64511034,MDEyOklzc3VlQ29tbWVudDY0NTExMDM0,1310570,2014-11-26T03:43:50Z,2014-11-26T03:43:50Z,OWNER,"will rerun on the weekend!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/33/comments,https://github.com/soumith/convnet-benchmarks/pull/33#issuecomment-64511034,https://api.github.com/repos/soumith/convnet-benchmarks/issues/33
soumith,convnet-benchmarks,49949739,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/65034217,65034217,MDEyOklzc3VlQ29tbWVudDY1MDM0MjE3,1310570,2014-12-01T08:41:09Z,2014-12-01T08:41:09Z,OWNER,"well, so I started running this thing (I reinstalled all the packages from scratch, according to the readme).

There seems to be something wrong. It's been running for more than an hour, and the numbers for CuDNN and CorrMM are completely bonkers:

```
> SKIP=legacy THEANO_FLAGS=floatX=float32 python pylearn2_benchmark.py

CONFIG: input = 3 x 128 x 128 * ker = 3 x 96 x 11 x 11 ( bs = 128 , stride = 1 )
Using gpu device 0: GeForce GTX TITAN Black
(experimental) meta-optimizer                      ==> fprop         ==>     265
(experimental) meta-optimizer                      ==> bprop inputs  ==>     204
(experimental) meta-optimizer                      ==> bprop weights ==>     225

theano.sandbox.cuda.fftconv.conv2d_fft             ==> fprop         ==>     419
theano.sandbox.cuda.fftconv.conv2d_fft             ==> bprop inputs  ==>     367
theano.sandbox.cuda.fftconv.conv2d_fft             ==> bprop weights ==>     316

(auto) theano.sandbox.cuda.dnn.GpuDnnConv          ==> fprop         ==>   17820
(auto) theano.sandbox.cuda.dnn.GpuDnnConv          ==> bprop inputs  ==>   20187
(auto) theano.sandbox.cuda.dnn.GpuDnnConv          ==> bprop weights ==>   16276

(auto) theano.sandbox.cuda.blas.GpuCorrMM          ==> fprop         ==>   17828
(auto) theano.sandbox.cuda.blas.GpuCorrMM          ==> bprop inputs  ==>   20186
(auto) theano.sandbox.cuda.blas.GpuCorrMM          ==> bprop weights ==>   16274

(manual) theano.sandbox.cuda.blas.GpuCorrMM        ==> fprop         ==>     272
(manual) theano.sandbox.cuda.blas.GpuCorrMM        ==> bprop inputs  ==>     202
(manual) theano.sandbox.cuda.blas.GpuCorrMM        ==> bprop weights ==>     310

(manual conv) theano.sandbox.cuda.dnn.GpuDnnConv   ==> fprop         ==>     262
(manual conv) theano.sandbox.cuda.dnn.GpuDnnConv   ==> bprop inputs  ==>     231
(manual conv) theano.sandbox.cuda.dnn.GpuDnnConv   ==> bprop weights ==>     223

pylearn2.sandbox.cuda_convnet(partial_sum=None)    ==> fprop         ==>     267
pylearn2.sandbox.cuda_convnet(partial_sum=None)    ==> bprop inputs  ==>     253
pylearn2.sandbox.cuda_convnet(partial_sum=None)    ==> bprop weights ==>     565

pylearn2.sandbox.cuda_convnet(partial_sum=1)       ==> fprop         ==>     266
pylearn2.sandbox.cuda_convnet(partial_sum=1)       ==> bprop inputs  ==>     253
pylearn2.sandbox.cuda_convnet(partial_sum=1)       ==> bprop weights ==>     315


CONFIG: input = 64 x 64 x 64 * ker = 64 x 128 x 9 x 9 ( bs = 128 , stride = 1 )
(experimental) meta-optimizer                      ==> fprop         ==>     288
(experimental) meta-optimizer                      ==> bprop inputs  ==>     350
(experimental) meta-optimizer                      ==> bprop weights ==>     403

theano.sandbox.cuda.fftconv.conv2d_fft             ==> fprop         ==>     167
theano.sandbox.cuda.fftconv.conv2d_fft             ==> bprop inputs  ==>     192
theano.sandbox.cuda.fftconv.conv2d_fft             ==> bprop weights ==>     166

(auto) theano.sandbox.cuda.dnn.GpuDnnConv          ==> fprop         ==>   75471
(auto) theano.sandbox.cuda.dnn.GpuDnnConv          ==> bprop inputs  ==>   89636
(auto) theano.sandbox.cuda.dnn.GpuDnnConv          ==> bprop weights ==>   69253

(auto) theano.sandbox.cuda.blas.GpuCorrMM          ==> fprop         ==>   75640
```
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/33/comments,https://github.com/soumith/convnet-benchmarks/pull/33#issuecomment-65034217,https://api.github.com/repos/soumith/convnet-benchmarks/issues/33
soumith,convnet-benchmarks,49949739,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/65044709,65044709,MDEyOklzc3VlQ29tbWVudDY1MDQ0NzA5,43829,2014-12-01T10:21:08Z,2014-12-01T10:21:08Z,NONE,"It must be an optimization-related issue, since the manual versions work fine. But even if the legacy implementation ends up being used due to an optimization failure, those numbers seem very high. So maybe it's just running on the CPU or something.

What's more interesting to me is the 2nd configuration, where conv2d_fft does better than the meta-optimizer for some reason. That shouldn't happen because the meta-optimizer includes conv2d_fft as one of the options.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/33/comments,https://github.com/soumith/convnet-benchmarks/pull/33#issuecomment-65044709,https://api.github.com/repos/soumith/convnet-benchmarks/issues/33
soumith,convnet-benchmarks,49949739,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/65044908,65044908,MDEyOklzc3VlQ29tbWVudDY1MDQ0OTA4,629706,2014-12-01T10:22:52Z,2014-12-01T10:22:52Z,CONTRIBUTOR,"> There seems to be something wrong. It's been running for more than an hour, and the numbers for CuDNN and CorrMM are completely bonkers:

Hmm, and they're both the same. Maybe they're using the legacy op now. Also for the second configuration, the meta-optimizer did not choose the FFT version although that was faster. First let's check that you're using the correct version of Theano. Can you please do:

``` bash
python -c 'import theano; print theano.__version__'
```

I've just updated to the latest version and everything looks fine:

```
$ python -c 'import theano; print theano.__version__'
0.6.0rc3.dev-dbc7091b93a42e1407d2b97bdae55576edf7db77
$ THEANO_FLAGS=device=gpu,allow_gc=1 SKIP=legacy,meta,fft,gemm,convnet python pylearn2_benchmark.py 4
Using gpu device 0: GeForce GT 640

CONFIG: input = 128 x 16 x 16 * ker = 128 x 128 x 7 x 7 ( bs = 128 , stride = 1 )
(auto) theano.sandbox.cuda.dnn.GpuDnnConv          ==> fprop         ==>      85
(auto) theano.sandbox.cuda.dnn.GpuDnnConv          ==> bprop inputs  ==>     125
(auto) theano.sandbox.cuda.dnn.GpuDnnConv          ==> bprop weights ==>     122

(manual conv) theano.sandbox.cuda.dnn.GpuDnnConv   ==> fprop         ==>      86
(manual conv) theano.sandbox.cuda.dnn.GpuDnnConv   ==> bprop inputs  ==>     125
(manual conv) theano.sandbox.cuda.dnn.GpuDnnConv   ==> bprop weights ==>     121
```

To check which op is used in your case, you can do:

``` bash
SKIP=legacy,meta,fft,gemm,convnet PRINT_GRAPH=1 python pylearn2_benchmark.py 4
```

and press CTRL+C after the fprop. The graph should have `GpuDnnConv` as the first node.

Finally, you can check if you don't have any bad options in your Theano configuration:

``` bash
cat ~/.theanorc
```

If it wasn't just the wrong Theano version being used, please post the output of what I suggested to run.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/33/comments,https://github.com/soumith/convnet-benchmarks/pull/33#issuecomment-65044908,https://api.github.com/repos/soumith/convnet-benchmarks/issues/33
soumith,convnet-benchmarks,49949739,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/65045476,65045476,MDEyOklzc3VlQ29tbWVudDY1MDQ1NDc2,629706,2014-12-01T10:27:36Z,2014-12-01T10:35:21Z,CONTRIBUTOR,"> So maybe it's just running on the CPU

Oh wait. Yes, that's it. Sorry, it seems you still need `THEANO_FLAGS=device=gpu`. I thought [`mode = theano.compile.get_default_mode().including('gpu')`](https://github.com/soumith/convnet-benchmarks/blob/master/theano/pylearn2_benchmark.py#L200) would do this already. @nouiz, what's the correct way to enforce GPU usage without `THEANO_FLAGS=device=gpu` being specified?

/edit: Found it. There were two lines overwriting the mode. I'll file a PR, but you can run it with `THEANO_FLAGS=device=gpu` for now if you want.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/33/comments,https://github.com/soumith/convnet-benchmarks/pull/33#issuecomment-65045476,https://api.github.com/repos/soumith/convnet-benchmarks/issues/33
soumith,convnet-benchmarks,49949739,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/65067652,65067652,MDEyOklzc3VlQ29tbWVudDY1MDY3NjUy,180987,2014-12-01T13:59:21Z,2014-12-01T13:59:21Z,CONTRIBUTOR,"theano.sandbox.cuda.use('gpu'). But this will let the driver select the GPU
to use. Why not put an assert like that:

assert theano.config.device.startswith('gpu') to be sure the user enabled
the GPU?

On Mon, Dec 1, 2014 at 5:27 AM, Jan SchlÃ¼ter notifications@github.com
wrote:

> So maybe it's just running on the CPU
> 
> Oh wait. Yes, that's it. Sorry, it seems you still need
> THEANO_FLAGS=device=gpu. I thought mode.optimizer_including('gpu') would
> do this already. @nouiz https://github.com/nouiz, what's the correct
> way to enforce GPU usage without THEANO_FLAGS=device=gpu being specified?
> 
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/pull/33#issuecomment-65045476
> .
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/33/comments,https://github.com/soumith/convnet-benchmarks/pull/33#issuecomment-65067652,https://api.github.com/repos/soumith/convnet-benchmarks/issues/33
soumith,convnet-benchmarks,49949739,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/65073806,65073806,MDEyOklzc3VlQ29tbWVudDY1MDczODA2,629706,2014-12-01T14:44:55Z,2014-12-01T14:44:55Z,CONTRIBUTOR,"> Why not put an assert like that: [...]

Thanks, but that's not necessary. Compiling the functions with `mode=theano.compile.get_default_mode().including('gpu')` is enough (I think I copied that from Theanos test suite, which also does not require the device to be set to `'gpu'`). There were just two forgotten lines in the code that reset the mode to default mode halfway through the benchmark, fixed in PR #34.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/33/comments,https://github.com/soumith/convnet-benchmarks/pull/33#issuecomment-65073806,https://api.github.com/repos/soumith/convnet-benchmarks/issues/33
soumith,convnet-benchmarks,49247762,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/63503935,63503935,MDEyOklzc3VlQ29tbWVudDYzNTAzOTM1,1310570,2014-11-18T16:57:17Z,2014-11-18T16:57:17Z,OWNER,"great, will benchmark them this week and post numbers. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/32/comments,https://github.com/soumith/convnet-benchmarks/pull/32#issuecomment-63503935,https://api.github.com/repos/soumith/convnet-benchmarks/issues/32
soumith,convnet-benchmarks,49247762,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/63505761,63505761,MDEyOklzc3VlQ29tbWVudDYzNTA1NzYx,629706,2014-11-18T17:06:30Z,2014-11-18T17:06:42Z,CONTRIBUTOR,"Thanks for merging! As I said, there's no need to benchmark again. The change in Theano to use the new convolutions by default happened in October, but your latest numbers are from September. So your numbers are correct. This PR just ensures the numbers are still correct when re-running with the latest Theano version.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/32/comments,https://github.com/soumith/convnet-benchmarks/pull/32#issuecomment-63505761,https://api.github.com/repos/soumith/convnet-benchmarks/issues/32
soumith,convnet-benchmarks,49247762,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/63508480,63508480,MDEyOklzc3VlQ29tbWVudDYzNTA4NDgw,1310570,2014-11-18T17:20:31Z,2014-11-18T17:20:31Z,OWNER,"ah ok thanks for clarifying.

I am finishing up full network benchmarks for the following networks:
- GoogleNet (inception)
- VGG 2014 (the 3x3 convs all the way)
- Overfeat
- AlexNet 2012 (without response-norm)

I have the model for cuda-convnet2, torch and CuDNN, but I have no idea how to do this for Theano.
Caffe, I have to check their inception thread, that might not be ready, but the other models, ready in caffe as well.

Any interest from you to put together the Theano parts?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/32/comments,https://github.com/soumith/convnet-benchmarks/pull/32#issuecomment-63508480,https://api.github.com/repos/soumith/convnet-benchmarks/issues/32
soumith,convnet-benchmarks,49247762,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/63509285,63509285,MDEyOklzc3VlQ29tbWVudDYzNTA5Mjg1,43829,2014-11-18T17:24:39Z,2014-11-18T17:24:39Z,NONE,"Unfortunately Theano currently misses the ""max pooling with zero padding"" feature that's needed for GoogLeNet. Of course you can do this padding manually but it will slow things down a bit, and it's rather involved. Maybe it's not too bad though, I guess the convolutions will dominate the runtime anyway. I might be able to help out with this as well, depending on how urgent it is :)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/32/comments,https://github.com/soumith/convnet-benchmarks/pull/32#issuecomment-63509285,https://api.github.com/repos/soumith/convnet-benchmarks/issues/32
soumith,convnet-benchmarks,49247762,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/63509549,63509549,MDEyOklzc3VlQ29tbWVudDYzNTA5NTQ5,1310570,2014-11-18T17:26:05Z,2014-11-18T17:26:05Z,OWNER,"there's no urgency, we are doing it all out of self-interest and boredom :)
The way we handle the padding for Inception is by padding after (so you can do inception in a layer-agnostic fashion). The extra run-time is very minimal.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/32/comments,https://github.com/soumith/convnet-benchmarks/pull/32#issuecomment-63509549,https://api.github.com/repos/soumith/convnet-benchmarks/issues/32
soumith,convnet-benchmarks,49247762,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/63510706,63510706,MDEyOklzc3VlQ29tbWVudDYzNTEwNzA2,43829,2014-11-18T17:32:17Z,2014-11-18T17:32:17Z,NONE,"What do you mean by padding after? just adding a border of zeros after pooling? What's the advantage of doing it after instead of before? Seems like the cost would roughly be the same, or am I thinking about this the wrong way?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/32/comments,https://github.com/soumith/convnet-benchmarks/pull/32#issuecomment-63510706,https://api.github.com/repos/soumith/convnet-benchmarks/issues/32
soumith,convnet-benchmarks,49247762,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/63510956,63510956,MDEyOklzc3VlQ29tbWVudDYzNTEwOTU2,1310570,2014-11-18T17:33:30Z,2014-11-18T17:33:30Z,OWNER,"I meant, padding before the pooling (after the convolutions). https://github.com/torch/nn/blob/master/doc/containers.md#nn.DepthConcat
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/32/comments,https://github.com/soumith/convnet-benchmarks/pull/32#issuecomment-63510956,https://api.github.com/repos/soumith/convnet-benchmarks/issues/32
soumith,convnet-benchmarks,49247762,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/63511742,63511742,MDEyOklzc3VlQ29tbWVudDYzNTExNzQy,43829,2014-11-18T17:38:21Z,2014-11-18T17:38:21Z,NONE,"Right, the concat requires a copy operation anyway, might as well pad in one go. But if I understand correctly this is not equivalent to what GoogLeNet does. Say you have a 20x20 feature map, and you want to max-pool it 3x3s1, as in the inception module. Then afaik the GoogLeNet way is to pad it to 22x22, so that the output of the pooling operation is 20x20 again. Your approach will instead pool the 20x20 feature map to 18x18, and then pad to 20x20 again in the next DepthConcat. Is that right?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/32/comments,https://github.com/soumith/convnet-benchmarks/pull/32#issuecomment-63511742,https://api.github.com/repos/soumith/convnet-benchmarks/issues/32
soumith,convnet-benchmarks,49247762,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/63512026,63512026,MDEyOklzc3VlQ29tbWVudDYzNTEyMDI2,1310570,2014-11-18T17:40:25Z,2014-11-18T17:40:25Z,OWNER,"no, I kept all the semantics the same. There'll be a ZeroPadding before the pooling as well.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/32/comments,https://github.com/soumith/convnet-benchmarks/pull/32#issuecomment-63512026,https://api.github.com/repos/soumith/convnet-benchmarks/issues/32
soumith,convnet-benchmarks,49247762,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/63512194,63512194,MDEyOklzc3VlQ29tbWVudDYzNTEyMTk0,43829,2014-11-18T17:41:29Z,2014-11-18T17:41:29Z,NONE,"So you're doing additional padding before the 3x3s1 pooling after all then? I thought you meant to say that you avoid the additional padding operation by folding it into DepthConcat, but I guess I misunderstood.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/32/comments,https://github.com/soumith/convnet-benchmarks/pull/32#issuecomment-63512194,https://api.github.com/repos/soumith/convnet-benchmarks/issues/32
soumith,convnet-benchmarks,49247762,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/63512440,63512440,MDEyOklzc3VlQ29tbWVudDYzNTEyNDQw,1310570,2014-11-18T17:43:11Z,2014-11-18T17:43:11Z,OWNER,"what I have so far is not EXACT to the paper, I'll make it as consistent as I can, but it would be as simple as adding nn.SpatialZeroPadding at places. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/32/comments,https://github.com/soumith/convnet-benchmarks/pull/32#issuecomment-63512440,https://api.github.com/repos/soumith/convnet-benchmarks/issues/32
soumith,convnet-benchmarks,49247762,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/63554649,63554649,MDEyOklzc3VlQ29tbWVudDYzNTU0NjQ5,629706,2014-11-18T22:08:42Z,2014-11-18T22:08:42Z,CONTRIBUTOR,"> Unfortunately Theano currently misses the ""max pooling with zero padding"" feature that's needed for GoogLeNet.

Which could be ported [from Caffe](https://github.com/BVLC/caffe/blob/master/src/caffe/layers/pooling_layer.cu). I'm mildly interested in doing that, but it's not on top of my list.
(On a side note: if we port all kinds of implementations to Theano, then what is the point in benchmarking it?) 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/32/comments,https://github.com/soumith/convnet-benchmarks/pull/32#issuecomment-63554649,https://api.github.com/repos/soumith/convnet-benchmarks/issues/32
soumith,convnet-benchmarks,43761731,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/56697303,56697303,MDEyOklzc3VlQ29tbWVudDU2Njk3MzAz,1310570,2014-09-24T16:27:52Z,2014-09-24T16:27:52Z,OWNER,"thanks will run theano numbers again now.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/30/comments,https://github.com/soumith/convnet-benchmarks/pull/30#issuecomment-56697303,https://api.github.com/repos/soumith/convnet-benchmarks/issues/30
soumith,convnet-benchmarks,42545268,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/55292980,55292980,MDEyOklzc3VlQ29tbWVudDU1MjkyOTgw,1310570,2014-09-11T16:48:12Z,2014-09-11T16:48:12Z,OWNER,"@f0k Can you by any chance change the timing prints to something like this:
print '{0:60} ==> {1:10}'.format(name + 'bprop weights:', math.floor(tm*1000))

So that we get fixed-width columns. It helps so much for readability
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/29/comments,https://github.com/soumith/convnet-benchmarks/pull/29#issuecomment-55292980,https://api.github.com/repos/soumith/convnet-benchmarks/issues/29
soumith,convnet-benchmarks,42545268,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/55336687,55336687,MDEyOklzc3VlQ29tbWVudDU1MzM2Njg3,1310570,2014-09-11T22:11:47Z,2014-09-11T22:11:47Z,OWNER,"fixed now.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/29/comments,https://github.com/soumith/convnet-benchmarks/pull/29#issuecomment-55336687,https://api.github.com/repos/soumith/convnet-benchmarks/issues/29
soumith,convnet-benchmarks,42523587,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/55284014,55284014,MDEyOklzc3VlQ29tbWVudDU1Mjg0MDE0,629706,2014-09-11T15:47:19Z,2014-09-11T15:47:19Z,CONTRIBUTOR,":+1: for merge!

@soumith: Note that this is work-in-progress again, it doesn't use cuDNN optimally yet (in particular, it uses the forward pass algorithm for all three operations). So it's expected to be a lot slower than the cuDNN wrapper of Torch or Caffe for now.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/28/comments,https://github.com/soumith/convnet-benchmarks/pull/28#issuecomment-55284014,https://api.github.com/repos/soumith/convnet-benchmarks/issues/28
soumith,convnet-benchmarks,42518887,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/55268425,55268425,MDEyOklzc3VlQ29tbWVudDU1MjY4NDI1,629706,2014-09-11T14:07:34Z,2014-09-11T14:07:34Z,CONTRIBUTOR,"I can do that when it's ready.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/27/comments,https://github.com/soumith/convnet-benchmarks/issues/27#issuecomment-55268425,https://api.github.com/repos/soumith/convnet-benchmarks/issues/27
soumith,convnet-benchmarks,42518887,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/55270835,55270835,MDEyOklzc3VlQ29tbWVudDU1MjcwODM1,180987,2014-09-11T14:23:34Z,2014-09-11T14:23:34Z,CONTRIBUTOR,"I already have another PR with this gh-28, I wanted to test it rapidly:)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/27/comments,https://github.com/soumith/convnet-benchmarks/issues/27#issuecomment-55270835,https://api.github.com/repos/soumith/convnet-benchmarks/issues/27
soumith,convnet-benchmarks,42518887,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/55290088,55290088,MDEyOklzc3VlQ29tbWVudDU1MjkwMDg4,1310570,2014-09-11T16:27:21Z,2014-09-11T16:27:21Z,OWNER,"added new theano output.log with CuDNN numbers.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/27/comments,https://github.com/soumith/convnet-benchmarks/issues/27#issuecomment-55290088,https://api.github.com/repos/soumith/convnet-benchmarks/issues/27
soumith,convnet-benchmarks,42106146,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/54721230,54721230,MDEyOklzc3VlQ29tbWVudDU0NzIxMjMw,1310570,2014-09-06T17:19:11Z,2014-09-06T17:19:11Z,OWNER,"awesome! let me rerun theano benchmarks now!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/25/comments,https://github.com/soumith/convnet-benchmarks/issues/25#issuecomment-54721230,https://api.github.com/repos/soumith/convnet-benchmarks/issues/25
soumith,convnet-benchmarks,41648565,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/54074586,54074586,MDEyOklzc3VlQ29tbWVudDU0MDc0NTg2,1310570,2014-09-01T15:57:48Z,2014-09-01T15:57:48Z,OWNER,"I think it makes sense to add a synchronize right after https://github.com/f0k/convnet-benchmarks/blob/theano-benchmark-additions/theano/pylearn2_benchmark.py#L93 as well dont you think? Otherwise, it is just streaming 10 calls (which would give you lower numbers)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/24/comments,https://github.com/soumith/convnet-benchmarks/pull/24#issuecomment-54074586,https://api.github.com/repos/soumith/convnet-benchmarks/issues/24
soumith,convnet-benchmarks,41648565,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/54075135,54075135,MDEyOklzc3VlQ29tbWVudDU0MDc1MTM1,629706,2014-09-01T16:04:40Z,2014-09-01T16:06:18Z,CONTRIBUTOR,"1. No, when a theano function exits, everything is synchronized already -- at least I think so because timing via events did not change anything compared to timing ""from the outside"".
2. No, even if all 10 calls are streamed, we are waiting for the second event to be triggered (`end.synchronize()`), which can only happen after all scheduled calls have finished. And as we are using the default stream, they will always be executed in sequence.

On a side note, I think taking the minimum over a number calls would give a more accurate estimate of the performance of the implementation, because then it's more independent of what the rest of your system is doing while benchmarking. I see the numbers always change quite a bit when you push a new `output.log`, even if the underlying implementation hasn't changed at all. (Of course this is something that needs to be done the same in all benchmarks.)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/24/comments,https://github.com/soumith/convnet-benchmarks/pull/24#issuecomment-54075135,https://api.github.com/repos/soumith/convnet-benchmarks/issues/24
soumith,convnet-benchmarks,41648565,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/54075853,54075853,MDEyOklzc3VlQ29tbWVudDU0MDc1ODUz,1310570,2014-09-01T16:13:48Z,2014-09-01T16:13:48Z,OWNER,"> I think taking the minimum over a number calls would give a more accurate estimate of the performance of the implementation

I'm not convinced minimum is better than mean (or even better, median) :)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/24/comments,https://github.com/soumith/convnet-benchmarks/pull/24#issuecomment-54075853,https://api.github.com/repos/soumith/convnet-benchmarks/issues/24
soumith,convnet-benchmarks,41648565,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/54077156,54077156,MDEyOklzc3VlQ29tbWVudDU0MDc3MTU2,629706,2014-09-01T16:31:34Z,2014-09-01T17:10:12Z,CONTRIBUTOR,"> I'm not convinced minimum is better than mean

It depends on what you/we want to measure:
- The time needed to execute the instructions for computing a particular convolution on a particular GPU, in a particular implementation
- The time needed to compute a particular convolution in a particular neural-network training library on a particular GPU with a particular operating system and other software running at the same time, the same that will usually run when actually training a network

For the former, [the minimum should give a better and more stable estimate](https://docs.python.org/2/library/timeit.html#timeit.Timer.repeat), and averaging over multiple runs would only be used if the timing method was not accurate enough compared to the execution time. For the latter, the mean is a better choice, or maybe the median. In both cases, the number of runs should be tweaked such that the variance of results is low enough to reliably compare different implementations -- i.e., in our case, timings shouldn't vary by more than 1ms when running the benchmark skript twice. Minimum and median will need fewer repetitions than the mean.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/24/comments,https://github.com/soumith/convnet-benchmarks/pull/24#issuecomment-54077156,https://api.github.com/repos/soumith/convnet-benchmarks/issues/24
soumith,convnet-benchmarks,41648565,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/54080449,54080449,MDEyOklzc3VlQ29tbWVudDU0MDgwNDQ5,1310570,2014-09-01T17:24:41Z,2014-09-01T17:24:41Z,OWNER,"added new numbers. strange error at the top is generated now, trying to allocate 8GB of mem, but that doesn't really stop the rest of the benchmark from running smoothly.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/24/comments,https://github.com/soumith/convnet-benchmarks/pull/24#issuecomment-54080449,https://api.github.com/repos/soumith/convnet-benchmarks/issues/24
soumith,convnet-benchmarks,41648565,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/54080561,54080561,MDEyOklzc3VlQ29tbWVudDU0MDgwNTYx,1310570,2014-09-01T17:26:36Z,2014-09-01T17:26:36Z,OWNER,"- [cuda-conv] partial_sum is kinda expensive (fails in layer 2)
- manual GpuCorrMM does seem a little faster for a few layers.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/24/comments,https://github.com/soumith/convnet-benchmarks/pull/24#issuecomment-54080561,https://api.github.com/repos/soumith/convnet-benchmarks/issues/24
soumith,convnet-benchmarks,41648565,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/54082330,54082330,MDEyOklzc3VlQ29tbWVudDU0MDgyMzMw,629706,2014-09-01T17:57:27Z,2014-09-01T17:57:27Z,CONTRIBUTOR,"> strange error at the top is generated now, trying to allocate 8GB of mem

Yep, that's what I meant by ""a more memory-hungry setting"" :) There are configurations where it works and makes a huge difference, though. E.g., try running the benchmark with ""i1x29x287,k32x6x6,b256"" on the command line. It may also work with a little more conservative setting such as `partial_sum=2`, but we probably don't want to fine-tune the implementation for each layer...

So I'd suggest to keep the table unchanged for now. The manual CorrMM is more close to Caffe and Torch because it avoids flipping the kernels (something that neither caffe nor Torch do), but it's probably not worth updating the table for the few milliseconds shaved off. For cuda-convnet we should decide what to do -- in L1 the difference in bprop weights is significant, but for other layers the memory-hungry setting is actually slower.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/24/comments,https://github.com/soumith/convnet-benchmarks/pull/24#issuecomment-54082330,https://api.github.com/repos/soumith/convnet-benchmarks/issues/24
soumith,convnet-benchmarks,41200421,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/53466352,53466352,MDEyOklzc3VlQ29tbWVudDUzNDY2MzUy,1310570,2014-08-26T18:25:07Z,2014-08-26T18:25:07Z,OWNER,"you can see this from the diffs of the last few log commits. The green regions are 6.5, red regions are 6.0 (except for ccn2 where some code was changed for performance improvement):

https://github.com/soumith/convnet-benchmarks/commit/5d58ad3117b24fe05a3879d480d522cae51a6307
https://github.com/soumith/convnet-benchmarks/commit/26f60dd2be6fa918aeb4835080ed69a1926563c9
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/23/comments,https://github.com/soumith/convnet-benchmarks/issues/23#issuecomment-53466352,https://api.github.com/repos/soumith/convnet-benchmarks/issues/23
soumith,convnet-benchmarks,41079530,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/53293791,53293791,MDEyOklzc3VlQ29tbWVudDUzMjkzNzkx,1310570,2014-08-25T17:14:39Z,2014-08-25T17:14:39Z,OWNER,"I already added corrmm to theano benchmarks, didn't find time to add it to the table yet.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/22/comments,https://github.com/soumith/convnet-benchmarks/issues/22#issuecomment-53293791,https://api.github.com/repos/soumith/convnet-benchmarks/issues/22
soumith,convnet-benchmarks,40658871,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/52733720,52733720,MDEyOklzc3VlQ29tbWVudDUyNzMzNzIw,1310570,2014-08-20T04:49:28Z,2014-08-20T04:49:28Z,OWNER,"thanks for pointing it out, looking at it now, only ran it 15 days ago
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/20/comments,https://github.com/soumith/convnet-benchmarks/issues/20#issuecomment-52733720,https://api.github.com/repos/soumith/convnet-benchmarks/issues/20
soumith,convnet-benchmarks,40658871,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/52735202,52735202,MDEyOklzc3VlQ29tbWVudDUyNzM1MjAy,1310570,2014-08-20T05:23:09Z,2014-08-20T05:23:09Z,OWNER,"fixed instructions.

> the gradient is not computed with respect to the bottom data, as one would expect for the first layer of a network.

oh, I'm assuming that you are saying gradient w.r.t. inputs is not computed.  Can you say how you came to this conclusion? A couple of us looked at the code about 15 days ago and we found that it does compute it, but maybe things changed?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/20/comments,https://github.com/soumith/convnet-benchmarks/issues/20#issuecomment-52735202,https://api.github.com/repos/soumith/convnet-benchmarks/issues/20
soumith,convnet-benchmarks,40658871,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/52809184,52809184,MDEyOklzc3VlQ29tbWVudDUyODA5MTg0,8460517,2014-08-20T17:07:53Z,2014-08-20T17:07:53Z,NONE,"> oh, I'm assuming that you are saying gradient w.r.t. inputs is not computed.

Right, the block of ConvolutionLayer<Dtype>::Backward_gpu with the comment

```
    // gradient w.r.t. bottom data, if necessary
    if (propagate_down[i]) {
```

is not entered in this benchmark. It is easy to verify this by putting a printf after the if statement.

Gradient w.r.t. inputs is computed if ""force_backward: true"" is added to the conv*.protoxt files.

There is a short explanation of force_backward here:
https://github.com/BVLC/caffe/issues/583
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/20/comments,https://github.com/soumith/convnet-benchmarks/issues/20#issuecomment-52809184,https://api.github.com/repos/soumith/convnet-benchmarks/issues/20
soumith,convnet-benchmarks,40658871,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/52809762,52809762,MDEyOklzc3VlQ29tbWVudDUyODA5NzYy,1310570,2014-08-20T17:12:10Z,2014-08-20T17:12:10Z,OWNER,"Thanks, this gives me so much relief wrt numbers as we were having a hard time reproducing caffe numbers in theano and torch.

I'll change the proto files in a bit to force the gradinput
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/20/comments,https://github.com/soumith/convnet-benchmarks/issues/20#issuecomment-52809762,https://api.github.com/repos/soumith/convnet-benchmarks/issues/20
soumith,convnet-benchmarks,40658871,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/52852434,52852434,MDEyOklzc3VlQ29tbWVudDUyODUyNDM0,8460517,2014-08-20T22:13:31Z,2014-08-20T22:13:31Z,NONE,"Great! Maybe we should also rename the layers in the proto files to ""conv1"", ""conv2"", .., ""conv5"", to improve readability of the log file. Now they are all named ""conv1"".
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/20/comments,https://github.com/soumith/convnet-benchmarks/issues/20#issuecomment-52852434,https://api.github.com/repos/soumith/convnet-benchmarks/issues/20
soumith,convnet-benchmarks,40658871,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/52874700,52874700,MDEyOklzc3VlQ29tbWVudDUyODc0NzAw,1310570,2014-08-21T03:32:37Z,2014-08-21T03:32:37Z,OWNER,"added the naming change as well, and forced gradInput computation too. thanks for pointing this out, very very helpful.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/20/comments,https://github.com/soumith/convnet-benchmarks/issues/20#issuecomment-52874700,https://api.github.com/repos/soumith/convnet-benchmarks/issues/20
soumith,convnet-benchmarks,40370805,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/52341147,52341147,MDEyOklzc3VlQ29tbWVudDUyMzQxMTQ3,1310570,2014-08-15T18:26:56Z,2014-08-15T18:26:56Z,OWNER,":D awesome. 

I added the code into convnetjs/benchmark.js with instructions and ran it on the benchmark rig (added output.log)

There's a 3x3 convolution filter in webgl here:
https://github.com/phoboslab/WebGLImageFilter/blob/master/webgl-image-filter.js#L461
But webgl mostly goes over my head, as I don't have a formal understanding of the graphics shader pipeline. 

I'd be excited to see more on the WebGL frontend!

I looked into hooking torch's backend C/CUDA libraries into nodejs, but writing bindings for nodejs is such a pain! (and the FFI interface has a non-trivial overhead).
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/19/comments,https://github.com/soumith/convnet-benchmarks/issues/19#issuecomment-52341147,https://api.github.com/repos/soumith/convnet-benchmarks/issues/19
soumith,convnet-benchmarks,40370805,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/52348443,52348443,MDEyOklzc3VlQ29tbWVudDUyMzQ4NDQz,241138,2014-08-15T19:34:17Z,2014-08-15T19:34:17Z,NONE,"Thanks, I found the link you mentioned a while ago and it has been helpful. Unfortunately things get complicated because a lot of code (like this one) assume 4depth input (RGBA) and 4depth output. But the extensions are relatively straight forward (though a little annoying).

Another possibility is Emscritpen? Not sure if anyone has tried to look into it. I think it can compile C++ code into JS but I haven't fully explored it.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/19/comments,https://github.com/soumith/convnet-benchmarks/issues/19#issuecomment-52348443,https://api.github.com/repos/soumith/convnet-benchmarks/issues/19
soumith,convnet-benchmarks,40167020,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/52065733,52065733,MDEyOklzc3VlQ29tbWVudDUyMDY1NzMz,1310570,2014-08-13T15:36:42Z,2014-08-13T15:36:42Z,OWNER,"Yea, Alex mentions that in his document as well, it's tricky to tune this thing universally
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/18/comments,https://github.com/soumith/convnet-benchmarks/issues/18#issuecomment-52065733,https://api.github.com/repos/soumith/convnet-benchmarks/issues/18
soumith,convnet-benchmarks,40167020,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/52591520,52591520,MDEyOklzc3VlQ29tbWVudDUyNTkxNTIw,180987,2014-08-19T05:17:36Z,2014-08-19T05:17:36Z,CONTRIBUTOR,"What about giving the 2 extreme values? With the default and the more
memory hungry version?

On Wed, Aug 13, 2014 at 11:36 AM, Soumith Chintala <notifications@github.com

> wrote:
> 
> Yea, Alex mentions that in his document as well, it's tricky to tune this
> thing universally
> 
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/issues/18#issuecomment-52065733
> .
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/18/comments,https://github.com/soumith/convnet-benchmarks/issues/18#issuecomment-52591520,https://api.github.com/repos/soumith/convnet-benchmarks/issues/18
soumith,convnet-benchmarks,40167020,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/54082388,54082388,MDEyOklzc3VlQ29tbWVudDU0MDgyMzg4,629706,2014-09-01T17:58:42Z,2014-09-01T17:58:42Z,CONTRIBUTOR,"> What about giving the 2 extreme values? With the default and the more memory hungry version?

Addressed in #24. The ""more memory hungry version"" does not execute on standard GPUs for one of the layers, it asks for some 8 GiB of memory :)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/18/comments,https://github.com/soumith/convnet-benchmarks/issues/18#issuecomment-54082388,https://api.github.com/repos/soumith/convnet-benchmarks/issues/18
soumith,convnet-benchmarks,40146537,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/52068885,52068885,MDEyOklzc3VlQ29tbWVudDUyMDY4ODg1,1310570,2014-08-13T15:57:09Z,2014-08-13T15:57:09Z,OWNER,"added new output.log! thanks.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/17/comments,https://github.com/soumith/convnet-benchmarks/pull/17#issuecomment-52068885,https://api.github.com/repos/soumith/convnet-benchmarks/issues/17
soumith,convnet-benchmarks,40146537,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/52071547,52071547,MDEyOklzc3VlQ29tbWVudDUyMDcxNTQ3,629706,2014-08-13T16:15:43Z,2014-08-13T16:15:43Z,CONTRIBUTOR,"Thanks for merging! The timings for `conv_gemm` are suspiciously close to the non-experimental Theano standard convolution, though... I think you will need to update to the latest Theano and possibly also delete your build directory to be sure (`~/.theano/compiledir-*`).
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/17/comments,https://github.com/soumith/convnet-benchmarks/pull/17#issuecomment-52071547,https://api.github.com/repos/soumith/convnet-benchmarks/issues/17
soumith,convnet-benchmarks,40146537,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/52073605,52073605,MDEyOklzc3VlQ29tbWVudDUyMDczNjA1,1310570,2014-08-13T16:31:06Z,2014-08-13T16:31:06Z,OWNER,"oh right that's true, i'll do that.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/17/comments,https://github.com/soumith/convnet-benchmarks/pull/17#issuecomment-52073605,https://api.github.com/repos/soumith/convnet-benchmarks/issues/17
soumith,convnet-benchmarks,40146537,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/52239860,52239860,MDEyOklzc3VlQ29tbWVudDUyMjM5ODYw,1310570,2014-08-14T20:30:40Z,2014-08-14T20:30:40Z,OWNER,"I just reinstalled all the libraries again and ran the script (checked in the new output.log). Looks like the numbers have changed quite a bit for the default theano conv2d, something looks fishy, is it not getting GPU-compiled anymore?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/17/comments,https://github.com/soumith/convnet-benchmarks/pull/17#issuecomment-52239860,https://api.github.com/repos/soumith/convnet-benchmarks/issues/17
soumith,convnet-benchmarks,40146537,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/52247278,52247278,MDEyOklzc3VlQ29tbWVudDUyMjQ3Mjc4,3520613,2014-08-14T21:28:11Z,2014-08-14T21:28:11Z,CONTRIBUTOR,"No its an issue with the current Theano build:

-------- Forwarded Message --------
Subject:    [Lisa_labo] Theano's GPU Backend Has Become Extremely Slow
Date:   Thu, 14 Aug 2014 14:24:24 -0700
From:   Mehdi Mirza memirzamo@gmail.com
To:     lisa_labo Labo lisa_labo@iro.umontreal.ca, FrÃ©dÃ©ric Bastien nouiz@nouiz.org

There is something very wrong has happened with the recent changes to Theano's GPU backend.
For an example pylearn2/scripts/papers/maxout.yaml
It should normally run in less than 10secs/epoch. Now it's taking more than 3 hours.
I have tried this multiple times, with clean head of the branches, clearing theano cache and on three different clusters and 4 machines.
Even compilation is taking > 20mins
And the same script with cpu runs without a problem around 25sec/epoch.
There is someone on the pylearn-users mailing list mentioning this too:
Ã‚ https://groups.google.com/forum/#!topic/pylearn-users/_W_Ytoq2dNU
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/17/comments,https://github.com/soumith/convnet-benchmarks/pull/17#issuecomment-52247278,https://api.github.com/repos/soumith/convnet-benchmarks/issues/17
soumith,convnet-benchmarks,40146537,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/52347094,52347094,MDEyOklzc3VlQ29tbWVudDUyMzQ3MDk0,629706,2014-08-15T19:20:54Z,2014-08-15T19:20:54Z,CONTRIBUTOR,"@soumith: The Theano problem [seems to have been fixed](https://github.com/Theano/Theano/pull/2037). If you update Theano and delete your `~/.theano/compiledir*`, it should be fine. The other libraries do not need to be reinstalled. But no need to hurry, it will be more interesting to re-run the benchmark when the corrmm port is finished.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/17/comments,https://github.com/soumith/convnet-benchmarks/pull/17#issuecomment-52347094,https://api.github.com/repos/soumith/convnet-benchmarks/issues/17
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51194754,51194754,MDEyOklzc3VlQ29tbWVudDUxMTk0NzU0,1310570,2014-08-05T13:11:39Z,2014-08-05T13:11:39Z,OWNER,"propagate_down is indeed true! [This is the benchmark code, and the place where it is set to true.](https://github.com/BVLC/caffe/blob/b1c4f121b4f01b538eef6997ba3af6c9a71afd31/tools/net_speed_benchmark.cpp#L90)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-51194754,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51205831,51205831,MDEyOklzc3VlQ29tbWVudDUxMjA1ODMx,3520613,2014-08-05T14:35:03Z,2014-08-05T14:35:03Z,CONTRIBUTOR,"Are you sure caffe also updates the parameters (or accumulates the parameter gradients) in the code?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-51205831,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51205954,51205954,MDEyOklzc3VlQ29tbWVudDUxMjA1OTU0,1310570,2014-08-05T14:35:53Z,2014-08-05T14:35:53Z,OWNER,"yes, it does that here: https://github.com/BVLC/caffe/blob/master/src/caffe/layers/conv_layer.cu#L79
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-51205954,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51206106,51206106,MDEyOklzc3VlQ29tbWVudDUxMjA2MTA2,1310570,2014-08-05T14:37:04Z,2014-08-05T14:37:04Z,OWNER,"if we want a double confirmation, we can ask @rbgirshick who is watching this repo. Ross, what do you say?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-51206106,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51206547,51206547,MDEyOklzc3VlQ29tbWVudDUxMjA2NTQ3,3520613,2014-08-05T14:40:01Z,2014-08-05T14:40:01Z,CONTRIBUTOR,"Yeah your link seems to confirm it. Yet the difference between forward and backward is so much smaller for caffe. I wonder what their secret is. Isn't SpatialConvolutionMM supposed to be using the same tricks anyhow?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-51206547,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51206849,51206849,MDEyOklzc3VlQ29tbWVudDUxMjA2ODQ5,1310570,2014-08-05T14:41:57Z,2014-08-05T14:41:57Z,OWNER,"> Isn't SpatialConvolutionMM supposed to be using the same tricks anyhow?

Yes, SpatialConvolutionMM is borrowed from Caffe (thanks guys), but we are using the cublas v1 interface (whereas Caffe uses cublas v2), we should fix that soon.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-51206849,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51207824,51207824,MDEyOklzc3VlQ29tbWVudDUxMjA3ODI0,629706,2014-08-05T14:48:13Z,2014-08-05T14:48:13Z,CONTRIBUTOR,"Another question: When you time, do you synchronize with the device somewhere? (Otherwise it would synchronize on the next `cudaMemset` call or similarly, which may be hidden in the next `caffe_net` call.) I don't see it in your benchmark, but maybe `layers[i]->Backward` does more than just calling `Backward_gpu` or I missed something else.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-51207824,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51208637,51208637,MDEyOklzc3VlQ29tbWVudDUxMjA4NjM3,1310570,2014-08-05T14:53:53Z,2014-08-05T14:53:53Z,OWNER,"@f0k that does seem to be a good point! the utility I use is Caffe's own, and looking through (https://github.com/BVLC/caffe/blob/master/include/caffe/layer.hpp), I don't see an explicit synchronize either. I'll have to look into this further.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-51208637,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51210069,51210069,MDEyOklzc3VlQ29tbWVudDUxMjEwMDY5,629706,2014-08-05T15:03:07Z,2014-08-05T15:03:37Z,CONTRIBUTOR,"I see, the code is here: https://github.com/BVLC/caffe/blob/master/src/caffe/util/benchmark.cpp
So basically you insert a start and stop event into the stream (not sure if that's the correct terminology) with `cudaEventRecord`, then wait for the stop event to be run over, and finally return the time difference between the events with `cudaEventElapsedTime`. Seems proper, but maybe the other benchmarks should do the same to ensure only the GPU time is measured, or the caffe benchmark should time ""from the outside"" just like the others.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-51210069,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51211029,51211029,MDEyOklzc3VlQ29tbWVudDUxMjExMDI5,1310570,2014-08-05T15:09:33Z,2014-08-05T15:09:33Z,OWNER,"> but maybe the other benchmarks should do the same to ensure only the GPU time is measured, or the caffe benchmark should time ""from the outside"" just like the others.

I could write a caffe benchmark to time ""from the outside"", but I doubt the speed is going to change by much, if at all.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-51211029,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51217105,51217105,MDEyOklzc3VlQ29tbWVudDUxMjE3MTA1,3520613,2014-08-05T15:50:58Z,2014-08-05T15:50:58Z,CONTRIBUTOR,"@f0k I think a call `cudaDeviceSynchronize` after the calls to forward/backward. Simpler than events, and conforms to other benchmarks.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-51217105,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51217873,51217873,MDEyOklzc3VlQ29tbWVudDUxMjE3ODcz,629706,2014-08-05T15:56:08Z,2014-08-05T15:56:08Z,CONTRIBUTOR,"> @f0k I think a call  cudaDeviceSynchronize  after the calls to forward/backward. Simpler than events, and conforms to other benchmarks.

@nicholas-leonard I was just explaining what the currently used caffe benchmark code does, to answer my own question of whether it's properly synchronized. So yes, it is properly synchronized, but it does things differently than the other benchmarks.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-51217873,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51218044,51218044,MDEyOklzc3VlQ29tbWVudDUxMjE4MDQ0,180987,2014-08-05T15:57:15Z,2014-08-05T15:57:15Z,CONTRIBUTOR,"The Theano benchmark have a sync before the timming too. I don't know if
that is useful, but I think consistency between software is better.:

```
    theano.sandbox.cuda.synchronize()
    start = time.time()
    for i in range(steps):
        fprop()
    theano.sandbox.cuda.synchronize()
    tm = (time.time()-start)/steps
```

On Tue, Aug 5, 2014 at 11:50 AM, Nicholas LÃ©onard notifications@github.com
wrote:

> @f0k https://github.com/f0k I think a call cudaDeviceSynchronize after
> the calls to forward/backward. Simpler than events, and conforms to other
> benchmarks.
> 
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-51217105
> .
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-51218044,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/52828584,52828584,MDEyOklzc3VlQ29tbWVudDUyODI4NTg0,8460517,2014-08-20T19:11:08Z,2014-08-20T19:11:08Z,NONE,"Perhaps the gradient w.r.t. inputs should be left out of the L1 benchmark, because it would not actually be computed in the first layer of a real network. Theano results seem particularly skewed by this factor.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-52828584,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/52832359,52832359,MDEyOklzc3VlQ29tbWVudDUyODMyMzU5,180987,2014-08-20T19:38:40Z,2014-08-20T19:38:40Z,CONTRIBUTOR,"I think it should be available as information somewhere, as for the middle
layer it is computed. But maybe we could do 2 summary (fprop + grad
weidght) and (fprop + the 2 grad).

On Wed, Aug 20, 2014 at 3:11 PM, Andrew Lavin notifications@github.com
wrote:

> Perhaps the gradient w.r.t. inputs should be left out of the L1 benchmark,
> because it would not actually be computed in the first layer of a real
> network. Theano results seem particularly skewed by this factor.
> 
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-52828584
> .
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-52832359,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/52843858,52843858,MDEyOklzc3VlQ29tbWVudDUyODQzODU4,8460517,2014-08-20T21:04:28Z,2014-08-20T21:04:28Z,NONE,"@nouiz L1 is clearly not a middle layer, so the only necessary change is to remove the gradInput calculation from the L1 results.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-52843858,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/53374210,53374210,MDEyOklzc3VlQ29tbWVudDUzMzc0MjEw,1310570,2014-08-26T04:29:14Z,2014-08-26T04:29:14Z,OWNER,"I benchmarked theano, torch, ccn2 and caffe with cuda 6.5. 
Also Alex updated ccn2 to add some more performance improvements.
Also, fixed theano benchmark to average over 10 iterations (like the others), rather than taking the minimum of 10 iterations.
Also added theano's corrMM to the table.

ccn2 is now a very close second to Caffe (caffe is cheating a bit in the benchmark because of timing just around the cuda kernel, and not the rest of the cpu cleanup around it).
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-53374210,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/53374272,53374272,MDEyOklzc3VlQ29tbWVudDUzMzc0Mjcy,4626592,2014-08-26T04:30:34Z,2014-08-26T04:30:34Z,NONE,"Hey @soumith , the **\* next to corrMM is not correct (uses a lot of extra memory).
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-53374272,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/53374477,53374477,MDEyOklzc3VlQ29tbWVudDUzMzc0NDc3,1310570,2014-08-26T04:35:02Z,2014-08-26T04:35:02Z,OWNER,"right, fixed.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-53374477,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/53374519,53374519,MDEyOklzc3VlQ29tbWVudDUzMzc0NTE5,4626592,2014-08-26T04:35:53Z,2014-08-26T04:35:53Z,NONE,"Thanks! And thanks for updating the table.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-53374519,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/54655826,54655826,MDEyOklzc3VlQ29tbWVudDU0NjU1ODI2,1377402,2014-09-05T17:29:19Z,2014-09-05T17:29:19Z,NONE,"I agree with @andravin about omitting the L1 grad-wrt-inputs from the overall summary (at least in a special column). That _really_ hurts the Theano legacy numbers, and it isn't even relevant for how these routines would actually be used. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-54655826,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/54724750,54724750,MDEyOklzc3VlQ29tbWVudDU0NzI0NzUw,1310570,2014-09-06T19:03:14Z,2014-09-06T19:03:14Z,OWNER,"Thanks to @nouiz who fixed an issue with theano fft gradInput, now Theano FFT implementation is 1.5x faster than Caffe (which is 2nd place).

Now is the time to also start a conversation about 3x3 convolutions, as this year's Imagenet challenge had the VGG team lead by Karen Simonyan use just 3x3 convolutions across the whole network, starting from 224x224 image!
http://arxiv.org/abs/1409.1556/

Should I add a few layer benchmarks that are relevant?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-54724750,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/54725113,54725113,MDEyOklzc3VlQ29tbWVudDU0NzI1MTEz,4626592,2014-09-06T19:12:02Z,2014-09-06T19:12:21Z,NONE,"Thanks a lot for sharing this paper!

Wow  16â€“19 weight layers?! Perhaps we are really moving towards the networks like in the brain. Too bad recurrent nets are still hard to train. We definitely also need to be able to train a model on multiple gpu's in the future. @soumith, does torch plan to do anything on these lines in the near future(multiple GPU support)? 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-54725113,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/54725461,54725461,MDEyOklzc3VlQ29tbWVudDU0NzI1NDYx,1310570,2014-09-06T19:24:09Z,2014-09-06T19:24:09Z,OWNER,"@stencilman it depends on how transparent you want it to be (multigpu can already be done in torch if you are careful with a few things), but this is not the thread to discuss it, let's open a thread in cutorch
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-54725461,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/54725483,54725483,MDEyOklzc3VlQ29tbWVudDU0NzI1NDgz,4626592,2014-09-06T19:25:03Z,2014-09-06T19:25:03Z,NONE,"@soumith Yes, sorry, lets open a thread in cutorch.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-54725483,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/54725698,54725698,MDEyOklzc3VlQ29tbWVudDU0NzI1Njk4,1310570,2014-09-06T19:32:10Z,2014-09-06T19:32:10Z,OWNER,"@ebattenberg I was waiting for this year's imagenet challenge to finish, but now you see why the GradInput and GradWeight numbers for L1 are starting to get relevant!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-54725698,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/54795206,54795206,MDEyOklzc3VlQ29tbWVudDU0Nzk1MjA2,629706,2014-09-08T09:33:41Z,2014-09-08T09:33:41Z,CONTRIBUTOR,"> Thanks to @nouiz who fixed an issue with theano fft gradInput, now Theano FFT implementation is 1.5x faster than Caffe (which is 2nd place).

Oh cool! Do you know what exactly was fixed?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-54795206,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/54795755,54795755,MDEyOklzc3VlQ29tbWVudDU0Nzk1NzU1,1310570,2014-09-08T09:37:26Z,2014-09-08T09:37:26Z,OWNER,"https://github.com/nouiz/Theano/commit/092e81e009cd531776ef804633f19be1d4f8787a
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-54795755,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/54822694,54822694,MDEyOklzc3VlQ29tbWVudDU0ODIyNjk0,180987,2014-09-08T14:08:34Z,2014-09-08T14:08:34Z,CONTRIBUTOR,"About the next convolution benchmark, what about 3d convolution for video?

On Mon, Sep 8, 2014 at 5:37 AM, Soumith Chintala notifications@github.com
wrote:

> nouiz/Theano@092e81e
> https://github.com/nouiz/Theano/commit/092e81e009cd531776ef804633f19be1d4f8787a
> 
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-54795755
> .
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-54822694,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/54841576,54841576,MDEyOklzc3VlQ29tbWVudDU0ODQxNTc2,1310570,2014-09-08T15:51:54Z,2014-09-08T15:51:54Z,OWNER,"@nouiz I am totally up for it, Theano and Caffe have Volumetric convolutions!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-54841576,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/55196251,55196251,MDEyOklzc3VlQ29tbWVudDU1MTk2MjUx,1310570,2014-09-10T22:57:58Z,2014-09-10T22:57:58Z,OWNER,"Added NVidia CuDNN results to table
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-55196251,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/55946177,55946177,MDEyOklzc3VlQ29tbWVudDU1OTQ2MTc3,43829,2014-09-17T19:23:50Z,2014-09-17T19:23:50Z,NONE,"Although the CuDNN results in this benchmark are not too spectacular, I tried it out myself today with Theano, and compared it with some other available implementations (legacy conv2d, fftconv, GpuCorrMM).

CuDNN mainly seems to have an edge for convolutions with lots of small filters (i.e. 3x3). I saw speedups of almost 2x compared to GpuCorrMM, and it was often faster than fftconv as well (I didn't compare to cuda-convnet because the call signature is different).

Considering that the 2nd placed entry in this year's ImageNet competition was a very deep convnet with 3x3 filters everywhere, this might actually turn out to be an interesting configuration to benchmark, imho.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-55946177,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/55948500,55948500,MDEyOklzc3VlQ29tbWVudDU1OTQ4NTAw,1310570,2014-09-17T19:41:03Z,2014-09-17T19:41:03Z,OWNER,"if you see the torch7 folder, i benchmarked all the VGG layers here:
https://github.com/soumith/convnet-benchmarks/blob/master/torch7/vgg.bench
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-55948500,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/55949098,55949098,MDEyOklzc3VlQ29tbWVudDU1OTQ5MDk4,43829,2014-09-17T19:45:39Z,2014-09-17T19:45:39Z,NONE,"Interesting, from those results the conclusion would be pretty different, if I'm reading them correctly. I wonder why it makes such a big difference for what I tried (very simple network on MNIST, C3-C3-P2-C3-C3-P2-F-F-F). I guess there could be other parameters that play a role, or the fact that I'm using Theano, or maybe the GTX 680 I ran it on.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-55949098,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/55949443,55949443,MDEyOklzc3VlQ29tbWVudDU1OTQ5NDQz,4626592,2014-09-17T19:48:27Z,2014-09-17T19:48:27Z,NONE,"@benanne: if you look at https://github.com/soumith/convnet-benchmarks/commit/4210e80c37da9d0d550317c433ee02d80f286a64, it seems to be that in Theano only the fprop has been optimized for CuDNN. Others not.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-55949443,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/55949734,55949734,MDEyOklzc3VlQ29tbWVudDU1OTQ5NzM0,1310570,2014-09-17T19:50:35Z,2014-09-17T19:50:35Z,OWNER,"@benanne the input and output image sizes also matter quite a bit. So MNIST wouldn't be representational of the performance of the VGG layers even though the filter sizes might be the same
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-55949734,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/55949876,55949876,MDEyOklzc3VlQ29tbWVudDU1OTQ5ODc2,43829,2014-09-17T19:51:36Z,2014-09-17T19:51:36Z,NONE,"@stencilman I see, so it's just using the forward pass implementation for all three operations (forward, backward w.r.t. weights, backward w.r.t. input). But that doesn't explain why it seems to come out on top when I test it.

@soumith That, however, would explain some things :)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-55949876,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/55994632,55994632,MDEyOklzc3VlQ29tbWVudDU1OTk0NjMy,1310570,2014-09-18T04:48:02Z,2014-09-18T04:48:02Z,OWNER,"Just added benchmarks for Maxime Oquab's (@qassemoquab) BHWD layout kernels. 
https://github.com/qassemoquab/nnbhwd
They perform pretty decent for the current benchmark layers, but this benchmark doesn't really do justice.
His module really shines when there are lots and lots of feature maps (applicable to certain domains).
He does banded unrolling + sgemm
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-55994632,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/56034862,56034862,MDEyOklzc3VlQ29tbWVudDU2MDM0ODYy,629706,2014-09-18T13:05:11Z,2014-09-18T13:05:11Z,CONTRIBUTOR,"> > @benanne: if you look at  4210e80 , it seems to be that in Theano only the fprop has been optimized for CuDNN.
> 
> @stencilman I see, so it's just using the forward pass implementation for all three operations (forward, backward w.r.t. weights, backward w.r.t. input).

Hmm, that's interesting. @abergeron [convinced me](https://github.com/Theano/Theano/pull/2096#issuecomment-55294921) that cuDNN shouldn't perform worse when just using `cudnnConvolutionForward` for everything, because it should select the optimal kernel to use under the hood -- e.g., asking it to do a forward pass with padding matching the kernel size should be as fast as asking it to do a backward pass wrt. input (full convolution). Maybe that's not the case. We should investigate and file a bug with Nvidia.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-56034862,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/56062554,56062554,MDEyOklzc3VlQ29tbWVudDU2MDYyNTU0,1002504,2014-09-18T16:10:44Z,2014-09-18T16:10:44Z,NONE,"To see if that is really a problem you can try the new version of the op that is https://github.com/Theano/Theano/pull/2117. If used directly the grad will use the Backward calls.

Also, I don't know what sort of timings you have, but I know that if you rely on the optimization to use cudnn for the gradient, you will end up with a bunch of potentially useless copies (because theano flips the kernels manually before the convolution and I have to do a call to gpu_contiguous).  This might be the source of the slowdown on the gradient part.

It might be possible to avoid the gpu_contigous call in most cases, but cudnn officially does not support negative strides which tend to happen somewhat often in the convolution case.  More work (but not too much) would be needed to make it work without it.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-56062554,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/56235339,56235339,MDEyOklzc3VlQ29tbWVudDU2MjM1MzM5,1002504,2014-09-19T21:05:19Z,2014-09-19T21:06:30Z,NONE,"I re-ran the test with this patch:

``` patch
diff --git a/theano/pylearn2_benchmark.py b/theano/pylearn2_benchmark.py
index a110628..be4c75d 100644
--- a/theano/pylearn2_benchmark.py
+++ b/theano/pylearn2_benchmark.py
@@ -203,7 +203,17 @@ for run in runs:
         mode = theano.compile.get_default_mode()
         mode = mode.including('cudnn')
         benchmark_three_ways('(experimental, auto) theano.sandbox.cuda.dnn.GpuDnnConv',
-                                sharedX, sharedY, sharedW, X, Y, gW, gX, mode)
+                             sharedX, sharedY, sharedW, X, Y, gW, gX, mode)
+
+        mode = theano.compile.get_default_mode()
+        mode = mode.including('gpu')
+        dnnX = theano.sandbox.cuda.CudaNdarrayType((False, False, False, False))(name='dnnX')
+        dnnY = theano.sandbox.cuda.dnn.dnn_conv(sharedX, sharedW, border_mode='valid', subsample=(dw,dh))
+        dnngW = theano.grad(None, wrt=sharedW, known_grads={dnnY: sharedY})
+        dnngX = theano.grad(None, wrt=sharedX, known_grads={dnnY: sharedY})
+        benchmark_three_ways('(experimental, manual) '
+                             'theano.sandbox.cuda.dnn.GpuDnnConv',
+                             sharedX, sharedY, sharedW, dnnX, dnnY, dnngW, dnngX, mode)

     # benchmark caffe-like gemm convolution
     # Mimic Theano flag THEANO_FLAGS=optimizer_including=conv_gemm
```

And got these results:

```
Using gpu device 0: GeForce GTX 750 Ti
Note: pycuda not available, no timing via CUDA events possible

CONFIG: input = 3 x 128 x 128 * ker = 3 x 96 x 11 x 11 ( bs = 128 , stride = 1 )
theano.tensor.nnet.conv.conv2d                                         ==> fprop           ==>     1885.0
theano.tensor.nnet.conv.conv2d                                         ==> bprop inputs    ==>    54169.0
theano.tensor.nnet.conv.conv2d                                         ==> bprop weights   ==>     1836.0

(experimental) theano.sandbox.cuda.fftconv.conv2d_fft fprop: FAILED
(experimental) theano.sandbox.cuda.fftconv.conv2d_fft bprop inputs: FAILED
(experimental) theano.sandbox.cuda.fftconv.conv2d_fft bprop weights: FAILED

(experimental, auto) theano.sandbox.cuda.dnn.GpuDnnConv                ==> fprop           ==>      268.0
(experimental, auto) theano.sandbox.cuda.dnn.GpuDnnConv                ==> bprop inputs    ==>     3262.0
(experimental, auto) theano.sandbox.cuda.dnn.GpuDnnConv                ==> bprop weights   ==>      596.0

(experimental, manual) theano.sandbox.cuda.dnn.GpuDnnConv              ==> fprop           ==>      269.0
(experimental, manual) theano.sandbox.cuda.dnn.GpuDnnConv              ==> bprop inputs    ==>      288.0
(experimental, manual) theano.sandbox.cuda.dnn.GpuDnnConv              ==> bprop weights   ==>      326.0

(experimental, auto) theano.sandbox.cuda.blas.GpuCorrMM                ==> fprop           ==>      393.0
(experimental, auto) theano.sandbox.cuda.blas.GpuCorrMM                ==> bprop inputs    ==>      415.0
(experimental, auto) theano.sandbox.cuda.blas.GpuCorrMM                ==> bprop weights   ==>      580.0

(experimental, manual) theano.sandbox.cuda.blas.GpuCorrMM              ==> fprop           ==>      393.0
(experimental, manual) theano.sandbox.cuda.blas.GpuCorrMM              ==> bprop inputs    ==>      415.0
(experimental, manual) theano.sandbox.cuda.blas.GpuCorrMM              ==> bprop weights   ==>      582.0
```

I know these might seem slow compared to what is in the repo, but I don't have a Titan Black, just a 750 Ti.

It seems that the grad_input case is not really optimized or there is just too much copying when using the optimization.  But when manually using it, it's fine.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-56235339,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39501492,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/229425962,229425962,MDEyOklzc3VlQ29tbWVudDIyOTQyNTk2Mg==,2287800,2016-06-29T17:19:44Z,2016-06-29T17:19:44Z,NONE,"Do you happen to have some kind of evaluation of the error rates across each platform? I'm curious to see how widely they vary from one approach to the next.

Thanks!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16/comments,https://github.com/soumith/convnet-benchmarks/issues/16#issuecomment-229425962,https://api.github.com/repos/soumith/convnet-benchmarks/issues/16
soumith,convnet-benchmarks,39500208,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51174489,51174489,MDEyOklzc3VlQ29tbWVudDUxMTc0NDg5,629706,2014-08-05T09:36:44Z,2014-08-05T09:36:44Z,CONTRIBUTOR,"I've created #16 as a place to discuss results. While doing so, I noticed that the links in your table usually point to the implementation, not to the documentation of a particular library's convolution routine. I can update this PR to point to Theano's implementations instead if needed.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/15/comments,https://github.com/soumith/convnet-benchmarks/pull/15#issuecomment-51174489,https://api.github.com/repos/soumith/convnet-benchmarks/issues/15
soumith,convnet-benchmarks,39500208,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51194413,51194413,MDEyOklzc3VlQ29tbWVudDUxMTk0NDEz,1310570,2014-08-05T13:08:28Z,2014-08-05T13:08:28Z,OWNER,"indeed, i point to the implementation, so it would be good to do the same with this PR.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/15/comments,https://github.com/soumith/convnet-benchmarks/pull/15#issuecomment-51194413,https://api.github.com/repos/soumith/convnet-benchmarks/issues/15
soumith,convnet-benchmarks,39500208,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51198893,51198893,MDEyOklzc3VlQ29tbWVudDUxMTk4ODkz,629706,2014-08-05T13:46:19Z,2014-08-05T13:46:19Z,CONTRIBUTOR,"I see it's merged already now. Anyway, the implementation links would be: [conv2d](https://github.com/Theano/Theano/blob/master/theano/sandbox/cuda/blas.py#L674) and [conv2d_fft](https://github.com/Theano/Theano/blob/master/theano/sandbox/cuda/fftconv.py). The former will soon be outdated because it points at a line number in a larger file, but feel free to use these links the next time you update the table.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/15/comments,https://github.com/soumith/convnet-benchmarks/pull/15#issuecomment-51198893,https://api.github.com/repos/soumith/convnet-benchmarks/issues/15
soumith,convnet-benchmarks,39500208,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51201961,51201961,MDEyOklzc3VlQ29tbWVudDUxMjAxOTYx,1310570,2014-08-05T14:08:18Z,2014-08-05T14:08:18Z,OWNER,"updated!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/15/comments,https://github.com/soumith/convnet-benchmarks/pull/15#issuecomment-51201961,https://api.github.com/repos/soumith/convnet-benchmarks/issues/15
soumith,convnet-benchmarks,39371395,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50992206,50992206,MDEyOklzc3VlQ29tbWVudDUwOTkyMjA2,1310570,2014-08-03T14:42:43Z,2014-08-03T14:42:43Z,OWNER,"i dont have a cuda-enabled machine on osx, all the benchmarks are run on Ubuntu 14.04 64-bit (as you see from the readme). what's the issue you are seeing?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/13/comments,https://github.com/soumith/convnet-benchmarks/issues/13#issuecomment-50992206,https://api.github.com/repos/soumith/convnet-benchmarks/issues/13
soumith,convnet-benchmarks,39371395,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50992377,50992377,MDEyOklzc3VlQ29tbWVudDUwOTkyMzc3,6293104,2014-08-03T14:48:33Z,2014-08-03T14:48:33Z,NONE,"i saw that u run it on ubuntu. i would like to run it on osx...is that possible?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/13/comments,https://github.com/soumith/convnet-benchmarks/issues/13#issuecomment-50992377,https://api.github.com/repos/soumith/convnet-benchmarks/issues/13
soumith,convnet-benchmarks,39371395,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50992392,50992392,MDEyOklzc3VlQ29tbWVudDUwOTkyMzky,1310570,2014-08-03T14:49:32Z,2014-08-03T14:49:32Z,OWNER,"yes, there's nothing about the install instructions or the benchmark script of cuda-convnet2 that is platform-specific. it should run on either OSX or Linux.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/13/comments,https://github.com/soumith/convnet-benchmarks/issues/13#issuecomment-50992392,https://api.github.com/repos/soumith/convnet-benchmarks/issues/13
soumith,convnet-benchmarks,39371395,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50992580,50992580,MDEyOklzc3VlQ29tbWVudDUwOTkyNTgw,1310570,2014-08-03T14:56:27Z,2014-08-03T14:56:27Z,OWNER,"> where did you read that?

I wrote the scripts, and the parts of cuda-convnet2 I use, i dont see anything platform specific either, I only use the CUDA kernels.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/13/comments,https://github.com/soumith/convnet-benchmarks/issues/13#issuecomment-50992580,https://api.github.com/repos/soumith/convnet-benchmarks/issues/13
soumith,convnet-benchmarks,39371395,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50992648,50992648,MDEyOklzc3VlQ29tbWVudDUwOTkyNjQ4,6293104,2014-08-03T14:59:38Z,2014-08-03T14:59:52Z,NONE,"from what i see, the cuda-convnet makefile needs to be adapted...
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/13/comments,https://github.com/soumith/convnet-benchmarks/issues/13#issuecomment-50992648,https://api.github.com/repos/soumith/convnet-benchmarks/issues/13
soumith,convnet-benchmarks,39371395,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50992704,50992704,MDEyOklzc3VlQ29tbWVudDUwOTkyNzA0,1310570,2014-08-03T15:01:50Z,2014-08-03T15:01:50Z,OWNER,"i dont use the makefile, the wrapper I wrote has a CMake based install that uses FindCUDA.cmake that finds nvcc correctly.

https://github.com/soumith/cuda-convnet2.torch/blob/master/CMakeLists.txt
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/13/comments,https://github.com/soumith/convnet-benchmarks/issues/13#issuecomment-50992704,https://api.github.com/repos/soumith/convnet-benchmarks/issues/13
soumith,convnet-benchmarks,39371395,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51165395,51165395,MDEyOklzc3VlQ29tbWVudDUxMTY1Mzk1,6293104,2014-08-05T08:36:06Z,2014-08-05T08:36:06Z,NONE,"ok.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/13/comments,https://github.com/soumith/convnet-benchmarks/issues/13#issuecomment-51165395,https://api.github.com/repos/soumith/convnet-benchmarks/issues/13
soumith,convnet-benchmarks,39369126,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50989486,50989486,MDEyOklzc3VlQ29tbWVudDUwOTg5NDg2,629706,2014-08-03T12:40:18Z,2014-08-03T12:41:50Z,CONTRIBUTOR,"When comparing the timings to the original pylearn2 wrapper, I've noticed something odd. Specifying both the input shape and filter shape is the only way to make the FFT backward pass work, but it is not the fastest for the standard conv2d: For the gradient wrt weights, it helps to omit the flter shape. I guess this should be addressed in Theano. For details, please see [this timings file](http://pastebin.com/F0HqBEWF).
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12/comments,https://github.com/soumith/convnet-benchmarks/pull/12#issuecomment-50989486,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12
soumith,convnet-benchmarks,39369126,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51060016,51060016,MDEyOklzc3VlQ29tbWVudDUxMDYwMDE2,1310570,2014-08-04T13:33:48Z,2014-08-04T13:33:48Z,OWNER,"this isn't going to be a huge timing difference, but just to keep in mind, the new Theano benchmark by @f0k doesn't have a bias term for the experimental fft version.

I am fixing cuda-convnet2 for layers greater than 512MB in tensor size, hence the delay in the table of bprop numbers (ccn2 fails for case1's gradWeights). It's a known bug because of CUDA texture mem limits.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12/comments,https://github.com/soumith/convnet-benchmarks/pull/12#issuecomment-51060016,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12
soumith,convnet-benchmarks,39369126,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51064047,51064047,MDEyOklzc3VlQ29tbWVudDUxMDY0MDQ3,629706,2014-08-04T14:07:05Z,2014-08-04T14:07:05Z,CONTRIBUTOR,"It doesn't have a bias term for the cuda-convnet wrapper either. I thought the bias term in pylearn2 was left in there unintentionally. We could agree on using a bias term and a specific nonlinearity for all benchmarks, but I think it's more appropriate to time all implementations without bias and linear output because we're just interested in the convolution performance. (Or at least I am... what was your original idea about the benchmark?)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12/comments,https://github.com/soumith/convnet-benchmarks/pull/12#issuecomment-51064047,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12
soumith,convnet-benchmarks,39369126,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51064926,51064926,MDEyOklzc3VlQ29tbWVudDUxMDY0OTI2,1310570,2014-08-04T14:12:06Z,2014-08-04T14:12:06Z,OWNER,"@f0k i could do with and without bias, but so far all the layers benchmarked have a bias term. For ccn2, for case 1, the bias term added 100GFlop/s of overhead for :forward (from my memory), so it's not a completely trivial term, but it isn't a big deal either.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12/comments,https://github.com/soumith/convnet-benchmarks/pull/12#issuecomment-51064926,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12
soumith,convnet-benchmarks,39369126,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51068900,51068900,MDEyOklzc3VlQ29tbWVudDUxMDY4OTAw,629706,2014-08-04T14:42:20Z,2014-08-04T14:42:20Z,CONTRIBUTOR,"Well, I can add this, but what about removing it everywhere? And what about the transfer function, what do you use in the other benchmarks? And for backpropagation, do you do a separate timing for the gradient wrt. the bias, is it included in the weights, or ignored altogether?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12/comments,https://github.com/soumith/convnet-benchmarks/pull/12#issuecomment-51068900,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12
soumith,convnet-benchmarks,39369126,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51070840,51070840,MDEyOklzc3VlQ29tbWVudDUxMDcwODQw,1310570,2014-08-04T14:56:14Z,2014-08-04T14:56:14Z,OWNER,"> Well, I can add this, but what about removing it everywhere?

Everywhere except torch, it is optional, but i shall make it optional in torch as well, and add new numbers for that.

> And for backpropagation, do you do a separate timing for the gradient wrt. the bias, is it included in the weights, or ignored altogether?

I don't do separate timings for gradBias, it is included in the timing of gradWeights.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12/comments,https://github.com/soumith/convnet-benchmarks/pull/12#issuecomment-51070840,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12
soumith,convnet-benchmarks,39369126,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51073936,51073936,MDEyOklzc3VlQ29tbWVudDUxMDczOTM2,629706,2014-08-04T15:17:38Z,2014-08-04T15:17:38Z,CONTRIBUTOR,"> Everywhere except torch, it is optional, but i shall make it optional in torch as well, and add new numbers for that.

That would be great! At least I expect all libraries will perform about the same in adding the bias terms, so differences between libraries will be more pronounced when omitting the bias terms. Of course I may be wrong; maybe there are some subtleties in the implementation I cannot imagine.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12/comments,https://github.com/soumith/convnet-benchmarks/pull/12#issuecomment-51073936,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12
soumith,convnet-benchmarks,39369126,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51079538,51079538,MDEyOklzc3VlQ29tbWVudDUxMDc5NTM4,3520613,2014-08-04T16:01:01Z,2014-08-04T16:01:01Z,CONTRIBUTOR,"Personally, I would like to see the final benchmark include Convolution + Pooling, including biases. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12/comments,https://github.com/soumith/convnet-benchmarks/pull/12#issuecomment-51079538,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12
soumith,convnet-benchmarks,39369126,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51079721,51079721,MDEyOklzc3VlQ29tbWVudDUxMDc5NzIx,3520613,2014-08-04T16:02:22Z,2014-08-04T16:02:22Z,CONTRIBUTOR,"Is this why SpatialConvolutionMM is now faster than convnet (you removed the biases from the MM benchmark)?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12/comments,https://github.com/soumith/convnet-benchmarks/pull/12#issuecomment-51079721,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12
soumith,convnet-benchmarks,39369126,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51081958,51081958,MDEyOklzc3VlQ29tbWVudDUxMDgxOTU4,1310570,2014-08-04T16:19:31Z,2014-08-04T16:19:31Z,OWNER,"@nicholas-leonard no, spatialconvolutonMM still has biases.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12/comments,https://github.com/soumith/convnet-benchmarks/pull/12#issuecomment-51081958,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12
soumith,convnet-benchmarks,39369126,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51082083,51082083,MDEyOklzc3VlQ29tbWVudDUxMDgyMDgz,1310570,2014-08-04T16:20:25Z,2014-08-04T16:20:25Z,OWNER,"we're almost getting to a place where we can universally benchmark conv+nonlin+pool
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12/comments,https://github.com/soumith/convnet-benchmarks/pull/12#issuecomment-51082083,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12
soumith,convnet-benchmarks,39369126,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51083917,51083917,MDEyOklzc3VlQ29tbWVudDUxMDgzOTE3,629706,2014-08-04T16:35:33Z,2014-08-04T16:36:12Z,CONTRIBUTOR,"Maybe we can have a ""naked"" or ""vanilla"" version (just convolution, nothing else) and a ""fully-fledged"" version (convolution plus biases, tanh or relu nonlinearity, and 2x2 max-pooling). I can update the Theano script once you decided on the specifications.

I'm still interested in the vanilla timings because that's what's interesting when cherry-picking implementations. Hmm, I'm getting hungry now.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12/comments,https://github.com/soumith/convnet-benchmarks/pull/12#issuecomment-51083917,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12
soumith,convnet-benchmarks,39369126,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51084451,51084451,MDEyOklzc3VlQ29tbWVudDUxMDg0NDUx,1310570,2014-08-04T16:40:00Z,2014-08-04T16:40:00Z,OWNER,"> Hmm, I'm getting hungry now.

lol. I just fixed the ccn2 texture limits bug for large layers, will update the output.log in just a sec.

wrt the table, I have two options:
- split the table into (a) main table with library, total forward, total backward, total time, limitations and (b) separate tables for :forward, :backward

or 
- keep everything as a giant table, but because of how github formats things, convert the table into html and host it on gh-pages. 

which of the options do you guys prefer? My vote is for option 1.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12/comments,https://github.com/soumith/convnet-benchmarks/pull/12#issuecomment-51084451,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12
soumith,convnet-benchmarks,39369126,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51085229,51085229,MDEyOklzc3VlQ29tbWVudDUxMDg1MjI5,629706,2014-08-04T16:45:54Z,2014-08-04T16:45:54Z,CONTRIBUTOR,"I vote for option 1 as well. I would even split :backward into two tables, or at least find a way to distinguish the two separate timings whenever available and allow the cherry-pick row to be maximally picky.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12/comments,https://github.com/soumith/convnet-benchmarks/pull/12#issuecomment-51085229,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12
soumith,convnet-benchmarks,39369126,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51086930,51086930,MDEyOklzc3VlQ29tbWVudDUxMDg2OTMw,43829,2014-08-04T16:59:18Z,2014-08-04T16:59:18Z,NONE,"I agree, I'd definitely prefer benchmarks that are as granular as possible. That said, it would also be nice to include an 'integration' benchmark which tests a bunch of stuff together. I think it would be better to use an entire convnet for that though, not an arbitrary combination of a convolutional layer and a pooling layer.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12/comments,https://github.com/soumith/convnet-benchmarks/pull/12#issuecomment-51086930,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12
soumith,convnet-benchmarks,39369126,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/51097329,51097329,MDEyOklzc3VlQ29tbWVudDUxMDk3MzI5,3520613,2014-08-04T18:21:23Z,2014-08-04T18:21:23Z,CONTRIBUTOR,"I also vote for 1. One main concise table with aggregated benchmarks, and other tables with detailed benchmarks.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12/comments,https://github.com/soumith/convnet-benchmarks/pull/12#issuecomment-51097329,https://api.github.com/repos/soumith/convnet-benchmarks/issues/12
soumith,convnet-benchmarks,39284910,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50881144,50881144,MDEyOklzc3VlQ29tbWVudDUwODgxMTQ0,1310570,2014-08-01T13:02:38Z,2014-08-01T13:02:38Z,OWNER,"awesome! 

> Thinking again, are you actually interested in computing the gradient wrt. the weights, or rather the gradient wrt. the input? Or both? Or should that be two separate benchmarks? Let me know and I'll update the pull request.

both the gradient wrt weights and gradient wrt input. If that's two separate timings, that's cool, if not it's fine (I have them separate in torch, but for caffe and ccv they are combined).
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/11/comments,https://github.com/soumith/convnet-benchmarks/pull/11#issuecomment-50881144,https://api.github.com/repos/soumith/convnet-benchmarks/issues/11
soumith,convnet-benchmarks,39284910,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50889643,50889643,MDEyOklzc3VlQ29tbWVudDUwODg5NjQz,629706,2014-08-01T14:21:57Z,2014-08-01T14:21:57Z,CONTRIBUTOR,"Updated accordingly. They're timed separately now. Out of curiosity I added a combined timing for the basic Theano convolution and it performs the same as timing separately and adding up the results. I've left it in anyway.

I'll leave the FLOP computation open, if that's okay :) You can still change it after merging if needed.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/11/comments,https://github.com/soumith/convnet-benchmarks/pull/11#issuecomment-50889643,https://api.github.com/repos/soumith/convnet-benchmarks/issues/11
soumith,convnet-benchmarks,39284910,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50889724,50889724,MDEyOklzc3VlQ29tbWVudDUwODg5NzI0,1310570,2014-08-01T14:22:42Z,2014-08-01T14:22:42Z,OWNER,"awesome, thanks for the changes, will add the output.log into the same folder shortly.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/11/comments,https://github.com/soumith/convnet-benchmarks/pull/11#issuecomment-50889724,https://api.github.com/repos/soumith/convnet-benchmarks/issues/11
soumith,convnet-benchmarks,39284910,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50892550,50892550,MDEyOklzc3VlQ29tbWVudDUwODkyNTUw,43829,2014-08-01T14:46:52Z,2014-08-01T14:46:52Z,NONE,"Nice work! I think it makes sense to time the backprop wrt input and weights separately where possible.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/11/comments,https://github.com/soumith/convnet-benchmarks/pull/11#issuecomment-50892550,https://api.github.com/repos/soumith/convnet-benchmarks/issues/11
soumith,convnet-benchmarks,39284910,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50892708,50892708,MDEyOklzc3VlQ29tbWVudDUwODkyNzA4,1310570,2014-08-01T14:48:15Z,2014-08-01T14:48:15Z,OWNER,"@benanne I get an assertion error for the FFT module during bprop for the last configuration.
AssertionError: in conv2d_fft: width is not even

Is this a requirement for the FFT experimental module?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/11/comments,https://github.com/soumith/convnet-benchmarks/pull/11#issuecomment-50892708,https://api.github.com/repos/soumith/convnet-benchmarks/issues/11
soumith,convnet-benchmarks,39284910,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50893113,50893113,MDEyOklzc3VlQ29tbWVudDUwODkzMTEz,43829,2014-08-01T14:51:41Z,2014-08-01T14:51:41Z,NONE,"Yes, I believe it is. I'm not sure why though, this limitation was discovered after it was merged into Theano. There is an argument to enable padding, but you have to enable it manually when needed (it cannot detect this situation automatically), which is tricky for the gradient. I think this is being worked on though.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/11/comments,https://github.com/soumith/convnet-benchmarks/pull/11#issuecomment-50893113,https://api.github.com/repos/soumith/convnet-benchmarks/issues/11
soumith,convnet-benchmarks,39284910,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50899123,50899123,MDEyOklzc3VlQ29tbWVudDUwODk5MTIz,1310570,2014-08-01T15:39:40Z,2014-08-01T15:39:40Z,OWNER,"okay, backprop numbers are ready for Theano, Torch, cuda-convnet, cuda-convnet2, Caffe. CCV will be done over the weekend (i can barely get the hang of the internal undocumented api).

Theano and cuda-convnet: https://github.com/soumith/convnet-benchmarks/blob/master/theano/output.log
Torch and cuda-convnet2: https://github.com/soumith/convnet-benchmarks/blob/master/torch7/output.log
Caffe: https://github.com/soumith/convnet-benchmarks/blob/master/caffe/output.log
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/11/comments,https://github.com/soumith/convnet-benchmarks/pull/11#issuecomment-50899123,https://api.github.com/repos/soumith/convnet-benchmarks/issues/11
soumith,convnet-benchmarks,39284910,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50904104,50904104,MDEyOklzc3VlQ29tbWVudDUwOTA0MTA0,629706,2014-08-01T16:20:03Z,2014-08-01T16:20:03Z,CONTRIBUTOR,"Actually padding is enabled automatically depending on the shape, _if it is known at compile time_: https://github.com/Theano/Theano/blob/master/theano/sandbox/cuda/opt.py#L1221
However, pylearn2's `ConvElemwise` does not allow to specify the image shape. It would probably be better to use Theano's `theano.tensor.nnet.conv.conv2d` directly instead of going via a pylearn2 MLP. I can't do that today, but next week if it isn't fixed by then.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/11/comments,https://github.com/soumith/convnet-benchmarks/pull/11#issuecomment-50904104,https://api.github.com/repos/soumith/convnet-benchmarks/issues/11
soumith,convnet-benchmarks,39284910,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50933478,50933478,MDEyOklzc3VlQ29tbWVudDUwOTMzNDc4,180987,2014-08-01T20:52:57Z,2014-08-01T20:52:57Z,CONTRIBUTOR,"The other option is to add that parameter to pylearn2 and have it passed to
the theano op. I think it would be better.

On Fri, Aug 1, 2014 at 12:20 PM, Jan SchlÃ¼ter notifications@github.com
wrote:

> Actually padding is enabled automatically depending on the shape, _if it
> is known at compile time_:
> https://github.com/Theano/Theano/blob/master/theano/sandbox/cuda/opt.py#L1221
> However, pylearn2's ConvElemwise does not allow to specify the image
> shape. It would probably be better to use Theano's
> theano.tensor.nnet.conv.conv2d directly instead of going via a pylearn2
> MLP. I can't do that today, but next week if it isn't fixed by then.
> 
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/pull/11#issuecomment-50904104
> .
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/11/comments,https://github.com/soumith/convnet-benchmarks/pull/11#issuecomment-50933478,https://api.github.com/repos/soumith/convnet-benchmarks/issues/11
soumith,convnet-benchmarks,39284910,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50989042,50989042,MDEyOklzc3VlQ29tbWVudDUwOTg5MDQy,629706,2014-08-03T12:16:36Z,2014-08-03T12:16:36Z,CONTRIBUTOR,"Addressed in PR #12.

@nouiz: It would be good to enable this in pylearn2, of course, but for the purpose of benchmarking the theano convolution code it seems more reasonable to avoid the diversion via pylearn2 altogether. This being said, there's something weird with the timings depending on the shape specifications; I'll post a comment to PR #12 in a minute.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/11/comments,https://github.com/soumith/convnet-benchmarks/pull/11#issuecomment-50989042,https://api.github.com/repos/soumith/convnet-benchmarks/issues/11
soumith,convnet-benchmarks,39280258,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50892373,50892373,MDEyOklzc3VlQ29tbWVudDUwODkyMzcz,43829,2014-08-01T14:45:21Z,2014-08-01T14:45:21Z,NONE,"Cool! So basically use cuda-convnet2 for the first layer, and go with FFTs for the rest. Awesome :) I wonder how the backward pass will affect these results though. I think that might be where the weakness of the FFT-based approach lies.

Maybe it even makes sense to do the forward pass using one implementation, and the backward pass using another. That would create an interesting situation for Theano: afaik it only supports using the same implementation for the forward and backward passes at the moment, but it could be doable to make it support any combination of implementations.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/10/comments,https://github.com/soumith/convnet-benchmarks/pull/10#issuecomment-50892373,https://api.github.com/repos/soumith/convnet-benchmarks/issues/10
soumith,convnet-benchmarks,39280258,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50898058,50898058,MDEyOklzc3VlQ29tbWVudDUwODk4MDU4,629706,2014-08-01T15:30:51Z,2014-08-01T15:30:51Z,CONTRIBUTOR,"The way it's currently done in Theano, every convolution (regardless of forward or backward pass)  becomes a `GpuConv` that can conditionally be replaced by any other implementation during graph optimization. So it would definitely be possible to have different implementations not only for different layers, but also for the different directions (even for the two different convolutions involved in the backward pass). The only problem is figuring out when to use which implementation -- it's probably not only dependent on input and filter shape, but also on the GPU.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/10/comments,https://github.com/soumith/convnet-benchmarks/pull/10#issuecomment-50898058,https://api.github.com/repos/soumith/convnet-benchmarks/issues/10
soumith,convnet-benchmarks,39280258,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50898647,50898647,MDEyOklzc3VlQ29tbWVudDUwODk4NjQ3,43829,2014-08-01T15:35:42Z,2014-08-01T15:35:42Z,NONE,"Indeed. I think it's a little optimistic to expect Theano to figure out all of this stuff automatically. Maybe it could have some basic 'intelligence' about what to use when, but if something like this is ever implemented, I think it is essential for the user to have fine-grained control over which implementation is used where, and if possible, separately for the forward pass and each of the two types of backward passes. That way, you can optimize the hell out of it and get the fastest possible network for your specific GPU by trying every combination, if you really want to.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/10/comments,https://github.com/soumith/convnet-benchmarks/pull/10#issuecomment-50898647,https://api.github.com/repos/soumith/convnet-benchmarks/issues/10
soumith,convnet-benchmarks,39280258,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50900902,50900902,MDEyOklzc3VlQ29tbWVudDUwOTAwOTAy,180987,2014-08-01T15:53:43Z,2014-08-01T15:53:43Z,CONTRIBUTOR,"It would be easy to allow the user to specify the version in the grad.
Currently, there is version parameter to conv2d. We could add a
version_grad_input and version_grad_weight version. We could add support
for string there.

For the auto, people already have a tendency to specify the shape to the
convolution. Theano could try all of them during a Theano function
compilation and use the fastest. The only ""problem"" is that if the GPU is
shared by other things, like the screen, this could mess with the timing.
So we could use bad version. So I would make this optional and if the user
know nothing is already running on the GPU, it would work correctly.

On Fri, Aug 1, 2014 at 11:35 AM, Sander Dieleman notifications@github.com
wrote:

> Indeed. I think it's a little optimistic to expect Theano to figure out
> all of this stuff automatically. Maybe it could have some basic
> 'intelligence' about what to use when, but if something like this is ever
> implemented, I think it is essential for the user to have fine-grained
> control over which implementation is used where, and if possible,
> separately for the forward pass and each of the two types of backward
> passes. That way, you can optimize the hell out of it and get the fastest
> possible network for your specific GPU by trying every combination, if you
> really want to.
> 
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/pull/10#issuecomment-50898647
> .
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/10/comments,https://github.com/soumith/convnet-benchmarks/pull/10#issuecomment-50900902,https://api.github.com/repos/soumith/convnet-benchmarks/issues/10
soumith,convnet-benchmarks,39216298,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50791058,50791058,MDEyOklzc3VlQ29tbWVudDUwNzkxMDU4,1310570,2014-07-31T17:28:32Z,2014-07-31T17:28:32Z,OWNER,"while i really want to see this as well, there's no easy way to do this, afaik. if someone knows how to measure GPU memory consumed exactly (other than running nvidia-smi at the exact time and hoping to catch a glimpse), i'd love to know.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/9/comments,https://github.com/soumith/convnet-benchmarks/issues/9#issuecomment-50791058,https://api.github.com/repos/soumith/convnet-benchmarks/issues/9
soumith,convnet-benchmarks,39216298,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50791479,50791479,MDEyOklzc3VlQ29tbWVudDUwNzkxNDc5,51059,2014-07-31T17:31:44Z,2014-07-31T17:31:44Z,NONE,"Theano has `theano.sandbox.cuda.cuda_ndarray.cuda_ndarray.mem_info()`:
(https://github.com/Theano/Theano/blob/master/theano/sandbox/cuda/cuda_ndarray.cu#L3418)

I am not sure about the other frameworks. Also you have to make that function call.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/9/comments,https://github.com/soumith/convnet-benchmarks/issues/9#issuecomment-50791479,https://api.github.com/repos/soumith/convnet-benchmarks/issues/9
soumith,convnet-benchmarks,39216298,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50793544,50793544,MDEyOklzc3VlQ29tbWVudDUwNzkzNTQ0,1310570,2014-07-31T17:47:19Z,2014-07-31T17:47:19Z,OWNER,"right the Cuda API has cudaMemGetInfo, but the problem is that you dont know the peak memory usage while the kernels are running, you only get the mem usage before and after (or by chance in between, if the call itself is asynchronous and the timing is just right).
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/9/comments,https://github.com/soumith/convnet-benchmarks/issues/9#issuecomment-50793544,https://api.github.com/repos/soumith/convnet-benchmarks/issues/9
soumith,convnet-benchmarks,39216298,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50793702,50793702,MDEyOklzc3VlQ29tbWVudDUwNzkzNzAy,1310570,2014-07-31T17:48:33Z,2014-07-31T17:48:33Z,OWNER,"the functions that launch the kernels might have mallocs and frees before and after launching the kernel, we would want to be first-party to the mallocs before the frees happen.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/9/comments,https://github.com/soumith/convnet-benchmarks/issues/9#issuecomment-50793702,https://api.github.com/repos/soumith/convnet-benchmarks/issues/9
soumith,convnet-benchmarks,39216298,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50793739,50793739,MDEyOklzc3VlQ29tbWVudDUwNzkzNzM5,180987,2014-07-31T17:48:51Z,2014-07-31T17:48:51Z,CONTRIBUTOR,"This can be run only between theano fct call. So you won't know the peak.

I see 2 ways to do this:
1) give the formula for the extra memory requested for the gpu.
2) run experiments bigger and bigger until it crash with missing memory.
Then you compare the biggest one that was working.

On Thu, Jul 31, 2014 at 1:31 PM, Alex Rothberg notifications@github.com
wrote:

> Theano has theano.sandbox.cuda.cuda_ndarray.cuda_ndarray.mem_info():
> (
> https://github.com/Theano/Theano/blob/master/theano/sandbox/cuda/cuda_ndarray.cu#L3418
> )
> 
> I am not sure about the other frameworks. Also you have to make that
> function call.
> 
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/issues/9#issuecomment-50791479
> .
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/9/comments,https://github.com/soumith/convnet-benchmarks/issues/9#issuecomment-50793739,https://api.github.com/repos/soumith/convnet-benchmarks/issues/9
soumith,convnet-benchmarks,39216298,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50793825,50793825,MDEyOklzc3VlQ29tbWVudDUwNzkzODI1,1310570,2014-07-31T17:49:29Z,2014-07-31T17:49:29Z,OWNER,"i like @nouiz 's formula approach, we can calculate it from the method used.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/9/comments,https://github.com/soumith/convnet-benchmarks/issues/9#issuecomment-50793825,https://api.github.com/repos/soumith/convnet-benchmarks/issues/9
soumith,convnet-benchmarks,39216298,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50793917,50793917,MDEyOklzc3VlQ29tbWVudDUwNzkzOTE3,1310570,2014-07-31T17:50:11Z,2014-07-31T17:50:11Z,OWNER,"@nouiz I can add the formulae, could you help out in adding time benchmarks for the backprop calls for Theano, i have finished backprop numbers for all the others.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/9/comments,https://github.com/soumith/convnet-benchmarks/issues/9#issuecomment-50793917,https://api.github.com/repos/soumith/convnet-benchmarks/issues/9
soumith,convnet-benchmarks,39216298,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50814214,50814214,MDEyOklzc3VlQ29tbWVudDUwODE0MjE0,180987,2014-07-31T20:31:08Z,2014-07-31T20:31:08Z,CONTRIBUTOR,"I'm pretty buzy, I don't know when I can do it. What do you need exactly, a
benchmark with the full forward/backward or just the backward? Is there an
example I can follow?

On Thu, Jul 31, 2014 at 1:50 PM, Soumith Chintala notifications@github.com
wrote:

> @nouiz https://github.com/nouiz I can add the formulae, could you help
> out in adding time benchmarks for the backprop calls for Theano, i have
> finished backprop numbers for all the others.
> 
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/issues/9#issuecomment-50793917
> .
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/9/comments,https://github.com/soumith/convnet-benchmarks/issues/9#issuecomment-50814214,https://api.github.com/repos/soumith/convnet-benchmarks/issues/9
soumith,convnet-benchmarks,39216298,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50814387,50814387,MDEyOklzc3VlQ29tbWVudDUwODE0Mzg3,1310570,2014-07-31T20:32:28Z,2014-07-31T20:32:28Z,OWNER,"> What do you need exactly, a benchmark with the full forward/backward or just the backward?

Adding backward timings to the current benchmark is what I'm looking for:
https://github.com/soumith/convnet-benchmarks/blob/master/theano/pylearn2_benchmark.py
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/9/comments,https://github.com/soumith/convnet-benchmarks/issues/9#issuecomment-50814387,https://api.github.com/repos/soumith/convnet-benchmarks/issues/9
soumith,convnet-benchmarks,39216298,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50816679,50816679,MDEyOklzc3VlQ29tbWVudDUwODE2Njc5,3520613,2014-07-31T20:50:33Z,2014-07-31T20:50:33Z,CONTRIBUTOR,"Okay so you want one timing for the entirety of the `forward/backward`.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/9/comments,https://github.com/soumith/convnet-benchmarks/issues/9#issuecomment-50816679,https://api.github.com/repos/soumith/convnet-benchmarks/issues/9
soumith,convnet-benchmarks,39216298,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50817124,50817124,MDEyOklzc3VlQ29tbWVudDUwODE3MTI0,3520613,2014-07-31T20:54:07Z,2014-07-31T20:54:07Z,CONTRIBUTOR,"Oh, an just so we are clear, does this includes an inplace parameter update? (in torch, `backward` or `backwardUpdate` vs just `updateGradInput`).
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/9/comments,https://github.com/soumith/convnet-benchmarks/issues/9#issuecomment-50817124,https://api.github.com/repos/soumith/convnet-benchmarks/issues/9
soumith,convnet-benchmarks,39216298,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50817402,50817402,MDEyOklzc3VlQ29tbWVudDUwODE3NDAy,1310570,2014-07-31T20:56:17Z,2014-07-31T20:56:17Z,OWNER,"> Okay so you want one timing for the entirety of the forward/backward.

Correct.

> Oh, an just so we are clear, does this includes an inplace parameter update?

backward (so updateGradInput + accGradParameters)

Thanks nick!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/9/comments,https://github.com/soumith/convnet-benchmarks/issues/9#issuecomment-50817402,https://api.github.com/repos/soumith/convnet-benchmarks/issues/9
soumith,convnet-benchmarks,39216298,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50817922,50817922,MDEyOklzc3VlQ29tbWVudDUwODE3OTIy,3520613,2014-07-31T21:00:17Z,2014-07-31T21:00:17Z,CONTRIBUTOR,"French:
Ok donc Fred ce sera benchmark qui inclu le forward, ainsi qu'un update des parametres utilisant les gradient. Et le update peut Ãªtre fait inplace.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/9/comments,https://github.com/soumith/convnet-benchmarks/issues/9#issuecomment-50817922,https://api.github.com/repos/soumith/convnet-benchmarks/issues/9
soumith,convnet-benchmarks,39216298,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50895604,50895604,MDEyOklzc3VlQ29tbWVudDUwODk1NjA0,3520613,2014-08-01T15:10:47Z,2014-08-01T15:10:47Z,CONTRIBUTOR,"Okay so the theano backward is complete: https://github.com/soumith/convnet-benchmarks/pull/11.Thanks @f0k.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/9/comments,https://github.com/soumith/convnet-benchmarks/issues/9#issuecomment-50895604,https://api.github.com/repos/soumith/convnet-benchmarks/issues/9
soumith,convnet-benchmarks,39051271,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50560561,50560561,MDEyOklzc3VlQ29tbWVudDUwNTYwNTYx,1310570,2014-07-30T00:53:04Z,2014-07-30T00:53:04Z,OWNER,"thanks, let me bench this and add it to the main table.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/7/comments,https://github.com/soumith/convnet-benchmarks/pull/7#issuecomment-50560561,https://api.github.com/repos/soumith/convnet-benchmarks/issues/7
soumith,convnet-benchmarks,39047863,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50558301,50558301,MDEyOklzc3VlQ29tbWVudDUwNTU4MzAx,1310570,2014-07-30T00:22:03Z,2014-07-30T00:22:03Z,OWNER,"i made it clearer right at the top of the table, thanks for that. I haven't put it in the table because of github formatting which cuts off part of the table when putting all units.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/6/comments,https://github.com/soumith/convnet-benchmarks/pull/6#issuecomment-50558301,https://api.github.com/repos/soumith/convnet-benchmarks/issues/6
soumith,convnet-benchmarks,39033975,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50537336,50537336,MDEyOklzc3VlQ29tbWVudDUwNTM3MzM2,180987,2014-07-29T20:56:34Z,2014-07-29T20:56:34Z,CONTRIBUTOR,"You probably want to rewrite my modification to the README, as my English need upgrade:)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5/comments,https://github.com/soumith/convnet-benchmarks/pull/5#issuecomment-50537336,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5
soumith,convnet-benchmarks,39033975,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50537444,50537444,MDEyOklzc3VlQ29tbWVudDUwNTM3NDQ0,1310570,2014-07-29T20:57:17Z,2014-07-29T20:57:17Z,OWNER,"thanks Frederic! I will :)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5/comments,https://github.com/soumith/convnet-benchmarks/pull/5#issuecomment-50537444,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5
soumith,convnet-benchmarks,39033975,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50537936,50537936,MDEyOklzc3VlQ29tbWVudDUwNTM3OTM2,1310570,2014-07-29T21:00:52Z,2014-07-29T21:00:52Z,OWNER,"@nouiz do I have to reinstall theano? or is there a custom module that I have to install? Right now it gives me this:

```
> THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python pylearn2_benchmark.py
Using gpu device 0: GeForce GTX TITAN Black

CONFIG: input = 3 x 128 x 128 * ker = 3 x 96 x 11 x 11 ( bs = 128 , stride = 1 )
Input shape: (128, 128)
Detector space: (118, 118)
Output space: (118, 118)
pylearn2.models.mlp.ConvElemwise: 296.912916004 GFLOP/s ( tm = 0.418362498283 )
Traceback (most recent call last):
  File ""pylearn2_benchmark.py"", line 103, in <module>
    on_unused_input='ignore', mode=mode)
  File ""/home/fatbox/code/convnet-benchmarks/theano/Theano/theano/compile/function.py"", line 223, in function
    profile=profile)
  File ""/home/fatbox/code/convnet-benchmarks/theano/Theano/theano/compile/pfunc.py"", line 511, in pfunc
    on_unused_input=on_unused_input)
  File ""/home/fatbox/code/convnet-benchmarks/theano/Theano/theano/compile/function_module.py"", line 1332, in orig_function
    defaults)
  File ""/home/fatbox/code/convnet-benchmarks/theano/Theano/theano/compile/function_module.py"", line 1198, in create
    _fn, _i, _o = self.linker.make_thunk(input_storage=input_storage_lists)
  File ""/home/fatbox/code/convnet-benchmarks/theano/Theano/theano/gof/link.py"", line 489, in make_thunk
    output_storage=output_storage)[:3]
  File ""/home/fatbox/code/convnet-benchmarks/theano/Theano/theano/gof/vm.py"", line 882, in make_all
    no_recycling))
  File ""/home/fatbox/code/convnet-benchmarks/theano/Theano/theano/sandbox/cuda/fftconv.py"", line 58, in make_thunk
    from theano.misc.pycuda_utils import to_gpuarray
  File ""/home/fatbox/code/convnet-benchmarks/theano/Theano/theano/misc/pycuda_utils.py"", line 2, in <module>
    import pycuda.gpuarray
ImportError: ('The following error happened while compiling the node', CuFFTOp(GpuContiguous.0), '\n', 'No module named pycuda.gpuarray')
```
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5/comments,https://github.com/soumith/convnet-benchmarks/pull/5#issuecomment-50537936,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5
soumith,convnet-benchmarks,39033975,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50538123,50538123,MDEyOklzc3VlQ29tbWVudDUwNTM4MTIz,43829,2014-07-29T21:02:21Z,2014-07-29T21:02:21Z,NONE,"The current FFT-based implementation in Theano depends on pyCUDA, and (unless they've modified it in the meantime) scikits.cuda. So you will need those two packages to be able to run this.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5/comments,https://github.com/soumith/convnet-benchmarks/pull/5#issuecomment-50538123,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5
soumith,convnet-benchmarks,39033975,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50538321,50538321,MDEyOklzc3VlQ29tbWVudDUwNTM4MzIx,1310570,2014-07-29T21:03:43Z,2014-07-29T21:03:43Z,OWNER,"@benanne could you help me modify the README.md in the theano section to have additional setup instructions.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5/comments,https://github.com/soumith/convnet-benchmarks/pull/5#issuecomment-50538321,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5
soumith,convnet-benchmarks,39033975,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50538537,50538537,MDEyOklzc3VlQ29tbWVudDUwNTM4NTM3,43829,2014-07-29T21:05:27Z,2014-07-29T21:05:27Z,NONE,"What kind of instructions do you mean? Just the added dependencies?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5/comments,https://github.com/soumith/convnet-benchmarks/pull/5#issuecomment-50538537,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5
soumith,convnet-benchmarks,39033975,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50538636,50538636,MDEyOklzc3VlQ29tbWVudDUwNTM4NjM2,1310570,2014-07-29T21:06:12Z,2014-07-29T21:06:12Z,OWNER,"right now these are the instructions:
Install Theano:

```
git clone git://github.com/Theano/Theano.git
cd Theano
sudo python setup.py develop
```

Install pylearn2:

```
git clone git://github.com/lisa-lab/pylearn2.git
cd pylearn2
sudo python setup.py develop
```

Launch the script:

```
THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python pylearn2_benchmark.py 
```
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5/comments,https://github.com/soumith/convnet-benchmarks/pull/5#issuecomment-50538636,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5
soumith,convnet-benchmarks,39033975,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50538713,50538713,MDEyOklzc3VlQ29tbWVudDUwNTM4NzEz,1310570,2014-07-29T21:06:44Z,2014-07-29T21:06:44Z,OWNER,"I'm assuming I have to add install instructions for pycuda and scikit-learn, correct?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5/comments,https://github.com/soumith/convnet-benchmarks/pull/5#issuecomment-50538713,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5
soumith,convnet-benchmarks,39033975,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50538818,50538818,MDEyOklzc3VlQ29tbWVudDUwNTM4ODE4,43829,2014-07-29T21:07:37Z,2014-07-29T21:07:37Z,NONE,"I see. That should suffice for the legacy kernels and the wrapped cuda-convnet code. For the FFT-based implementation you will indeed need pycuda / scikits.cuda (not scikit-learn) as dependencies. 
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5/comments,https://github.com/soumith/convnet-benchmarks/pull/5#issuecomment-50538818,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5
soumith,convnet-benchmarks,39033975,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50540424,50540424,MDEyOklzc3VlQ29tbWVudDUwNTQwNDI0,1310570,2014-07-29T21:20:08Z,2014-07-29T21:20:08Z,OWNER,"cool, made some more progress, but still errors out, is there a version of pycuda/scikits.cuda that i require?

```
> THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python pylearn2_benchmark.py
Using gpu device 0: GeForce GTX TITAN Black

CONFIG: input = 3 x 128 x 128 * ker = 3 x 96 x 11 x 11 ( bs = 128 , stride = 1 )
Input shape: (128, 128)
Detector space: (118, 118)
Output space: (118, 118)
pylearn2.models.mlp.ConvElemwise: 291.007529832 GFLOP/s ( tm = 0.426852285862 )
Traceback (most recent call last):
  File ""pylearn2_benchmark.py"", line 108, in <module>
    fprop()
  File ""/home/fatbox/code/convnet-benchmarks/theano/Theano/theano/compile/function_module.py"", line 589, in __call__
    self.fn.thunks[self.fn.position_of_error])
  File ""/home/fatbox/code/convnet-benchmarks/theano/Theano/theano/compile/function_module.py"", line 579, in __call__
    outputs = self.fn()
  File ""/home/fatbox/code/convnet-benchmarks/theano/Theano/theano/sandbox/cuda/fftconv.py"", line 328, in thunk
    output_b_pycuda)
  File ""/home/fatbox/code/convnet-benchmarks/theano/Theano/theano/sandbox/cuda/fftconv.py"", line 275, in sc_complex_dot_batched
    cublas.cublasCgemmBatched(handle, transb, transa, m, n, k, alpha,
AttributeError: 'module' object has no attribute 'cublasCgemmBatched'
Apply node that caused the error: BatchedComplexDotOp(GpuContiguous.0, GpuContiguous.0)
Inputs types: [CudaNdarrayType(float32, 4D), CudaNdarrayType(float32, 4D)]
Inputs shapes: [(8320, 128, 3, 2), (8320, 3, 96, 2)]
Inputs strides: [(768, 6, 2, 1), (576, 192, 2, 1)]
Inputs scalar values: ['not scalar', 'not scalar']

HINT: Re-running with most Theano optimization disabled could give you a back-traces when this node was created. This can be done with by setting the Theano flags optimizer=fast_compile
HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint of this apply node.
PyCUDA WARNING: a clean-up operation failed (dead context maybe?)
cuMemFree failed: invalid value
PyCUDA WARNING: a clean-up operation failed (dead context maybe?)
cuMemFree failed: invalid value
PyCUDA WARNING: a clean-up operation failed (dead context maybe?)
cuMemFree failed: invalid value
-------------------------------------------------------------------
PyCUDA ERROR: The context stack was not empty upon module cleanup.
-------------------------------------------------------------------
A context was still active when the context stack was being
cleaned up. At this point in our execution, CUDA may already
have been deinitialized, so there is no way we can finish
cleanly. The program will be aborted now.
Use Context.pop() to avoid this problem.
-------------------------------------------------------------------
Aborted (core dumped)
```
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5/comments,https://github.com/soumith/convnet-benchmarks/pull/5#issuecomment-50540424,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5
soumith,convnet-benchmarks,39033975,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50540628,50540628,MDEyOklzc3VlQ29tbWVudDUwNTQwNjI4,1310570,2014-07-29T21:21:41Z,2014-07-29T21:21:41Z,OWNER,"I installed the latest versions listed on pypi

https://pypi.python.org/pypi/pycuda
https://pypi.python.org/pypi/scikits.cuda
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5/comments,https://github.com/soumith/convnet-benchmarks/pull/5#issuecomment-50540628,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5
soumith,convnet-benchmarks,39033975,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50540970,50540970,MDEyOklzc3VlQ29tbWVudDUwNTQwOTcw,43829,2014-07-29T21:24:14Z,2014-07-29T21:24:14Z,NONE,"Yeah, that scikits.cuda version is too old, you'll need 0.5.0 at least. The cublassCgemmBatched wrapper was something I added when I worked on this. Sorry, I should have mentioned this earlier.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5/comments,https://github.com/soumith/convnet-benchmarks/pull/5#issuecomment-50540970,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5
soumith,convnet-benchmarks,39033975,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50541256,50541256,MDEyOklzc3VlQ29tbWVudDUwNTQxMjU2,1310570,2014-07-29T21:26:23Z,2014-07-29T21:26:23Z,OWNER,"ok great, works now! thanks.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5/comments,https://github.com/soumith/convnet-benchmarks/pull/5#issuecomment-50541256,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5
soumith,convnet-benchmarks,39033975,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50541576,50541576,MDEyOklzc3VlQ29tbWVudDUwNTQxNTc2,1310570,2014-07-29T21:29:00Z,2014-07-29T21:29:00Z,OWNER,"just to confirm (so that this place isn't another war zone), the fft version looks to be about 2x faster than ConvElemwise for the first case.

Does that sound about right?
pylearn2.models.mlp.ConvElemwise: 299.74149507 GFLOP/s ( tm = 0.414414525032 )
(fft experimental) pylearn2.models.mlp.ConvElemwise: 595.44122732 GFLOP/s ( tm = 0.208613753319 )
 pylearn2.sandbox.cuda_convnet: 1354.6420678 GFLOP/s ( tm = 0.0916974544525 )
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5/comments,https://github.com/soumith/convnet-benchmarks/pull/5#issuecomment-50541576,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5
soumith,convnet-benchmarks,39033975,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50542792,50542792,MDEyOklzc3VlQ29tbWVudDUwNTQyNzky,43829,2014-07-29T21:38:34Z,2014-07-29T21:39:14Z,NONE,"Could be, I haven't tested the current implementation myself. It'll also depend on the input size a lot. I believe you're using 3 input feature maps at the moment - in my experience, the FFT-based version will be mostly beneficial when there are a lot of input feature maps, because this becomes the inner dimension of a batched dot product in the Fourier domain.

Note that it will also have some overhead on the first run, because the FFT plan has to be created. Subsequent runs should be faster.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5/comments,https://github.com/soumith/convnet-benchmarks/pull/5#issuecomment-50542792,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5
soumith,convnet-benchmarks,39033975,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50542915,50542915,MDEyOklzc3VlQ29tbWVudDUwNTQyOTE1,1310570,2014-07-29T21:39:33Z,2014-07-29T21:39:33Z,OWNER,"indeed, it is very fast for the later layers. Full log here:
https://github.com/soumith/convnet-benchmarks/blob/master/theano/output.log
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5/comments,https://github.com/soumith/convnet-benchmarks/pull/5#issuecomment-50542915,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5
soumith,convnet-benchmarks,39033975,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50544438,50544438,MDEyOklzc3VlQ29tbWVudDUwNTQ0NDM4,43829,2014-07-29T21:52:23Z,2014-07-29T21:52:23Z,NONE,"very cool :) I should mention though that the Gflop/s metric doesn't really make sense for the FFT implementation, it's not actually performing this many floating point operations, the FFT approach just needs fewer. 7 Tflop/s is actually more than the maximum that the Titan is capable of (about 4.5 Tflop/s).

I suppose the same goes for the Toeplitz matrix approach that Caffe uses, it will also need a different number of flops for a given convolution.

That said it's still useful to see how many Gflop/s it is equivalent to compared to a naive implementation.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5/comments,https://github.com/soumith/convnet-benchmarks/pull/5#issuecomment-50544438,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5
soumith,convnet-benchmarks,39033975,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50544500,50544500,MDEyOklzc3VlQ29tbWVudDUwNTQ0NTAw,1310570,2014-07-29T21:53:01Z,2014-07-29T21:53:01Z,OWNER,"I am changing the metrics as we speak :)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5/comments,https://github.com/soumith/convnet-benchmarks/pull/5#issuecomment-50544500,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5
soumith,convnet-benchmarks,39033975,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50548621,50548621,MDEyOklzc3VlQ29tbWVudDUwNTQ4NjIx,1310570,2014-07-29T22:31:53Z,2014-07-29T22:31:53Z,OWNER,"One last question before I add this entry into the table:
- Where is the code located for this (experiemental FFT) module?
- What is the status of this wrt usability (does it have :backward() code as well, is it unit tested etc.)?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5/comments,https://github.com/soumith/convnet-benchmarks/pull/5#issuecomment-50548621,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5
soumith,convnet-benchmarks,39033975,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50548946,50548946,MDEyOklzc3VlQ29tbWVudDUwNTQ4OTQ2,43829,2014-07-29T22:35:35Z,2014-07-29T22:35:35Z,NONE,"you can find the code here: https://github.com/Theano/Theano/blob/master/theano/sandbox/cuda/fftconv.py

Regarding usability: afaik there are tests, I don't know if anyone has tried using it 'in production' though. The main problem with it is that it uses a lot of memory, so it isn't applicable for every use case. No free lunch! :)

By :backward() I assume you mean the gradient. The way this is implemented is as an optimization that replaces Theano's own ConvOp with the FFT-based one. Because this only happens in the optimization phase, the gradient has already been calculated at that point, so the convolutions that are part of the gradient are also replaced by their FFT versions automatically. In short, it does not have its own gradient implementation, but because of the way Theano works, this is not necessary. The implementation of Theano's own ConvOp is reused.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5/comments,https://github.com/soumith/convnet-benchmarks/pull/5#issuecomment-50548946,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5
soumith,convnet-benchmarks,39033975,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50549214,50549214,MDEyOklzc3VlQ29tbWVudDUwNTQ5MjE0,1310570,2014-07-29T22:38:27Z,2014-07-29T22:38:27Z,OWNER,"Awesome, I am going to add this in for now with the information you provided. Thank you.

I have finished up the benchmark code for all the other libraries (except ccv) for the backpropagation as well (i.e. calculate gradients wrt image, gradients wrt parameters). Would anyone from LISA Lab modify this test to add the backpropagation timings as well?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5/comments,https://github.com/soumith/convnet-benchmarks/pull/5#issuecomment-50549214,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5
soumith,convnet-benchmarks,39033975,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50551439,50551439,MDEyOklzc3VlQ29tbWVudDUwNTUxNDM5,1310570,2014-07-29T23:02:17Z,2014-07-29T23:02:17Z,OWNER,"Added! and made the table report numbers for 5 configurations. This module is currently the FASTEST!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5/comments,https://github.com/soumith/convnet-benchmarks/pull/5#issuecomment-50551439,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5
soumith,convnet-benchmarks,39033975,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50551694,50551694,MDEyOklzc3VlQ29tbWVudDUwNTUxNjk0,43829,2014-07-29T23:04:57Z,2014-07-29T23:04:57Z,NONE,"Cool! I have a feeling that might change when you get to the backward pass ;)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5/comments,https://github.com/soumith/convnet-benchmarks/pull/5#issuecomment-50551694,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5
soumith,convnet-benchmarks,39033975,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50551837,50551837,MDEyOklzc3VlQ29tbWVudDUwNTUxODM3,127987,2014-07-29T23:06:36Z,2014-07-29T23:06:36Z,CONTRIBUTOR,"I could help with making ccv work with your configuration. For larger kernel window, FFT is the best, although AlexNet and MattNet has small kernels. Probably small kernel works better just because we have fewer samples though.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5/comments,https://github.com/soumith/convnet-benchmarks/pull/5#issuecomment-50551837,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5
soumith,convnet-benchmarks,39033975,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50551909,50551909,MDEyOklzc3VlQ29tbWVudDUwNTUxOTA5,1310570,2014-07-29T23:07:36Z,2014-07-29T23:07:36Z,OWNER,"thanks, i will add the :backward numbers for all the modules this weekend (I've already finished it for ccn2, caffe and torch) i hope someone can change the Theano benchmark to incorporate the backward pass numbers as well.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5/comments,https://github.com/soumith/convnet-benchmarks/pull/5#issuecomment-50551909,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5
soumith,convnet-benchmarks,39033975,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50551968,50551968,MDEyOklzc3VlQ29tbWVudDUwNTUxOTY4,1310570,2014-07-29T23:08:14Z,2014-07-29T23:08:14Z,OWNER,"@liuliu that would be great! the way ccv benchmark works right now, it is a little hacky and i didn't find the time to modify cwc-bench for each of the configurations. Thanks!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5/comments,https://github.com/soumith/convnet-benchmarks/pull/5#issuecomment-50551968,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5
soumith,convnet-benchmarks,39033975,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50552217,50552217,MDEyOklzc3VlQ29tbWVudDUwNTUyMjE3,127987,2014-07-29T23:11:05Z,2014-07-29T23:11:05Z,CONTRIBUTOR,"@soumith , yeah, looking closer to your layer configuration, it is a bit hard to gets ccv's number because each layer's output doesn't match the other. Due to all the assertion check for layer output / input consistency, you have to go all the way to call the actual CUDA kernel probably to get some numbers out.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5/comments,https://github.com/soumith/convnet-benchmarks/pull/5#issuecomment-50552217,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5
soumith,convnet-benchmarks,39033975,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50552394,50552394,MDEyOklzc3VlQ29tbWVudDUwNTUyMzk0,127987,2014-07-29T23:13:05Z,2014-07-29T23:13:05Z,CONTRIBUTOR,"Also, you probably want to run with some real data (such as image or something) rather than the allocated uninitialized memory, I noticed that for all zero regions, my TITAN card sometimes cheats and have a shorter running time.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5/comments,https://github.com/soumith/convnet-benchmarks/pull/5#issuecomment-50552394,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5
soumith,convnet-benchmarks,39033975,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50563006,50563006,MDEyOklzc3VlQ29tbWVudDUwNTYzMDA2,180987,2014-07-30T01:26:25Z,2014-07-30T01:26:25Z,CONTRIBUTOR,"I just saw the update table results. I think it you should keep the fft
with the experimental.

Also, maybe it would be great to add a section with the limitation of each
module. For example, Theano(legacy) is the slowest, but most versatile in
the size/shape accepted.

Also, what about a new column with the extra temps memory needed for each
method? caffe need memory for the toeplitz matrix and the fft method need
extra memory too. For example, we could put the extra memory needed as the
formula for the shape like batchsize*kernelsize.

On Tue, Jul 29, 2014 at 7:13 PM, Liu Liu notifications@github.com wrote:

> Also, you probably want to run with some real data (such as image or
> something) rather than the allocated uninitialized memory, I noticed that
> for all zero regions, my TITAN card sometimes cheats and have a shorter
> running time.
> 
> â€”
> Reply to this email directly or view it on GitHub
> https://github.com/soumith/convnet-benchmarks/pull/5#issuecomment-50552394
> .
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5/comments,https://github.com/soumith/convnet-benchmarks/pull/5#issuecomment-50563006,https://api.github.com/repos/soumith/convnet-benchmarks/issues/5
soumith,convnet-benchmarks,38967918,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50474745,50474745,MDEyOklzc3VlQ29tbWVudDUwNDc0NzQ1,1310570,2014-07-29T13:21:01Z,2014-07-29T13:21:01Z,OWNER,"> I noticed that the previous cuda-convnet result was replaced by a better one, and this time the wrapper used is pylearn2. I think both results are relevant and interesting, as both the kernel and the library used will affect performance.

That seems fair, I'll add a column in there.

> It would also be very interesting (for me personally at least) to see how the pylearn2-wrapped cuda-convnet compares with using cuda-convnet's own Python bindings, for example (and of course Torch's).

It looks like doing benchmarking for kernels around cuda-convnet's python bindings is quite difficult.

> On a somewhat related note, I apologize if I was a bit too eager spreading the link to this repo around, as some people seem to be reacting strongly to these results 

Neither did I think these results were even relevant, i haven't even finished them (benchmarking one layer gives you a rough idea, but doesn't really tell you much else!), hope bridges are not broken over these.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/4/comments,https://github.com/soumith/convnet-benchmarks/issues/4#issuecomment-50474745,https://api.github.com/repos/soumith/convnet-benchmarks/issues/4
soumith,convnet-benchmarks,38277583,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/49612146,49612146,MDEyOklzc3VlQ29tbWVudDQ5NjEyMTQ2,1310570,2014-07-21T14:27:35Z,2014-07-21T14:27:35Z,OWNER,"thanks! I'll have a look at this later today.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/3/comments,https://github.com/soumith/convnet-benchmarks/issues/3#issuecomment-49612146,https://api.github.com/repos/soumith/convnet-benchmarks/issues/3
soumith,convnet-benchmarks,38277583,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50388963,50388963,MDEyOklzc3VlQ29tbWVudDUwMzg4OTYz,1310570,2014-07-28T19:47:07Z,2014-07-28T19:47:07Z,OWNER,"This library uses the cuda kernels from Alex Khrizevsky's cuda-convnet, so there's no point in benchmarking it.

Kernels located here: https://github.com/TorontoDeepLearning/convnet/blob/master/src/cudamat_conv_kernels.cuh
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/3/comments,https://github.com/soumith/convnet-benchmarks/issues/3#issuecomment-50388963,https://api.github.com/repos/soumith/convnet-benchmarks/issues/3
soumith,convnet-benchmarks,38267255,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50298233,50298233,MDEyOklzc3VlQ29tbWVudDUwMjk4MjMz,1310570,2014-07-28T03:56:26Z,2014-07-28T03:56:26Z,OWNER,"Added the benchmarks using torch wrappers, included the benchmark in the table.

It's the fastest right now among all, by quite a bit, and it's even better for smaller layers, blows the competition. For example, a mid-level layer like 
CONFIG: input = 384x13x13 \* ker = 384x384x3x3 (bs = 128, stride = 1)
cuda-convnet2 is almost 3x faster compared to Caffe/nn.SpatialConvolutionMM

cuda-convnet1: 986.23791188616 GFLOP/s (tm = 0.041682004928589)
Caffe/nn.SpatialConvolutionMM: 736.80179999304 GFLOP/s (tm = 0.055792987346649)
**ccn2.SpatialConvolution: 2274.8261325257 GFLOP/s (tm = 0.018070995807648)**
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/2/comments,https://github.com/soumith/convnet-benchmarks/issues/2#issuecomment-50298233,https://api.github.com/repos/soumith/convnet-benchmarks/issues/2
soumith,convnet-benchmarks,38267255,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50298286,50298286,MDEyOklzc3VlQ29tbWVudDUwMjk4Mjg2,188959,2014-07-28T03:57:54Z,2014-07-28T03:57:54Z,COLLABORATOR,"Pretty cool!
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/2/comments,https://github.com/soumith/convnet-benchmarks/issues/2#issuecomment-50298286,https://api.github.com/repos/soumith/convnet-benchmarks/issues/2
soumith,convnet-benchmarks,38267255,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50359990,50359990,MDEyOklzc3VlQ29tbWVudDUwMzU5OTkw,3520613,2014-07-28T16:12:47Z,2014-07-28T16:12:47Z,CONTRIBUTOR,"Wow.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/2/comments,https://github.com/soumith/convnet-benchmarks/issues/2#issuecomment-50359990,https://api.github.com/repos/soumith/convnet-benchmarks/issues/2
soumith,convnet-benchmarks,38267255,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50360424,50360424,MDEyOklzc3VlQ29tbWVudDUwMzYwNDI0,127987,2014-07-28T16:16:01Z,2014-07-28T16:16:01Z,CONTRIBUTOR,"The preloading trick works pretty well for Alex seems :)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/2/comments,https://github.com/soumith/convnet-benchmarks/issues/2#issuecomment-50360424,https://api.github.com/repos/soumith/convnet-benchmarks/issues/2
soumith,convnet-benchmarks,38267255,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50360478,50360478,MDEyOklzc3VlQ29tbWVudDUwMzYwNDc4,3520613,2014-07-28T16:16:28Z,2014-07-28T16:16:28Z,CONTRIBUTOR,"So I guess this means we should update SpatialConvolutionCUDA to use convnet2?
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/2/comments,https://github.com/soumith/convnet-benchmarks/issues/2#issuecomment-50360478,https://api.github.com/repos/soumith/convnet-benchmarks/issues/2
soumith,convnet-benchmarks,38267255,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50360861,50360861,MDEyOklzc3VlQ29tbWVudDUwMzYwODYx,1310570,2014-07-28T16:19:29Z,2014-07-28T16:19:29Z,OWNER,"you guys can look at the full output.log [here](https://github.com/soumith/convnet-benchmarks/blob/master/torch7/output.log)

There's numbers for approximate configurations for Initial layers, Mid-level layers, Small layers etc.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/2/comments,https://github.com/soumith/convnet-benchmarks/issues/2#issuecomment-50360861,https://api.github.com/repos/soumith/convnet-benchmarks/issues/2
soumith,convnet-benchmarks,38267255,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50361082,50361082,MDEyOklzc3VlQ29tbWVudDUwMzYxMDgy,1310570,2014-07-28T16:21:08Z,2014-07-28T16:21:08Z,OWNER,"> So I guess this means we should update SpatialConvolutionCUDA to use convnet2?

@nicholas-leonard https://github.com/soumith/cuda-convnet2.torch/ much cleaner, and we can import all the other modules (like BlockSparse convolution layer, Response normalization across feature maps etc.). I converted all the NVMatrix references in cudaconv3 to be THTensor references.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/2/comments,https://github.com/soumith/convnet-benchmarks/issues/2#issuecomment-50361082,https://api.github.com/repos/soumith/convnet-benchmarks/issues/2
soumith,convnet-benchmarks,38267255,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50361284,50361284,MDEyOklzc3VlQ29tbWVudDUwMzYxMjg0,3520613,2014-07-28T16:22:49Z,2014-07-28T16:22:49Z,CONTRIBUTOR,"That does seem pretty clean. Nice.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/2/comments,https://github.com/soumith/convnet-benchmarks/issues/2#issuecomment-50361284,https://api.github.com/repos/soumith/convnet-benchmarks/issues/2
soumith,convnet-benchmarks,38267255,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50372614,50372614,MDEyOklzc3VlQ29tbWVudDUwMzcyNjE0,43829,2014-07-28T17:48:03Z,2014-07-28T17:48:03Z,NONE,"This is pretty nuts. I'd like to learn more about this 'preloading trick', is it documented anywhere? Or is this his secret sauce :) I hope this ends up in Theano/pylearn2 soon.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/2/comments,https://github.com/soumith/convnet-benchmarks/issues/2#issuecomment-50372614,https://api.github.com/repos/soumith/convnet-benchmarks/issues/2
soumith,convnet-benchmarks,38267255,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50382623,50382623,MDEyOklzc3VlQ29tbWVudDUwMzgyNjIz,127987,2014-07-28T18:59:27Z,2014-07-28T18:59:54Z,CONTRIBUTOR,"@benanne , I just find that Alex's trick is spiritually similar to this discussion: http://on-demand.gputechconf.com/gtc/2014/presentations/S4297-optimizations-high-perf-2d-convolutions.pdf
In that, they load global memory directly into register to avoid the bottleneck from global memory to shared memory.
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/2/comments,https://github.com/soumith/convnet-benchmarks/issues/2#issuecomment-50382623,https://api.github.com/repos/soumith/convnet-benchmarks/issues/2
soumith,convnet-benchmarks,38267255,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/50384535,50384535,MDEyOklzc3VlQ29tbWVudDUwMzg0NTM1,43829,2014-07-28T19:13:13Z,2014-07-28T19:13:13Z,NONE,"Awesome, thanks for the reference! Not sure if I'll understand any of it but I'll have a look :)
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/2/comments,https://github.com/soumith/convnet-benchmarks/issues/2#issuecomment-50384535,https://api.github.com/repos/soumith/convnet-benchmarks/issues/2
soumith,convnet-benchmarks,37844193,https://api.github.com/repos/soumith/convnet-benchmarks/issues/comments/48983593,48983593,MDEyOklzc3VlQ29tbWVudDQ4OTgzNTkz,1310570,2014-07-15T02:26:04Z,2014-07-15T02:26:04Z,OWNER,"that's a good idea. I am first trying to get an initial baseline for a single module with all the libraries, then I will add more complete benchmarks .
",NA,https://api.github.com/repos/soumith/convnet-benchmarks/issues/1/comments,https://github.com/soumith/convnet-benchmarks/issues/1#issuecomment-48983593,https://api.github.com/repos/soumith/convnet-benchmarks/issues/1
soumith,dcgan.torch,684043050,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/678677748,678677748,MDEyOklzc3VlQ29tbWVudDY3ODY3Nzc0OA==,63679270,2020-08-22T18:51:03Z,2020-08-22T18:51:03Z,NONE,"Solved it.
When you expect it to be, say `gpu=0 net=experiment1_25_net_G.t7 th generate.lua` actually the argument for the net parameter should be `net=checkpoints/experiment1_25_net_G.t7`",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/95/comments,https://github.com/soumith/dcgan.torch/issues/95#issuecomment-678677748,https://api.github.com/repos/soumith/dcgan.torch/issues/95
soumith,dcgan.torch,466797948,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/510848355,510848355,MDEyOklzc3VlQ29tbWVudDUxMDg0ODM1NQ==,22466907,2019-07-12T11:12:41Z,2019-07-12T11:12:41Z,NONE,"Hi

Can one please help me with this",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/93/comments,https://github.com/soumith/dcgan.torch/issues/93#issuecomment-510848355,https://api.github.com/repos/soumith/dcgan.torch/issues/93
soumith,dcgan.torch,392754540,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/448750069,448750069,MDEyOklzc3VlQ29tbWVudDQ0ODc1MDA2OQ==,1310570,2018-12-19T21:28:28Z,2018-12-19T21:28:28Z,OWNER,thanks!,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/91/comments,https://github.com/soumith/dcgan.torch/pull/91#issuecomment-448750069,https://api.github.com/repos/soumith/dcgan.torch/issues/91
soumith,dcgan.torch,370528796,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/435361787,435361787,MDEyOklzc3VlQ29tbWVudDQzNTM2MTc4Nw==,22891788,2018-11-02T12:25:19Z,2018-11-02T12:25:19Z,NONE,"I also get this issue, did you find a solution?",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/88/comments,https://github.com/soumith/dcgan.torch/issues/88#issuecomment-435361787,https://api.github.com/repos/soumith/dcgan.torch/issues/88
soumith,dcgan.torch,370528796,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/446145207,446145207,MDEyOklzc3VlQ29tbWVudDQ0NjE0NTIwNw==,44195331,2018-12-11T10:08:42Z,2018-12-11T10:08:42Z,NONE,Nope still got no solution for this.,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/88/comments,https://github.com/soumith/dcgan.torch/issues/88#issuecomment-446145207,https://api.github.com/repos/soumith/dcgan.torch/issues/88
soumith,dcgan.torch,288414985,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/448724450,448724450,MDEyOklzc3VlQ29tbWVudDQ0ODcyNDQ1MA==,288140,2018-12-19T19:58:00Z,2018-12-19T19:58:00Z,CONTRIBUTOR,"I've been needing this functionality when training on small data sets... @jonaslund, I've made a pull request on this repo but if you still need this, my fork has this option available now.",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/85/comments,https://github.com/soumith/dcgan.torch/issues/85#issuecomment-448724450,https://api.github.com/repos/soumith/dcgan.torch/issues/85
soumith,dcgan.torch,288414985,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/448750114,448750114,MDEyOklzc3VlQ29tbWVudDQ0ODc1MDExNA==,1310570,2018-12-19T21:28:37Z,2018-12-19T21:28:37Z,OWNER,fixed via https://github.com/soumith/dcgan.torch/pull/91,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/85/comments,https://github.com/soumith/dcgan.torch/issues/85#issuecomment-448750114,https://api.github.com/repos/soumith/dcgan.torch/issues/85
soumith,dcgan.torch,282100796,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/526320114,526320114,MDEyOklzc3VlQ29tbWVudDUyNjMyMDExNA==,3686747,2019-08-29T19:05:48Z,2019-08-29T19:05:48Z,NONE,"imagemagick is a handy command line tool that can (among other things) convert tifs to jpgs
https://imagemagick.org/script/convert.php",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/83/comments,https://github.com/soumith/dcgan.torch/issues/83#issuecomment-526320114,https://api.github.com/repos/soumith/dcgan.torch/issues/83
soumith,dcgan.torch,276532891,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/346766756,346766756,MDEyOklzc3VlQ29tbWVudDM0Njc2Njc1Ng==,16058650,2017-11-24T08:09:32Z,2017-11-24T08:09:32Z,NONE,"I've found. 
Thanks.",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/82/comments,https://github.com/soumith/dcgan.torch/issues/82#issuecomment-346766756,https://api.github.com/repos/soumith/dcgan.torch/issues/82
soumith,dcgan.torch,276532891,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/442048346,442048346,MDEyOklzc3VlQ29tbWVudDQ0MjA0ODM0Ng==,42356823,2018-11-27T12:55:49Z,2018-11-27T12:55:49Z,NONE,@coderSkyChen  Can you please share the link. I am not able to find,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/82/comments,https://github.com/soumith/dcgan.torch/issues/82#issuecomment-442048346,https://api.github.com/repos/soumith/dcgan.torch/issues/82
soumith,dcgan.torch,245389123,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/354588695,354588695,MDEyOklzc3VlQ29tbWVudDM1NDU4ODY5NQ==,5436950,2017-12-31T06:54:13Z,2017-12-31T06:54:13Z,NONE,"This is a common problem in GANs. Try setting ngf = ndf * 4. As an example, if you have enough memory, try ngf = 256; ndf = 64",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/81/comments,https://github.com/soumith/dcgan.torch/issues/81#issuecomment-354588695,https://api.github.com/repos/soumith/dcgan.torch/issues/81
soumith,dcgan.torch,230778157,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/303466798,303466798,MDEyOklzc3VlQ29tbWVudDMwMzQ2Njc5OA==,1310570,2017-05-23T17:01:22Z,2017-05-23T17:01:22Z,OWNER,celebA needs a subfolder with images.,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/77/comments,https://github.com/soumith/dcgan.torch/issues/77#issuecomment-303466798,https://api.github.com/repos/soumith/dcgan.torch/issues/77
soumith,dcgan.torch,221654648,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/295402853,295402853,MDEyOklzc3VlQ29tbWVudDI5NTQwMjg1Mw==,6764173,2017-04-19T19:28:20Z,2017-04-19T19:28:20Z,NONE,"Look at https://github.com/soumith/dcgan.torch/blob/master/data/donkey_folder.lua#L67. When the data is loaded, images are randomly flipped. Commenting this line may solve your problem.",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/73/comments,https://github.com/soumith/dcgan.torch/issues/73#issuecomment-295402853,https://api.github.com/repos/soumith/dcgan.torch/issues/73
soumith,dcgan.torch,221654648,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/295419652,295419652,MDEyOklzc3VlQ29tbWVudDI5NTQxOTY1Mg==,18741096,2017-04-19T20:10:20Z,2017-04-19T20:10:20Z,NONE,"Thanks a lot, that makes sense!
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/73/comments,https://github.com/soumith/dcgan.torch/issues/73#issuecomment-295419652,https://api.github.com/repos/soumith/dcgan.torch/issues/73
soumith,dcgan.torch,220479436,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/313709385,313709385,MDEyOklzc3VlQ29tbWVudDMxMzcwOTM4NQ==,182098,2017-07-07T15:10:34Z,2017-07-07T15:10:34Z,NONE,"You could try to put  
```
require 'cunn'
require 'cudnn'
```
before the torch.load() line.",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/72/comments,https://github.com/soumith/dcgan.torch/issues/72#issuecomment-313709385,https://api.github.com/repos/soumith/dcgan.torch/issues/72
soumith,dcgan.torch,220479436,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/353873418,353873418,MDEyOklzc3VlQ29tbWVudDM1Mzg3MzQxOA==,29484350,2017-12-25T14:39:57Z,2017-12-25T14:39:57Z,NONE,Did you solve that problem of resuming training from a specific epoch ? If yes then kindly help me how to do it. I couldnâ€™t understand the solution #62 ,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/72/comments,https://github.com/soumith/dcgan.torch/issues/72#issuecomment-353873418,https://api.github.com/repos/soumith/dcgan.torch/issues/72
soumith,dcgan.torch,218704577,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/296206746,296206746,MDEyOklzc3VlQ29tbWVudDI5NjIwNjc0Ng==,6764173,2017-04-21T14:31:02Z,2017-04-21T14:31:02Z,NONE,"If `gradParameters` are reset to 0 after the first forward pass then this forward pass becomes useless because you lose the gradients you have computed. The `backward` method calls the `accGradParameters` which accumulate the gradients (it adds them to the `gradParameters` tensor). Since you want to train on both real and fake examples you need to call this method twice without resetting the `gradParameters` tensor before the second call. 

At the end the `gradParameters` tensor that is returned contains the sum of the gradients with respect to the real and the fake data.",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/71/comments,https://github.com/soumith/dcgan.torch/issues/71#issuecomment-296206746,https://api.github.com/repos/soumith/dcgan.torch/issues/71
soumith,dcgan.torch,214090359,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/286443171,286443171,MDEyOklzc3VlQ29tbWVudDI4NjQ0MzE3MQ==,15846113,2017-03-14T14:44:36Z,2017-03-14T14:44:36Z,NONE,"Problem solved, I commented the line cutorch.setDevice() when debugging and the default 0 gpu card was occupied. So it seems that the error above was due to memory insufficiency. I did the following: (1) luarocks install cutorch (2) enough memory",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/67/comments,https://github.com/soumith/dcgan.torch/issues/67#issuecomment-286443171,https://api.github.com/repos/soumith/dcgan.torch/issues/67
soumith,dcgan.torch,214090359,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/411983090,411983090,MDEyOklzc3VlQ29tbWVudDQxMTk4MzA5MA==,41097763,2018-08-10T05:55:17Z,2018-08-10T05:55:17Z,NONE,"I also had this problem. The first step have finished, but i don't know how to get enough memory?
Can you tell me ?
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/67/comments,https://github.com/soumith/dcgan.torch/issues/67#issuecomment-411983090,https://api.github.com/repos/soumith/dcgan.torch/issues/67
soumith,dcgan.torch,209787184,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/284239714,284239714,MDEyOklzc3VlQ29tbWVudDI4NDIzOTcxNA==,469493,2017-03-05T16:07:19Z,2017-03-05T16:07:19Z,NONE,"If you have fewer than 10000 images in your dataset you need to increase the number of epochs the model trains for.  Set nist=500 or 100 and let it run.

Then use one of the higher saved models for the image generation. For example
gpu=0 batchSize=64 net=experiment1_500_net_G.t7 th generate.lua ",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/66/comments,https://github.com/soumith/dcgan.torch/issues/66#issuecomment-284239714,https://api.github.com/repos/soumith/dcgan.torch/issues/66
soumith,dcgan.torch,209787184,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/285245406,285245406,MDEyOklzc3VlQ29tbWVudDI4NTI0NTQwNg==,26160577,2017-03-09T03:36:54Z,2017-03-09T03:36:54Z,NONE,I also encountered this problem using gpu and over 200k images,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/66/comments,https://github.com/soumith/dcgan.torch/issues/66#issuecomment-285245406,https://api.github.com/repos/soumith/dcgan.torch/issues/66
soumith,dcgan.torch,208899411,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/281124504,281124504,MDEyOklzc3VlQ29tbWVudDI4MTEyNDUwNA==,18741096,2017-02-20T16:32:01Z,2017-02-20T16:32:01Z,NONE,"Ok, I solved it: it happened with multiple GPUs.
Setting CUDA_VISIBLE_DEVICES=1 fixed it.",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/65/comments,https://github.com/soumith/dcgan.torch/issues/65#issuecomment-281124504,https://api.github.com/repos/soumith/dcgan.torch/issues/65
soumith,dcgan.torch,208898625,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/381882349,381882349,MDEyOklzc3VlQ29tbWVudDM4MTg4MjM0OQ==,934683,2018-04-17T07:42:33Z,2018-04-17T07:42:54Z,NONE,"Robbie Barrat wrote a nice [script to convert GPU trained models to be usable on CPU only systems.](https://github.com/robbiebarrat/art-DCGAN/blob/master/utils/gpu2cpu.lua)
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/64/comments,https://github.com/soumith/dcgan.torch/issues/64#issuecomment-381882349,https://api.github.com/repos/soumith/dcgan.torch/issues/64
soumith,dcgan.torch,200562428,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/272394627,272394627,MDEyOklzc3VlQ29tbWVudDI3MjM5NDYyNw==,16005208,2017-01-13T09:07:05Z,2017-01-13T09:07:05Z,NONE,"Ok, it seems to work with the pretrainend checkpoints from the readme file. I have no idea what's going wrong with my checkpoints. ",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/63/comments,https://github.com/soumith/dcgan.torch/issues/63#issuecomment-272394627,https://api.github.com/repos/soumith/dcgan.torch/issues/63
soumith,dcgan.torch,200562428,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/272515377,272515377,MDEyOklzc3VlQ29tbWVudDI3MjUxNTM3Nw==,1310570,2017-01-13T18:46:47Z,2017-01-13T18:46:47Z,OWNER,fixed it in master via https://github.com/soumith/dcgan.torch/commit/5864a3148d66067a376c1a29a1f6614a8bcd545f,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/63/comments,https://github.com/soumith/dcgan.torch/issues/63#issuecomment-272515377,https://api.github.com/repos/soumith/dcgan.torch/issues/63
soumith,dcgan.torch,200562428,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/272696400,272696400,MDEyOklzc3VlQ29tbWVudDI3MjY5NjQwMA==,16005208,2017-01-15T13:49:56Z,2017-01-15T13:49:56Z,NONE,"Thaaaaaaaaaaanks !

2017-01-13 19:46 GMT+01:00 Soumith Chintala <notifications@github.com>:

> Closed #63 <https://github.com/soumith/dcgan.torch/issues/63>.
>
> â€”
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/soumith/dcgan.torch/issues/63#event-922651550>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/APQ4WKW6dNV--wMbsGVCeFpEl4ZKKoCDks5rR8aYgaJpZM4Lilmk>
> .
>



-- 






------------------------------



Mathieu Arbez Hermoso
0033 (0)6  67  95  96  74

http://mathieu-arbez-hermoso.net/
http://www.twogeesineggs.com/
http://dcmnts.pysgs.net/
http://mmmwww.net
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/63/comments,https://github.com/soumith/dcgan.torch/issues/63#issuecomment-272696400,https://api.github.com/repos/soumith/dcgan.torch/issues/63
soumith,dcgan.torch,200468632,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/281836536,281836536,MDEyOklzc3VlQ29tbWVudDI4MTgzNjUzNg==,2930635,2017-02-22T23:14:00Z,2017-02-22T23:14:00Z,NONE,"@mathieuarbezhermoso did you resolve this? I'd also like to know how to continue from a particular checkpoint.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/62/comments,https://github.com/soumith/dcgan.torch/issues/62#issuecomment-281836536,https://api.github.com/repos/soumith/dcgan.torch/issues/62
soumith,dcgan.torch,200468632,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/283195693,283195693,MDEyOklzc3VlQ29tbWVudDI4MzE5NTY5Mw==,18741096,2017-02-28T23:31:18Z,2017-02-28T23:31:18Z,NONE,@mathieuarbezhermoso I'd like to know this too!,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/62/comments,https://github.com/soumith/dcgan.torch/issues/62#issuecomment-283195693,https://api.github.com/repos/soumith/dcgan.torch/issues/62
soumith,dcgan.torch,200468632,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/283199614,283199614,MDEyOklzc3VlQ29tbWVudDI4MzE5OTYxNA==,18741096,2017-02-28T23:52:53Z,2017-02-28T23:52:53Z,NONE,"I think I managed to do that, it should be relatively simple. I've added netD and netG options in the opt = { ... } portion of main.lua, and then simply

```
if (opt.netD ~= '') then
  print('Initializing discriminator network from' .. opt.netD)
  netD = torch.load(opt.netD)
else 
  netD = nn.Sequential()
 ....
end
```

and same thing for opt.netG.
That should work, but you should hard code the epoch count by hand if you need to change it; also not sure about how the adam learning rates will behave, but I assume they'll be just reset as if you started from scratch.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/62/comments,https://github.com/soumith/dcgan.torch/issues/62#issuecomment-283199614,https://api.github.com/repos/soumith/dcgan.torch/issues/62
soumith,dcgan.torch,200468632,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/283298787,283298787,MDEyOklzc3VlQ29tbWVudDI4MzI5ODc4Nw==,2930635,2017-03-01T10:09:38Z,2017-03-01T10:09:38Z,NONE,@danieleghisi oh great! I'm gonna try this tonight. Thanks for sharing your findings!,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/62/comments,https://github.com/soumith/dcgan.torch/issues/62#issuecomment-283298787,https://api.github.com/repos/soumith/dcgan.torch/issues/62
soumith,dcgan.torch,200468632,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/353873560,353873560,MDEyOklzc3VlQ29tbWVudDM1Mzg3MzU2MA==,29484350,2017-12-25T14:42:19Z,2017-12-25T14:42:19Z,NONE,@rjpeart did you able to update that solution in the code ? If yesh then kindly help me too,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/62/comments,https://github.com/soumith/dcgan.torch/issues/62#issuecomment-353873560,https://api.github.com/repos/soumith/dcgan.torch/issues/62
soumith,dcgan.torch,200468632,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/353874987,353874987,MDEyOklzc3VlQ29tbWVudDM1Mzg3NDk4Nw==,2930635,2017-12-25T15:10:44Z,2017-12-25T15:10:44Z,NONE,"Sadly no, I didnâ€™t try this in the end.
On Mon, 25 Dec 2017 at 14:42, Maryam Shah <notifications@github.com> wrote:

> @rjpeart <https://github.com/rjpeart> did you able to update that
> solution in the code ? If yesh then kindly help me too
>
> â€”
> You are receiving this because you were mentioned.
>
>
> Reply to this email directly, view it on GitHub
> <https://github.com/soumith/dcgan.torch/issues/62#issuecomment-353873560>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ACy3y8k0kqIKZI5K5NCLy9t2cziYI-DEks5tD7RNgaJpZM4LiL6W>
> .
>
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/62/comments,https://github.com/soumith/dcgan.torch/issues/62#issuecomment-353874987,https://api.github.com/repos/soumith/dcgan.torch/issues/62
soumith,dcgan.torch,200468632,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/353875243,353875243,MDEyOklzc3VlQ29tbWVudDM1Mzg3NTI0Mw==,29484350,2017-12-25T15:15:41Z,2017-12-25T15:15:41Z,NONE,Thanks so much for quick response. Well i am stuck at resuming epoch from a certain value. Could you explain me the solution mentioned above? I would be extremely thankful to you.,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/62/comments,https://github.com/soumith/dcgan.torch/issues/62#issuecomment-353875243,https://api.github.com/repos/soumith/dcgan.torch/issues/62
soumith,dcgan.torch,200468632,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/353881494,353881494,MDEyOklzc3VlQ29tbWVudDM1Mzg4MTQ5NA==,2930635,2017-12-25T17:27:49Z,2017-12-25T17:27:49Z,NONE,"I would love to be able to help, but Iâ€™m not skilled at this! It would take
me a long time to work it out through trial-and-error. Hope you find your
way through.
On Mon, 25 Dec 2017 at 15:15, Maryam Shah <notifications@github.com> wrote:

> Thanks so much for quick response. Well i am stuck at resuming epoch from
> a certain value. Could you explain me the solution mentioned above? I would
> be extremely thankful to you.
>
> â€”
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/soumith/dcgan.torch/issues/62#issuecomment-353875243>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ACy3yy81t11hv6iuTQPIPSD0CLTLstJUks5tD7wegaJpZM4LiL6W>
> .
>
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/62/comments,https://github.com/soumith/dcgan.torch/issues/62#issuecomment-353881494,https://api.github.com/repos/soumith/dcgan.torch/issues/62
soumith,dcgan.torch,200468632,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/353881656,353881656,MDEyOklzc3VlQ29tbWVudDM1Mzg4MTY1Ng==,29484350,2017-12-25T17:30:39Z,2017-12-25T17:30:39Z,NONE,Yeah sure well I found one solution... will check it first thing in the morning (since its 2:30 am at my place).Anyway I truly appreciate your concern and kind words. Thank you so much ! ,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/62/comments,https://github.com/soumith/dcgan.torch/issues/62#issuecomment-353881656,https://api.github.com/repos/soumith/dcgan.torch/issues/62
soumith,dcgan.torch,200362488,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/286442927,286442927,MDEyOklzc3VlQ29tbWVudDI4NjQ0MjkyNw==,15846113,2017-03-14T14:43:52Z,2017-03-14T14:43:52Z,NONE,"Problem solved, I commented the line cutorch.setDevice() when debugging and the default 0 gpu card was occupied. So it seems that the error above was due to memory insufficiency. I did the following: (1) luarocks install cutorch (2) enough memory",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/61/comments,https://github.com/soumith/dcgan.torch/issues/61#issuecomment-286442927,https://api.github.com/repos/soumith/dcgan.torch/issues/61
soumith,dcgan.torch,200362488,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/286443368,286443368,MDEyOklzc3VlQ29tbWVudDI4NjQ0MzM2OA==,15846113,2017-03-14T14:45:13Z,2017-03-14T14:45:13Z,NONE,commented at the wrong place. ...,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/61/comments,https://github.com/soumith/dcgan.torch/issues/61#issuecomment-286443368,https://api.github.com/repos/soumith/dcgan.torch/issues/61
soumith,dcgan.torch,198812321,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/270490429,270490429,MDEyOklzc3VlQ29tbWVudDI3MDQ5MDQyOQ==,1840180,2017-01-04T21:22:18Z,2017-01-04T21:22:18Z,NONE,"I upgraded everything (torch, nn, cudnn, optim)... thought issue was with an outdated nn library. Now, I get the following error -

```
Dataset: folder	 Size: 	168950	
/home/saurabh/torch/install/bin/luajit: /home/saurabh/torch/install/share/lua/5.1/nn/Container.lua:67: 
In 1 module of nn.Sequential:
/home/saurabh/torch/install/share/lua/5.1/nn/THNN.lua:110: bad argument #4 to 'v' (weight tensor must be 2D (nOutputPlane,nInputPlane*kH*kW) at /tmp/luarocks_cunn-scm-1-6954/cunn/lib/THCUNN/SpatialConvolutionMM.cu:13)
stack traceback:
	[C]: in function 'v'
	/home/saurabh/torch/install/share/lua/5.1/nn/THNN.lua:110: in function 'SpatialConvolutionMM_updateOutput'
	...bh/torch/install/share/lua/5.1/nn/SpatialConvolution.lua:79: in function <...bh/torch/install/share/lua/5.1/nn/SpatialConvolution.lua:76>
	[C]: in function 'xpcall'
	/home/saurabh/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'
	/home/saurabh/torch/install/share/lua/5.1/nn/Sequential.lua:44: in function 'forward'
	main.lua:162: in function 'opfunc'
	/home/saurabh/torch/install/share/lua/5.1/optim/adam.lua:37: in function 'adam'
	main.lua:213: in main chunk
	[C]: in function 'dofile'
	...rabh/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:145: in main chunk
	[C]: at 0x00406670

WARNING: If you see a stack trace below, it doesn't point to the place where this error occurred. Please use only the one above.
stack traceback:
	[C]: in function 'error'
	/home/saurabh/torch/install/share/lua/5.1/nn/Container.lua:67: in function 'rethrowErrors'
	/home/saurabh/torch/install/share/lua/5.1/nn/Sequential.lua:44: in function 'forward'
	main.lua:162: in function 'opfunc'
	/home/saurabh/torch/install/share/lua/5.1/optim/adam.lua:37: in function 'adam'
	main.lua:213: in main chunk
	[C]: in function 'dofile'
	...rabh/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:145: in main chunk
	[C]: at 0x00406670

```

I'm out of ideas at the moment. Would appreciate any pointers on where to look.",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/60/comments,https://github.com/soumith/dcgan.torch/issues/60#issuecomment-270490429,https://api.github.com/repos/soumith/dcgan.torch/issues/60
soumith,dcgan.torch,198812321,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/270491723,270491723,MDEyOklzc3VlQ29tbWVudDI3MDQ5MTcyMw==,1310570,2017-01-04T21:27:45Z,2017-01-04T21:27:45Z,OWNER,"update cutorch and cunn :)

luarocks install cutorch
luarocks install cunn",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/60/comments,https://github.com/soumith/dcgan.torch/issues/60#issuecomment-270491723,https://api.github.com/repos/soumith/dcgan.torch/issues/60
soumith,dcgan.torch,198812321,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/270497235,270497235,MDEyOklzc3VlQ29tbWVudDI3MDQ5NzIzNQ==,1840180,2017-01-04T21:51:10Z,2017-01-04T21:51:10Z,NONE,"Oh God! That's it... thanks! :flushed: 

I created this script so that I don't forget to do this again (has happened before) -

```
$ cat ~/update_torch.sh 
#!/bin/bash

cat packages.torch | while read package
do
  echo updating $package
  luarocks install $package
done

echo ""You are now free to have some fun""

```
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/60/comments,https://github.com/soumith/dcgan.torch/issues/60#issuecomment-270497235,https://api.github.com/repos/soumith/dcgan.torch/issues/60
soumith,dcgan.torch,198812321,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/270498798,270498798,MDEyOklzc3VlQ29tbWVudDI3MDQ5ODc5OA==,1310570,2017-01-04T21:57:50Z,2017-01-04T21:57:50Z,OWNER,"we have an update script in ~/torch for this precise reason. To update the core packages, one simply has to do:
cd ~/torch
./clean.sh
./update.sh
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/60/comments,https://github.com/soumith/dcgan.torch/issues/60#issuecomment-270498798,https://api.github.com/repos/soumith/dcgan.torch/issues/60
soumith,dcgan.torch,198812321,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/280276422,280276422,MDEyOklzc3VlQ29tbWVudDI4MDI3NjQyMg==,24761636,2017-02-16T09:17:31Z,2017-02-16T09:17:31Z,NONE,"I had the same problem when running the code for church_outdoor dataset (on Ubuntu 14.04).
I updated all the packages concerned with this issue (cleaned up torch and re-installed it again), 
but still got the error.

Error detail is;

{
  ntrain : inf
  beta1 : 0.5
  name : ""experiment1""
  niter : 25
  batchSize : 64
  ndf : 64
  fineSize : 64
  nz : 100
  loadSize : 96
  gpu : 1
  ngf : 64
  dataset : ""lsun""
  lr : 0.0002
  noise : ""normal""
  nThreads : 4
  display_id : 10
  display : 1
}
Random Seed: 4252	
Starting donkey with id: 2 seed: 4254
table: 0x4074fa48
Starting donkey with id: 1 seed: 4253
table: 0x41f07c00
Starting donkey with id: 4 seed: 4256
table: 0x40305f48
Starting donkey with id: 3 seed: 4255
table: 0x40b27350
Classes:
1	church_outdoor
Classes:
1	church_outdoor
Classes:
Classes:
1	church_outdoor
1	church_outdoor
initializing train loader
initializing: 	church_outdoor
initializing train loader
initializing: 	church_outdoor
initializing train loader
initializing: 	church_outdoor
initializing train loader
initializing: 	church_outdoor
Dataset: lsun	 Size: 	126227	
/home/yashima/torch/install/bin/luajit: main.lua:45: attempt to call method 'noBias' (a nil value)
stack traceback:
	main.lua:45: in function 'callback'
	/home/yashima/.luarocks/share/lua/5.1/nn/Module.lua:318: in function 'apply'
	/home/yashima/.luarocks/share/lua/5.1/nn/Module.lua:322: in function 'apply'
	main.lua:81: in main chunk
	[C]: in function 'dofile'
	...hima/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
	[C]: at 0x00406670


Any comments would be appreciated. Thanks.",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/60/comments,https://github.com/soumith/dcgan.torch/issues/60#issuecomment-280276422,https://api.github.com/repos/soumith/dcgan.torch/issues/60
soumith,dcgan.torch,196319720,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/267869663,267869663,MDEyOklzc3VlQ29tbWVudDI2Nzg2OTY2Mw==,1310570,2016-12-19T02:40:41Z,2016-12-19T02:40:41Z,OWNER,"have a look at this thread, it'll answer your questions: https://github.com/soumith/dcgan.torch/issues/2#issuecomment-164862299",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/59/comments,https://github.com/soumith/dcgan.torch/issues/59#issuecomment-267869663,https://api.github.com/repos/soumith/dcgan.torch/issues/59
soumith,dcgan.torch,196319720,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/267869705,267869705,MDEyOklzc3VlQ29tbWVudDI2Nzg2OTcwNQ==,22454672,2016-12-19T02:41:10Z,2016-12-19T02:41:10Z,NONE,"woah thanks for that fast answer, I'm going on it now ",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/59/comments,https://github.com/soumith/dcgan.torch/issues/59#issuecomment-267869705,https://api.github.com/repos/soumith/dcgan.torch/issues/59
soumith,dcgan.torch,196319720,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/267870223,267870223,MDEyOklzc3VlQ29tbWVudDI2Nzg3MDIyMw==,22454672,2016-12-19T02:46:20Z,2016-12-19T02:46:20Z,NONE,"Okay, so I don't get everything on that thread but I'll try to manage the 128x128 .
But, just curious, if I wanted to have like 1920x1080 generations, would it be possible ? Or is it just unrealistic dream ?",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/59/comments,https://github.com/soumith/dcgan.torch/issues/59#issuecomment-267870223,https://api.github.com/repos/soumith/dcgan.torch/issues/59
soumith,dcgan.torch,196319720,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/267870307,267870307,MDEyOklzc3VlQ29tbWVudDI2Nzg3MDMwNw==,1310570,2016-12-19T02:47:16Z,2016-12-19T02:47:16Z,OWNER,"wrt GANs at the moment, 1920x1080 is an unrealistic dream. We cant even do 256x256 reasonably on a diverse set of problems.",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/59/comments,https://github.com/soumith/dcgan.torch/issues/59#issuecomment-267870307,https://api.github.com/repos/soumith/dcgan.torch/issues/59
soumith,dcgan.torch,196319720,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/267870903,267870903,MDEyOklzc3VlQ29tbWVudDI2Nzg3MDkwMw==,22454672,2016-12-19T02:53:14Z,2016-12-19T02:53:14Z,NONE,"I saw that person : https://github.com/hardmaru/cppn-gan-vae-tensorflow reffering to your code as an inspiration. So I was thinking maybe. Anyway, thanks so much for everything (:",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/59/comments,https://github.com/soumith/dcgan.torch/issues/59#issuecomment-267870903,https://api.github.com/repos/soumith/dcgan.torch/issues/59
soumith,dcgan.torch,196319720,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/267871040,267871040,MDEyOklzc3VlQ29tbWVudDI2Nzg3MTA0MA==,1310570,2016-12-19T02:54:24Z,2016-12-19T02:54:24Z,OWNER,"you can implement a VAE-GAN and maybe produce a 512px on MNIST, but that's about it. Open research :)",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/59/comments,https://github.com/soumith/dcgan.torch/issues/59#issuecomment-267871040,https://api.github.com/repos/soumith/dcgan.torch/issues/59
soumith,dcgan.torch,195977984,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/267916009,267916009,MDEyOklzc3VlQ29tbWVudDI2NzkxNjAwOQ==,10444110,2016-12-19T09:12:44Z,2016-12-19T09:12:44Z,NONE,"problem solved: larger image set successfully trained

",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/58/comments,https://github.com/soumith/dcgan.torch/issues/58#issuecomment-267916009,https://api.github.com/repos/soumith/dcgan.torch/issues/58
soumith,dcgan.torch,195977984,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/443824417,443824417,MDEyOklzc3VlQ29tbWVudDQ0MzgyNDQxNw==,15966304,2018-12-03T18:50:50Z,2018-12-03T18:50:50Z,NONE,"> problem solved: larger image set successfully trained

Hi @michaelByrne ,

As it's been a while since your last update here, I'm hoping that you recall what you did in order to get this working correctly.

I'm getting the same errors as you describe.

Thanks in advance for any insight into the issue.

",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/58/comments,https://github.com/soumith/dcgan.torch/issues/58#issuecomment-443824417,https://api.github.com/repos/soumith/dcgan.torch/issues/58
soumith,dcgan.torch,193135686,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/264473863,264473863,MDEyOklzc3VlQ29tbWVudDI2NDQ3Mzg2Mw==,1310570,2016-12-02T15:02:53Z,2016-12-02T15:02:53Z,OWNER,DATA_ROOT needs to point to the folder where the images are...,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/57/comments,https://github.com/soumith/dcgan.torch/issues/57#issuecomment-264473863,https://api.github.com/repos/soumith/dcgan.torch/issues/57
soumith,dcgan.torch,193135686,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/264478660,264478660,MDEyOklzc3VlQ29tbWVudDI2NDQ3ODY2MA==,5426040,2016-12-02T15:22:08Z,2016-12-02T15:22:08Z,NONE,Yes I'm correctly specify the DATA_ROOT and it has find 202659 images. but also give me the mistake..,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/57/comments,https://github.com/soumith/dcgan.torch/issues/57#issuecomment-264478660,https://api.github.com/repos/soumith/dcgan.torch/issues/57
soumith,dcgan.torch,193135686,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/268963569,268963569,MDEyOklzc3VlQ29tbWVudDI2ODk2MzU2OQ==,5723047,2016-12-23T09:36:56Z,2016-12-23T09:36:56Z,NONE,Encountered the same problem when DATA_ROOT is pointing to the correct path.,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/57/comments,https://github.com/soumith/dcgan.torch/issues/57#issuecomment-268963569,https://api.github.com/repos/soumith/dcgan.torch/issues/57
soumith,dcgan.torch,193135686,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/268964068,268964068,MDEyOklzc3VlQ29tbWVudDI2ODk2NDA2OA==,5723047,2016-12-23T09:41:06Z,2016-12-23T09:41:06Z,NONE,"error report:
```
$ DATA_ROOT=. dataset=folder th main.lua 
{
  ntrain : inf
  beta1 : 0.5
  name : ""experiment1""
  niter : 25
  batchSize : 64
  ndf : 64
  fineSize : 64
  nz : 100
  loadSize : 96
  gpu : 1
  ngf : 64
  dataset : ""folder""
  lr : 0.0002
  noise : ""normal""
  nThreads : 4
  display_id : 10
  display : 1
}
Random Seed: 9189	
Starting donkey with id: 2 seed: 9191
table: 0x41f545b0
Starting donkey with id: 3 seed: 9192
table: 0x40c3bcb8
Starting donkey with id: 1 seed: 9190
table: 0x414aaeb0
Starting donkey with id: 4 seed: 9193
table: 0x4189ab40
Creating train metadata
table: 0x402261b0
running ""find"" on each class directory, and concatenate all those filenames into a single file containing all image paths for a given class
Creating train metadata
table: 0x41bf63c0
running ""find"" on each class directory, and concatenate all those filenames into a single file containing all image paths for a given class
Creating train metadata
table: 0x414a9ac8
running ""find"" on each class directory, and concatenate all those filenames into a single file containing all image paths for a given class
Creating train metadata
table: 0x41c12140
running ""find"" on each class directory, and concatenate all those filenames into a single file containing all image paths for a given class
now combine all the files to a single large file
now combine all the files to a single large file
now combine all the files to a single large file
now combine all the files to a single large file
load the large concatenated list of sample paths to self.imagePath
load the large concatenated list of sample paths to self.imagePath
load the large concatenated list of sample paths to self.imagePath
load the large concatenated list of sample paths to self.imagePath
202604 samples found.===================== 200000/202604 ============================>..]  ETA: 1ms | Step: 0ms         
Updating classList and imageClass appropriately
202604 samples found.===================== 200000/202604 ============================>..]  ETA: 1ms | Step: 0ms         
Updating classList and imageClass appropriately
/home/john/torch/install/bin/luajit: /home/john/torch/install/share/lua/5.1/threads/threads.lua:183: [thread 2 callback] /home/john/cv17-gan/dcgan.torch/data/dataset.lua:228: Class has zero samples
stack traceback:
	[C]: in function 'error'
	/home/john/cv17-gan/dcgan.torch/data/dataset.lua:228: in function '__init'
	/home/john/torch/install/share/lua/5.1/torch/init.lua:91: in function </home/john/torch/install/share/lua/5.1/torch/init.lua:87>
	[C]: in function 'dataLoader'
	/home/john/cv17-gan/dcgan.torch/data/donkey_folder.lua:82: in main chunk
	[C]: in function 'dofile'
	/home/john/cv17-gan/dcgan.torch/data/data.lua:42: in function </home/john/cv17-gan/dcgan.torch/data/data.lua:32>
	[C]: in function 'xpcall'
	/home/john/torch/install/share/lua/5.1/threads/threads.lua:234: in function 'callback'
	/home/john/torch/install/share/lua/5.1/threads/queue.lua:65: in function </home/john/torch/install/share/lua/5.1/threads/queue.lua:41>
	[C]: in function 'pcall'
	/home/john/torch/install/share/lua/5.1/threads/queue.lua:40: in function 'dojob'
	[string ""  local Queue = require 'threads.queue'...""]:13: in main chunk
stack traceback:
	[C]: in function 'error'
	/home/john/torch/install/share/lua/5.1/threads/threads.lua:183: in function 'dojob'
	/home/john/torch/install/share/lua/5.1/threads/threads.lua:264: in function 'synchronize'
	/home/john/torch/install/share/lua/5.1/threads/threads.lua:142: in function 'specific'
	/home/john/torch/install/share/lua/5.1/threads/threads.lua:125: in function 'Threads'
	/home/john/cv17-gan/dcgan.torch/data/data.lua:30: in function 'new'
	main.lua:38: in main chunk
	[C]: in function 'dofile'
	...john/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:145: in main chunk
	[C]: at 0x00405b40

```
I extracted celebA into the root dir of source tree instead of `celebA/`.",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/57/comments,https://github.com/soumith/dcgan.torch/issues/57#issuecomment-268964068,https://api.github.com/repos/soumith/dcgan.torch/issues/57
soumith,dcgan.torch,193135686,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/268964611,268964611,MDEyOklzc3VlQ29tbWVudDI2ODk2NDYxMQ==,5723047,2016-12-23T09:45:40Z,2016-12-23T09:45:40Z,NONE,"I'm using the latest version of torch (Dec 23, 2016) and the latest version of dcgan.torch .",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/57/comments,https://github.com/soumith/dcgan.torch/issues/57#issuecomment-268964611,https://api.github.com/repos/soumith/dcgan.torch/issues/57
soumith,dcgan.torch,193135686,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/268998834,268998834,MDEyOklzc3VlQ29tbWVudDI2ODk5ODgzNA==,5426040,2016-12-23T14:37:10Z,2016-12-23T14:37:10Z,NONE,@CDLuminate I solved the problem. you should ensure there is one folder in your DATA_PATH...,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/57/comments,https://github.com/soumith/dcgan.torch/issues/57#issuecomment-268998834,https://api.github.com/repos/soumith/dcgan.torch/issues/57
soumith,dcgan.torch,193135686,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/268999286,268999286,MDEyOklzc3VlQ29tbWVudDI2ODk5OTI4Ng==,1310570,2016-12-23T14:40:48Z,2016-12-23T14:41:08Z,OWNER,"@CDLuminate if your images are at data/celebA/*.png, then your DATA_ROOT=data",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/57/comments,https://github.com/soumith/dcgan.torch/issues/57#issuecomment-268999286,https://api.github.com/repos/soumith/dcgan.torch/issues/57
soumith,dcgan.torch,193135686,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/268999441,268999441,MDEyOklzc3VlQ29tbWVudDI2ODk5OTQ0MQ==,1310570,2016-12-23T14:42:00Z,2016-12-23T14:42:00Z,OWNER,"usually after you unzip the celebA dataset in a folder called celebA, all the images are:
celebA/img_align_celeba/*.jpg

in this case, the DATA_ROOT=celebA",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/57/comments,https://github.com/soumith/dcgan.torch/issues/57#issuecomment-268999441,https://api.github.com/repos/soumith/dcgan.torch/issues/57
soumith,dcgan.torch,193135686,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/269064262,269064262,MDEyOklzc3VlQ29tbWVudDI2OTA2NDI2Mg==,5723047,2016-12-24T02:13:30Z,2016-12-24T02:13:30Z,NONE,@tzzcl @soumith Thank you both. I moved the `img_aligned_celeba` directory to `celebA` and things get working.,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/57/comments,https://github.com/soumith/dcgan.torch/issues/57#issuecomment-269064262,https://api.github.com/repos/soumith/dcgan.torch/issues/57
soumith,dcgan.torch,191783989,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/263041773,263041773,MDEyOklzc3VlQ29tbWVudDI2MzA0MTc3Mw==,14287863,2016-11-26T03:30:07Z,2016-11-26T03:30:07Z,NONE,"sorry ,I have  solve the low level error ...",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/56/comments,https://github.com/soumith/dcgan.torch/issues/56#issuecomment-263041773,https://api.github.com/repos/soumith/dcgan.torch/issues/56
soumith,dcgan.torch,191783989,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/263044309,263044309,MDEyOklzc3VlQ29tbWVudDI2MzA0NDMwOQ==,1310570,2016-11-26T04:57:28Z,2016-11-26T04:57:28Z,OWNER,ok glad you solved it,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/56/comments,https://github.com/soumith/dcgan.torch/issues/56#issuecomment-263044309,https://api.github.com/repos/soumith/dcgan.torch/issues/56
soumith,dcgan.torch,189284734,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/262334705,262334705,MDEyOklzc3VlQ29tbWVudDI2MjMzNDcwNQ==,1310570,2016-11-22T19:08:05Z,2016-11-22T19:08:05Z,OWNER,it depends on which experiment you refer to from the paper. some used 512 some 1024,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/55/comments,https://github.com/soumith/dcgan.torch/issues/55#issuecomment-262334705,https://api.github.com/repos/soumith/dcgan.torch/issues/55
soumith,dcgan.torch,188733711,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/259976749,259976749,MDEyOklzc3VlQ29tbWVudDI1OTk3Njc0OQ==,1310570,2016-11-11T15:06:05Z,2016-11-11T15:06:05Z,OWNER,"no, it's just for normalization. And the Conv weights are initialized in a way that [-1,1] is expected as the input range.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/54/comments,https://github.com/soumith/dcgan.torch/issues/54#issuecomment-259976749,https://api.github.com/repos/soumith/dcgan.torch/issues/54
soumith,dcgan.torch,188384925,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/259586770,259586770,MDEyOklzc3VlQ29tbWVudDI1OTU4Njc3MA==,1310570,2016-11-10T02:36:03Z,2016-11-10T02:36:03Z,OWNER,"the data loader has been repurposed across several projects. in this case it's not relevant, as there's no separate training / testing (like for example in image classification where in training we want images to be random-cropped and jittered, but in testing we might want them to be center-cropped)
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/53/comments,https://github.com/soumith/dcgan.torch/issues/53#issuecomment-259586770,https://api.github.com/repos/soumith/dcgan.torch/issues/53
soumith,dcgan.torch,188105704,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/259262483,259262483,MDEyOklzc3VlQ29tbWVudDI1OTI2MjQ4Mw==,1310570,2016-11-08T21:18:06Z,2016-11-08T21:18:06Z,OWNER,"it is not available.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/52/comments,https://github.com/soumith/dcgan.torch/issues/52#issuecomment-259262483,https://api.github.com/repos/soumith/dcgan.torch/issues/52
soumith,dcgan.torch,187822123,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/262334882,262334882,MDEyOklzc3VlQ29tbWVudDI2MjMzNDg4Mg==,1310570,2016-11-22T19:08:41Z,2016-11-22T19:08:41Z,OWNER,this paper / code does not use embeddings,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/51/comments,https://github.com/soumith/dcgan.torch/issues/51#issuecomment-262334882,https://api.github.com/repos/soumith/dcgan.torch/issues/51
soumith,dcgan.torch,186117282,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/262341191,262341191,MDEyOklzc3VlQ29tbWVudDI2MjM0MTE5MQ==,1310570,2016-11-22T19:31:50Z,2016-11-22T19:31:50Z,OWNER,are your second set of generations on a pre-trained network?,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/49/comments,https://github.com/soumith/dcgan.torch/issues/49#issuecomment-262341191,https://api.github.com/repos/soumith/dcgan.torch/issues/49
soumith,dcgan.torch,186117282,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/262777901,262777901,MDEyOklzc3VlQ29tbWVudDI2Mjc3NzkwMQ==,4658916,2016-11-24T13:36:50Z,2016-11-24T13:36:50Z,NONE,"No, I had trained both the networks from scratch on Celeb dataset, with the only difference of how I created the input to generator, as I explained in my comment. ",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/49/comments,https://github.com/soumith/dcgan.torch/issues/49#issuecomment-262777901,https://api.github.com/repos/soumith/dcgan.torch/issues/49
soumith,dcgan.torch,186117282,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/272659511,272659511,MDEyOklzc3VlQ29tbWVudDI3MjY1OTUxMQ==,4658916,2017-01-14T22:54:39Z,2017-01-14T22:54:39Z,NONE,"Seems like this is a mode collapse problem where the generator is modeling the distribution to one Gaussian rather than a mixture of Gaussians. More pointers on the mode collapse problem can be found in Ian Goodfellow's 2016 NIPS tutorial, https://arxiv.org/abs/1701.00160.",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/49/comments,https://github.com/soumith/dcgan.torch/issues/49#issuecomment-272659511,https://api.github.com/repos/soumith/dcgan.torch/issues/49
soumith,dcgan.torch,186117282,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/286837090,286837090,MDEyOklzc3VlQ29tbWVudDI4NjgzNzA5MA==,18741096,2017-03-15T18:30:08Z,2017-03-15T18:30:08Z,NONE,@vasavig Did you find a way to overcome this problem?,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/49/comments,https://github.com/soumith/dcgan.torch/issues/49#issuecomment-286837090,https://api.github.com/repos/soumith/dcgan.torch/issues/49
soumith,dcgan.torch,186117282,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/286897393,286897393,MDEyOklzc3VlQ29tbWVudDI4Njg5NzM5Mw==,18741096,2017-03-15T22:17:59Z,2017-03-15T22:17:59Z,NONE,"In [this paper](https://arxiv.org/abs/1611.02163) the authors claim to have solved the mode collapse issue via ""unrolling"" the Adam objective. 
I wouldn't be able to implement any of this in the current model, but it might be cool to consider it...?",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/49/comments,https://github.com/soumith/dcgan.torch/issues/49#issuecomment-286897393,https://api.github.com/repos/soumith/dcgan.torch/issues/49
soumith,dcgan.torch,185867405,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/256944532,256944532,MDEyOklzc3VlQ29tbWVudDI1Njk0NDUzMg==,1310570,2016-10-28T15:01:47Z,2016-10-28T15:01:47Z,OWNER,"there are batchnorm interactions that will be present.

If you want to avoid such interactions between batch samples, put the model in evaluate mode, i.e. G:evaluate() and then push the replicated noise vectors through.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/48/comments,https://github.com/soumith/dcgan.torch/issues/48#issuecomment-256944532,https://api.github.com/repos/soumith/dcgan.torch/issues/48
soumith,dcgan.torch,185867405,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/257021949,257021949,MDEyOklzc3VlQ29tbWVudDI1NzAyMTk0OQ==,4658916,2016-10-28T20:36:23Z,2016-10-28T20:36:23Z,NONE,"Thanks, soumith! That clears a lot of confusion.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/48/comments,https://github.com/soumith/dcgan.torch/issues/48#issuecomment-257021949,https://api.github.com/repos/soumith/dcgan.torch/issues/48
soumith,dcgan.torch,185585751,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/262342599,262342599,MDEyOklzc3VlQ29tbWVudDI2MjM0MjU5OQ==,1310570,2016-11-22T19:36:43Z,2016-11-22T19:36:43Z,OWNER,we pass in images and look at the activations. ,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/47/comments,https://github.com/soumith/dcgan.torch/issues/47#issuecomment-262342599,https://api.github.com/repos/soumith/dcgan.torch/issues/47
soumith,dcgan.torch,185575305,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/262342793,262342793,MDEyOklzc3VlQ29tbWVudDI2MjM0Mjc5Mw==,1310570,2016-11-22T19:37:26Z,2016-11-22T19:37:26Z,OWNER,"well, DATA_ROOT=xxx is an example of a folder. replace it with the path to the folder where your dataset lies.",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/46/comments,https://github.com/soumith/dcgan.torch/issues/46#issuecomment-262342793,https://api.github.com/repos/soumith/dcgan.torch/issues/46
soumith,dcgan.torch,185302294,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/259163637,259163637,MDEyOklzc3VlQ29tbWVudDI1OTE2MzYzNw==,1310570,2016-11-08T15:15:48Z,2016-11-08T15:15:48Z,OWNER,"it looks for images with the following extensions https://github.com/soumith/dcgan.torch/blob/master/data/dataset.lua#L146

maybe it did not find any such.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/45/comments,https://github.com/soumith/dcgan.torch/issues/45#issuecomment-259163637,https://api.github.com/repos/soumith/dcgan.torch/issues/45
soumith,dcgan.torch,185300353,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/259163400,259163400,MDEyOklzc3VlQ29tbWVudDI1OTE2MzQwMA==,1310570,2016-11-08T15:14:56Z,2016-11-08T15:14:56Z,OWNER,"you should choose a normal distribution.
Tom White gives a good set of arguments here: https://github.com/soumith/dcgan.torch/issues/14 as well as in his latest paper.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/44/comments,https://github.com/soumith/dcgan.torch/issues/44#issuecomment-259163400,https://api.github.com/repos/soumith/dcgan.torch/issues/44
soumith,dcgan.torch,184124649,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/262342938,262342938,MDEyOklzc3VlQ29tbWVudDI2MjM0MjkzOA==,1310570,2016-11-22T19:37:57Z,2016-11-22T19:37:57Z,OWNER,you cannot do this by design. the picture has to be generated by the network.,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/43/comments,https://github.com/soumith/dcgan.torch/issues/43#issuecomment-262342938,https://api.github.com/repos/soumith/dcgan.torch/issues/43
soumith,dcgan.torch,182728656,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/253520154,253520154,MDEyOklzc3VlQ29tbWVudDI1MzUyMDE1NA==,1310570,2016-10-13T13:55:58Z,2016-10-13T13:55:58Z,OWNER,"pixel values of real images is -1 to 1 too. they are mean-normalized
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/42/comments,https://github.com/soumith/dcgan.torch/issues/42#issuecomment-253520154,https://api.github.com/repos/soumith/dcgan.torch/issues/42
soumith,dcgan.torch,180088855,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/262343109,262343109,MDEyOklzc3VlQ29tbWVudDI2MjM0MzEwOQ==,1310570,2016-11-22T19:38:39Z,2016-11-22T19:38:39Z,OWNER,the batch discrimination is implemented here: https://github.com/qassemoquab/stnbhwd/blob/batchDisc/BatchDiscrimination.lua,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/41/comments,https://github.com/soumith/dcgan.torch/issues/41#issuecomment-262343109,https://api.github.com/repos/soumith/dcgan.torch/issues/41
soumith,dcgan.torch,179715370,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/262343236,262343236,MDEyOklzc3VlQ29tbWVudDI2MjM0MzIzNg==,1310570,2016-11-22T19:39:04Z,2016-11-22T19:39:04Z,OWNER,it can be used. what's the issue that you see?,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/40/comments,https://github.com/soumith/dcgan.torch/issues/40#issuecomment-262343236,https://api.github.com/repos/soumith/dcgan.torch/issues/40
soumith,dcgan.torch,179715370,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/274342555,274342555,MDEyOklzc3VlQ29tbWVudDI3NDM0MjU1NQ==,15867677,2017-01-22T16:50:33Z,2017-01-22T16:50:33Z,NONE,"Hello, I also trained the GAN using the Celeb-A dataset on the GPU. However arithmetic.lua does not work on the checkpoints generated doing this. (with the checkpoints, which I downloaded from the repository it worked.) 
When I run net=checkpoints/experiment1_1_net_G.t7 gpu=0 qlua arithmetic.lua I get the following error:
table: 0x41a862a8
qlua: /home/catherine/torch/install/share/lua/5.1/torch/File.lua:343: unknown Torch class <torch.CudaTensor>
stack traceback:
	[C]: at 0x7f7f513a0f50
	[C]: in function 'error'
	/home/catherine/torch/install/share/lua/5.1/torch/File.lua:343: in function 'readObject'
	/home/catherine/torch/install/share/lua/5.1/torch/File.lua:369: in function 'readObject'
	/home/catherine/torch/install/share/lua/5.1/nn/Module.lua:192: in function 'read'
	/home/catherine/torch/install/share/lua/5.1/torch/File.lua:351: in function 'readObject'
	/home/catherine/torch/install/share/lua/5.1/torch/File.lua:409: in function 'load'
	arithmetic.lua:27: in main chunk
When I try to run it in gpu mode I get the same error. I tried to include require cunn and require cudnn at the beginning of arithmetic.lua, but then I get different errors. Thank you. ",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/40/comments,https://github.com/soumith/dcgan.torch/issues/40#issuecomment-274342555,https://api.github.com/repos/soumith/dcgan.torch/issues/40
soumith,dcgan.torch,177308506,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/247476659,247476659,MDEyOklzc3VlQ29tbWVudDI0NzQ3NjY1OQ==,4658916,2016-09-15T22:49:42Z,2016-09-15T22:49:42Z,NONE,"Found the definition as ""trainHook"" function in donkey_folder file.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/36/comments,https://github.com/soumith/dcgan.torch/issues/36#issuecomment-247476659,https://api.github.com/repos/soumith/dcgan.torch/issues/36
soumith,dcgan.torch,175558585,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/247455205,247455205,MDEyOklzc3VlQ29tbWVudDI0NzQ1NTIwNQ==,9437360,2016-09-15T21:04:42Z,2016-09-15T21:04:42Z,NONE,"You may have solved this by now, but can add those require's at the beginning(right at the top) of the generate.lua and it should work now.. 
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/35/comments,https://github.com/soumith/dcgan.torch/issues/35#issuecomment-247455205,https://api.github.com/repos/soumith/dcgan.torch/issues/35
soumith,dcgan.torch,175558585,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/247524600,247524600,MDEyOklzc3VlQ29tbWVudDI0NzUyNDYwMA==,15064373,2016-09-16T06:01:32Z,2016-09-16T13:19:31Z,NONE,"""add those require's at the beginning(right at the top) of the generate.lua and it should work now..""

That is exactly what I wrote in my posting. It was not a problem for me, just wanted to point a possible error in the code.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/35/comments,https://github.com/soumith/dcgan.torch/issues/35#issuecomment-247524600,https://api.github.com/repos/soumith/dcgan.torch/issues/35
soumith,dcgan.torch,175558585,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/247650567,247650567,MDEyOklzc3VlQ29tbWVudDI0NzY1MDU2Nw==,9437360,2016-09-16T16:49:51Z,2016-09-16T16:49:51Z,NONE,"Oops. Sorry about that. 
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/35/comments,https://github.com/soumith/dcgan.torch/issues/35#issuecomment-247650567,https://api.github.com/repos/soumith/dcgan.torch/issues/35
soumith,dcgan.torch,175558585,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/264449523,264449523,MDEyOklzc3VlQ29tbWVudDI2NDQ0OTUyMw==,14287863,2016-12-02T13:03:42Z,2016-12-02T13:03:42Z,NONE,"@htoyryla ,I have the same problem,and I want to ask u for whether the size of your model 'XXX.t7' is 154MB, and my model is so large,I wander if I run the experiment wrong?",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/35/comments,https://github.com/soumith/dcgan.torch/issues/35#issuecomment-264449523,https://api.github.com/repos/soumith/dcgan.torch/issues/35
soumith,dcgan.torch,174139398,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/244480665,244480665,MDEyOklzc3VlQ29tbWVudDI0NDQ4MDY2NQ==,6722968,2016-09-02T20:33:18Z,2016-09-02T20:33:18Z,NONE,"![screen shot 2016-09-03 at 8 27 47 am](https://cloud.githubusercontent.com/assets/6722968/18217346/5f1c8f18-71b0-11e6-81ab-305c99b248ba.png)
![screen shot 2016-09-03 at 8 30 47 am](https://cloud.githubusercontent.com/assets/6722968/18217444/f7fbd770-71b0-11e6-9166-182e116c7674.png)
For example, here's the progressive results on a net trained on 500+ images for 400 Epochs.
But I can't seem to reach the kind of smooth, coherent results in your demos. Should I be reaching some kind of consistent figures in the Err_G and Err_D fields?
I'm guessing I just need to train more, but I don't know how to continue training from the 400 mark..?
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/34/comments,https://github.com/soumith/dcgan.torch/issues/34#issuecomment-244480665,https://api.github.com/repos/soumith/dcgan.torch/issues/34
soumith,dcgan.torch,174139398,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/255000146,255000146,MDEyOklzc3VlQ29tbWVudDI1NTAwMDE0Ng==,7991905,2016-10-20T03:29:19Z,2016-10-20T03:29:19Z,NONE,"more pictures
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/34/comments,https://github.com/soumith/dcgan.torch/issues/34#issuecomment-255000146,https://api.github.com/repos/soumith/dcgan.torch/issues/34
soumith,dcgan.torch,174139398,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/281833825,281833825,MDEyOklzc3VlQ29tbWVudDI4MTgzMzgyNQ==,2930635,2017-02-22T23:01:14Z,2017-02-22T23:01:14Z,NONE,I would also love to know how to do this.,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/34/comments,https://github.com/soumith/dcgan.torch/issues/34#issuecomment-281833825,https://api.github.com/repos/soumith/dcgan.torch/issues/34
soumith,dcgan.torch,173320032,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/242594848,242594848,MDEyOklzc3VlQ29tbWVudDI0MjU5NDg0OA==,9437360,2016-08-26T01:25:11Z,2016-08-26T01:25:11Z,NONE,"Just to update, luarocks install cudnn again did it. Closing the issue. 

Thanks,
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/33/comments,https://github.com/soumith/dcgan.torch/issues/33#issuecomment-242594848,https://api.github.com/repos/soumith/dcgan.torch/issues/33
soumith,dcgan.torch,171411446,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/240157185,240157185,MDEyOklzc3VlQ29tbWVudDI0MDE1NzE4NQ==,1310570,2016-08-16T16:27:00Z,2016-08-16T16:27:00Z,OWNER,"Thanks!
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/32/comments,https://github.com/soumith/dcgan.torch/pull/32#issuecomment-240157185,https://api.github.com/repos/soumith/dcgan.torch/issues/32
soumith,dcgan.torch,170091617,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/398503956,398503956,MDEyOklzc3VlQ29tbWVudDM5ODUwMzk1Ng==,5029845,2018-06-19T18:43:37Z,2018-06-19T18:43:37Z,NONE,maxmin will encourage fake generations,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/31/comments,https://github.com/soumith/dcgan.torch/issues/31#issuecomment-398503956,https://api.github.com/repos/soumith/dcgan.torch/issues/31
soumith,dcgan.torch,169495614,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/238052291,238052291,MDEyOklzc3VlQ29tbWVudDIzODA1MjI5MQ==,1310570,2016-08-06T22:27:16Z,2016-08-06T22:27:16Z,OWNER,"should be fixed if you install the latest cudnn.torch bindings: luarocks install torch
fixed via commit: https://github.com/soumith/cudnn.torch/commit/556c272b8be5b3b61a8f1f2b8a7daf8ce73681c1
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/30/comments,https://github.com/soumith/dcgan.torch/issues/30#issuecomment-238052291,https://api.github.com/repos/soumith/dcgan.torch/issues/30
soumith,dcgan.torch,169495614,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/247933642,247933642,MDEyOklzc3VlQ29tbWVudDI0NzkzMzY0Mg==,1022717,2016-09-19T08:13:22Z,2016-09-19T08:13:22Z,NONE,"Actually, it's not fixed. Or maybe is re-introduced. Anyway, I get the same error with the latest versions. 

The problem is `self.adjW` and `self.adjH` are still not initialized when calling `SpatialFullConvolution:createIODescriptors(...)`. `SpatialFullConvolution:read(...)` is never called when the network is created via `torch.load(...)`, so the fix doesn't solve the problem. 

One way to solve this is to add the two lines:

```
self.adjW = self.adjW or 0
self.adjH = self.adjH or 0
```

to `cudnn.SpatialFullConvolution.lua` line 111. 

Although I'm not sure if this is the correct way and does not introduce new problems. 
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/30/comments,https://github.com/soumith/dcgan.torch/issues/30#issuecomment-247933642,https://api.github.com/repos/soumith/dcgan.torch/issues/30
soumith,dcgan.torch,168035544,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/236405583,236405583,MDEyOklzc3VlQ29tbWVudDIzNjQwNTU4Mw==,1310570,2016-07-31T03:02:01Z,2016-07-31T03:02:01Z,OWNER,"fixed via https://github.com/soumith/dcgan.torch/commit/f709ad999d8e5f13457b0a0a7c2d7d8c4ffa8696
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/29/comments,https://github.com/soumith/dcgan.torch/issues/29#issuecomment-236405583,https://api.github.com/repos/soumith/dcgan.torch/issues/29
soumith,dcgan.torch,167945269,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/235724024,235724024,MDEyOklzc3VlQ29tbWVudDIzNTcyNDAyNA==,9153650,2016-07-27T21:17:39Z,2016-07-27T21:29:33Z,NONE,"This happened to me, too. The code is loading the generator which is stored as a cuda tensor before importing cunn and cudnn, so it doesn't recognize cuda tensors.
Just add this after opt is initialized:

```
if opt.gpu > 0 then
  require 'cunn'
  require 'cudnn'
end
```

Also, this means that after this last commit e057802, you can't load a network in CPU if it was stored in GPU. This could be done with the util.save function which was erased in the mentioned commit.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/28/comments,https://github.com/soumith/dcgan.torch/issues/28#issuecomment-235724024,https://api.github.com/repos/soumith/dcgan.torch/issues/28
soumith,dcgan.torch,167945269,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/235730356,235730356,MDEyOklzc3VlQ29tbWVudDIzNTczMDM1Ng==,623781,2016-07-27T21:41:59Z,2016-07-27T21:41:59Z,NONE,"Thanks!! That did the trick!!!
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/28/comments,https://github.com/soumith/dcgan.torch/issues/28#issuecomment-235730356,https://api.github.com/repos/soumith/dcgan.torch/issues/28
soumith,dcgan.torch,166066652,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/233298440,233298440,MDEyOklzc3VlQ29tbWVudDIzMzI5ODQ0MA==,9080094,2016-07-18T10:51:13Z,2016-07-18T10:51:13Z,NONE,"found an answer here: https://github.com/cmusatyalab/openface/issues/23#issuecomment-149175986
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/27/comments,https://github.com/soumith/dcgan.torch/issues/27#issuecomment-233298440,https://api.github.com/repos/soumith/dcgan.torch/issues/27
soumith,dcgan.torch,166066652,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/233298581,233298581,MDEyOklzc3VlQ29tbWVudDIzMzI5ODU4MQ==,9080094,2016-07-18T10:52:03Z,2016-07-18T10:52:03Z,NONE,"now I get this:

/Users/WS18/torch/install/bin/luajit: main.lua:45: attempt to call method 'noBias' (a nil value)
stack traceback:
    main.lua:45: in function 'callback'
    /Users/WS18/torch/install/share/lua/5.1/nn/Module.lua:318: in function 'apply'
    /Users/WS18/torch/install/share/lua/5.1/nn/Module.lua:322: in function 'apply'
    main.lua:81: in main chunk
    [C]: in function 'dofile'
    ...WS18/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:145: in main chunk
    [C]: at 0x0103607d10
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/27/comments,https://github.com/soumith/dcgan.torch/issues/27#issuecomment-233298581,https://api.github.com/repos/soumith/dcgan.torch/issues/27
soumith,dcgan.torch,166066652,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/233301686,233301686,MDEyOklzc3VlQ29tbWVudDIzMzMwMTY4Ng==,9080094,2016-07-18T11:11:45Z,2016-07-18T11:11:45Z,NONE,"fixed it by updating the stack:

luarocks install torch
luarocks install nn
luarocks install cutorch
luarocks install cunn
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/27/comments,https://github.com/soumith/dcgan.torch/issues/27#issuecomment-233301686,https://api.github.com/repos/soumith/dcgan.torch/issues/27
soumith,dcgan.torch,165812457,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/573372102,573372102,MDEyOklzc3VlQ29tbWVudDU3MzM3MjEwMg==,10707361,2020-01-12T01:42:53Z,2020-01-12T01:42:53Z,NONE,"Hi, now 3 years passed I faced up with the similar problem as you had 3 years ago.  I would like to know how you get it done. Thanks.",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/26/comments,https://github.com/soumith/dcgan.torch/issues/26#issuecomment-573372102,https://api.github.com/repos/soumith/dcgan.torch/issues/26
soumith,dcgan.torch,165708284,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/235495366,235495366,MDEyOklzc3VlQ29tbWVudDIzNTQ5NTM2Ng==,1310570,2016-07-27T06:06:42Z,2016-07-27T06:06:42Z,OWNER,"@jwyang you could look at this repo which has very good svhn https://github.com/openai/improved-gan
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/25/comments,https://github.com/soumith/dcgan.torch/issues/25#issuecomment-235495366,https://api.github.com/repos/soumith/dcgan.torch/issues/25
soumith,dcgan.torch,165630806,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/232801572,232801572,MDEyOklzc3VlQ29tbWVudDIzMjgwMTU3Mg==,9153650,2016-07-14T21:43:57Z,2016-07-14T21:43:57Z,NONE,"I have found the origin of the problem.

https://github.com/soumith/dcgan.torch/blob/e0578020ecd5c1b5df6356e7059b2b46670ae5a8/main.lua#L240

Generator is saved as `netG:clearState()`. If saved without `clearState()`, images on evaluation mode are generated properly.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/24/comments,https://github.com/soumith/dcgan.torch/issues/24#issuecomment-232801572,https://api.github.com/repos/soumith/dcgan.torch/issues/24
soumith,dcgan.torch,165630806,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/235495989,235495989,MDEyOklzc3VlQ29tbWVudDIzNTQ5NTk4OQ==,1310570,2016-07-27T06:11:48Z,2016-07-27T06:11:48Z,OWNER,"@szagoruyko does clearState clear running_mean / running_std? I dont think so right...wonder what's going on here. 
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/24/comments,https://github.com/soumith/dcgan.torch/issues/24#issuecomment-235495989,https://api.github.com/repos/soumith/dcgan.torch/issues/24
soumith,dcgan.torch,165630806,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/235514601,235514601,MDEyOklzc3VlQ29tbWVudDIzNTUxNDYwMQ==,4953728,2016-07-27T08:01:48Z,2016-07-27T08:01:48Z,CONTRIBUTOR,"I trained a model for a couple of epochs and saved one with clearState and one without clearState, they both produce identical images (with the same seed).
@Guim3 did you modify the code in any way? can you post the 2 models so that we can debug the issue?
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/24/comments,https://github.com/soumith/dcgan.torch/issues/24#issuecomment-235514601,https://api.github.com/repos/soumith/dcgan.torch/issues/24
soumith,dcgan.torch,164861149,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/231766162,231766162,MDEyOklzc3VlQ29tbWVudDIzMTc2NjE2Mg==,1310570,2016-07-11T15:17:14Z,2016-07-11T15:17:14Z,OWNER,"hi! while the need for GPUs isn't essential for training DCGANs, it is practically needed. Training DCGANs on CPU will take months.

The gpu=0 flag will let you train DCGANs without the GPU.
The generation scripts, which use pre-trained models are more suited to try out when you dont have a GPU.
https://github.com/soumith/dcgan.torch#2-use-a-pre-trained-generator-to-generate-images
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/23/comments,https://github.com/soumith/dcgan.torch/issues/23#issuecomment-231766162,https://api.github.com/repos/soumith/dcgan.torch/issues/23
soumith,dcgan.torch,164861149,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/231780435,231780435,MDEyOklzc3VlQ29tbWVudDIzMTc4MDQzNQ==,229521,2016-07-11T16:02:44Z,2016-07-11T16:02:44Z,NONE,"Thanks for the swift reply. I appreciate the length of time, and am planning to move to AWS, but just making sure I understand how everything works.

Now training with the command (for others' reference):

`gpu=0 DATA_ROOT=celebA dataset=folder th main.lua`
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/23/comments,https://github.com/soumith/dcgan.torch/issues/23#issuecomment-231780435,https://api.github.com/repos/soumith/dcgan.torch/issues/23
soumith,dcgan.torch,164819880,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/232800032,232800032,MDEyOklzc3VlQ29tbWVudDIzMjgwMDAzMg==,9153650,2016-07-14T21:37:16Z,2016-07-14T21:37:16Z,NONE,"You need to install threads package.

`luarocks install threads`
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/22/comments,https://github.com/soumith/dcgan.torch/issues/22#issuecomment-232800032,https://api.github.com/repos/soumith/dcgan.torch/issues/22
soumith,dcgan.torch,164819880,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/241335659,241335659,MDEyOklzc3VlQ29tbWVudDI0MTMzNTY1OQ==,7868520,2016-08-22T07:38:40Z,2016-08-22T07:38:40Z,NONE,"Thanks :)
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/22/comments,https://github.com/soumith/dcgan.torch/issues/22#issuecomment-241335659,https://api.github.com/repos/soumith/dcgan.torch/issues/22
soumith,dcgan.torch,164685776,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/231557176,231557176,MDEyOklzc3VlQ29tbWVudDIzMTU1NzE3Ng==,9153650,2016-07-09T21:35:55Z,2016-07-09T21:35:55Z,NONE,"It doesn't make much of a difference. You would only simply give the discriminator one training iteration extra training compared to the generator. And in fact, you are interested on not overpowering the discriminator, as this might yield an unstable training. 
Think it like this way: if you were to compute the output with the updated netD, why wouldn't you update the parameters again with this new output? You could add as many extra iterations you wanted to the discriminator for one iteration from the generator, but this doesn't might be desired for a robust model.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/21/comments,https://github.com/soumith/dcgan.torch/issues/21#issuecomment-231557176,https://api.github.com/repos/soumith/dcgan.torch/issues/21
soumith,dcgan.torch,164685776,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/231557482,231557482,MDEyOklzc3VlQ29tbWVudDIzMTU1NzQ4Mg==,9464836,2016-07-09T21:42:39Z,2016-07-09T21:46:08Z,NONE,"Unless I'm misunderstanding something, I don't think that's what happens in the code.  What happens in the code is that we call forward on netD in fDx, then update netD's parameters, then call backward on netD in fGx.  That means that the gradInput is being computed using the updated parameters, while the loss was computed using the parameters before they were updated.  That means the gradient doesn't actually make any sense at all.

Neural networks are often robust to this kind of error, so it's not surprising that it works anyway, but it might work much better if things were done in the correct sequence.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/21/comments,https://github.com/soumith/dcgan.torch/issues/21#issuecomment-231557482,https://api.github.com/repos/soumith/dcgan.torch/issues/21
soumith,dcgan.torch,164685776,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/231833018,231833018,MDEyOklzc3VlQ29tbWVudDIzMTgzMzAxOA==,9153650,2016-07-11T19:06:40Z,2016-07-11T19:24:46Z,NONE,"Hi again,

I misunderstood your question, sorry! 
When you say ""then call backward on netD in fGx"", you are refering [to this line of code](https://github.com/soumith/dcgan.torch/blob/master/main.lua#L200), right? 

`local df_dg = netD:updateGradInput(input, df_do)`

Firstly, notice that a call to [updateGradInput](https://github.com/torch/nn/blob/master/doc/module.md#gradinput-backwardinput-gradoutput) _does not_ compute a whole backpropagation step. In order to do so, you would also need to call accGradParameters, which is not called on D in fGx.

Secondly, why is necessary to call updateGradInput on D if we are not updating D at all? Because we need the gradients with respect to the fake images generated by G (`input` in fGx). This is what updateGradInput returns, `df_dg`.

So, once we get `df_dg`, we are now able to do a backward step on G with the correct gradients:

`netG:backward(noise, df_dg)`

PS: [Notice that `df_do`](https://github.com/soumith/dcgan.torch/blob/master/main.lua#L199) does not contain the gradients of the generated images `input` but the gradients of D output, which is a one dimensional vector.  That's the reason we can't use `df_do` on G's backward step.

Hope I have answered your question this time! :P
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/21/comments,https://github.com/soumith/dcgan.torch/issues/21#issuecomment-231833018,https://api.github.com/repos/soumith/dcgan.torch/issues/21
soumith,dcgan.torch,164685776,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/231838952,231838952,MDEyOklzc3VlQ29tbWVudDIzMTgzODk1Mg==,9464836,2016-07-11T19:29:25Z,2016-07-11T19:35:04Z,NONE,"Sorry, I misspoke - I meant we call updateGradInput on netD in fGx, but that doesn't address my point at all.  The problem is that netD had different parameters when we call forward on it in fDx than when we call updateGradInput on it in fGx since we ran adam on fDx before calling fGx.  That means that we are taking the gradient with respect to netD's input using different parameters than we used to compute netD's output, so the loss and gradient being computed don't match, although they will be close assuming the parameter update is small, which is why the model still works even though the gradient computation is incorrect.  I've corrected it in my own code and it doesn't make a huge difference, but I would still recommend correcting it here or adding a note that it is not technically correct.

Maybe a pseudo code simplification of what's going on will make this more clear:

1.) fDx
z <- N(0, 1)
x_real <- load from dataset 
x_fake <- netG(z)
y_real <- netD(x_real)
...criterion stuff...
...gradInput stuff...
gradParamsD <- dnetD_dparamsD
y_fake <- netD(x_fake)
...criterion stuff...
...gradInput stuff...
gradParamsD <- dnetD_dparamsD 

2.) adam 

paramsD <- adam(gradParamsD) 

3.) fGx 

...criterion stuff using y_fake from 1... 
gradInputD <- dnetD_dx_fake (the params of netD changed in 2 and we computed y_fake in 1!) 
...netG stuff using gradInputD...

4.) adam 

paramsG <- adam(gradParamsG)
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/21/comments,https://github.com/soumith/dcgan.torch/issues/21#issuecomment-231838952,https://api.github.com/repos/soumith/dcgan.torch/issues/21
soumith,dcgan.torch,164685776,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/231866345,231866345,MDEyOklzc3VlQ29tbWVudDIzMTg2NjM0NQ==,9153650,2016-07-11T21:11:07Z,2016-07-11T21:11:07Z,NONE,"This time I think I got your point.
I agree with you.
Then you would simply change [this line](https://github.com/soumith/dcgan.torch/blob/master/main.lua#L197):

`local output = netD.output -- netD:forward(input) was already executed in fDx, so save computation`

For this one:

`local output = netD:forward(input)`

It remains to be seen if the time saved by inaccurately updating the parameters pays off, though.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/21/comments,https://github.com/soumith/dcgan.torch/issues/21#issuecomment-231866345,https://api.github.com/repos/soumith/dcgan.torch/issues/21
soumith,dcgan.torch,164685776,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/231878031,231878031,MDEyOklzc3VlQ29tbWVudDIzMTg3ODAzMQ==,9464836,2016-07-11T21:56:16Z,2016-07-11T21:56:16Z,NONE,"Yeah, that's sort of what I did.  It's actually possible to do it correctly without doing netD:forward(input) - what you do is create a closure that does the full computation for both netD and netG and returns as its result a pair of functions that return the D loss and gradParams and G loss and gradParams separately.  Then in training we call the single closure and then apply adam to functions it returns.  This easily generalizes for multiple G updates per D update if desired.  To give a better idea we create:

```
function trainBatch()
    ...do all computations...

    local function trainBatchD(p) return lossD, gradParamsD end
    local function trainBatchG(p) return lossG, gradParamsG end

    return trainBatchD, trainBatchG 
end

then in training we do:

local trainBatchD, trainBatchG = trainBatch()

_, lossD = optim.adam(trainBatchD, paramsD, optimStateD)
_, lossG = optim.adam(trainBatchG, paramsG, optimStateG)
```
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/21/comments,https://github.com/soumith/dcgan.torch/issues/21#issuecomment-231878031,https://api.github.com/repos/soumith/dcgan.torch/issues/21
soumith,dcgan.torch,162166012,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/228369922,228369922,MDEyOklzc3VlQ29tbWVudDIyODM2OTkyMg==,1310570,2016-06-24T15:01:02Z,2016-06-24T15:01:02Z,OWNER,"this is because of the interactions in batchnorm. Put the model in :evaluate() mode to not have this effect.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/19/comments,https://github.com/soumith/dcgan.torch/issues/19#issuecomment-228369922,https://api.github.com/repos/soumith/dcgan.torch/issues/19
soumith,dcgan.torch,162166012,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/228381325,228381325,MDEyOklzc3VlQ29tbWVudDIyODM4MTMyNQ==,9153650,2016-06-24T15:45:13Z,2016-06-24T15:45:13Z,NONE,"Thanks for your quick answer. However, that solved the problem but created a new one. Now I'm able to replicate the results. The problem is that the generated images with evaluate mode are very different to the ones created with training mode:

Training mode
![training](https://cloud.githubusercontent.com/assets/9153650/16342497/ce439356-3a32-11e6-9636-380de1494476.png)

Evaluation mode
![evaluate](https://cloud.githubusercontent.com/assets/9153650/16342512/dbaed0aa-3a32-11e6-943a-17b7fc3294c9.png)

Generated images with evaluation mode are not as close to MNIST as training mode, they have grey background and more bold numbers. 
With training mode activated I assume that batchnorm is using the batch mean and std, while with evaluate mode it uses the mean of all the data, right? May be this is the explanation to this result? Is there any way to avoid this behaviour while being able to replicate results on evaluation time?

Thanks again.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/19/comments,https://github.com/soumith/dcgan.torch/issues/19#issuecomment-228381325,https://api.github.com/repos/soumith/dcgan.torch/issues/19
soumith,dcgan.torch,162166012,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/228607934,228607934,MDEyOklzc3VlQ29tbWVudDIyODYwNzkzNA==,1310570,2016-06-26T15:53:46Z,2016-06-26T15:53:46Z,OWNER,"i think this is a problem with the running_mean and running_var being a bit off from training.
Before saving the model, do 100 mini-batches of generations (no training) so that they are re-estimated correctly, and then put the model into :evaluate mode and save it.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/19/comments,https://github.com/soumith/dcgan.torch/issues/19#issuecomment-228607934,https://api.github.com/repos/soumith/dcgan.torch/issues/19
soumith,dcgan.torch,162166012,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/228613558,228613558,MDEyOklzc3VlQ29tbWVudDIyODYxMzU1OA==,9153650,2016-06-26T17:56:43Z,2016-06-26T17:56:43Z,NONE,"Thanks once more! I did what you said and now evaluation mode and training mode look the same.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/19/comments,https://github.com/soumith/dcgan.torch/issues/19#issuecomment-228613558,https://api.github.com/repos/soumith/dcgan.torch/issues/19
soumith,dcgan.torch,157805541,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/222890687,222890687,MDEyOklzc3VlQ29tbWVudDIyMjg5MDY4Nw==,1310570,2016-06-01T04:38:43Z,2016-06-01T04:38:43Z,OWNER,"you could stand up from your chair and clap your hands ;)
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/18/comments,https://github.com/soumith/dcgan.torch/issues/18#issuecomment-222890687,https://api.github.com/repos/soumith/dcgan.torch/issues/18
soumith,dcgan.torch,157805541,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/222944444,222944444,MDEyOklzc3VlQ29tbWVudDIyMjk0NDQ0NA==,18051187,2016-06-01T09:45:37Z,2016-06-01T09:45:37Z,NONE,"dataset=folder
I misunderstand that the value of dataset 'folder' must be substitute a existing folder

I think dataset='folder' is  good 
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/18/comments,https://github.com/soumith/dcgan.torch/issues/18#issuecomment-222944444,https://api.github.com/repos/soumith/dcgan.torch/issues/18
soumith,dcgan.torch,151031090,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/214623378,214623378,MDEyOklzc3VlQ29tbWVudDIxNDYyMzM3OA==,1310570,2016-04-26T05:54:59Z,2016-04-26T05:54:59Z,OWNER,"my guess is that you need a bigger dataset. there's only ~1800 images in your dataset.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/16/comments,https://github.com/soumith/dcgan.torch/issues/16#issuecomment-214623378,https://api.github.com/repos/soumith/dcgan.torch/issues/16
soumith,dcgan.torch,151031090,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/214628664,214628664,MDEyOklzc3VlQ29tbWVudDIxNDYyODY2NA==,214133,2016-04-26T06:27:47Z,2016-04-26T06:27:47Z,NONE,"I agree a bigger dataset would be best, but as long as the discriminator doesn't win out totally by the end of training (Err_D stable at 0.01 and less) you can always try increasing the number of iterations (niter). 

It probably won't cover the input space very well if you do - that is, there will be a lot of images in your training set that it never will make anything like - but the ones it can make will be better than the noise you currently get, at least.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/16/comments,https://github.com/soumith/dcgan.torch/issues/16#issuecomment-214628664,https://api.github.com/repos/soumith/dcgan.torch/issues/16
soumith,dcgan.torch,151031090,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/215154095,215154095,MDEyOklzc3VlQ29tbWVudDIxNTE1NDA5NQ==,18605143,2016-04-27T17:15:02Z,2016-04-27T17:15:39Z,NONE,"Tried training it overnight with a dataset of ~2100 images and niter set to 250 instead of 25. The result obviously isn't as good as it would be with a larger dataset, but I'm pretty impressed with it. Thanks for the tip! 
![computerroom](https://cloud.githubusercontent.com/assets/18605143/14861160/06fe62ac-0c7a-11e6-8e6d-c522d7561ce1.png)
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/16/comments,https://github.com/soumith/dcgan.torch/issues/16#issuecomment-215154095,https://api.github.com/repos/soumith/dcgan.torch/issues/16
soumith,dcgan.torch,149127387,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/211359458,211359458,MDEyOklzc3VlQ29tbWVudDIxMTM1OTQ1OA==,214133,2016-04-18T12:34:21Z,2016-04-18T12:34:21Z,NONE,"Exactly the same as this issue: 

https://github.com/soumith/dcgan.torch/issues/10

Your torch may be correctly installed, but it isn't recent. Whenever something like this appears, the first thing I try is luarocks install torch, nn, cunn etc. 
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/15/comments,https://github.com/soumith/dcgan.torch/issues/15#issuecomment-211359458,https://api.github.com/repos/soumith/dcgan.torch/issues/15
soumith,dcgan.torch,138904898,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/193308733,193308733,MDEyOklzc3VlQ29tbWVudDE5MzMwODczMw==,1310570,2016-03-07T15:49:47Z,2016-03-07T15:49:47Z,OWNER,"I think it depends on the shape of your latent space. If your latent space is spherical already (i.e. you learnt to sample from a gaussian while training, rather than from uniform), linear interpolation seems okay. If you sample Z from a cube (uniform), a spherical interpolation seems like a much better idea.

https://github.com/soumith/dcgan.torch/blob/master/main.lua#L23
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/14/comments,https://github.com/soumith/dcgan.torch/issues/14#issuecomment-193308733,https://api.github.com/repos/soumith/dcgan.torch/issues/14
soumith,dcgan.torch,138904898,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/194211796,194211796,MDEyOklzc3VlQ29tbWVudDE5NDIxMTc5Ng==,945979,2016-03-09T09:41:44Z,2016-03-11T09:30:49Z,NONE,"Agreed it depends on the shape of the latent space. But at z=100, switching from [a prior of noise:uniform(-1, 1)](https://github.com/soumith/dcgan.torch/blob/29b8dbce7ea5d8711c971729244aeec5072ace65/main.lua#L167) to a [prior of noise:normal(0,1)](https://github.com/soumith/dcgan.torch/blob/29b8dbce7ea5d8711c971729244aeec5072ace65/main.lua#L169) yields the same result: points along the linear interpolation between two randomly selected points will go way outside the expected distribution.

---

To clarify my main point, calling the following code:

``` lua
local noise1 = torch.Tensor(opt.batchSize, 100, 1, 1)
noise1:normal(0, 1)
local noise2 = torch.Tensor(opt.batchSize, 100, 1, 1)
noise2:normal(0, 1)
```

Will always result in two 100 dim vectors with length about 10. If you choose to linearly interpolate between them, you will invariably get a ""tentpole"" effect in which the length decreases from 10 to 7 at the midpoint, which is over 4 standard deviations away from the expected length. Shouldn't the interpolated points instead be from the same distribution as the original samples?

---

Happy to uncover my own conceptual flaw in how latent spaces are sampled, which is certainly possible. My ipython notebook code to replicate this is below.

``` python
from matplotlib import pylab as plt
%matplotlib inline
import numpy as np
```

``` python
# random_points = np.random.uniform(low=-1, high=1, size=(1000,100))
random_points = np.random.normal(loc=0, scale=1, size=(1000,100))
lengths = map(np.linalg.norm, random_points)
print(""Mean length is {:3.2f} and std is {:3.2f}"".format(np.mean(lengths), np.std(lengths)))
n, bins, patches = plt.hist(lengths, 50, normed=1, facecolor='green', alpha=0.75)
plt.show()
```

![image](https://cloud.githubusercontent.com/assets/945979/13630817/2621fe92-e596-11e5-9f88-615fcbf361b4.png)

``` python
# take midpoint of two adjacent points in vector
def midpoint_length(points, ix):
    num_points = len(points)
    next_ix = (ix + 1) % num_points
    avg = (points[ix] + points[next_ix]) / 2.0
    return np.linalg.norm(avg)

mid_lengths = []
for i in range(len(random_points)):
    mid_lengths.append(midpoint_length(random_points, i))
print(""Mean length is {:3.2f} and std is {:3.2f}"".format(np.mean(mid_lengths), np.std(mid_lengths)))
n, bins, patches = plt.hist(mid_lengths, 50, normed=1, facecolor='green', alpha=0.75)
plt.show()
```

![image](https://cloud.githubusercontent.com/assets/945979/13630860/653e6fde-e596-11e5-8a9f-2cfa7bbd4f0d.png)
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/14/comments,https://github.com/soumith/dcgan.torch/issues/14#issuecomment-194211796,https://api.github.com/repos/soumith/dcgan.torch/issues/14
soumith,dcgan.torch,138904898,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/195302200,195302200,MDEyOklzc3VlQ29tbWVudDE5NTMwMjIwMA==,945979,2016-03-11T09:55:05Z,2016-03-11T09:55:05Z,NONE,"To visually demonstrate the relevance to this codebase, I constructed 5 (uniform) random interpolations from the pre-trained `bedrooms_4_net_G.t7` model:

![bedroom_pairs1](https://cloud.githubusercontent.com/assets/945979/13697789/ab812dc6-e724-11e5-9e6b-0c6685b15c80.png)

Each interpolation is presented in pairs: the first line is linear interpolation and the second is spherical interpolation. To my eye, the first line often suffers from blurring in the center or other visual washout while the second line stays crisper and more visually consistent with the style of the endpoints. This is a pattern I've seen in other latent spaces as well.

We can also visualize the tentpole effect by graphing the lengths of all of the vectors across the interpolation. Here are the five linear interpolations:

<img width=""480"" alt=""tentpole1"" src=""https://cloud.githubusercontent.com/assets/945979/13698556/9a6d65cc-e729-11e5-9430-3faaf180b70c.png"">

The lengths at each end are about 5.75, but in the center they sag down to just above 4. This is exactly what is predicted by the distributions in the original comment. Arguably, this sag correlates exactly with the visual artifacts in the rendered images above.

We can compare that to the lengths when using spherical interpolation, which of course won't sag:

<img width=""480"" alt=""tentpole2"" src=""https://cloud.githubusercontent.com/assets/945979/13698642/3943c8a8-e72a-11e5-9227-6aa401b26a92.png"">

Getting the shape of the latent space right is important, which is why I've spent time making this case here. If my argument is right, then this has implications for how to most accurately compute interpolations, extrapolations, flythroughs, averages, etc. in latent space. Alternately, a different prior could perhaps be used so that these operations could remain linear.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/14/comments,https://github.com/soumith/dcgan.torch/issues/14#issuecomment-195302200,https://api.github.com/repos/soumith/dcgan.torch/issues/14
soumith,dcgan.torch,138904898,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/195436007,195436007,MDEyOklzc3VlQ29tbWVudDE5NTQzNjAwNw==,1310570,2016-03-11T16:17:09Z,2016-03-11T16:17:09Z,OWNER,"After reading this through, I am convinced that spherical interpolation is essential as well. The weak generations in the center are something I've noticed as well. Thanks for pointing this out. I think the overall fix is to always keep the intermediate vectors constant.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/14/comments,https://github.com/soumith/dcgan.torch/issues/14#issuecomment-195436007,https://api.github.com/repos/soumith/dcgan.torch/issues/14
soumith,dcgan.torch,138904898,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/195521866,195521866,MDEyOklzc3VlQ29tbWVudDE5NTUyMTg2Ng==,1310570,2016-03-11T19:56:25Z,2016-03-11T19:56:25Z,OWNER,"I just spoke to Arthur Szlam who smacked me on the head, because he's been telling this to me for months, and i conveniently ignored it.

He says the latent space should also be sampled from points on an n-dimensional hypersphere, and when doing interpolations, you just take the path on the great circle.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/14/comments,https://github.com/soumith/dcgan.torch/issues/14#issuecomment-195521866,https://api.github.com/repos/soumith/dcgan.torch/issues/14
soumith,dcgan.torch,138904898,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/199171316,199171316,MDEyOklzc3VlQ29tbWVudDE5OTE3MTMxNg==,945979,2016-03-21T08:31:50Z,2016-03-21T08:31:50Z,NONE,"Thanks for thinking this through with me. Having just revisited Domingos' classic [A Few Useful Things to Know about Machine Learning](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf), this quote now meant more to me:

> INTUITION FAILS IN HIGH DIMENSIONS
> After overfitting, the biggest problem in machine learning is the curse of dimensionality. [...] Our intuitions, which come from a three dimensional world, often do not apply in high-dimensional ones. In high dimensions, most of the mass of a multivariate Gaussian distribution is not near the mean, but in an increasingly distant â€œshellâ€ around it; and most of the volume of a high-dimensional orange is in the skin, not the pulp.

So are you suggesting constraining all latent vectors to lie exactly on the unit n-sphere? That would be an interesting simplification if it worked. I instead wrote my own interpolator which does a great circle path with elevation changes. Feel free to adapt it to this codebase if you'd like.

``` python
def slerp(val, low, high):
    omega = np.arccos(np.dot(low/np.linalg.norm(low), high/np.linalg.norm(high)))
    so = np.sin(omega)
    return np.sin((1.0-val)*omega) / so * low + np.sin(val*omega)/so * high
```
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/14/comments,https://github.com/soumith/dcgan.torch/issues/14#issuecomment-199171316,https://api.github.com/repos/soumith/dcgan.torch/issues/14
soumith,dcgan.torch,138904898,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/200025792,200025792,MDEyOklzc3VlQ29tbWVudDIwMDAyNTc5Mg==,1908017,2016-03-22T21:04:35Z,2016-03-22T21:04:35Z,NONE,"Thanks for the slerp implementation, I started using it and thought I'd share some fixed edge cases. Not sure what the convention should be for the degenerate opposite vectors case, currently it just lerps them.

``` python
def slerp(val, low, high):
    omega = np.arccos(np.clip(np.dot(low/np.linalg.norm(low), high/np.linalg.norm(high)), -1, 1))
    so = np.sin(omega)
    if so == 0:
        return (1.0-val) * low + val * high # L'Hopital's rule/LERP
    return np.sin((1.0-val)*omega) / so * low + np.sin(val*omega) / so * high

print(slerp(0, np.array([1,0,0]), np.array([1,0,0])))
print(slerp(0.5, np.array([1,0,0]), np.array([1,0,0])))
print(slerp(0, np.array([1,0,0]), np.array([0.5,0,0])))
print(slerp(0.5, np.array([1,0,0]), np.array([0.5,0,0])))
print(slerp(0, np.array([1,0,0]), np.array([-1,0,0])))
print(slerp(0.5, np.array([1,0,0]), np.array([-1,0,0])))
# [ 1.  0.  0.]
# [ 1.  0.  0.]
# [ 1.  0.  0.]
# [ 0.75  0.    0.  ]
# [ 1.  0.  0.]
# [ 0.  0.  0.]
```
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/14/comments,https://github.com/soumith/dcgan.torch/issues/14#issuecomment-200025792,https://api.github.com/repos/soumith/dcgan.torch/issues/14
soumith,dcgan.torch,138904898,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/349261809,349261809,MDEyOklzc3VlQ29tbWVudDM0OTI2MTgwOQ==,3956593,2017-12-05T10:27:00Z,2017-12-05T10:27:18Z,NONE,"@pqn As far as I understood this you're using `slerp` only for interpolating between two vectors (`low` and `high`). How do you sample `z` for the generator during GAN training?  
Is it 

    z = np.random.normal(loc=0, scale=1, size=(1000,100))
as in the [comment](https://github.com/soumith/dcgan.torch/issues/14#issuecomment-194211796) above by dribnet?



",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/14/comments,https://github.com/soumith/dcgan.torch/issues/14#issuecomment-349261809,https://api.github.com/repos/soumith/dcgan.torch/issues/14
soumith,dcgan.torch,138904898,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/349411050,349411050,MDEyOklzc3VlQ29tbWVudDM0OTQxMTA1MA==,1908017,2017-12-05T19:17:50Z,2017-12-05T19:17:50Z,NONE,@mgarbade I actually just stole the slerp for feature vector interpolation when playing with [this paper](https://arxiv.org/abs/1411.5928) (which is not a GAN) in MXNet. The equivalent input `z` was always just a one-hot vector for a specific 3D model and not randomly sampled.,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/14/comments,https://github.com/soumith/dcgan.torch/issues/14#issuecomment-349411050,https://api.github.com/repos/soumith/dcgan.torch/issues/14
soumith,dcgan.torch,138904898,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/390214877,390214877,MDEyOklzc3VlQ29tbWVudDM5MDIxNDg3Nw==,144230,2018-05-18T13:54:56Z,2018-05-18T13:54:56Z,NONE,"Hey, this is a super useful thread, thanks to all!

In case it's useful for anyone (this thread ranks high on SEO :), multivariate gaussian converges to hypersphere with radius of sqrt(dim-1) (can approximate as sqrt(dim) for large dim) and variance of 0.5. (Technically chi distribution with variance 1, but for high dim corresponds to gaussian with variance 0.5). 

See below for empirical code example, and nice brief explanation at https://www.johndcook.com/blog/2011/09/01/multivariate-normal-shell/) 

```
import math
import numpy as np
for i in range(1,15):
    n = 10000 # sample size
    dim = i*i*i*i
    z = np.random.normal(size=[n,dim])
    r = np.linalg.norm(z, axis=1)
    mean, var = np.mean(r), np.var(r)
    print('dim:{}, sqrt(dim-1):{}, mean:{}, var:{}'.format(dim, math.sqrt(dim-1), mean, var))
  
    
dim:1, sqrt(dim-1):0.0, mean:0.795150900024, var:0.366173691484
dim:16, sqrt(dim-1):3.87298334621, mean:3.93135579711, var:0.491231344077
dim:81, sqrt(dim-1):8.94427191, mean:8.96891458959, var:0.489195309416
dim:256, sqrt(dim-1):15.9687194227, mean:15.9929240032, var:0.498972907482
dim:625, sqrt(dim-1):24.9799919936, mean:24.9836431855, var:0.490930587752
dim:1296, sqrt(dim-1):35.9861084309, mean:35.9903244552, var:0.496412523249
dim:2401, sqrt(dim-1):48.9897948557, mean:49.0022012748, var:0.498904908632
dim:4096, sqrt(dim-1):63.9921870231, mean:64.0026298725, var:0.497066494871
dim:6561, sqrt(dim-1):80.9938269253, mean:80.9966366457, var:0.490071214555
dim:10000, sqrt(dim-1):99.994999875, mean:99.9974767903, var:0.516483725478
dim:14641, sqrt(dim-1):120.995867698, mean:120.997610236, var:0.493307838545
dim:20736, sqrt(dim-1):143.996527736, mean:143.998235492, var:0.500247771622
dim:28561, sqrt(dim-1):168.997041394, mean:168.998178752, var:0.509050583724
dim:38416, sqrt(dim-1):195.997448963, mean:196.013193242, var:0.497141009441
```",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/14/comments,https://github.com/soumith/dcgan.torch/issues/14#issuecomment-390214877,https://api.github.com/repos/soumith/dcgan.torch/issues/14
soumith,dcgan.torch,138904898,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/525160139,525160139,MDEyOklzc3VlQ29tbWVudDUyNTE2MDEzOQ==,10319863,2019-08-27T06:34:48Z,2019-08-27T06:37:29Z,NONE,"Hey, super useful thank you! I needed a batchwise, n dimensional slerp that does a different range of interpolation steps for each sample for my purposes, so I adapted your code to tensorflow. Here it is in case anyone needs it: 
```
def tf_slerp(val, low, high):
  # Val must be Batch_size, n_timesteps
 # low must be batch_size, n_dimensions
  # high must be batch_size, n_dimensions
  
  dim_size = low.shape[-1]
  time_steps = val.shape[-1]
  p1 = low/tf.tile( tf.expand_dims(tf.norm(low, axis = 1),axis = 1), [1,dim_size])
  p2 = high/tf.tile( tf.expand_dims(tf.norm(high, axis = 1),axis = 1), [1,dim_size])
  dot = tf.reduce_sum(p1*p2, axis = -1) # batchwise dot of our Batch*num_dims.

  omega = tf.acos(tf.clip_by_value(
      dot, -1,1,))
  so = tf.sin(omega)
  # if (so == 0):
  # return (1.0-val)*low + val * high
  so = tf.tile(tf.expand_dims(tf.expand_dims(so, axis = 1),axis = 2), [1,time_steps,dim_size])
  omega = tf.tile(tf.expand_dims(tf.expand_dims(omega, axis = 1), axis = 2), [1,time_steps,dim_size])
  val = tf.tile(tf.expand_dims(val, axis = 2), [1,1,dim_size])
  low = tf.tile(tf.expand_dims(low, axis = 1), [1,time_steps,1])
  high = tf.tile(tf.expand_dims(high, axis = 1), [1,time_steps,1])
  return np.sin((1.0-val)*omega) / so * low + np.sin(val*omega) / so * high
```

And usage to visualise it in the 2D case

```low = np.array([[1.0,0.0], [3.0,0.0]])
high = np.array([[0.0,1.0], [0.0,3.0]])
values = np.array([np.linspace(0,1,10), np.linspace(0,0.5,10)])

array = tf_slerp(values, low, high)

plt.scatter(array[0,:,0], array[0,:,1])
plt.show()
plt.scatter(array[1,:,0], array[1,:,1])
plt.show()```",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/14/comments,https://github.com/soumith/dcgan.torch/issues/14#issuecomment-525160139,https://api.github.com/repos/soumith/dcgan.torch/issues/14
soumith,dcgan.torch,138904898,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/717639608,717639608,MDEyOklzc3VlQ29tbWVudDcxNzYzOTYwOA==,8037983,2020-10-28T01:34:23Z,2020-10-28T01:34:23Z,NONE,"> Thanks for the slerp implementation, I started using it and thought I'd share some fixed edge cases. Not sure what the convention should be for the degenerate opposite vectors case, currently it just lerps them.
> 
> ```python
> def slerp(val, low, high):
>     omega = np.arccos(np.clip(np.dot(low/np.linalg.norm(low), high/np.linalg.norm(high)), -1, 1))
>     so = np.sin(omega)
>     if so == 0:
>         return (1.0-val) * low + val * high # L'Hopital's rule/LERP
>     return np.sin((1.0-val)*omega) / so * low + np.sin(val*omega) / so * high
> 
> print(slerp(0, np.array([1,0,0]), np.array([1,0,0])))
> print(slerp(0.5, np.array([1,0,0]), np.array([1,0,0])))
> print(slerp(0, np.array([1,0,0]), np.array([0.5,0,0])))
> print(slerp(0.5, np.array([1,0,0]), np.array([0.5,0,0])))
> print(slerp(0, np.array([1,0,0]), np.array([-1,0,0])))
> print(slerp(0.5, np.array([1,0,0]), np.array([-1,0,0])))
> # [ 1.  0.  0.]
> # [ 1.  0.  0.]
> # [ 1.  0.  0.]
> # [ 0.75  0.    0.  ]
> # [ 1.  0.  0.]
> # [ 0.  0.  0.]
> ```

@pqn, thank you for this snippet. Could you explain how is the case (`so==0`) is related to ""L'Hopital's rule""?",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/14/comments,https://github.com/soumith/dcgan.torch/issues/14#issuecomment-717639608,https://api.github.com/repos/soumith/dcgan.torch/issues/14
soumith,dcgan.torch,138904898,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/718389349,718389349,MDEyOklzc3VlQ29tbWVudDcxODM4OTM0OQ==,1908017,2020-10-29T06:25:47Z,2020-10-29T06:25:47Z,NONE,"@cocoaaa It's been a while since I thought about this but IIRC, if `omega` (thus `np.sin(omega)`) is at 0, then the usual slerp formula will become undefined since that's in the denominator. But the slerp formula can still be smoothly extended at 0 by calculating the limit via L'Hopital's rule. This simplifies to linear interpolation, which also probably gives better numerical stability in the region near 0 as well. Seems the [Wikipedia article](https://en.wikipedia.org/wiki/Slerp) corroborates my memory.",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/14/comments,https://github.com/soumith/dcgan.torch/issues/14#issuecomment-718389349,https://api.github.com/repos/soumith/dcgan.torch/issues/14
soumith,dcgan.torch,134061435,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/184820480,184820480,MDEyOklzc3VlQ29tbWVudDE4NDgyMDQ4MA==,1310570,2016-02-16T18:40:53Z,2016-02-16T18:40:53Z,OWNER,"image.scale will help scale up the visualization.
image.save (as you have seen) will help visualize outside interactive notebook.
Otherwise, the display package will also work, as seen in the dcgan.torch code
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/13/comments,https://github.com/soumith/dcgan.torch/issues/13#issuecomment-184820480,https://api.github.com/repos/soumith/dcgan.torch/issues/13
soumith,dcgan.torch,134061435,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/184881325,184881325,MDEyOklzc3VlQ29tbWVudDE4NDg4MTMyNQ==,3698659,2016-02-16T21:27:34Z,2016-02-16T21:27:34Z,NONE,"I think I am misunderstanding how these weights can be visualized. Is the image I posted above the entire set of kernels for that layer? I'm not sure I understand exactly how to project the other layers into pixel space. For instance if I take the first conv layer (weight:size = 100x1024x4x4), does it need to be narrowed to a three channel mapping as:

filters=net:get(1).weight
filters=filters:narrow(2, 1, 3)
image.toDisplayTensor{input=filters}
image.save(""filters.png"", filters)

And then, how would I visualize the features that are maximally activated by the training data? I guess I am looking at the [deconvnet paper](https://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf) and wonder if there are any examples of this in torch?
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/13/comments,https://github.com/soumith/dcgan.torch/issues/13#issuecomment-184881325,https://api.github.com/repos/soumith/dcgan.torch/issues/13
soumith,dcgan.torch,134061435,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/184895278,184895278,MDEyOklzc3VlQ29tbWVudDE4NDg5NTI3OA==,1310570,2016-02-16T22:08:14Z,2016-02-16T22:08:14Z,OWNER,"the filters from other layers have to be visualized as single-channel images.
so, reshape weight to become size: 100 \* 1024 x 4 x 4
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/13/comments,https://github.com/soumith/dcgan.torch/issues/13#issuecomment-184895278,https://api.github.com/repos/soumith/dcgan.torch/issues/13
soumith,dcgan.torch,131846447,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/180802155,180802155,MDEyOklzc3VlQ29tbWVudDE4MDgwMjE1NQ==,1310570,2016-02-06T16:18:20Z,2016-02-06T16:18:20Z,OWNER,"@AjayTalati could you try reinstalling image? (  luarocks install image )
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/12/comments,https://github.com/soumith/dcgan.torch/issues/12#issuecomment-180802155,https://api.github.com/repos/soumith/dcgan.torch/issues/12
soumith,dcgan.torch,131846447,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/180869072,180869072,MDEyOklzc3VlQ29tbWVudDE4MDg2OTA3Mg==,10279780,2016-02-06T21:25:45Z,2016-02-06T21:25:45Z,NONE,"@soumith Great, that did it! Just needed to clean and reinstall Torch, it's training nicely now.

Thanks a lot mate :+1: 
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/12/comments,https://github.com/soumith/dcgan.torch/issues/12#issuecomment-180869072,https://api.github.com/repos/soumith/dcgan.torch/issues/12
soumith,dcgan.torch,131619941,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/278608223,278608223,MDEyOklzc3VlQ29tbWVudDI3ODYwODIyMw==,2556642,2017-02-09T10:48:51Z,2017-02-09T10:48:51Z,NONE,I have the same questionï¼Œ is there anyone explained thatï¼Ÿ,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/11/comments,https://github.com/soumith/dcgan.torch/issues/11#issuecomment-278608223,https://api.github.com/repos/soumith/dcgan.torch/issues/11
soumith,dcgan.torch,131619941,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/314240804,314240804,MDEyOklzc3VlQ29tbWVudDMxNDI0MDgwNA==,13528274,2017-07-10T21:01:33Z,2017-07-10T21:01:33Z,NONE,"(Just my two cents)

I think its a typo of sorts since there is no way the image is getting up-scaled to 64x64 if kernal size of 5 is used. 

Kernal size 4 cuts it just right given the parameter definitions and the formula used here:
https://github.com/torch/nn/blob/master/doc/convolution.md#nn.SpatialFullConvolution",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/11/comments,https://github.com/soumith/dcgan.torch/issues/11#issuecomment-314240804,https://api.github.com/repos/soumith/dcgan.torch/issues/11
soumith,dcgan.torch,131619941,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/314241700,314241700,MDEyOklzc3VlQ29tbWVudDMxNDI0MTcwMA==,1310570,2017-07-10T21:03:40Z,2017-07-10T21:03:40Z,OWNER,"there's a way to use kernel=5 and get 64x64, you just force-pad the edges. We did kernel size of 5 in the theano code (because of the way the cudnn Transpose was implemented with Theano), and we did ksz=4 with Torch code.",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/11/comments,https://github.com/soumith/dcgan.torch/issues/11#issuecomment-314241700,https://api.github.com/repos/soumith/dcgan.torch/issues/11
soumith,dcgan.torch,131619941,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/314243724,314243724,MDEyOklzc3VlQ29tbWVudDMxNDI0MzcyNA==,13528274,2017-07-10T21:08:10Z,2017-07-10T21:37:52Z,NONE,"Apologies. 
Thanks for this info. 
I was referring to the Torch code as such - didn't think Theano was in context too. ",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/11/comments,https://github.com/soumith/dcgan.torch/issues/11#issuecomment-314243724,https://api.github.com/repos/soumith/dcgan.torch/issues/11
soumith,dcgan.torch,127116991,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/172376283,172376283,MDEyOklzc3VlQ29tbWVudDE3MjM3NjI4Mw==,1310570,2016-01-17T20:12:14Z,2016-01-17T20:12:22Z,OWNER,"You have a very outdated version of torch and packages. I'd suggest that you reinstall torch from scratch on your machine via:
rm -rf ~/torch-distro

Then following the instructions on our website: http://torch.ch/docs/getting-started.html#_
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/10/comments,https://github.com/soumith/dcgan.torch/issues/10#issuecomment-172376283,https://api.github.com/repos/soumith/dcgan.torch/issues/10
soumith,dcgan.torch,126527297,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/171501422,171501422,MDEyOklzc3VlQ29tbWVudDE3MTUwMTQyMg==,1310570,2016-01-14T02:07:03Z,2016-01-14T02:07:03Z,OWNER,"The fake images update too. It's just that the network has converged, and the changes are very minor. 
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/9/comments,https://github.com/soumith/dcgan.torch/issues/9#issuecomment-171501422,https://api.github.com/repos/soumith/dcgan.torch/issues/9
soumith,dcgan.torch,126527297,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/171530075,171530075,MDEyOklzc3VlQ29tbWVudDE3MTUzMDA3NQ==,3698659,2016-01-14T04:59:10Z,2016-01-14T04:59:10Z,NONE,"@soumith Well I would have thought so too, but why is the network converging so quickly? I ran the code on the same data set, but at epoch 100, the fake output looks exactly the same as it did at epoch 2 (mostly noise). This was not the case training on my other machine. 
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/9/comments,https://github.com/soumith/dcgan.torch/issues/9#issuecomment-171530075,https://api.github.com/repos/soumith/dcgan.torch/issues/9
soumith,dcgan.torch,126527297,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/171558471,171558471,MDEyOklzc3VlQ29tbWVudDE3MTU1ODQ3MQ==,3698659,2016-01-14T07:11:26Z,2016-01-16T02:31:35Z,NONE,"Actually after running a few more tests I'm fairly sure this is some issue related to the gpu.

Here are two logs using the same random seed and data, the first is cpu and the second is gpu. The gpu converges extremely quickly, towards the end of epoch 1. The cpu never really converges and the error seems to occasionally spike. 

I also installed the cudnn with the r4 bindings because I was getting the error mentioned in this post. Not sure if this could be related...
https://github.com/karpathy/neuraltalk2/issues/44

GPU Log:

```
~/dc$ gpu=1 display_id=41 DATA_ROOT=/media/aferriss/SHARED/allImgs/Users/adamferriss/Desktop/dcgan.torch/myimages dataset=folder nThreads=12 th main.lua
```

Full log: https://gist.github.com/soumith/58cdb2ef9f8e0dbbf71a
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/9/comments,https://github.com/soumith/dcgan.torch/issues/9#issuecomment-171558471,https://api.github.com/repos/soumith/dcgan.torch/issues/9
soumith,dcgan.torch,126527297,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/171572335,171572335,MDEyOklzc3VlQ29tbWVudDE3MTU3MjMzNQ==,3698659,2016-01-14T08:37:58Z,2016-01-14T08:38:16Z,NONE,"Ok, after a few frustrating hours I figured it out. I believe there is a problem with cudnn 4. I downgraded to cudnn 3 and now everything seems to work fine.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/9/comments,https://github.com/soumith/dcgan.torch/issues/9#issuecomment-171572335,https://api.github.com/repos/soumith/dcgan.torch/issues/9
soumith,dcgan.torch,126527297,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/171593468,171593468,MDEyOklzc3VlQ29tbWVudDE3MTU5MzQ2OA==,1310570,2016-01-14T10:05:14Z,2016-01-14T10:05:14Z,OWNER,"Hmm that's weird, I'll look into it too.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/9/comments,https://github.com/soumith/dcgan.torch/issues/9#issuecomment-171593468,https://api.github.com/repos/soumith/dcgan.torch/issues/9
soumith,dcgan.torch,126527297,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/172144749,172144749,MDEyOklzc3VlQ29tbWVudDE3MjE0NDc0OQ==,2220731,2016-01-16T01:43:03Z,2016-01-16T01:43:03Z,NONE,"I also encountered this issue on Ubuntu with GPU training and disabling the checkpoint save statements at the end of the epoch seemed to fix the issue.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/9/comments,https://github.com/soumith/dcgan.torch/issues/9#issuecomment-172144749,https://api.github.com/repos/soumith/dcgan.torch/issues/9
soumith,dcgan.torch,126527297,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/172148352,172148352,MDEyOklzc3VlQ29tbWVudDE3MjE0ODM1Mg==,1310570,2016-01-16T02:34:32Z,2016-01-16T02:34:32Z,OWNER,"Thanks to Kenneth Marino from CMU, I figured out the bug. 
Pushing a patch shortly, sorry about this.
The issue is that the flattened parameter buffers gotten by the getParameters call get untied after util.save, because in util.save I typecast the network to CPU and then back to GPU.
I added util.save / util.load while making my code ready for release, so I did not notice this issue in my experiments.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/9/comments,https://github.com/soumith/dcgan.torch/issues/9#issuecomment-172148352,https://api.github.com/repos/soumith/dcgan.torch/issues/9
soumith,dcgan.torch,126527297,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/172148927,172148927,MDEyOklzc3VlQ29tbWVudDE3MjE0ODkyNw==,1310570,2016-01-16T02:46:56Z,2016-01-16T02:46:56Z,OWNER,"The bug is fixed now via: https://github.com/soumith/dcgan.torch/commit/5b093d3982f00e390571e0bcf83710c33f82e041

Sorry about this.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/9/comments,https://github.com/soumith/dcgan.torch/issues/9#issuecomment-172148927,https://api.github.com/repos/soumith/dcgan.torch/issues/9
soumith,dcgan.torch,126527297,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/172151856,172151856,MDEyOklzc3VlQ29tbWVudDE3MjE1MTg1Ng==,3698659,2016-01-16T03:42:16Z,2016-01-16T03:42:16Z,NONE,"@soumith Thanks so much! Glad to know I'm not crazy :+1: 
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/9/comments,https://github.com/soumith/dcgan.torch/issues/9#issuecomment-172151856,https://api.github.com/repos/soumith/dcgan.torch/issues/9
soumith,dcgan.torch,125155637,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/169385795,169385795,MDEyOklzc3VlQ29tbWVudDE2OTM4NTc5NQ==,1310570,2016-01-06T16:52:15Z,2016-01-06T16:52:15Z,OWNER,"it simply shares the .output buffer for every alternating module. So in total only two .output buffers are needed for the whole network. This can be done in the inference phase, where we dont need to keep around the .output for backward computation.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/8/comments,https://github.com/soumith/dcgan.torch/issues/8#issuecomment-169385795,https://api.github.com/repos/soumith/dcgan.torch/issues/8
soumith,dcgan.torch,125155637,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/169565018,169565018,MDEyOklzc3VlQ29tbWVudDE2OTU2NTAxOA==,6665222,2016-01-07T06:13:14Z,2016-01-07T06:13:14Z,NONE,"From what I understand, we need the `.output` during _backward computation_ because the _output_ for the current layer, acts as _input_ to the next layer. 

In that case, why exactly do we need to share every alternating module (have two buffers), instead of just having one buffer ?
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/8/comments,https://github.com/soumith/dcgan.torch/issues/8#issuecomment-169565018,https://api.github.com/repos/soumith/dcgan.torch/issues/8
soumith,dcgan.torch,125155637,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/169573645,169573645,MDEyOklzc3VlQ29tbWVudDE2OTU3MzY0NQ==,1310570,2016-01-07T06:56:20Z,2016-01-07T06:56:20Z,OWNER,"because while you are computing a layer, you cant have it's input and output be the same. As you write to the output, you cant read from the output and expect it to be input.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/8/comments,https://github.com/soumith/dcgan.torch/issues/8#issuecomment-169573645,https://api.github.com/repos/soumith/dcgan.torch/issues/8
soumith,dcgan.torch,125155637,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/169580016,169580016,MDEyOklzc3VlQ29tbWVudDE2OTU4MDAxNg==,1309728,2016-01-07T07:16:19Z,2016-01-07T07:16:19Z,NONE,"So, is it possible to set up double buffering for training? Would the required changes simply involve having buffers for `gradInput` instead of `output`?
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/8/comments,https://github.com/soumith/dcgan.torch/issues/8#issuecomment-169580016,https://api.github.com/repos/soumith/dcgan.torch/issues/8
soumith,dcgan.torch,125155637,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/169580958,169580958,MDEyOklzc3VlQ29tbWVudDE2OTU4MDk1OA==,1310570,2016-01-07T07:18:19Z,2016-01-07T07:18:19Z,OWNER,"yea you can (for gradInput), but it requires a little bit of change to our method dispatch in backward.
In current backward, all modules' updateGradInput is called, and then all modules' accGradParameters.
This has to be changed such that gradInput dependencies are better scoped.

We have internal patches for this, will open PRs slowly and patch things.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/8/comments,https://github.com/soumith/dcgan.torch/issues/8#issuecomment-169580958,https://api.github.com/repos/soumith/dcgan.torch/issues/8
soumith,dcgan.torch,125155637,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/169609277,169609277,MDEyOklzc3VlQ29tbWVudDE2OTYwOTI3Nw==,9110200,2016-01-07T09:34:16Z,2016-01-07T09:34:16Z,NONE,"@soumith I think that currently all modules in nn/cunn do not use `gradInput` in `accGradParameters` (and that should be always the case except for rare optimizations), so optimizing `gradInput` for memory is already possible. Even the newly added `SpatialBatchNormalization` (which implements only a `backward` in C) would work.
Or am I missing something ? 
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/8/comments,https://github.com/soumith/dcgan.torch/issues/8#issuecomment-169609277,https://api.github.com/repos/soumith/dcgan.torch/issues/8
soumith,dcgan.torch,125155637,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/169695515,169695515,MDEyOklzc3VlQ29tbWVudDE2OTY5NTUxNQ==,1310570,2016-01-07T15:30:07Z,2016-01-07T15:30:07Z,OWNER,"cc: @sgross see fmassa's comment wdyt
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/8/comments,https://github.com/soumith/dcgan.torch/issues/8#issuecomment-169695515,https://api.github.com/repos/soumith/dcgan.torch/issues/8
soumith,dcgan.torch,125155637,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/169772455,169772455,MDEyOklzc3VlQ29tbWVudDE2OTc3MjQ1NQ==,9110200,2016-01-07T18:53:09Z,2016-01-07T18:53:09Z,NONE,"@soumith did you mean @colesbury ?
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/8/comments,https://github.com/soumith/dcgan.torch/issues/8#issuecomment-169772455,https://api.github.com/repos/soumith/dcgan.torch/issues/8
soumith,dcgan.torch,125155637,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/169772580,169772580,MDEyOklzc3VlQ29tbWVudDE2OTc3MjU4MA==,1310570,2016-01-07T18:53:39Z,2016-01-07T18:53:39Z,OWNER,"oops, yea. sgross is his internal username.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/8/comments,https://github.com/soumith/dcgan.torch/issues/8#issuecomment-169772580,https://api.github.com/repos/soumith/dcgan.torch/issues/8
soumith,dcgan.torch,125155637,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/169792082,169792082,MDEyOklzc3VlQ29tbWVudDE2OTc5MjA4Mg==,655866,2016-01-07T20:10:12Z,2016-01-07T20:10:12Z,NONE,"@fmassa, it also requires all nn.Containers to properly override 'backward' so that they call 'backward' on their submodules. Some, like nn.Sequential, already do this, but others, like nn.ConcatTable, do not.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/8/comments,https://github.com/soumith/dcgan.torch/issues/8#issuecomment-169792082,https://api.github.com/repos/soumith/dcgan.torch/issues/8
soumith,dcgan.torch,125155637,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/169793612,169793612,MDEyOklzc3VlQ29tbWVudDE2OTc5MzYxMg==,9110200,2016-01-07T20:17:09Z,2016-01-07T20:17:09Z,NONE,"@colesbury that's needed if we want to optimize backward for speed (by avoiding redundant calls in updateGradInput and accGradParameters).
If what we want is to optimize for space, then I think it's not needed, because gradInput is not used in accGradParameters, I think this refactoring to allow sharing the gradInputs is not needed (but care must be take with parallel containers)
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/8/comments,https://github.com/soumith/dcgan.torch/issues/8#issuecomment-169793612,https://api.github.com/repos/soumith/dcgan.torch/issues/8
soumith,dcgan.torch,125155637,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/169794057,169794057,MDEyOklzc3VlQ29tbWVudDE2OTc5NDA1Nw==,1310570,2016-01-07T20:18:58Z,2016-01-07T20:18:58Z,OWNER,"@fmassa accGradParameters uses gradOutput, even though it doesn't use gradInput. If we share gradInput among all alternating layers, now do you see the problem....
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/8/comments,https://github.com/soumith/dcgan.torch/issues/8#issuecomment-169794057,https://api.github.com/repos/soumith/dcgan.torch/issues/8
soumith,dcgan.torch,125155637,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/169795268,169795268,MDEyOklzc3VlQ29tbWVudDE2OTc5NTI2OA==,9110200,2016-01-07T20:24:38Z,2016-01-07T20:24:38Z,NONE,"Ops... you are right. My bad..
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/8/comments,https://github.com/soumith/dcgan.torch/issues/8#issuecomment-169795268,https://api.github.com/repos/soumith/dcgan.torch/issues/8
soumith,dcgan.torch,124160654,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/167715390,167715390,MDEyOklzc3VlQ29tbWVudDE2NzcxNTM5MA==,1310570,2015-12-29T04:10:46Z,2015-12-29T04:10:46Z,OWNER,"@aferriss you just need to update your nn and cunn packages:

luarocks install torch
luarocks install cutorch
luarocks install nn
luarocks install cunn
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/7/comments,https://github.com/soumith/dcgan.torch/issues/7#issuecomment-167715390,https://api.github.com/repos/soumith/dcgan.torch/issues/7
soumith,dcgan.torch,124160654,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/167716140,167716140,MDEyOklzc3VlQ29tbWVudDE2NzcxNjE0MA==,3698659,2015-12-29T04:22:51Z,2015-12-29T04:22:51Z,NONE,"ah that was stupid of me, thanks @soumith 
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/7/comments,https://github.com/soumith/dcgan.torch/issues/7#issuecomment-167716140,https://api.github.com/repos/soumith/dcgan.torch/issues/7
soumith,dcgan.torch,122325315,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/164837084,164837084,MDEyOklzc3VlQ29tbWVudDE2NDgzNzA4NA==,1310570,2015-12-15T17:38:32Z,2015-12-15T17:38:32Z,OWNER,"okay :)
on my machine n=1 somehow works fine 
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/3/comments,https://github.com/soumith/dcgan.torch/pull/3#issuecomment-164837084,https://api.github.com/repos/soumith/dcgan.torch/issues/3
soumith,dcgan.torch,122175376,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/164839184,164839184,MDEyOklzc3VlQ29tbWVudDE2NDgzOTE4NA==,1310570,2015-12-15T17:47:21Z,2015-12-15T17:47:21Z,OWNER,"So, the hard-wired 64x64 is in the model. Getting around it is actually trivial. Let me write a bit more detailed post, with code references.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/2/comments,https://github.com/soumith/dcgan.torch/issues/2#issuecomment-164839184,https://api.github.com/repos/soumith/dcgan.torch/issues/2
soumith,dcgan.torch,122175376,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/164843250,164843250,MDEyOklzc3VlQ29tbWVudDE2NDg0MzI1MA==,352559,2015-12-15T18:04:29Z,2015-12-15T18:04:29Z,CONTRIBUTOR,"I look forward to that. I know that simply increasing net depth/size didn't necessarily help much in past work, but we hope that with more visible detail, maybe that will make a difference. (Perhaps our datasets are too heterogeneous or we're using the wrong hyperparameters, but we're having a hard time getting past fuzzy blobs and getting the fantastic results like the faces/rooms/flowers.)
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/2/comments,https://github.com/soumith/dcgan.torch/issues/2#issuecomment-164843250,https://api.github.com/repos/soumith/dcgan.torch/issues/2
soumith,dcgan.torch,122175376,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/164862299,164862299,MDEyOklzc3VlQ29tbWVudDE2NDg2MjI5OQ==,1310570,2015-12-15T19:16:09Z,2015-12-15T19:16:09Z,OWNER,"Ok, so the data loader is pretty generic. It has two control variables.
`opt.loadSize` and `opt.fineSize` [here](https://github.com/soumith/dcgan.torch/blob/master/main.lua#L9-L10).  
Right now, `loadSize` has to be greater than `fineSize` (because of a bug in the cropping logic). So it's okay to have `loadSize=65 fineSize=64 th main.lua`
If the dataset if well-aligned (like the faces dataset for example), keeping `loadSize = fineSize + 1` will get you better generations, as DCGANs have a hard time understanding and aligning generations.  
I think it is because the discriminator is a standard convnet that builds spatial invariances layer by layer. One could build a DCGAN that not only plays the true/fake game on the whole image, but also on local regions. This will make object alignment an explicit objective in the game, and I think it will get you much better generations.

Now, coming to the next part. To do generations of size 128, all you have to do is make the following changes:
`loadSize=129 fineSize=128 th main.lua`
And change the generator definition to this:

``` lua
local netG = nn.Sequential()
-- input is Z, going into a convolution
netG:add(SpatialFullConvolution(nz, ngf * 16, 4, 4))
netG:add(SpatialBatchNormalization(ngf * 16)):add(nn.ReLU(true))
-- state size: (ngf*16) x 4 x 4
netG:add(SpatialFullConvolution(ngf * 16, ngf * 8, 4, 4, 2, 2, 1, 1))
netG:add(SpatialBatchNormalization(ngf * 8)):add(nn.ReLU(true))
-- state size: (ngf*8) x 8 x 8
netG:add(SpatialFullConvolution(ngf * 8, ngf * 4, 4, 4, 2, 2, 1, 1))
netG:add(SpatialBatchNormalization(ngf * 4)):add(nn.ReLU(true))
-- state size: (ngf*4) x 16 x 16
netG:add(SpatialFullConvolution(ngf * 4, ngf * 2, 4, 4, 2, 2, 1, 1))
netG:add(SpatialBatchNormalization(ngf * 2)):add(nn.ReLU(true))
-- state size: (ngf * 2) x 32 x 32
netG:add(SpatialFullConvolution(ngf * 2, ngf, 4, 4, 2, 2, 1, 1))
netG:add(SpatialBatchNormalization(ngf)):add(nn.ReLU(true))
-- state size: (ngf) x 64 x 64
netG:add(SpatialFullConvolution(ngf, nc, 4, 4, 2, 2, 1, 1))
netG:add(nn.Tanh())
-- state size: (nc) x 128 x 128
```

And change the discriminator similarly:

``` lua
local netD = nn.Sequential()

-- input is (nc) x 128 x 128
netD:add(SpatialConvolution(nc, ndf, 4, 4, 2, 2, 1, 1))
netD:add(nn.LeakyReLU(0.2, true))
-- state size: (ndf) x 64 x 64
netD:add(SpatialConvolution(ndf, ndf * 2, 4, 4, 2, 2, 1, 1))
netD:add(SpatialBatchNormalization(ndf * 2)):add(nn.LeakyReLU(0.2, true))
-- state size: (ndf*2) x 32 x 32
netD:add(SpatialConvolution(ndf * 2, ndf * 4, 4, 4, 2, 2, 1, 1))
netD:add(SpatialBatchNormalization(ndf * 4)):add(nn.LeakyReLU(0.2, true))
-- state size: (ndf*4) x 16 x 16
netD:add(SpatialConvolution(ndf * 4, ndf * 8, 4, 4, 2, 2, 1, 1))
netD:add(SpatialBatchNormalization(ndf * 8)):add(nn.LeakyReLU(0.2, true))
-- state size: (ndf*8) x 8 x 8
netD:add(SpatialConvolution(ndf * 8, ndf * 16, 4, 4, 2, 2, 1, 1))
netD:add(SpatialBatchNormalization(ndf * 16)):add(nn.LeakyReLU(0.2, true))
-- state size: (ndf*16) x 4 x 4
netD:add(SpatialConvolution(ndf * 16, 1, 4, 4))
netD:add(nn.Sigmoid())
-- state size: 1 x 1 x 1
netD:add(nn.View(1):setNumInputDims(3))
-- state size: 1
```

You could write a function to essentially generate both the networks automatically for a given generation size, but to keep the code more readable, I defined them manually.

As you see, and unlike what you think, `ngf` and `ndf` are note related to the generation size, but they control the number of feature maps in the generator / discriminator.

Hope this helps.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/2/comments,https://github.com/soumith/dcgan.torch/issues/2#issuecomment-164862299,https://api.github.com/repos/soumith/dcgan.torch/issues/2
soumith,dcgan.torch,122175376,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/164919869,164919869,MDEyOklzc3VlQ29tbWVudDE2NDkxOTg2OQ==,352559,2015-12-15T22:39:15Z,2015-12-15T22:39:15Z,CONTRIBUTOR,">  And change the generator definition to this:

OK, I see. So to expand it we just need to add another base layer where the argument is the max-size and then we adjust each 'higher' layer to tweak the numbers appropriately. So if we wanted to try out not just 128x128 but 256x256, we would just add another line and tweak accordingly?

(FWIW, I seem to be getting better fuzz from 128x128, but it's too soon for me to be sure that it'll get me nice images in the end when it finishes training. Maybe tomorrow morning I'll know.)

> As you see, and unlike what you think, ngf and ndf are note related to the generation size, but they control the number of feature maps in the generator / discriminator.

Oh. I was a little confused because in the Eyescream page, it mentions that if you penalize the discriminator's net size by giving it a fraction of the parameters that the generator gets, you get more stable training (presumably this is because the discriminator has the easier job and too often wins in my runs), and I found that making sure that `ngf` was >2x `ndf` did indeed make training much more stable and helped balance error rates.

BTW, I couldn't help but wonder: discriminator vs generator reminds me a lot of actor-critic in reinforcement learning, and seems to have many of the same problems.

Has anyone ever tried to make dcgans stabler by borrowing some of the techniques from there, like freezing networks and having experience-replay buffers? I mean, for example, if D's errors drop to ~0.10, where it's about to collapse and abort training, D's weights could be frozen & no more learning done until G starts to do better at detecting fakes and D's error goes up to something more reasonable like 0.5/1/2; and similarly, if G starts winning almost 100% and is about to reach 0 errors and destroy learning, it could be frozen until D learns enough to start pushing its error rates up to 1. Or a buffer of past images which fooled D could be saved to train on occasionally (to prevent a total collapse when D becomes perfect & wins).
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/2/comments,https://github.com/soumith/dcgan.torch/issues/2#issuecomment-164919869,https://api.github.com/repos/soumith/dcgan.torch/issues/2
soumith,dcgan.torch,122175376,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/164937130,164937130,MDEyOklzc3VlQ29tbWVudDE2NDkzNzEzMA==,1310570,2015-12-15T23:41:59Z,2015-12-15T23:41:59Z,OWNER,"It does indeed seem very similar to actor-critic.

There's some trial on doing the network freezing / iterative scheduled optimization.
This blog post details some:
http://torch.ch/blog/2015/11/13/gan.html#balancing-the-gan-game

I've tried it in eyescream with no luck. It did not help things overall:
https://github.com/facebook/eyescream/blob/master/lsun/train.lua#L122-L129

But since eyescream, lots of progress happened. DCGANs might be a good candidate to try this stuff.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/2/comments,https://github.com/soumith/dcgan.torch/issues/2#issuecomment-164937130,https://api.github.com/repos/soumith/dcgan.torch/issues/2
soumith,dcgan.torch,122175376,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/236356865,236356865,MDEyOklzc3VlQ29tbWVudDIzNjM1Njg2NQ==,623781,2016-07-30T10:01:40Z,2016-07-30T10:04:58Z,NONE,"Tried the resize code posted above for 128x128 and finding that the Discriminator flatlines to 0.0000 around size 10. Anything I might be missing? Changed the discriminator & generator code as well as the command line parameters as specified. Testing 128x128 as a size greater than 64x64 toward eventually trying 320x200. 
Getting a lot of cool outputs even with the 64x64, thanks for the work on this!
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/2/comments,https://github.com/soumith/dcgan.torch/issues/2#issuecomment-236356865,https://api.github.com/repos/soumith/dcgan.torch/issues/2
soumith,dcgan.torch,122175376,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/236372244,236372244,MDEyOklzc3VlQ29tbWVudDIzNjM3MjI0NA==,352559,2016-07-30T15:43:14Z,2016-07-30T15:43:14Z,CONTRIBUTOR,"You can try changing the learning rates to favor the discriminator, or changing the filter counts, or increasing minibatch size.

Alternately, you could try switching to the new improved-gan in Tensorflow which has additional tricks intended to stabilize training. (It's not that hard to rewrite the Imagenet data processing script to use whatever set of images you may have; just need to edit all the hardwired paths and delete some asserts.) In my experience so far, improved-gan works faster and better but only if you can fit minibatches of at least 32 into your GPU (and probably, ideally 64 or 128) - the catch being that somehow the codebase is wired to assume minibatches which are powers of 2 or something, as anything else crashes and minibatches of 2/4/8/16 diverge almost instantly.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/2/comments,https://github.com/soumith/dcgan.torch/issues/2#issuecomment-236372244,https://api.github.com/repos/soumith/dcgan.torch/issues/2
soumith,dcgan.torch,122175376,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/271099384,271099384,MDEyOklzc3VlQ29tbWVudDI3MTA5OTM4NA==,4195648,2017-01-07T18:02:10Z,2017-01-07T18:02:10Z,NONE,I ran into a similar issue with flatlining to zero.  Setting ndf to something around ngf/2 or ngf/4 led to stable learning. (That is for 128^2),NA,https://api.github.com/repos/soumith/dcgan.torch/issues/2/comments,https://github.com/soumith/dcgan.torch/issues/2#issuecomment-271099384,https://api.github.com/repos/soumith/dcgan.torch/issues/2
soumith,dcgan.torch,122175376,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/283347331,283347331,MDEyOklzc3VlQ29tbWVudDI4MzM0NzMzMQ==,2930635,2017-03-01T14:01:34Z,2017-03-01T14:01:34Z,NONE,"I also ran into the flatlining issue when trying with 128x128, so I set ndf to ngf/4. The resulting images have a lovely crisp resolution, but after 1000 epochs are very repetitive, nowhere near as much variation as when using 64x64 and keeping ndf and ngf at 64. See the attached. Trying again with ndf at ngf/2. Will report back.

![generation2](https://cloud.githubusercontent.com/assets/2930635/23462885/8ec22ba4-feca-11e6-9d59-add1aff38876.png)
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/2/comments,https://github.com/soumith/dcgan.torch/issues/2#issuecomment-283347331,https://api.github.com/repos/soumith/dcgan.torch/issues/2
soumith,dcgan.torch,122175376,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/283359552,283359552,MDEyOklzc3VlQ29tbWVudDI4MzM1OTU1Mg==,2930635,2017-03-01T14:47:30Z,2017-03-01T14:48:22Z,NONE,"It was a quick test in the end. With ndf set to ngf/2, training flatlines in 2nd epoch. Any clues as to how I might keep up the variation in the images when using 128x128?

<img width=""763"" alt=""screen shot 2017-03-01 at 10 47 51 pm"" src=""https://cloud.githubusercontent.com/assets/2930635/23464766/27308538-fed1-11e6-9895-a4fe45fa21fb.png"">

",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/2/comments,https://github.com/soumith/dcgan.torch/issues/2#issuecomment-283359552,https://api.github.com/repos/soumith/dcgan.torch/issues/2
soumith,dcgan.torch,122175376,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/283758716,283758716,MDEyOklzc3VlQ29tbWVudDI4Mzc1ODcxNg==,4195648,2017-03-02T19:45:02Z,2017-03-02T19:45:02Z,NONE,"@rjpeart Have you tried using any other ""tricks"" like label smoothing or injecting white noise into the input of the discriminator? That also helped stabilise training for me and is a recommended ""fix"" for problematic networks."" Also how much variation is in your dataset?",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/2/comments,https://github.com/soumith/dcgan.torch/issues/2#issuecomment-283758716,https://api.github.com/repos/soumith/dcgan.torch/issues/2
soumith,dcgan.torch,122175376,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/283851547,283851547,MDEyOklzc3VlQ29tbWVudDI4Mzg1MTU0Nw==,2930635,2017-03-03T02:57:45Z,2017-03-03T02:57:45Z,NONE,"@LukasMosser Thanks for your response. I was not aware of those tricks so haven't tried (still on that steep learning curve), but I will do, thanks for the leads! I'm using ~950 samples in this dataset, which although not huge has given me great results at 64x64px, so I was surprised at the level of repetition at a higher resolution. I guess it's because of a diminished discriminator? ",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/2/comments,https://github.com/soumith/dcgan.torch/issues/2#issuecomment-283851547,https://api.github.com/repos/soumith/dcgan.torch/issues/2
soumith,dcgan.torch,122175376,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/283982237,283982237,MDEyOklzc3VlQ29tbWVudDI4Mzk4MjIzNw==,2930635,2017-03-03T15:22:59Z,2017-03-03T15:23:52Z,NONE,"@LukasMosser adding white noise stabilised the learning perfectly. Thanks so much for your advice. For anyone else struggling with this, here's how I defined the discriminator (it's the code provided by @soumith above, but with white noise added at the 5th line down)

```
local netD = nn.Sequential()
-- input is (nc) x 128 x 128
netD:add(SpatialConvolution(nc, ndf, 4, 4, 2, 2, 1, 1))
netD:add(nn.LeakyReLU(0.2, true))
--add white noise
netD:add(WhiteNoise(0,0.1))
-- state size: (ndf) x 64 x 64
netD:add(SpatialConvolution(ndf, ndf * 2, 4, 4, 2, 2, 1, 1))
netD:add(SpatialBatchNormalization(ndf * 2)):add(nn.LeakyReLU(0.2, true))
-- state size: (ndf*2) x 32 x 32
netD:add(SpatialConvolution(ndf * 2, ndf * 4, 4, 4, 2, 2, 1, 1))
netD:add(SpatialBatchNormalization(ndf * 4)):add(nn.LeakyReLU(0.2, true))
-- state size: (ndf*4) x 16 x 16
netD:add(SpatialConvolution(ndf * 4, ndf * 8, 4, 4, 2, 2, 1, 1))
netD:add(SpatialBatchNormalization(ndf * 8)):add(nn.LeakyReLU(0.2, true))
-- state size: (ndf*8) x 8 x 8
netD:add(SpatialConvolution(ndf * 8, ndf * 16, 4, 4, 2, 2, 1, 1))
netD:add(SpatialBatchNormalization(ndf * 16)):add(nn.LeakyReLU(0.2, true))
-- state size: (ndf*16) x 4 x 4
netD:add(SpatialConvolution(ndf * 16, 1, 4, 4))
netD:add(nn.Sigmoid())
-- state size: 1 x 1 x 1
netD:add(nn.View(1):setNumInputDims(3))
-- state size: 1

netD:apply(weights_init)
```",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/2/comments,https://github.com/soumith/dcgan.torch/issues/2#issuecomment-283982237,https://api.github.com/repos/soumith/dcgan.torch/issues/2
soumith,dcgan.torch,122175376,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/284067822,284067822,MDEyOklzc3VlQ29tbWVudDI4NDA2NzgyMg==,4195648,2017-03-03T20:56:09Z,2017-03-03T20:56:09Z,NONE,"@rjpeart glad I could help! Also interesting that you add white noise after the first LeakyRelu, I added it before the first convolutional layer and it worked as well, although I believe one can define it in any layer (or all of them) except the last.

Here more tricks: https://github.com/soumith/ganhacks

And here an article why adding noise works (and how): http://www.inference.vc/instance-noise-a-trick-for-stabilising-gan-training/",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/2/comments,https://github.com/soumith/dcgan.torch/issues/2#issuecomment-284067822,https://api.github.com/repos/soumith/dcgan.torch/issues/2
soumith,dcgan.torch,122175376,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/301027036,301027036,MDEyOklzc3VlQ29tbWVudDMwMTAyNzAzNg==,16591441,2017-05-12T09:16:49Z,2017-05-12T09:16:56Z,NONE,"@LukasMosser @rjpeart hey guys! I'm having a hard time adding the WhiteNoise at the discriminator. When I replaced the original discriminator code with @rjpeart modified code, I'm getting this error when training

`/home/'myusername'/torch/install/bin/luajit: main2.lua:89: attempt to call global 'WhiteNoise' (a nil value)
stack traceback:
	main2.lua:89: in main chunk
	[C]: in function 'dofile'
	...tion/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
	[C]: at 0x00405d50
`

Any suggestions? (also on that steep learning curve)",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/2/comments,https://github.com/soumith/dcgan.torch/issues/2#issuecomment-301027036,https://api.github.com/repos/soumith/dcgan.torch/issues/2
soumith,dcgan.torch,122175376,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/301241817,301241817,MDEyOklzc3VlQ29tbWVudDMwMTI0MTgxNw==,4195648,2017-05-13T11:16:07Z,2017-05-13T11:16:07Z,NONE,"@kubmin you probably didn't import the dpnn torch package.
https://github.com/Element-Research/dpnn/blob/master/WhiteNoise.lua

Hope that helps!",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/2/comments,https://github.com/soumith/dcgan.torch/issues/2#issuecomment-301241817,https://api.github.com/repos/soumith/dcgan.torch/issues/2
soumith,dcgan.torch,122175376,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/301455731,301455731,MDEyOklzc3VlQ29tbWVudDMwMTQ1NTczMQ==,16591441,2017-05-15T12:06:07Z,2017-05-15T12:06:07Z,NONE,"@LukasMosser thank you! totally didn't import the required package.

cheers!",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/2/comments,https://github.com/soumith/dcgan.torch/issues/2#issuecomment-301455731,https://api.github.com/repos/soumith/dcgan.torch/issues/2
soumith,dcgan.torch,122175376,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/306690551,306690551,MDEyOklzc3VlQ29tbWVudDMwNjY5MDU1MQ==,14958112,2017-06-07T05:28:09Z,2017-06-07T05:28:09Z,NONE,"@rjpeart @soumith Did any of you succeed in making larger sizes, perhaps up to 512x512? What would the extra lines in the generator/discriminator definition look like? Thank you!",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/2/comments,https://github.com/soumith/dcgan.torch/issues/2#issuecomment-306690551,https://api.github.com/repos/soumith/dcgan.torch/issues/2
soumith,dcgan.torch,122175376,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/307230146,307230146,MDEyOklzc3VlQ29tbWVudDMwNzIzMDE0Ng==,2930635,2017-06-08T21:17:32Z,2017-06-08T21:17:32Z,NONE,"@plugimi I got up to 128, but had the repetition problems as stated above. I think you could probably work out what the lines in the generator / discriminator look like by following that pattern. However, I seem to recall reading a thread that mentioned a 512 res would be too computationally intensive to complete. Can't find the thread right now though :/ 
Maybe @LukasMosser knows?",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/2/comments,https://github.com/soumith/dcgan.torch/issues/2#issuecomment-307230146,https://api.github.com/repos/soumith/dcgan.torch/issues/2
soumith,dcgan.torch,122175376,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/307234143,307234143,MDEyOklzc3VlQ29tbWVudDMwNzIzNDE0Mw==,4195648,2017-06-08T21:35:02Z,2017-06-08T21:35:02Z,NONE,"@plugimi @rjpeart I'm sure you can work out the pattern. The computational side is another, if you manage to fit your models in a GPU, you may have to use labelsmoothing, noise annealing or label flipping to stabilise.",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/2/comments,https://github.com/soumith/dcgan.torch/issues/2#issuecomment-307234143,https://api.github.com/repos/soumith/dcgan.torch/issues/2
soumith,dcgan.torch,122175376,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/309578659,309578659,MDEyOklzc3VlQ29tbWVudDMwOTU3ODY1OQ==,11282440,2017-06-19T21:27:08Z,2017-06-19T21:27:08Z,NONE,Could someone please provide the code for the generator / discriminator nets (in main.py) with dimensions of 256x256? I can't figure it out from the examples - i'm super new to torch :(,NA,https://api.github.com/repos/soumith/dcgan.torch/issues/2/comments,https://github.com/soumith/dcgan.torch/issues/2#issuecomment-309578659,https://api.github.com/repos/soumith/dcgan.torch/issues/2
soumith,dcgan.torch,122175376,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/495007322,495007322,MDEyOklzc3VlQ29tbWVudDQ5NTAwNzMyMg==,15966304,2019-05-22T23:08:58Z,2019-05-22T23:08:58Z,NONE,"Soumith, if you see this, or anyone else if you know the solution, could you verify if the following is correct for 256x256? I believe I followed the pattern correctly, although am not certain. I was able to get training to work with these changes, but it took significantly longer and even after training for much longer the results were still just fuzzy/static.

Ultimately, my goal is to create much larger AI generated images.

Thanks in advance if you're able to help with this.

For 256x256 I changed the following training config:
`loadSize=257 fineSize=256 th main.lua`

Changes I made to the generator:

```lua
  -- input is Z, going into a convolution

  -- changes by John for 256x256
  netG:add(SpatialFullConvolution(nz, ngf * 32, 4, 4))
  netG:add(SpatialBatchNormalization(ngf * 32)):add(nn.ReLU(true))
  netG:add(SpatialFullConvolution(ngf * 32, ngf * 16, 4, 4, 2, 2, 1, 1))
  netG:add(SpatialBatchNormalization(ngf * 16)):add(nn.ReLU(true))
  -- / end changes by John for 256x256

  netG:add(SpatialFullConvolution(ngf * 16, ngf * 8, 4, 4, 2, 2, 1, 1))
  netG:add(SpatialBatchNormalization(ngf * 8)):add(nn.ReLU(true))
  -- state size: (ngf*8) x 8 x 8
  netG:add(SpatialFullConvolution(ngf * 8, ngf * 4, 4, 4, 2, 2, 1, 1))
  netG:add(SpatialBatchNormalization(ngf * 4)):add(nn.ReLU(true))
  -- state size: (ngf*4) x 16 x 16
  netG:add(SpatialFullConvolution(ngf * 4, ngf * 2, 4, 4, 2, 2, 1, 1))
  netG:add(SpatialBatchNormalization(ngf * 2)):add(nn.ReLU(true))
  -- state size: (ngf * 2) x 32 x 32
  netG:add(SpatialFullConvolution(ngf * 2, ngf, 4, 4, 2, 2, 1, 1))
  netG:add(SpatialBatchNormalization(ngf)):add(nn.ReLU(true))
  -- state size: (ngf) x 64 x 64
  netG:add(SpatialFullConvolution(ngf, nc, 4, 4, 2, 2, 1, 1))
  netG:add(nn.Tanh())
  -- state size: (nc) x 128 x 128
  ```
  
  And changes I made to the discriminator:
  
  ```lua
    -- input is (nc) x 128 x 128
  netD:add(SpatialConvolution(nc, ndf, 4, 4, 2, 2, 1, 1))
  netD:add(nn.LeakyReLU(0.2, true))
  -- state size: (ndf) x 64 x 64
  netD:add(SpatialConvolution(ndf, ndf * 2, 4, 4, 2, 2, 1, 1))
  netD:add(SpatialBatchNormalization(ndf * 2)):add(nn.LeakyReLU(0.2, true))
  -- state size: (ndf*2) x 32 x 32
  netD:add(SpatialConvolution(ndf * 2, ndf * 4, 4, 4, 2, 2, 1, 1))
  netD:add(SpatialBatchNormalization(ndf * 4)):add(nn.LeakyReLU(0.2, true))
  -- state size: (ndf*4) x 16 x 16
  netD:add(SpatialConvolution(ndf * 4, ndf * 8, 4, 4, 2, 2, 1, 1))
  netD:add(SpatialBatchNormalization(ndf * 8)):add(nn.LeakyReLU(0.2, true))
  -- state size: (ndf*8) x 8 x 8
  netD:add(SpatialConvolution(ndf * 8, ndf * 16, 4, 4, 2, 2, 1, 1))
  netD:add(SpatialBatchNormalization(ndf * 16)):add(nn.LeakyReLU(0.2, true))
  -- state size: (ndf*16) x 4 x 4

  -- changes by John for 256x256
  netD:add(SpatialConvolution(ndf * 16, ndf * 32, 4, 4, 2, 2, 1, 1))
  netD:add(SpatialBatchNormalization(ndf * 32)):add(nn.LeakyReLU(0.2, true))
  -- state size: (ndf*16) x 4 x 4
  netD:add(SpatialConvolution(ndf * 32, 1, 4, 4))
  netD:add(nn.Sigmoid())
  -- state size: 1 x 1 x 1
  -- / end changes by John for 256x256

  netD:add(nn.View(1):setNumInputDims(3))
  -- state size: 1
  ```",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/2/comments,https://github.com/soumith/dcgan.torch/issues/2#issuecomment-495007322,https://api.github.com/repos/soumith/dcgan.torch/issues/2
soumith,dcgan.torch,122175376,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/496731124,496731124,MDEyOklzc3VlQ29tbWVudDQ5NjczMTEyNA==,1310570,2019-05-28T23:49:31Z,2019-05-28T23:49:31Z,OWNER,"if you want large GANs, look at https://github.com/ajbrock/BigGAN-PyTorch

DCGAN is a bit outdated ;-)",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/2/comments,https://github.com/soumith/dcgan.torch/issues/2#issuecomment-496731124,https://api.github.com/repos/soumith/dcgan.torch/issues/2
soumith,dcgan.torch,122175376,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/497107610,497107610,MDEyOklzc3VlQ29tbWVudDQ5NzEwNzYxMA==,15966304,2019-05-29T21:01:23Z,2019-05-29T21:01:23Z,NONE,"Soumith, thank you so much for the reply and info! Much appreciated!",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/2/comments,https://github.com/soumith/dcgan.torch/issues/2#issuecomment-497107610,https://api.github.com/repos/soumith/dcgan.torch/issues/2
soumith,dcgan.torch,121739901,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/163991806,163991806,MDEyOklzc3VlQ29tbWVudDE2Mzk5MTgwNg==,1310570,2015-12-11T17:05:14Z,2015-12-11T17:05:14Z,OWNER,"hmmm, weird. didn't know that. thanks.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/1/comments,https://github.com/soumith/dcgan.torch/pull/1#issuecomment-163991806,https://api.github.com/repos/soumith/dcgan.torch/issues/1
soumith,dcgan.torch,121739901,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/163992929,163992929,MDEyOklzc3VlQ29tbWVudDE2Mzk5MjkyOQ==,5733,2015-12-11T17:09:04Z,2015-12-11T17:09:04Z,CONTRIBUTOR,"It's definitely weird, because it has been listed on torch/rocks for a while. However, I tested on two machines and it only worked when I passed the specific URL for the rockspec to `luarocks install`.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/1/comments,https://github.com/soumith/dcgan.torch/pull/1#issuecomment-163992929,https://api.github.com/repos/soumith/dcgan.torch/issues/1
soumith,dcgan.torch,121739901,https://api.github.com/repos/soumith/dcgan.torch/issues/comments/164039731,164039731,MDEyOklzc3VlQ29tbWVudDE2NDAzOTczMQ==,1310570,2015-12-11T20:25:12Z,2015-12-11T20:25:12Z,OWNER,"I think the rockspec of display in the rocks server is out of date. i'll update it now.
",NA,https://api.github.com/repos/soumith/dcgan.torch/issues/1/comments,https://github.com/soumith/dcgan.torch/pull/1#issuecomment-164039731,https://api.github.com/repos/soumith/dcgan.torch/issues/1
