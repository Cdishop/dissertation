filter(dickeyp > 0.05) %>%
count() / nrow(store_results)
store_results %>%
filter(kpss < 0.05) %>%
count() / nrow(store_results)
(store_results %>%
filter(dickeyp > 0.05, kpss < 0.05) %>%
count()) / nrow(store_results)
library(flextable)
df$nicedate <- gsub(df$event_at, pattern = "00:00:00", replacement = "")
library(kableExtra)
store_results <- store_results %>%
mutate(DF = ifelse(dickeyp > 0.05, "Yes", "No")) %>%
mutate(KP = ifelse(kpss < 0.05, "Yes", "No"))
df <- left_join(df, store_results)
fitdf <- df %>%
filter(event_at_index == 0) %>%
select(uniqid, org, nicedate, numdays, dickey, dickeyp, DF) %>%
mutate(numdays_round = round(numdays, digits = 0)) %>%
mutate(dickey = round(dickey, digits = 2)) %>%
mutate(dickeyp = round(dickeyp, digits = 2)) %>%
select(uniqid, org, nicedate, numdays_round, dickey, dickeyp, DF) %>%
rename("Repo ID" = uniqid,
"Org" = org,
"Start Date" = nicedate,
"Length (Days)" = numdays_round,
"Dickey-Fuller" = dickey,
"P-Value" = dickeyp,
"Unit Root" = DF)
fitdf <- flextable(fitdf)
fitdf <- autofit(fitdf)
fitdf <- fontsize(fitdf, size = 9)
fitdf
df1 <- read.csv('../../github/analysis/initial-analysis-df.csv')
View(df1)
df1 <- read.csv('../../github/analysis/initial-analysis-df.csv')
numids <- length(unique(df1$repoid))
idlist <- data.frame(
'repoid' = c(unique(df1$repoid)),
'uniqid' = c(204:numids)
)
length(unique(df1$repoid))
unique(df1$repoid)
numids <- length(unique(df1$repoid))
idlist <- data.frame(
'repoid' = c(unique(df1$repoid)),
'uniqid' = c(204:(204+numids))
)
idlist <- data.frame(
'repoid' = c(unique(df1$repoid)),
'uniqid' = c(204:(205+numids))
)
idlist <- data.frame(
'repoid' = c(unique(df1$repoid)),
'uniqid' = c(seq(from = 204, to = (204+numids), by = 1))
)
numids <- length(unique(df1$repoid))
idlist <- data.frame(
'repoid' = c(unique(df1$repoid)),
'uniqid' = c(seq(from = 204, to = (203+numids), by = 1))
)
View(idlist)
df1 <- df1 %>% left_join(df1, idlist)
df1 <- left_join(df1, idlist)
dfcombine <- bind_rows(df, df1)
fitdf <- dfcombine %>%
filter(event_at_index == 0) %>%
select(uniqid, nicedate, numdays, dickey, dickeyp, DF) %>%
mutate(numdays_round = round(numdays, digits = 0)) %>%
mutate(dickey = round(dickey, digits = 2)) %>%
mutate(dickeyp = round(dickeyp, digits = 2)) %>%
select(uniqid, nicedate, numdays_round, dickey, dickeyp, DF) %>%
rename("Repo ID" = uniqid,
"Start Date" = nicedate,
"Length (Days)" = numdays_round,
"Dickey-Fuller" = dickey,
"P-Value" = dickeyp,
"Unit Root" = DF)
fitdf <- flextable(fitdf)
fitdf <- autofit(fitdf)
fitdf <- fontsize(fitdf, size = 9)
fitdf
fitdf <- dfcombine %>%
filter(event_at_index == 0) %>%
select(uniqid, nicedate, numdays, dickey, dickeyp, DF) %>%
mutate(numdays_round = round(numdays, digits = 0)) %>%
mutate(dickey = round(dickey, digits = 2)) %>%
mutate(dickeyp = round(dickeyp, digits = 2)) %>%
select(uniqid, nicedate, numdays_round, dickey, dickeyp, DF) %>%
rename("Repo ID" = uniqid,
"Start Date" = nicedate,
"Length (Days)" = numdays_round,
"Dickey-Fuller" = dickey,
"P-Value" = dickeyp,
"Unit Root" = DF)
sum(fitdf[, "Unit Root"] == 'Yes')
sum(fitdf[, "Unit Root"] == 'Yes') / nrow(fitdf)
df1 <- read.csv('../../github/analysis/initial-analysis-df.csv')
numids <- length(unique(df1$repoid))
idlist <- data.frame(
'repoid' = c(unique(df1$repoid)),
'uniqid' = c(seq(from = 204, to = (203+numids), by = 1))
)
df1 <- left_join(df1, idlist)
dfcombine <- bind_rows(df, df1)
fitdf <- dfcombine %>%
filter(event_at_index == 0) %>%
select(uniqid, nicedate, numdays, dickey, dickeyp, DF) %>%
mutate(numdays_round = round(numdays, digits = 0)) %>%
mutate(dickey = round(dickey, digits = 2)) %>%
mutate(dickeyp = round(dickeyp, digits = 2)) %>%
select(uniqid, nicedate, numdays_round, dickey, dickeyp, DF) %>%
rename("Repo ID" = uniqid,
"Start Date" = nicedate,
"Length (Days)" = numdays_round,
"Dickey-Fuller" = dickey,
"P-Value" = dickeyp,
"Unit Root" = DF)
sum(fitdf[, "Unit Root"] == 'Yes') / nrow(fitdf) # 81% of walks contain unit root
fitdf <- flextable(fitdf)
fitdf <- autofit(fitdf)
fitdf <- fontsize(fitdf, size = 9)
fitdf
View(dfcombine)
summarize(yy = ifelse(KP == DF, 'yes', 'no))
sdf]
}sdfsalkdjweakmnrf
)sdkl
''''
dfcombine %>%
group_by(uniqid) %>%
summarize(yy = ifelse(KP == DF, 'yes', 'no'))
View(dfcombine)
dfcombine %>%
group_by(uniqid) %>%
mutate(matched = ifelse(KP == DF, 'yes', 'no'))
dfcombine %>%
group_by(uniqid) %>%
select(uniqid, KP, DP)
dfcombine %>%
group_by(uniqid) %>%
select(uniqid, KP, DF)
library(tidyverse)
library(tseries)
library(plm)
df <- read.csv("../cleaning/merging-orgs/output/daily_issues.csv")
df %>%
distinct(repo) %>% nrow()
df %>%
group_by(repo) %>%
summarize(
numdays = max(event_at_index))
df %>%
group_by(repo) %>%
summarize(
numdays = max(event_at_index)) %>%
filter(numdays < 1001) %>%
pull(repo) -> dontuse
df <- df %>%
filter(! repo %in% c(dontuse))
repos <- df %>%
distinct(repo) %>% pull()
repids <- tibble(
'repo' = repos,
'uniqid' = c(1:length(repos))
)
df <- left_join(df, repids)
df %>%
filter(event_at_index == 0) %>%
select(uniqid, event_at, org) %>%
rename("Repo ID" = uniqid,
"Start Date" = event_at)
numdays <- df %>%
group_by(repo) %>%
summarize(
numdays = max(event_at_index))
df <- left_join(df, numdays)
df %>%
filter(event_at_index == 0) %>%
select(repoid, event_at, numdays, org) %>%
rename("Repo ID" = repoid,
"Start Date" = event_at,
"Length of Time Series (Days)" = numdays,
"Organization" = org)
df1 <- df %>%
filter(repoid == 1)
dfdickey <- plm.data(df1, index = c("repoid", "event_at_index"))
# not significant = random walk
adf.test(dfdickey[, "value"])["p.value"][[1]] # p value
adf.test(dfdickey[, "value"])$statistic[[1]] # test statistic
# significant = random walk
kpss.test(dfdickey[, "value"], null = "Trend")["p.value"][[1]]
runs <- nrow(repids)
store_results <- data.frame(
"dickeyp" = numeric(runs),
"kpss" = numeric(runs),
"repoid" = numeric(runs),
"dickey" = numeric(runs)
)
for(i in 1:runs){
dt <- df %>%
filter(uniqid == i)
dtdickey <- plm.data(dt, index = c("repoid", "event_at_index"))
# not significant = random walk
dy <- adf.test(dtdickey[, "value"])["p.value"][[1]]
dt <- adf.test(dtdickey[, "value"])$statistic[[1]]
# significant = random walk
kp <- kpss.test(dtdickey[, "value"], null = "Trend")["p.value"][[1]]
store_results[[i, "dickeyp"]] <- dy
store_results[[i, "kpss"]] <- kp
store_results[[i, "repoid"]] <- i
store_results[[i, "dickey"]] <- dt
}
store_results %>%
filter(dickeyp > 0.05) %>%
count() / nrow(store_results)
store_results %>%
filter(kpss < 0.05) %>%
count() / nrow(store_results)
(store_results %>%
filter(dickeyp > 0.05, kpss < 0.05) %>%
count()) / nrow(store_results)
knitr::opts_chunk$set(echo = F, warning = F, message = F, include = F)
library(tidyverse)
library(tseries)
library(plm)
df <- read.csv("../cleaning/merging-orgs/output/daily_issues.csv")
df %>%
distinct(repo) %>% nrow()
df %>%
group_by(repo) %>%
summarize(
numdays = max(event_at_index))
df %>%
group_by(repo) %>%
summarize(
numdays = max(event_at_index)) %>%
filter(numdays < 1001) %>%
pull(repo) -> dontuse
df <- df %>%
filter(! repo %in% c(dontuse))
repos <- df %>%
distinct(repo) %>% pull()
repids <- tibble(
'repo' = repos,
'uniqid' = c(1:length(repos))
)
df <- left_join(df, repids)
df %>%
filter(event_at_index == 0) %>%
select(uniqid, event_at, org) %>%
rename("Repo ID" = uniqid,
"Start Date" = event_at)
numdays <- df %>%
group_by(repo) %>%
summarize(
numdays = max(event_at_index))
df <- left_join(df, numdays)
df %>%
filter(event_at_index == 0) %>%
select(repoid, event_at, numdays, org) %>%
rename("Repo ID" = repoid,
"Start Date" = event_at,
"Length of Time Series (Days)" = numdays,
"Organization" = org)
df1 <- df %>%
filter(repoid == 1)
dfdickey <- plm.data(df1, index = c("repoid", "event_at_index"))
# not significant = random walk
adf.test(dfdickey[, "value"])["p.value"][[1]] # p value
adf.test(dfdickey[, "value"])$statistic[[1]] # test statistic
# significant = random walk
kpss.test(dfdickey[, "value"], null = "Trend")["p.value"][[1]]
runs <- nrow(repids)
store_results <- data.frame(
"dickeyp" = numeric(runs),
"kpss" = numeric(runs),
"repoid" = numeric(runs),
"dickey" = numeric(runs)
)
for(i in 1:runs){
dt <- df %>%
filter(uniqid == i)
dtdickey <- plm.data(dt, index = c("repoid", "event_at_index"))
# not significant = random walk
dy <- adf.test(dtdickey[, "value"])["p.value"][[1]]
dt <- adf.test(dtdickey[, "value"])$statistic[[1]]
# significant = random walk
kp <- kpss.test(dtdickey[, "value"], null = "Trend")["p.value"][[1]]
store_results[[i, "dickeyp"]] <- dy
store_results[[i, "kpss"]] <- kp
store_results[[i, "repoid"]] <- i
store_results[[i, "dickey"]] <- dt
}
store_results %>%
filter(dickeyp > 0.05) %>%
count() / nrow(store_results)
store_results %>%
filter(kpss < 0.05) %>%
count() / nrow(store_results)
(store_results %>%
filter(dickeyp > 0.05, kpss < 0.05) %>%
count()) / nrow(store_results)
knitr::opts_chunk$set(echo = F, warning = F, message = F, include = F)
library(tidyverse)
df <- read.csv("../cleaning/merging-orgs/output/daily_issues.csv")
df <- df %>%
dplyr::filter(org == "facebook")
allrepos <- df %>% distinct(repoid) %>% pull(repoid)
allrepos1 <- data.frame(repo1 = c(allrepos))
allrepos2 <- data.frame(repo2 = c(allrepos))
crossrepos <- subset(merge(allrepos1, allrepos2, by = NULL), repo1 <= repo2)
crossrepos <- crossrepos %>%
filter(repo1 != repo2)
savepair <- numeric(nrow(crossrepos))
for(j in 1:nrow(crossrepos)){
pair <- crossrepos %>%
slice(j)
pairdf <- df %>%
filter(repoid %in% c(pair$repo1, pair$repo2))
pairdf %>%
group_by(repo) %>%
summarize(
numdays = max(event_at_index)) %>%
arrange(numdays) %>%
slice(1) %>% pull(numdays) %>% floor() -> series_n
percent <- pairdf %>%
filter(event_at_index %in% c(1:series_n)) %>%
select(X, event_at_index, value, repoid, repo) %>%
group_by(event_at_index) %>%
mutate(rk = rank(-value)) %>%
filter(repoid == pair$repo1) %>%
filter(rk == 1) %>%
nrow() / series_n
savepair[j] <- percent
}
plotpair <- round(savepair, 1)
ggplot() + geom_histogram(aes(plotpair)) +
theme_bw() +
labs(x = "Fraction Of Periods With Max Requests", y = "Count")
hm <- data.frame(
'freqs' = c(plotpair)
)
hm <- hm %>%
count(freqs) %>%
mutate(perc = n / 325)
ggplot(hm, aes(x = freqs, y = perc)) +
geom_bar(stat = "identity") +
theme_bw() +
labs(x = "Time", y = "Probability of Spending n Periods in the Lead") +
theme(axis.text.x = element_blank()) +
theme(axis.title.y = element_text(angle = 0)) +
ggtitle("Facebook")
g <- ggplot(hm, aes(x = freqs, y = perc)) +
geom_bar(stat = "identity") +
theme_bw() +
labs(x = "Time", y = "Probability of Spending n Periods in the Lead") +
theme(axis.text.x = element_blank()) +
ggtitle("Facebook")
ggsave('fbook.pdf', g, width = 6, height = 4)
hm <- data.frame(
'freqs' = c(plotpair)
)
hm <- hm %>%
count(freqs) %>%
mutate(perc = n / 325)
ggplot(hm, aes(x = freqs, y = perc)) +
geom_bar(stat = "identity") +
theme_bw() +
labs(x = "Time", y = "Probability of Spending n Periods in the Lead") +
#theme(axis.text.x = element_blank()) +
theme(axis.title.y = element_text(angle = 0)) +
ggtitle("Facebook")
hm <- data.frame(
'freqs' = c(plotpair)
)
hm <- hm %>%
count(freqs) %>%
mutate(perc = n / 325)
ggplot(hm, aes(x = freqs, y = perc)) +
geom_bar(stat = "identity") +
theme_bw() +
labs(x = "Proportion of n Periods", y = "Probability of Spending n Periods in the Lead") +
#theme(axis.text.x = element_blank()) +
theme(axis.title.y = element_text(angle = 0)) +
ggtitle("Facebook")
View(hm)
hm <- hm %>%
mutate(org = "Facebook")
write.csv(hm, "fbook.csv", row.names= F)
knitr::opts_chunk$set(echo = F, warning = F, message = F, include = F)
library(tidyverse)
df <- read.csv("../cleaning/merging-orgs/output/daily_issues.csv")
df <- df %>%
dplyr::filter(org == "google")
allrepos <- df %>% distinct(repoid) %>% pull(repoid)
allrepos1 <- data.frame(repo1 = c(allrepos))
allrepos2 <- data.frame(repo2 = c(allrepos))
crossrepos <- subset(merge(allrepos1, allrepos2, by = NULL), repo1 <= repo2)
crossrepos <- crossrepos %>%
filter(repo1 != repo2)
savepair <- numeric(nrow(crossrepos))
for(j in 1:nrow(crossrepos)){
pair <- crossrepos %>%
slice(j)
pairdf <- df %>%
filter(repoid %in% c(pair$repo1, pair$repo2))
pairdf %>%
group_by(repo) %>%
summarize(
numdays = max(event_at_index)) %>%
arrange(numdays) %>%
slice(1) %>% pull(numdays) %>% floor() -> series_n
percent <- pairdf %>%
filter(event_at_index %in% c(1:series_n)) %>%
select(X, event_at_index, value, repoid, repo) %>%
group_by(event_at_index) %>%
mutate(rk = rank(-value)) %>%
filter(repoid == pair$repo1) %>%
filter(rk == 1) %>%
nrow() / series_n
savepair[j] <- percent
}
plotpair <- round(savepair, 1)
ggplot() + geom_histogram(aes(plotpair)) +
theme_bw() +
labs(x = "Fraction Of Periods With Max Requests", y = "Count")
hm <- data.frame(
'freqs' = c(plotpair)
)
hm <- hm %>%
count(freqs) %>%
mutate(perc = n / 3570)
ggplot(hm, aes(x = freqs, y = perc)) +
geom_bar(stat = "identity") +
theme_bw() +
labs(x = "Time", y = "Probability of Spending n Periods in the Lead") +
theme(axis.text.x = element_blank()) +
theme(axis.title.y = element_text(angle = 0)) +
ggtitle("Google")
g <- ggplot(hm, aes(x = freqs, y = perc)) +
geom_bar(stat = "identity") +
theme_bw() +
labs(x = "Time", y = "Probability of Spending n Periods in the Lead") +
theme(axis.text.x = element_blank()) +
ggtitle("Google")
ggsave('google.pdf', g, width = 6, height = 4)
hm <- hm %>%
mutate(org = "Google")
write.csv(hm, "google.csv", row.names= F)
knitr::opts_chunk$set(echo = F, warning = F, message = F, include = F)
library(tidyverse)
df <- read.csv("../cleaning/merging-orgs/output/daily_issues.csv")
df <- df %>%
dplyr::filter(org == "microsoft")
allrepos <- df %>% distinct(repoid) %>% pull(repoid)
allrepos1 <- data.frame(repo1 = c(allrepos))
allrepos2 <- data.frame(repo2 = c(allrepos))
crossrepos <- subset(merge(allrepos1, allrepos2, by = NULL), repo1 <= repo2)
crossrepos <- crossrepos %>%
filter(repo1 != repo2)
savepair <- numeric(nrow(crossrepos))
for(j in 1:nrow(crossrepos)){
pair <- crossrepos %>%
slice(j)
pairdf <- df %>%
filter(repoid %in% c(pair$repo1, pair$repo2))
pairdf %>%
group_by(repo) %>%
summarize(
numdays = max(event_at_index)) %>%
arrange(numdays) %>%
slice(1) %>% pull(numdays) %>% floor() -> series_n
percent <- pairdf %>%
filter(event_at_index %in% c(1:series_n)) %>%
select(X, event_at_index, value, repoid, repo) %>%
group_by(event_at_index) %>%
mutate(rk = rank(-value)) %>%
filter(repoid == pair$repo1) %>%
filter(rk == 1) %>%
nrow() / series_n
savepair[j] <- percent
}
plotpair <- round(savepair, 1)
ggplot() + geom_histogram(aes(plotpair)) +
theme_bw() +
labs(x = "Fraction Of Periods With Max Requests", y = "Count")
hm <- data.frame(
'freqs' = c(plotpair)
)
hm <- hm %>%
count(freqs) %>%
mutate(perc = n / 3741) %>%
filter(!is.na(freqs) == T) %>%
filter(!is.infinite(freqs) == T)
ggplot(hm, aes(x = freqs, y = perc)) +
geom_bar(stat = "identity") +
theme_bw() +
labs(x = "Time", y = "Probability of Spending n Periods in the Lead") +
theme(axis.text.x = element_blank()) +
theme(axis.title.y = element_text(angle = 0)) +
ggtitle("Microsoft")
g <- ggplot(hm, aes(x = freqs, y = perc)) +
geom_bar(stat = "identity") +
theme_bw() +
labs(x = "Time", y = "Probability of Spending n Periods in the Lead") +
theme(axis.text.x = element_blank()) +
ggtitle("Microsoft")
ggsave('microsoft.pdf', g, width = 6, height = 4)
hm <- hm %>%
mutate(org = "Microsoft")
write.csv(hm, "microsoft.csv", row.names= F)
